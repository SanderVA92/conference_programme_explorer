abstract,title,authors,session,all_keyword_ids,paper_id,session_name,stream,timeslot,track,stream_name,schedule,day,time,start_time,end_time,date,room,keywords,track_code
"There have been decades of research in the field of humanitarian logistics, and academics in the field of logistics are becoming more and more interested in it. That being said, we still acquire insight and identify new problems with each disaster. We have also observed various humanitarian logistics applications during COVID-19, e.g. for PCR test sites and vaccination centers. Unfortunately, the recent earthquake in Turkey has led us to re-evaluate the response cycle of disaster management. Close inspection reveals that this response phase actually leads to a variety of new applications of distribution logistics problems. We have conducted many meetings and workshops with municipalities that were very active during the response of the Maraş Earthquakes. Many municipalities aim to have “earthquake-resistant cities” with correct action plans and being ready for potential disasters. Based on our discussions with these municipalities, we have developed an “ideal action plan”. We also investigated the potential decision problems and linked these problems with OR literature.",Disaster Resilient Cities - An OR Approach to Disaster Management,[1379],37,"[65, 58]",3,Bahar Yetis Kara,62,14,01,Keynotes,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,Sportshallen [building - 101],"['Logistics', 'Humanitarian Applications']",WC-01
"Content analysis reveals that in many markets in the State of Israel where regular payment is required for continuous service, usually monthly, a situation can be seen where the price of the service creeps up consistently and even though the consumer has an option to switch to another company, at the starting price he paid, he remains loyal to the company that increases its service consistently and continuously. The purpose of this paper is to provide a theoretical model according to which the company that raises its' price capitalize on the consumer loyalty. The subgame perfect equilibrium resulting from the model supports the empirical evident. According to the perfect equilibrium the firm initially offers loss prices, the consumer remains loyal to the firm and the prices in subsequent periods compensate for the loss in the first periods.",Cooperative solution to noncooperative game,"[36242, 75640]",185,"[50, 33, 25]",7,Experimental economics and game theory 1,73,13,40,Experimental economics and game theory,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,96 [building - 306],"['Game Theory', 'Economic Modeling', 'Decision Analysis']",WB-40
"Many cities in the world have areas which can be called slums. Slums are areas where poor people live in poor circumstances. The houses are not comfortable or convenient. Often leaking roofs, no good sanitation; toilets and douches are missing, no proper cooking devices or electricity. Slums create a lot of trouble, for the inhabitants themself concerning safety and healthcare, but also for the whole city and for the country.
Changing slums is a complex societal problem, which should be handled according to the directions of the Compram methodology.
The Compram Methodology is a multidisciplinary approach for policy making on handling complex societal problems. These problems exceed the boundaries of a discipline. Therefore, a multidisciplinary approach is needed. In each complex societal problem there are elements of knowledge, power and emotion. All these elements should be addressed in the problem handling approach. 
The Compram methodology emphasizes the exchange of knowledge and understanding by communication among and between the experts, actors and politicians meanwhile keeping emotion in mind.
","An ethical complex societal problem, slums in developing countries.",[1254],77,"[15, 41, 139]",8,"Ethics and OR, societal complexity and public service",28,4,20,OR and Ethics,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,45 [building - 116],"['Complex Societal Problems', 'Ethics', 'Sustainable Development']",MC-20
"This paper revises the two-commodity network flow modelling approach for the Capacitated Vehicle Routing Problem [CVRP] with identical vehicles. The flows of equally sized two commodities considered in this paper are organized in the opposite directions along the collection of routes assigned to vehicles. The resulting mathematical programming formulations allow to formulate a general, asymmetric CVRP [even with presence of Time Windows constraints] using n[n - 1]/2 binary variables, where n is the number of customers involved in the definition of CVRP, which is the number of binary variables typically required for the symmetric CVRP only. The commodity flow formulations of the problem is a very flexible tool for modeling the problem that allows for various enhancements. For example, the family of Generalized Large Multistar valid inequalities can be incorporated in a commodity flow-based formulation of the problem without
using a single extra constraint. In addition, the compact mathematical formulations of the problem are further improved using Rounded Capacity Inequalities at the root node. The computational study demonstrates that a wide range of problem instances from the literature can be solved to optimality [or suboptimality with relatively small optimality gaps] using a simple solution methodology that solely employs a standard mixed integer linear programming solver.",Two-Commodity Opposite Direction Network Flows for Vehicle Routing Problems,"[56731, 47904]",787,"[145, 72]",10,MILPs for Vehicle Routing 2,5,12,58,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S07 [building - 101],"['Vehicle Routing', 'Mathematical Programming']",WA-58
"This paper contributes towards developing an efficient algorithm, relying on the Alternating-Direction of Multipliers [ADMM], for solving scenario-based Model Predictive Control arising in multi-period portfolio optimization problems.

We enhance the standard two-set splitting algorithm of the ADMM method by including inequality constraints through a so-called embedded splitting without recourse to an additional splitting set. 

We derive an alteration of the termination criterion using the probabilities assigned to the scenarios and provide a convergence analysis.",Enhancing portfolio optimization - an ADMM approach with embedded splitting for scenario-based model predictive control,[75618],245,"[21, 82, 83]",12,Portfolio optimization and sustainability,53,2,08,AI & Innovation in Sustainable Finance,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,1020 [building - 202],"['Convex Optimization', 'Optimal Control', 'Optimization in Financial Mathematics']",MA-08
"Companies have to use all their resources in an optimum way in order to be successful in a competitive environment. One of the important approaches in recent years is the effective use of the workforce and the effective use of human resources in order to achieve efficiency.Different in industry for production environments, widely parallelized to minimize production time in different production environments machine scheduling problems are used.In this study, it was applied in a defense industry company in Turkey. A multi-criteria mixed integer programming model has been developed for more efficient planning on two identical NC machines.The aim of the model is to minimize the total delay time and total completion time of the jobs.The aim is to ensure that many parts of different projects are correct on the cutting benches minimizing total delay and total completion time according to prioritization and under certain constraints.In addition,an iterative heuristic approach based on a mathematical model has been developed for the problem size for which the mathematical model is not sufficient. The methodology used is to increase the efficiency of the company by improving the total time to completion, maximum latency, total latency and number of delayed jobs.",A Multiple Criteria Mixed Integer for the Parallel Machine Scheduling Problem Programming Model - Common Sourcing,[62307],653,"[77, 72, 111]",13,Integer Programming for Decision Support,45,9,45,Decision Support Systems,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,30 [building - 324],"['Multi-Objective Decision Making', 'Mathematical Programming', 'Programming, Mixed-Integer']",TC-45
"In 1950, the world’s first OR journal, the Operational Research Quarterly, was published.  Hence, 2004 sees the 75th volume of, what is now, the Journal of the Operational Research Society.  This talk will discuss the origin of the journal, and changes in editor, frequency, publisher and content during these 75 years.  A current editor-in-chief will discuss JORS' 75th volume and future plans.",75 years of the world’s first OR journal,"[5870, 61687]",454,"[88, 106]",17,Moments in the history of OR  2,27,14,20,Moments in the history of OR,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,45 [building - 116],"['OR History', 'Profession of OR']",WC-20
"The talk is devoted to the optimal transport on irreversible metric spaces. While the compact setting is mostly similar to the reversible case developed by J. Lott, K.-T. Sturm and C. Villani, the noncompact case provides various surprising phenomena. Since the reversibility of noncompact irreversible spaces might be infinite, it is motivated to introduce a suitable nondecreasing function that bounds the reversibility of larger and larger balls. By this approach, we are able to prove satisfactory convergence/stability results in a suitable – reversibility depending – Gromov-Hausdorff topology. A wide class of irreversible spaces is provided by Finsler manifolds, which serve to construct various model examples by pointing out genuine differences between the reversible and irreversible settings. It is a joint work with Alexandru Kristaly.",Convergence and stability of optimal transport on irreversible metric spaces,[75642],86,"[81, 0]",19,Optimization on Geodesic Metric Spaces II - Nonsmooth case,69,7,41,Optimization on Geodesic Metric Spaces - Smooth and Nonsmooth,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,97 [building - 306],['Non-smooth Optimization'],TA-41
" Sparsity is a highly desired feature in deep neural networks [DNNs] since it ensures numerical efficiency, improves the interpretability of models [due to the smaller number of relevant features], and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the L1 norm [i.e., zero weights] and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity [L1 norm] as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the L1 norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto front for the above-mentioned objectives in a very efficient manner. We present numerical examples using both deterministic and stochastic gradients. We furthermore demonstrate that knowledge of the regularization path allows for a well-generalizing network parametrization. ",A multiobjective continuation method to compute the regularization path of deep neural networks,"[75646, 69711]",87,"[81, 66, 5]",20,Advances in Continuous Multiobjective Optimization,34,8,37,Multiobjective Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,33 [building - 306],"['Non-smooth Optimization', 'Machine Learning', 'Algorithms']",TB-37
"This study proposes a novel robust version of berth and quay crane allocation and scheduling problem integrated with a quay crane worker assignment problem. To align with on-field data, we consider in this version that the processing rates of the quay cranes workers is not deterministic, but subject to uncertainties. The uncertainty of this parameter is addressed using robust optimization, by considering an uncertainty set of scenarios. A novel two-stage mixed integer linear programming model is proposed to minimize the worst-case tardiness of vessels and find the worst-case scenario from the uncertainty set. The model is first tested using a commercial solver, but it could not be solved, even for small instances. Thus, as we noticed that the problem has a decomposable structure, we developed an exact decomposition algorithm that splits the model into a master problem and a set of subproblems to alleviate the complexity. We also strengthened the algorithm by introducing novel [re]formulation enhancement, valid inequalities and lower bounds. To assess the performance of the proposed algorithm, we conducted a series of computational experiments on a dataset of instances designed from a real-case database of a container terminal. Also, we performed a sensitivity analysis to evaluate the value of considering a robust version of the model compared to a non-robust version.",A novel robust exact decomposition algorithm for berth and quay crane allocation and scheduling problem considering uncertainty in worker productivity,"[75644, 58796, 75647]",82,"[70, 127, 129]",23,Seaside Planning III,52,7,62,OR in Port Operations,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S12 [building - 101],"['Maritime applications', 'Robust Optimization', 'Scheduling']",TA-62
"This study investigates the effects of uncertainty in time-to-build and regulation, which hinders immediate revenue generation after investment, on a firm's optimal investment decision. We show that in the absence of regulation, uncertainty in time-to-build always accelerates investment and enhances firm value. We also show that in the absence of time-to-build, uncertainty in regulation can mitigate the distortion of investment induced by regulation. Furthermore, in the presence of both time-to-build and regulation, there can exist harmless regulation that does not induce any distortion in the investment decision and does not harm firm value. Lastly, in the presence of both time-to-build and regulation, not only uncertainty in time-to-build but also its presence can accelerate investment.","Time-to-build, regulation, and investment",[58094],92,"[45, 33, 44]",24,Real Option Analysis,8,15,57,Real Option Analysis,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S06 [building - 101],"['Financial Modelling', 'Economic Modeling', 'Finance and Banking']",WD-57
"
The Menu Planning Problem [MPP] has gained increasing attention in recent years, driven by the need to address the complex interplay between nutrition, cost, palatability and environmental sustainability in our diets. This work presents an approach to fully incorporate these considerations using a Wierzbicky Achievement Function [WAF]. The MPP is a multi-objective optimization challenge that aims to generate daily menus that are not only nutritious and affordable but also palatable and environmentally sustainable. Traditional approaches have focused primarily on cost and nutritional aspects, often neglecting the environmental impact of food choices. However, there is a growing urgency to integrate sustainability into the MPP. This holistic approach results in a tri-objective problem that is optimized by using a WAF, a versatile tool that enables the simultaneous optimization of multiple conflicting objectives, to balance the trade-offs between cost, palatability, and environmental impact effectively. By assigning a wide range of weights to each objective, we can find menus that represent a diverse level of compromise between the objectives. Our research shows the effectiveness of this approach by generating an extensive Pareto Front of solutions, offering a spectrum of menu choices that cater to different sets of preferences. This study provides a valuable framework for incorporating multiple considerations into the MPP, paving the way for more responsible dietary choices.",Including Sustainability into the Menu Planning Problem using a Wierzbicky Achievement Function,"[65641, 47384, 24309]",380,"[14, 84, 100]",25,Sustainable Food Supply,78,14,13,Secure & Sustainable Food Supply,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,15 [building - 116],"['Combinatorial Optimization', 'Optimization Modeling', 'OR in Sustainability']",WC-13
"In response to recent global fluctuations, companies are turning to reverse logistics to minimize waste and increase production capacity. Implementing a reverse supply chain [RSC] is not without challenges, including obtaining e-waste with the right quality, preventing fraud, and disposing of sensitive information securely. Counterfeit electronic parts are a significant threat to the reliability and safety of electronic equipment, highlighting the need for traceable systems to prevent and detect fraudulent activities and protect consumers. Blockchain technology [BCT] offers several characteristics to address these issues while bringing several challenges. It embraces substantial investments, encompassing technological assets and the development of requisite infrastructure. Cultural and governance issues also influence the willingness of all supply chain parties to collaborate on BCT investment. To address these challenges, we model an evolutionary game to investigate the following research questions - how does parties' adoption tendency in the semiconductor industry evolve, considering technical cost, partners' collaboration, and the ultimate goal of achieving complete transparency through BCT adoption? What is the impact of external factors, including cryptocurrency fluctuations and evolving consumer behaviors, on parties' decision-making processes in RSC, particularly concerning their adoption or non-adoption of Blockchain technology?",Reverse Supply Chain in the Semiconductor Industry - Examining the Blockchain Implementation,"[78684, 53902, 70943, 31749, 16518]",7,"[125, 138, 50]",28,"Circular Economy, Remanufacturing and Recycling",18,2,23,"Circular Economy, Remanufacturing and Recycling ","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,82 [building - 116],"['Reverse Logistics / Remanufacturing', 'Supply Chain Management', 'Game Theory']",MA-23
"This study explores how operational decisions regarding a quality-differentiated product line determines the overall success of a multiproduct planning in airline market by using optimized integrated gradients of global airline industry from 2012-2022. Data visualization techniques are assigned to define contours and color shades in product and service design due to seat configuration, space allocation, seat design in time variants. In order to understand product variety dimensions, airline decisions are sliced into multiple cubic dimensions and assigned into different contoured colors, due to constrained in capacity in cabin deck and seat options, the density of colors returns individual decision made by each airline for given origin-destination of flight operations. The results suggested that critical areas of optimal product line choice are dependent not only on price discrimination and segmentation strategy, but the airlines with efficient aircraft layout favors service differentiation strategy in order to perform well in the airline market. The study reveals the interconnected relationship between constraint decisions and operational outcomes, shedding light on how factors such as space for passenger interactions, efficient seat configuration design, and in-flight seat design collectively impact both market share and load factors.",Big Data Visualization to Unveil Key Differentiated Service Dimensions of the Airlines Performances in COVID-19 Aftermath,[71527],256,"[4, 130, 85]",29,Airplane Boarding,85,8,54,Public Transport Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S01 [building - 101],"['Airline Applications', 'Service Operations', 'OR and the Arts\xa0']",TB-54
"Recently, improving the infrastructure for Electric vehicles [EVs] has been a major focus of organizations around the world. In this study, we present a service network design framework to improve the infrastructure for EVs with quality-of-service constraints taken into consideration. We present a novel charging station location model with capacity allocation, and we consider the community impact in locating these stations. First, we present descriptive analysis of community impact of charging stations in the United Staes. Then, we highlight how charging times at stations affect the quality of service in the whole service system by quantifying the improvements in flow coverage while reducing waiting times, in addition to maximizing the community impact. The value of faster chargers in increasing coverage and reducing the average waiting times is also highlighted in relation to a service provider’s budget constraints.",Locating Electric Vehicle Charging Stations and Environmental Justice ,"[62757, 75653, 75654]",580,"[5, 64, 94]",30,Location of Alternative Fuel and Charging Stations,29,3,61,Locational Analysis,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S10 [building - 101],"['Algorithms', 'Location', 'OR in Environment and Climate change']",MB-61
"COVID 19 infection spread to the whole world in a short time at the end of 2019 and affected the health system of many developed and developing world countries negatively. Türkiye was one of the most negatively affected countries in the world. In this paper we evaluated the performance of 81 provinces and 7 regions of Türkiye against COVID-19 disease using efficiency score which was calculated by data envelopment analysis [DEA] and compared the efficiency score with the different models which are used to evaluate the efficiency of healthcare system. We took input parameters as public health expenditure in a million, number of hospitals, number of hospital beds, percentage of health workers, population density, and number of infected. We divided output parameters into good and bad categories such as the number of recovered are taken as good output. The number of death is taken as bad outputs. Finally, the provinces and regions are ranked with the super efficiency score aand evaluated benchmarking for each states and regions.",Evaluation of Healthcare Systems during COVID-19 Pandemic in Türkiye,[16635],970,"[56, 24, 35]",34,DEA in healthcare,3,3,17,OR in Health Services [ORAHS],"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,40 [building - 116],"['Health Care', 'Data Envelopment Analysis', 'Efficiency Analysis']",MB-17
"The most relevant task for the implementation of the Balanced Scorecard is the mapping of strategies since it provides a structure to demonstrate how strategies link the intangible properties of an organization with the value creation process, in turn; it shows how the objectives of the different perspectives are related to achieving the vision. This research contributes to models and cutting-edge methodologies in the context of the systematic development of strategic maps of organizations through the creation of a framework to help in decision-making. Our objective is the design of a methodology that supports the construction of strategic maps in organizations through the implementation of multi-criteria decision making and optimization methods that allow for establishing and reducing causal relationships between the strategic objectives of the 4 perspectives of the BSC [Learning and Growth, Internal Processes, Customers and Financial]and the selection of the most representative for the fulfillment of the strategy. The MCDM used to meet the objective was DEMATEL determining the causal relationships and the effects of the variables of the strategic map with the information obtained from a group of experts. The linear programming optimization model that was applied is based on the reduction of arcs within the strategic map, eliminating the objectives that obtained the lowest rating in the expert consultation and the maximization of the selected financial objectives.",Development of strategic map [BSC] with fuzzy logic and multi-criteria optimization,"[71580, 31757, 58207, 58717]",477,"[77, 112, 137]",37,Multiple-Criteria Decision Support,45,7,45,Decision Support Systems,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,30 [building - 324],"['Multi-Objective Decision Making', 'Programming, Multi-Objective', 'Strategic Planning and Management']",TA-45
"We present a branch-and-bound method for multiobjective mixed-integer convex quadratic programs that computes a superset of efficient integer assignments and a coverage of the nondominated set. The method relies on outer approximations of the upper image set of continuous relaxations. These outer approximations are obtained addressing the dual formulations of specific subproblems where the values of certain integer variables are fixed. The devised pruning conditions and a tailored preprocessing phase allow a fast enumeration of the nodes. Despite we do not require any boundedness of the feasible set, we are able to prove that the method stops after having explored a finite number of nodes. Numerical experiments on a broad set of instances with two, three, and four objectives are presented.",Using Dual Relaxations in Multiobjective Mixed-Integer Convex Quadratic Programming,"[9249, 24206, 70992, 66903]",52,"[112, 111, 114]",38,Multiobjective Mixed-Integer Nonlinear Optimization,34,9,37,Multiobjective Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,33 [building - 306],"['Programming, Multi-Objective', 'Programming, Mixed-Integer', 'Programming, Quadratic']",TC-37
"During times of armed conflict, a number of security check-points are set up by authorities to regulate the movement of commercial and relief trucks into and out of the war region. These security check-points have become highly utilized, because of complex security procedures and increased truck traffic, which heavily slows down the delivery of relief aid. Our research aims to improve the process at the security check-points by redesigning the current process to reduce processing times and relieve congestion at check-point entrance gates. These efforts are then integrated with the proposed decision-support tool [the Clearing Function Distribution Model, CFDM] to minimize the effects of security check-point congestion on the entire humanitarian supply network using a hybrid simulation-optimization approach. Employing business process simulation, the current and re-engineered processes are both simulated, and the simulation output is used to estimate the clearing function [capacity as a function of the workload]. Our efforts will contribute to improving the planning of the humanitarian network experiencing congestion at security check-points by minimizing the impact of congestion on the delivery lead time of relief aid to the final destination. For this, the Kerem Abu Salem security check-point in the south of GAZA is used as a case study. The results show that the CFDM tool works better when the Re-engineered-Clearing function's output is applied.
",Hybrid simulation-optimization approach for planning relief-aid distribution ,"[73839, 45231, 75661]",773,"[58, 12, 131]",42,Post-Disaster Relief Distribution,38,12,21,OR in Humanitarian Operations [HOpe],"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,49 [building - 116],"['Humanitarian Applications', 'Capacity Planning', 'Simulation']",WA-21
"Business globalization and strategic sourcing have made supply chain networks become length and complex, which have increased it more vulnerable and uncertain to disruptions. Geographical wars, such as the  Russia–Ukraine war, have shown the disruptive effect of these catastrophes, which inspires me to think that traditional supply chain management fails to cope with such disruption challenges. To address these current and potentially future challenges, scholars and practitioners suggested that supply chain management must incorporate resilience, robustness, and risk management capability. There are several approaches to assessing a resilient supply chain. Due to the ambiguity of resilience assessment, most measures are described subjectively using linguistic terms. Abandon the traditional way of assessing resilience, this paper develops a Fuzzy Resilience Index in Supply Chain [FRISC] to measure the level of supply chain resilience. The FRISC comprises attribute ratings and corresponding weights and is aggregated by a fuzzy weighted average. Finally, an example is presented at the end of the paper to illustrate the procedure of the proposed method. The results show that the proposed model can provide more meaningful results to the analyst and can efficiently assist managers in dealing with both ambiguity and complexity in supply chain resilience auditing.",A Comprehensive Model for Supply Chain Resilience Measuring,"[73845, 46843]",308,"[138, 49]",46,MCDA applications,44,8,44,Multiple Criteria Decision Analysis,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,20 [building - 324],"['Supply Chain Management', 'Fuzzy Sets and Systems']",TB-44
"Retail is often the last supply chain node a product/service ‘visits’ before reaching the end customer. As a result, customers’ behavior towards different incentives is paramount to managing retail operations, which combine elements of customer relationship management, supply chain and inventory planning, product distribution and logistics, pricing, and store management. Consequently, we must acknowledge that it may be hard to adhere to the common assumptions of operations research in this context -- that all relevant variables and constraints can be identified and accounted for to achieve the ideal outcome. In this talk, we’ll give a framework and examples for conducting empirically driven operations research to improve retail operations. The “empirically driven” approach has two main components - find an empirical justification of model assumptions and parameters and perform an empirical assessment of model results and insights.",Empirically driven operations research for improving retail operations,[23114],31,"[138, 32, 130]",47,Pedro Amorim,62,5,01,Keynotes,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,Sportshallen [building - 101],"['Supply Chain Management', 'E-Commerce', 'Service Operations']",MD-01
"During certain national festivities, nations occasionally grant mass prisoner pardons based on specific standards. In this study, we introduce a novel technique to determine which prisoners qualify for early release using a multi-criteria decision analysis [MCDA]. The criteria used were gender, age, crime severity, proportion of sentence completed, and behavior while in jail. Criteria weights were calculated using the analytical hierarchy process [AHP], based on data provided by the authorities. The model produced transparent and impartial results, offering decision-makers a convenient and efficient tool. ",Applying MCDA to Determine Eligibility for Pardon,[64656],122,"[6, 26]",48,Pairwise comparisons and preference relations 1,44,9,44,Multiple Criteria Decision Analysis,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,20 [building - 324],"['Analytic Hierarchy Process', 'Decision Support Systems']",TC-44
"Health Technology Assessment [HTA] is an applied discipline which involves providing support to decision makers about the cost-effectiveness and budget impact of medical technologies, with a view to informing decisions about reimbursement in a given country’s health system.  However, HTA is itself resource-intensive, and requires analytic skills which may be scarce, particularly in low and middle-income countries.  In response, many countries have between explored with “adaptive” HTAs [aHTAs].  The idea in an aHTA is for analysts in a target country to adapt the analysis and findings from HTAs from other countries, rather than undertaking HTA de novo.  aHTA is thus less-resource intensive and quicker than full HTA, but at the expense of lower contextual relevance.  Drawing on systematic reviews and meta-analyses for a number of medical technologies, we run a series of simulations to quantify the financial and health loss of using aHTA for a typical country for a range of cost-effectiveness thresholds, and to explore the factors which drive high losses. This work is a staging post to building an optimisation model which would support a country to optimise its HTA budget across full and adaptive HTAs.","Bronze, silver or gold?  What’s the added value of investing more in the analysis of medical technologies?","[11885, 76212, 76213]",718,"[56, 0]",49,OR Innovations in Policy Making - B,26,7,13,Soft OR and Problem Structuring Methods,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,15 [building - 116],['Health Care'],TA-13
"Preventive maintenance is a very critical topic for many industries. Particularly in healthcare, often the associated costs [monetary and social] of unplanned downtimes are much higher than those coming from scheduled maintenance. However, deciding upon optimal schedules is a very complex task. Ideally, maintenance should be performed only when it is absolutely needed, minimising potential disruptions.  In this paper we collaborate with Siemens to analyse a large data set consisting in log-data obtained from Computerised Tomography [CT] scan machines, with the objective of developing preventive maintenance based on anomaly detection. Here, the end-user [e.g., Hospitals] will have to make the decision of performing such maintenance based on the model predictions [and therefore, having to re-schedule patient's appointments]. Having this into consideration we further develop explainable analytics that provide a visual representation containing warning time windows for future potential machine failures, and an explanation of why our model classifies them in that way.",Preventive maintenance for computerised tomography [CT] scan machines - an anomaly detection approach based on log-data.,[61170],72,"[7, 47, 56]",50,"Advancements of OR-analytics in statistics, machine learning and data science 1",16,2,28,"Advancements of OR-analytics in statistics, machine learning and data science","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,065 [building - 208],"['Analytics and Data Science', 'Forecasting', 'Health Care']",MA-28
"Set optimization is a generalization of multiobjective optimization where a value of the objective mapping is not a single vector but a set of vectors. Polyhedral convex set optimization is the simplest class of set optimization problems comparable to linear programming in the field of scalar optimization. We survey some recent developments including solution concepts, solution methods and applications.",A survey on polyhedral convex set optimization,[19168],12,"[110, 112, 26]",51,Objective Space-Based Approaches in Multiobjective Optimization,34,7,37,Multiobjective Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,33 [building - 306],"['Programming, Linear', 'Programming, Multi-Objective', 'Decision Support Systems']",TA-37
"In this study, we analyze a variant of the well-known Bin Packing Problem where items can be fragmented and some pairs of items cannot be packed into the same bin due to conflicts. The goal is to pack a given set of items into a minimum number of fixed-capacity bins while not packing fragments of conflicting items into the same bin. This problem is called the Bin Packing Problem with Conflicts and Item Fragmentation. We develop a lower bounding scheme based on identifying a maximal clique of the conflict graph for the problem and propose a heuristic algorithm called the Sequential Maximum Degree Packing Heuristic which sequentially packs the items starting from the highest degree item using a mixed-integer model. We test the proposed solution approach on the instances with random conflict graphs and compare its performance against that of the benchmark algorithms in the literature. We observe that our approach outperforms the benchmark algorithms especially when the conflict graph is dense. It provides solutions with up to 13% less optimality gap compared to the algorithms in the literature. Finally, the proposed lower bounding scheme improves the trivial continuous lower bound only if a large maximal clique of the conflict graph can be identified.",Bin Packing Problem with Conflicts and Item Fragmentation,[41315],877,"[23, 14, 5]",54,Heuristic Algorithms for Combinatorial Optimization Problems I [Contributed],64,14,52,Combinatorial Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,8003 [building - 202],"['Cutting and Packing', 'Combinatorial Optimization', 'Algorithms']",WC-52
"In this talk we survey some recent developments on the OR aspects of kidney exchange programmes [KEPs]. These programmes have been established in most of the Western countries in the last two decades to facilitate the exchange of living kidney donors for those recipients who have willing but immunologically incompatible donors. In the first part, we describe the European practices including the hierarchic optimisation used in the matching runs of the KEPs for computing optimal solutions. These IP-based methods have been implemented in the simulator software developed in the ENCKEP COST Action [2016-2021] and subsequently incorporated in the KEPSOFT software that provides a common IT-platform for European applications. In the second part, we describe an alternative solution concept based on the individual fairness notion of stability. We explain how this classical cooperative game theoretical concept can incentivise the recipients to register valuable donors or multiple donors in KEPs. Finally, in the third part, we present new results on international KEPs, where credit-based compensation schemes have been proposed to balance out the benefits of the countries when merging their national pools.",Optimality and fairness in kidney exchange programmes,[68355],33,"[50, 56, 111]",55,Peter Biro,62,12,01,Keynotes,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,Sportshallen [building - 101],"['Game Theory', 'Health Care', 'Programming, Mixed-Integer']",WA-01
"Various nonlinear Conjugate Gradient [CG] schemes have been observed to improve convergence rates in single-objective optimization compared to steepest descent, without requiring second-order derivatives. Pérez and Prudente translated many of the most popular CG directions to the multi-objective case. However, in order to prove convergence, the classical CG schemes require us to assume Wolfe conditions on the step-size. In some settings, fulfilling Wolfe conditions might not be possible. Thus, we present several example directions that provide guaranteed descent in all objective functions, independent of the step-size. This enables us to prove convergence to Pareto-critical points for a simple inexact line-search algorithm with backtracking to satisfy some modified Armijo-rule.
Our special CG directions are designed to be implementable without too much computational cost, and numerical experiments show the efficacy of the algorithm.

",Multi-Objective Nonlinear Conjugate Gradient Schemes with Guaranteed Descent,"[74055, 69711]",87,"[112, 19, 110]",57,Advances in Continuous Multiobjective Optimization,34,8,37,Multiobjective Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,33 [building - 306],"['Programming, Multi-Objective', 'Continuous Optimization', 'Programming, Linear']",TB-37
"This paper studies the use of the Regular Vine copula models in the context of the construction of portfolios with varying sizes. We find that copula-based portfolios outperform the naïve equally-weighted benchmark before transaction costs. Significantly reduced tail risk makes copula-based portfolios especially desirable for investors with high risk aversion. The superior performance is more pronounced during periods of high dependence asymmetry and high market volatility. Sparse vine models, in which independence pair-copulas prevail, provide significantly improved results for large portfolios across various performance measures, specifically reducing turnover. The improvement of portfolio performance, however, is attenuated as we take transaction costs into account. Limiting large portfolio turnover by increasing investment horizon or rebalancing error tolerance restores the outperformance of copula-based strategies.",Sparse Vine copula-based portfolio optimization,"[75693, 70628, 70627]",411,"[45, 25, 126]",60,Market risk in a volatile world,9,3,51,Risk management in finance,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,M5 [building - 101],"['Financial Modelling', 'Decision Analysis', 'Risk Analysis and Management']",MB-51
"We posit an oligopolistic competition scenario in which two companies compete for an identical product, while consumers have different preferences for them. One product uses a technology that generates pollution, while the other uses cleaner technology. Consumers show a degree of environmental sensitivity in favor of organic products over pollutants. The host country's government sets an optimal emissions limit for the polluting company. This emission limit is positive when the discomfort caused by pollution and/or the level of environmental awareness is high enough. However, the optimal emission limit is zero if the discomfort caused by pollution and/or the level of environmental awareness is low enough. The intensity of environmental awareness has a negative effect on the optimal limit of permitted emissions.",Environmental Sensitivity and Emission Limits,[75697],687,"[33, 40, 100]",62,Toward Climate Neutrality,80,8,53,Sustainable and Resilient Systems,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,8007 [building - 202],"['Economic Modeling', 'Environmental Management', 'OR in Sustainability']",TB-53
"We consider a real-options problem in which the underlying project value follows a geometric or exponential Levy process, capturing rare events besides continuous fluctuations. Such rare events lead to ambiguity because of inconclusive empirical data or market incompleteness. We use ambiguity theory---leveraging the notion of variational preferences and $g$-expectations---to pin down for the general case a pricing kernel under which to value real options and derive the firm's optimal exercise strategy under this kernel. We also provide sufficient conditions for the optimality of a threshold policy in the general case. For the case with multi-priors preferences, we obtain explicit expressions for the optimal investment threshold, expected investment time, and value function and prove comparative statics to assess analytically the effect of small jumps on these. Closed-form expressions are not available for multipliers preferences, but we provide approximate solutions for the cases with negligible and deep ambiguity. Rare events, which are priced under ambiguity aversion, generally lead to a higher investment threshold, delayed investment and higher option value.",Real options under ambiguity with rare events,"[75698, 75700, 37974]",71,"[20, 108, 127]",63,Robust decisions in finance and investments,74,7,57,Modern Decision Making in Finance and Insurance,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S06 [building - 101],"['Control Theory', 'Programming, Dynamic', 'Robust Optimization']",TA-57
"We construct a financial market model wherein market makers post limit orders to compete for liquidity. We show that market makers may relinquish liquidity provision because of competition if the fundamental value of the asset is volatile. Besides the general perception that cancellation harms market liquidity, we find that cancellation can sustain liquidity and prevent depletion.  Moreover, this finding, as well as others, leads to an important policy implication that the cancellation strategy, one of the most prominent characteristics of high-frequency trading, can positively affect market liquidity and should not be strictly regulated.",Competition in Liquidity Provision - Analysis of High Frequency Market Making and Policy Implications,"[75689, 75709]",411,"[44, 0]",64,Market risk in a volatile world,9,3,51,Risk management in finance,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,M5 [building - 101],['Finance and Banking'],MB-51
"Coordination of inland flows with the hinterland is a difficult task, and lack of it results in long truck queues at the gates and unsatisfactorily conditions for truck drivers. In addition, the lack of space at ports and the limited resources demand efficient container handling operations to provide the required service levels to the ships and external trucks for transferring cargo.  One critical operation related to container [or cargo in general] handling has to do with customs checks and inspections. The massive flow of cargo that is being handled at ports provides an opportunity for organized crime infiltration. For this reason, Customs plays an important role in securing international trade operations and stopping the flow of illicit products. However, inspections are time-consuming and costly, and container delays at an inspection facility can cause supply chain disruptions. Hence, one critical task for Customs and Port Authorities is to define strategies and operational policies to perform this operation, in which logistics infrastructure plays a critical role. In this paper, we propose a supply chain network design framework to locate different Customs facilities where the required inspections will be performed, with the aim of having more fluid operations in the dispatching of import containers. We assume that rather than having each port terminal its own Customs facility, the authorities have decided to locate external facilities to reduce container delays.",Port Supply Chain Network Design for Customs Operations,"[74140, 65430, 50208, 74616, 74453]",83,"[70, 64, 25]",69,Port-Hinterland Transportation & Corridors,52,4,62,OR in Port Operations,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S12 [building - 101],"['Maritime applications', 'Location', 'Decision Analysis']",MC-62
"Considering the consumption preference and fairness concern, this paper constructs a sub-channel remanufacturing supply chain in which manufacturer sell new products through retail channel and remanufactured products through direct channel. The optimal profit equilibrium strategy in the centralized and decentralized decision making of sub-channel remanufacturing supply chain with or without sales effort is studied by means of differential game analysis, considering the effect of time attenuation of advertising effect generated by dual sales effort of manufacturer and retailer. It is found that regardless of the initial advertising effect size of new products and remanufactured products, both manufacturer and retailer must take sales efforts in a certain period to stabilize the supply chain profits，and there exists a critical value for the time to take sales efforts. In addition, it is revealed that the changing trend and the final stable value of the advertising effect of products depend on the optimal sales effort level and the effect of sales effort on the advertising effect, thus finding the changing law of the profit of the members of the supply chain and the total profit over time. This study provides a theoretical basis for supply chain member enterprises to effectively implement sales efforts on the profit control strategy of the sub-channel remanufacturing supply chain.",Research on profit control strategies of remanufacturing supply chain considering dual sales effort ,[58237],72,"[125, 32, 50]",70,"Advancements of OR-analytics in statistics, machine learning and data science 1",16,2,28,"Advancements of OR-analytics in statistics, machine learning and data science","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,065 [building - 208],"['Reverse Logistics / Remanufacturing', 'E-Commerce', 'Game Theory']",MA-28
"Prolonged emergency department [ED] length of stay [LOS] is associated with detrimental effects on patient care and quality, including increased mortality, increased risk of hospital-acquired infections, and disrupted patient flow. There is also evidence that certain groups of patients experience longer LOS based on their gender or race, especially with regard to the part of LOS that is attributable to waiting to be seen by a clinician. This work tackles the patient prioritization and placement aspects of ED operations with the goal of improving throughput and wait time in a fair, equitable way. We present a novel Mixed Integer Linear Programming predictive-prescriptive formulation that incorporates a breakdown of predicted patient ED LOS into actionable pieces and allows for a more granular model of ED operations. We show how to incorporate considerations for fairness and reformulate the MILP formulation into a compact and computationally tractable formulation that can be solved efficiently in real time. This work was conducted in collaboration with a large US academic medical center. Data from more than 40000 patient visits were used to shape and evaluate the models. We provide an interpretable metamodel trained on the complex model’s recommendations in order to help with the operationalization of the algorithm. The method will be used by the hospital to improve patient flow and quality of care as well as to support more fair and consistent bed allocation decisions.",A granular approach to optimal and fair patient placement in hospital emergency departments,"[3024, 75726, 70470, 75725, 75724]",107,"[56, 5, 41]",72,Ethics of OR and artificial intelligence ,28,2,20,OR and Ethics,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,45 [building - 116],"['Health Care', 'Algorithms', 'Ethics']",MA-20
"Although TSP has been placed in the subject of NP-completeness, it might not be as complex as expected. Often the seed of a solution is in the problem structure that creates the complexity. The intrinsic difficulty of TSP is due to the combinatorial explosion of tours in the solution space S, which increases in [n-1]! as TSP size n increases. In TSP, a small set of n[n-1] edges make up a large set of [n-1]! tours in S. Let S’ be a subset of S, if we can exponentially reduce S to S’ using a polynomial-time local search algorithm, and S’ can be polynomially searched by an exhaustive search algorithm, there is a polynomial algorithm for TSP. A local search can reduce S to S’ quickly using the edge matrix E of TSP as a reduction mechanism, because if one edge is removed in E, [n-2]! tours are removed in S. Instead of searching for better tours in S, our effort is to remove bad edges in E. In a local search system, an attractor S’ drives search trajectories into the vicinity of a globally optimal point in S. S’ contains a small set of the most promising tours and is exponentially smaller than S, thus make an exhaustive search feasible. This new search paradigm is called optimizing with attractor. A new search algorithm consists of two search phases - local search phase quickly construct S’ in S, and exhaustive search phase examines S’ completely to find the optimal tour. TSP size can be reduced through its own data structure, thus it is a self-reducible problem.",Optimizing with attractor - combinatorial complexity of the traveling salesman problem,[5910],196,"[5, 14, 52]",73,"Discrete, continuous or stochastic optimization and control in networks, transportation and design I ",64,2,25,Combinatorial Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,011 [building - 208],"['Algorithms', 'Combinatorial Optimization', 'Global Optimization']",MA-25
Several mixed-integer programming models have been proposed for scheduling project activities under predefined resource capacities and precedence constraints. We introduce a model expansion that considers aggregated capacity limitations. A computational evaluation for three state-of-the-art models from the literature shows that the extension is advantageous for each of the tested models.,Workload-Based Extensions of Mixed-Integer Programming Models for Resource-Constrained Project Scheduling,"[125, 75739, 75740]",960,"[129, 12, 110]",76,RCPSP and extensions,35,5,60,Project Management and Scheduling,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S09 [building - 101],"['Scheduling', 'Capacity Planning', 'Programming, Linear']",MD-60
"This paper utilizes a cue diagnosticity framework to differentiate between high-scope cues [the founder’s social capital and online comments] and low-scope cues [the number of pictures, sentences and videos], and empirically investigates how these two types of cues are diagnostic in the process of assessing the product quality. Based on the cue diagnosticity framework, under different high-scope cues, this paper puts forward hypotheses about the influence of low-scope cues on backers’ quality perception of crowdfunding products. The findings show that regardless of the social capital and comments of founders, the number of pictures and videos in the introduction of reward-based crowdfunding will have a positive impact on the quality perception of crowdfunding products. When at least one of the founder’s social capital or comments is good, the number of sentences will have a positive effect. However, when the founder’s social capital and comments are poor, the number of sentences will have a negative impact on the quality perception of products. Furthermore, we replace the dependent variable and change the classification method of backer’s social capital to ensure the robustness of the results. This paper enriches the research of reward-based crowdfunding, and provides important implications for founders to raise more funds from backers by publishing more effective information.",The effect of multiple cues on the quality perception of crowdfunding products - A cue diagnosticity framework,[75736],548,"[32, 67]",78,"Digitization in Knolwedge, Technology, and Innovation",54,14,08,"Knowledge, Technology, and Innovation","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,1020 [building - 202],"['E-Commerce', 'Management Information Systems']",WC-08
"When considering the prevention of food waste, it is worth examining the strategies of perishable food retailers. Inventory management and sale price strategies of perishable food products are essential for retailers to maximize profit and minimize food waste. In our study, we examine the challenge faced by a retailer who simultaneously sells a single type of food product with different ages [old and new], allowing for demand shifts between these products based on sale prices and consumer behaviors. We develop a bi-objective dynamic programming model and find the optimal discounted price, sale price and order quantity of food products to maximize the retailer’s profit and minimize food waste. We formulate four commonly practised static and dynamic pricing policies and quantify the advantage of dynamic pricing and price differentiation between old and new products by considering both profit and waste. Our findings indicate that significant improvements can be obtained by dynamically determining the order quantity and the old product’s sale price, considering the current inventory at hand. Moreover, for our bi-objective function, we use the weighted-sum method and we analyze the results of different weight combinations of profit and waste in the objective function. The findings emphasize the significance of waste and sustainability issues, highlight the balance between waste and profit, and offer valuable insights to companies aiming to enhance their system results.",Optimizing Profit and Minimizing Waste - A Bi-Objective Dynamic Programming Model for Inventory Control and Pricing of Perishable Products,"[75750, 13439]",482,"[108, 61, 124]",89,Omni-Channel Retailing ,30,10,50,Retail Operations,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,M2 [building - 101],"['Programming, Dynamic', 'Inventory', 'Revenue Management and Pricing']",TD-50
"Product returns are an essential part of conducting business today, due to the emergence of e-commerce and the proliferation of generous return policies.  Such policies, however, can backfire due to their costly operations and unpredictability. To minimize the adverse impact, we develop and compare different collection policies to minimize the combined inventory and transportation costs in a three-echelon network consisting of [a] customers returning products to initial collection points [ICPs], [b] the ICPs, and [c] the centralized return centers [CRCs], that receive the products shipped from  the ICPs after they are aggregated into larger shipments to leverage economies of scale. Collection policies are developed for the cases of a single product and a single ICP and CRC; multiple products and a single ICP and CRC; and multiple products with a single ICP and multiple CRCs, by using individual shipment policy where ICPs ship out products to individual CRCs  and a combined shipment policy that allows shipping and routing from an ICP to multiple CRCs.  Mathematical models are developed to determine the collection period at the ICPs to minimize the combined inventory and transportation costs for these policies. An efficient algorithm is developed, and results are presented with an experimental dataset.",Optimal Policies for Handling Returned Products in the Reverse Supply Chain,"[42757, 2717, 75783]",482,"[125, 138, 84]",90,Omni-Channel Retailing ,30,10,50,Retail Operations,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,M2 [building - 101],"['Reverse Logistics / Remanufacturing', 'Supply Chain Management', 'Optimization Modeling']",TD-50
"The Highway Alignment Optimization [HAO] is an optimization model that has been used to find alignments for a new highway that is to be constructed. Traditionally, the approach has been to use Genetic Algorithms [GA] for producing highway alignments for short highway sections. In this study, a long highway that provides accessibility to settlements as it traverses across a wide geographical area was taken into consideration. The objective of the HAO model has been defined as improving the ease of access from these settlements to the highway. A solution methodology referred to as the Alignment Bacterial Foraging Optimization [ABFO] algorithm has been developed that is capable of generating highway alignments for this HAO variant. An accessibility index has been defined to represent the total accessibility from all the settlements to the highway under consideration. The numerical results of the ABFO indicates that a set of good highway alignments can be generated using this method. While, the ABFO can presently generate horizontal highway alignments, this method may be modified to generate 3D alignments in the future. The ABFO indicates promise and can provide decision-makers with a set of good highway alignments to enable better and well-informed decision making. ",An Alignment Bacterial Foraging Algorithm for Highway Alignment Optimization,"[75784, 75830, 75831]",523,"[5, 74, 143]",92,Optimization in transportation infrastructure design and management,6,5,56,Transportation,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S04 [building - 101],"['Algorithms', 'Metaheuristics', 'Transportation']",MD-56
"Modern computer networks are required, on one hand, to support high bandwidth applications, on the other hand, to have stringent Quality of Service [QoS] guarantees. This is a relevant practical issue, since many applications over IP networks require QoS in terms of real-time guarantees, that is, controlled end-to-end delay.  This implies that Internet Service Providers are required to negotiate delay bound within their Service Level Agreements, which in turn requires appropriate traffic engineering support.
From a mathematical point of view, this problem, known as Delay Constrained Routing [DCR],  can be formulated as a Mixed-Integer Nonlinear Program, where one needs to simultaneously compute paths and reserve resources along the paths of the network, since the maximum delay of a flow depends [in a non-linear way] on both.
Even in the single-flow case, DCR is significantly more difficult than classical shortest path routing problems. Yet, DCR presents an interesting mixture of combinatorial and continuous structures and naturally lends itself to decomposition methods. We propose a combined Lagrange-Benders approach that provides both upper and lower bounds of very good quality in extremely short computing times. ",A combined Lagrange-Benders approach to Delay Constrained Routing,"[969, 12770, 75795]",132,"[113, 150, 141]",93,Topics in Mixed Integer Nonlinear Programming 1,86,8,04,MINLP,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,1001 [building - 202],"['Programming, Nonlinear', 'Network Flows', 'Telecommunications']",TB-04
"Since the late 1960's, Uruguay has developed an important system of housing cooperatives, which are an affordable option with many advantages over traditional options such as buying or renting a house or apartment.

A housing cooperative in Uruguay usually involves between 20 and 80 members. After the buildings are completed, a assignment is performed, giving to each member [as long as he/she continues to participate in the cooperative] the right to live in the assigned unit. The assignment is most usually performed by a random draw; which does not take into account individual preferences of the members over the different units.

In this presentation, we discuss the use of optimization methods to perform assignments which take into account the preferences of the cooperative members. Working with some Uruguayan cooperatives, we found that the total satisfaction,  the minimum individual satisfactionwere important criteria.  Based on these insights, we developed a system employing a a two-stage Mixed Integer Linear Programming [MILP] optimization model taking into account the minimum individual satisfaction and the total satisfaction. This system has already been applied in more than 30 cooperatives, involving more than 1000 habitational units, obtaining large improvements in comparison to random draws. Further work is being performed to include other criteria such as minimizing the difference between individual satisfactions.",Preference-based unit assignment for Uruguayan housing cooperatives,[2542],56,"[72, 151, 28]",96,Developing green and sustainable communities [EWG-ORD Workshop 1],67,9,18,OR for Development and Developing Countries,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,42 [building - 116],"['Mathematical Programming', 'Practice of OR', 'Developing Countries']",TC-18
"To promote and encourage sustainable development in favour of ensuring an egalitarian and environmentally secure future, Yale and Columbia Universities jointly with World Economic Forum developed the Environmental Performance Index [EPI]. The EPI is a composite index of environmental and sustainability indicators that quantifies and assesses a country’s environmental performance based on three broad policy objectives - climate change, environmental health, and ecosystem vitality. This paper proposes an approach based on the cross-efficiency data envelopment analysis [DEA] for estimating the EPI. We propose cross-efficiency models for settings where there are only outputs [without inputs], as in composite indicator construction, that provides unique cross-efficiency scores. As cross-efficiency incorporates both self- and peer-evaluation, it offers a more acceptable and reasonable mechanism for computing the EPI. Utilizing the scores obtained via the cross-efficiency DEA approach, we extend our analysis by employing the regression to determine the relative importance and contribution of the variables used in EPI construction and find suitable weights that could be assigned to the indicators if the decision-maker desires to estimate the countries’ EPI scores via fixed weights. The limitations of the existing methodology for EPI estimation, which is based on a non-optimization approach with weights assigned to the indicators subjectively, are overcome in our proposed approach.",A cross-efficiency DEA approach for composite indicators - Revisiting Environmental Performance Index,"[75800, 23679, 75833]",940,"[24, 40, 35]",98,DEA applications in Environment and Sustainability I,89,8,48,Data Envelopment Analysis and its Application,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,60 [building - 324],"['Data Envelopment Analysis', 'Environmental Management', 'Efficiency Analysis']",TB-48
The aim of this work is to find the solutions of the Interval-valued Optimization Problem through the Variational Interval-valued Problem on Hadamard manifolds. We will use the gH-differentiability for interval-valued functions on Hadamard manifolds and we will extend the Stampacchia and Minty versions of these variational problems given by Euclidean spaces and real valued functions to interval-valued functions on Hadamard manifolds.,The Interval-valued optimization problem through variational inequality problems on Hadamard manifolds,"[19254, 13763, 18427, 62647]",49,"[19, 51, 72]",99,Vector and Set Optimization I,33,2,41,Vector and Set Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,97 [building - 306],"['Continuous Optimization', 'Generalized Convex Optimization', 'Mathematical Programming']",MA-41
"The best strongly polynomial bound for maximum flows in a graph with n nodes and m arcs is the O[nm] bound by Orlin [STOC 2013].  His algorithm runs in O[nm] time for graphs that are suitable sparse.  King, Rao, and Tarjan [J. Algorithms 1994] had already established the O[nm] run time for all other networks.

Strengthening Orlin's approach, we present a black-box framework that makes any weakly polynomial maximum flow algorithms to strongly polynomial. The algorithm relies on using Italiano’s data structure for maintaining an incremental dynamic transitive closure, which runs in O[nm] time.   Surprisingly, the remaining operations take much less time, except in the most dense networks.   If one can improve the run time for dynamic transitive closure, then one can speed up the running time for the max flow problem as well.   We also obtain improved running times for a range of special cases including instances with a bounded number of arcs with finite upper capacity and instances with bounded tree depth.
",From Incremental Transitive Closure to Strongly Polynomial Maximum Flow,"[2715, 56944, 73911, 66922]",872,"[150, 5, 14]",100,Optimization problems on graphs,64,5,26,Combinatorial Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,012 [building - 208],"['Network Flows', 'Algorithms', 'Combinatorial Optimization']",MD-26
"The selection of equilibria is a central issue in the management of multi-agent systems that can be partially controlled. Once the system has been modelled, the selection can be performed through an hierarchical program whose lower-level describes the equilibria of the system and the upper-level explicitly addresses the selection criterion through a suitable objective function. These  hierarchical programs are simpler than more general bilevel structures as the lower-level problems are non-parametric with respect to the upper level variables. In order to tackle them, suitable approximated versions are introduced. On the one hand, the approximation does not perturb the original [exact] program too much and allows for some additional flexibility in the choice. On the other hand, it allows relying on suitable exact penalty schemes by recovering those regularity conditions that the original problems do not satisfy. These penalization approaches are addressed in detail and their convergence properties are established.",Equilibrium selection via approximation and penalization,[11645],30,"[50, 81, 3]",101,Tools and algorithms for equilibrium detection,63,10,40,"Interfaces Between Optimization, Hierarchical Problems and Equilibrium Detection with Applications","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,96 [building - 306],"['Game Theory', 'Non-smooth Optimization', 'Agent Systems']",TD-40
"Balancing supply and demand in free-floating one-way carsharing systems is a critical operational challenge. This paper presents a novel approach that integrates a binary logit model into a mixed integer linear programming framework to optimize short-term pricing and fleet relocation. Demand modeling, based on a binary logit model, aggregates different trips under a unified utility model and improves estimation by incorporating information from similar trips. To speed up the estimation process, a binning approach is used, where variables such as location and time are categorized into a few bins, which is particularly beneficial for trips with limited observations.
The modeling framework adopts a dynamic structure where the binary logit model estimates demand using accumulated observations from past iterations at each decision point. This continuous learning environment allows for dynamic improvement in estimation and decision making. At the framework's core is a mathematical program that prescribes optimal levels of promotion and relocation. The framework then includes simulated market responses to the decisions, allowing for real-time adjustments to effectively balance supply and demand.
Computational experiments demonstrate the effectiveness of the proposed approach and highlight its potential for real-world applications. The continuous learning environment, combining demand modeling and operational decisions, opens avenues for future research in transportation systems.",Enhancing carsharing pricing and operations through integrated choice models,"[71153, 35712]",108,"[143, 124]",102,Revenue Management in Sharing/Platform Economy,11,3,59,Pricing and Revenue Management,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S08 [building - 101],"['Transportation', 'Revenue Management and Pricing']",MB-59
"HiGHS is open-source software for large scale linear optimization, and has established itself as offering the best benchmark performance due to its innovative interior point [IPM] solver for LP, and powerful MIP solver. In particular, the IPM solver has been hailed as a game-changer for the field of open-source energy system modelling. However, for some instances, the IPM solver is still prohibitively expensive. A new IPM solver is now under development, and this talk will outline the progress that has been made, as well as setting out other recent developments in HiGHS.",Progress towards a new interior point solver for HiGHS,[45576],239,"[110, 114, 134]",104,Continuous Solvers,76,4,30,Software for Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,53 [building - 208],"['Programming, Linear', 'Programming, Quadratic', 'Software']",MC-30
"Railway freight transportation plays a crucial role in China's economic growth and has received substantial investment in railway infrastructure. The introduction of the high-speed freight train presents the logistics market with a competitive alternative between air and land transport. This new service enables cargo loading and unloading at various stops along the journey in order to maximize carriage utilization. Given the tight transit time at each stop, the high-speed freight service needs to allocate freights into different carriages without any remarshalling — such as removing existing freights before putting everything back — between freights. As such, the freights must be handled in a First-In-Last-Out [FILO] order and requires a careful plan of carriage assignment and storage allocation for each freight, which motivates this multi-stop railway freight loading and unloading problem. Considering the aforementioned operational features, a fresh variant of the bin packing problem [BPP] can be defined, i.e., the Temporal BPP with FILO policy [TBPP-FILO]. This novel variant incorporates the sequential requirements of loading and unloading processes, presenting a unique and difficult challenge compared to existing BPP variants. A hybrid Benders decomposition approach is developed using the technique of Lagrangian relaxation, which adjusts the allowance of capacity violation through the update of Lagrangian multipliers. ",A Hybrid Benders Decomposition for Multi-stop Railway Freight Loading and Unloading Problem,"[61927, 75811, 75810]",504,"[65, 72, 122]",105,Freight railway transportation ,6,3,56,Transportation,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S04 [building - 101],"['Logistics', 'Mathematical Programming', 'Railway Applications']",MB-56
"The fusion of machine learning and optimization has the potential to deliver outcomes for engineering applications that the two technologies cannot achieve independently. This talk illustrates this potential with the concept of an optimization proxy that can produce, in real time, feasible and near-optimal solutions to classes of optimization problems. The talk reviews the theoretical foundations underlying optimization proxies and demonstrates its practical practical on applications in power systems, supply chains, and
mobility.",Fusing Learning and Optimization for Engineering,[75814],27,"[66, 14]",106,Pascal van Henteryck,61,11,01,Plenaries,"Tuesday, 16:30-17:30",T,E,16:30,17:30,2024-07-02,Sportshallen [building - 101],"['Machine Learning', 'Combinatorial Optimization']",TE-01
"In focus is a smaller Software-as-a-Software [SaaS] enterprise that is pursuing AI-led innovation as a key strategy. Get a front-row view of leader decision-making and organizational behavior as this enterprise is operating in turbulent market conditions. How are AI ethics made during an industry-critical 6-month period [2023/24]?

A contemporary interpretation of Aristotelian phronesis – the ethics of “practical wisdom” – and an adaptive systems lens form the basis of inquiry into the situated action at this SaaS business - How are AI ethics included, excluded, centralized, or marginalized while the technological landscape changes rapidly? Drivers for decision-making, such as the tangle of stakeholder relationships, the evolving AI-based products themselves, leader mindsets, policies, reputation management, and design processes come into view.

The situated exploration is set in the context of broadly discussed AI ethics frameworks, such as the EU guidelines, UNESCO recommendations, and current academic research. Together, they outline a map of gaps and blind spots, as well as effective practices. This map informs a discussion of the opportunities and constraints for embedding greater AI-ethics “practical wisdom” into turbulent organizational life – and for strengthening positive trajectories with AI technologies.
",AI ethics-in-the-making - a situated perspective,[75680],107,"[8, 41, 10]",109,Ethics of OR and artificial intelligence ,28,2,20,OR and Ethics,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,45 [building - 116],"['Artificial Intelligence', 'Ethics', 'Behavioural OR']",MA-20
"Abstract - 
This study investigates the retrial machine repair problem with warm standbys and working vacation, in which the server works with different repair rates rather than completely halting the repair during a vacation period. It is assumed that the server begins working vacation when the system is empty. When a failed machine finds that the server is idle upon arrival, it is immediately repaired by the server. If the failed machine finds that the server is busy upon arrival, it leaves the repair facility to join the retrial orbit and attempts repair again. The failure, repair, vacation, and retrial times are assumed to be exponentially distributed. A matrix analytic method was used to compute steady-state probabilities and several system performance measures. A multi-objective model for the cost and expected waiting time was derived. The epsilon-constraint method and non-dominated sorting genetic algorithm were used to identify and compare the Pareto optimal set and make comparisons. The Pareto front set was obtained and illustrated to improve the quality of decision making for system engineers.",Multi-objective Optimization of Retrial Machine Repair Problem with Warm Standbys and Working Vacation,[75837],200,"[121, 84, 135]",113,Multi-objective Combinatorial Optimization,64,4,52,Combinatorial Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,8003 [building - 202],"['Queuing Systems', 'Optimization Modeling', 'Stochastic Models']",MC-52
"We study the decision problem faced by the operator of a private 5G network, known as a private cell, who must allocate available capacity to meet the resource needs of the primary user of the network. The operator may lease excess capacity to external secondary users to generate additional revenue. Private cells are privately owned wireless networks. Industries such as manufacturing and transportation utilize private cells to prevent downtime for their automated operations. Private cells use network slicing, a technological advancement made available by 5G, to meet differentiated application needs. Under slicing, the network's resources can be dynamically segmented to support specific applications. Network slicing also allows the resource capacity currently not used by the primary user to be leased to a secondary user, such as a broker. Given this setting, we study the problem faced by an operator whose main responsibility is to serve the slice instances of the private cell's primary user but who can also lease excess resources to a secondary user to generate revenue. Primary user slice instances require a specific combination of network resources, such as spectrum, computation, or storage. The operator determines which instances will be admitted to the network for service and which resources can be leased to the secondary user. We use an MDP model to formulate this problem and characterize the optimal admission and leasing decisions.  ",Dynamic Capacity Management in 5G Networks,"[74278, 51180, 74282]",800,"[141, 135, 108]",119,Analysis of Stochastic Models II,50,15,39,Stochastic Modelling,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,35 [building - 306],"['Telecommunications', 'Stochastic Models', 'Programming, Dynamic']",WD-39
"As we think about what impact we would like operations research [OR] to have on the world, it can be helpful to look to the past for guidance and inspiration. This talk overviews the early stages of operations research becoming a discipline and academic field of study following World War II. In this talk, I will introduce “fun facts” about OR history, including the piece of OR history that inspired a scene in the film Good Will Hunting. I will also discuss early attempts to define the field of operations research, drawing upon the writings of Philip McCord Morse. The young field of OR experienced some growing pains, when some leaders in the field expressed their concerns about the demise and possible death of OR. Ultimately, OR flourished in the following decades. A theme of the talk is that various efforts taken to tackle hard problems defined the field of OR, opened up fruitful areas for exploration, and guided the evolution of OR.",Tackling hard problems - on the evolution of operations research ,[61471],454,"[88, 0]",120,Moments in the history of OR  2,27,14,20,Moments in the history of OR,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,45 [building - 116],['OR History'],WC-20
"Rooted in the theories of transaction cost economics and contingency theory for technology transfer, this study is being undertaken to examine supplier reputation aligned with the competitive needs of an organization as an important criterion for the selection of supplier-sourced innovative new technologies. Competitive advantage for an organization through supplier-sourced innovative technologies would be empirically investigated by i] analyzing the relationship of supplier reputation fit with the competitive advantage of the organization, and ii] analyzing the relationship between competitive advantage and the successful adoption of supplier-sourced innovative technology in the organization. As such, the study aims to make an important theoretical contribution in the area of buyer-supplier innovative technology diffusion, transfer, and integration. Further, as more organizations are undertaking digital transformation initiatives, the proposed study can provide empirical evidence for the managers on the importance of supplier reputation as a critical selection criterion for supplier-sourced innovative new technology.",Supplier reputation and Supplier-sourced innovative new technologies adoption ,"[75839, 75842]",61,"[138, 67, 132]",121,"Theory of Knowledge, Technology, and Innovation",54,12,08,"Knowledge, Technology, and Innovation","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1020 [building - 202],"['Supply Chain Management', 'Management Information Systems', 'Social Networks']",WA-08
"This study delves into the emerging field of Supply Chain Digital Transformation [SCDT] with a specific focus on fostering resilience and sustainability within the Indian manufacturing context. Identifying and analyzing the Critical Success Factors [CSFs] underpinning SCDT success is the central theme. Through an extensive literature review and collaboration with industry experts, fourteen CSFs were pinpointed. The study employs a unique approach — Grey Causal Modelling, to unravel the interdependencies and relative importance of these factors. Two pivotal cause factors, Vision for Digital Operations and Coordination and Collaboration, emerged as crucial in driving SCDT success. By prioritizing these factors, manufacturing firms can enhance their chances of achieving successful SCDT with resilience and sustainability objectives. The research not only contributes valuable insights for organizations navigating the digital landscape but also sheds light on the intricacies of SCDT in the dynamic environment of Indian manufacturing. The study underscores the strategic importance of identifying and nurturing specific factors, offering guidance for firms aiming to thrive in today's digitally-driven world.
",Unveiling Success in Supply Chain Digital Transformation - A Comprehensive Analysis of Critical Factors for Resilience and Sustainability in Indian Manufacturing,"[75827, 42684, 75850, 75851]",308,"[69, 77, 138]",124,MCDA applications,44,8,44,Multiple Criteria Decision Analysis,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,20 [building - 324],"['Manufacturing', 'Multi-Objective Decision Making', 'Supply Chain Management']",TB-44
"As a different paradigm from Selten's perfection for rationality on strategy perturbation, the concept of quasi-perfect equilibrium was formulated by van Damme through backward induction for finite extensive-form games with perfect recall. The admissibility of quasi-perfect equilibrium gains itself an advantage over Selten's perfect equilibrium. Nevertheless, the formulation provides insufficient information on how to find such an equilibrium so that its existence was derived from the existence of a normal-form proper equilibrium [sufficient but unnecessary]. To address this issue, this paper develops with a separation technique an equivalent definition of quasi-perfect equilibrium through the introduction of epsilon-quasi-perfect equilibrium and establishes directly the existence of a quasi-perfect equilibrium. To demonstrate its computational effectiveness, we illustrate with simple examples how one can employ the definition to analytically find all the quasi-perfect equilibria. As a byproduct, we acquire an equivalent definition of sequential equilibrium. A further application of the definition leads to a differentiable path-following method to compute quasi-perfect equilibria.    ",Computing quasi-perfect equilibria - from a characterization to a differentiable path-following method,"[10076, 71349]",30,"[50, 19, 113]",125,Tools and algorithms for equilibrium detection,63,10,40,"Interfaces Between Optimization, Hierarchical Problems and Equilibrium Detection with Applications","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,96 [building - 306],"['Game Theory', 'Continuous Optimization', 'Programming, Nonlinear']",TD-40
"We study a backlogging inventory model of a manufacturing system with two facilities that periodically produces, transfers and sells a product to stochastic and price-sensitive demands. By expounding an insightful difference in reactive transshipment decisions between lost-sales and backlogging models, we characterize inefficient backorder transshipment around the facilities in the backlogging situations and demonstrate, on account of the inefficient backorder transshipment, that optimal production,
pricing, and transshipment decisions can retain complementarity for a group of variables, and behave monotonically. Further, we perform the monotone comparative statics of the complementary variables to establish sensitivities for substitutable demands to optima pricing decisions.","Production, pricing and transshipment control on a backlogging manufacturing system",[75963],73,"[61, 108, 138]",130,"Advancements of OR-analytics in statistics, machine learning and data science 2",16,3,28,"Advancements of OR-analytics in statistics, machine learning and data science","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,065 [building - 208],"['Inventory', 'Programming, Dynamic', 'Supply Chain Management']",MB-28
"In real life, such as fashionable commodities and high-tech products with short product life cycle, the length of the waiting time for the next replenishment is the main factor for deciding whether the backlogging with be accepted or not. On the other hand, such as seasonal items, first-hand vegetables, and fruits during the expiration period, there was no deterioration occurring immediately. In addition, several studies have shown that the impact of inflation on lot-sizing decision-making is often not negligible. We also noted that sales at the retail level tend to be proportional to the amount of inventory displayed. To reflect these phenomenon, this paper presents an inventory model for non-instantaneous deteriorating items with price- and stock-dependent demand under inflation and time value of money over a finite planning horizon. In the model, shortages are allowed and the unsatisfied demand is partially backlogged at the exponential rate with respect to the waiting time. We establish the theoretical results and provide an efficient solution procedure to find the optimal number of replenishment, the cycle time and selling price. Then the optimal order quantity and the total present value of profits are obtained. Finally, numerical examples are used to illustrate the proposed model. In addition, sensitivity analysis of the model parameters and some managerial insights are also discussed.",Optimal Ordering Policies for Non-instantaneous Deterioration Items with Price and Stock Sensitive Demand under Inflation and Time Discounting,"[75816, 27549]",833,"[61, 105, 151]",132,Stochastic inventory systems,32,12,49,"Lot Sizing, Lot Scheduling and Production Planning","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,M1 [building - 101],"['Inventory', 'Production and Inventory Systems', 'Practice of OR']",WA-49
"Public authorities are obliged to prepare annual budgets. The planning process regularly takes place in a two-way process between the administrative board and specialist departments and is predominantly based on a qualitative forecasting methodology and the updating of the previous year's plans. However, subsequent plan/actual comparisons regularly show that the actual values realized deviate significantly from the planned values in some cases. The uncertainty of developments in the next financial year appears to be too great for the predominantly applied qualitative methods for determining budget estimates to be sufficient to generate budgets that are stable in the sense of low deviations between planned and actual figures. This is taken as an opportunity to develop a quantitative planning methodology with the aid of Monte Carlo simulation in order to take better account of distribution and event risks. A multi-stage procedure is presented, which takes into account the special features of public administration operations and includes suggestions for practical implementation.",Bandwidth planning for public authorities using the example of german municipalities,[61435],255,"[47, 101, 131]",134,"OR in Accounting - Planning, Taxation, and Reporting",7,13,59,OR in Financial and Management Accounting,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S08 [building - 101],"['Forecasting', 'OR/MS and the Public Sector', 'Simulation']",WB-59
"A multitude of studies within the domain of tourism and hospitality sector have delved into the influence of perceived value on diverse aspects of consumer behaviour. Perceived value is regarded as a crucial element in comprehending a customers' tangible actions. The present study encompasses functional value, FV, social value, SV, and emotional value, EV, as the three distinct dimensions contributing to perceived value and explores their impact on millennial tourist satisfaction, MTS, and their revisit intention, MTRI, in the context of homestay experience in the Indian Hill State of Uttarakhand.  Responses from 329 millennial tourist respondents were analysed. The results of multiple regression analysis reveal that an amalgamation of FV, SV and EV effectively explains 71.2% of the observed variability in MTS. Additionally, the calculated Durbin-Watson statistic is 1.913 indicating the absence of autocorrelation within the residuals, reinforcing the model's reliability. Digressing further, it is observed that FV and EV significantly impact MTS [p < 0.001; β = 0.451 p = 0.002; β = 0.157, respectively], but the same effect is not observed in the case of SV [p = 0.347; β = 0.057]. Further, MTS is found to significantly impact MTRI [p < 0.001; β = 0.868]. Homestays occupy a substantial section of the hospitality industry in this hill state. The findings have important implications for designing a strategy for augmenting millennial tourist revisit in the present context. ","Unravelling the Role of Perceived Value Dimensions in Shaping Millennials' Revisit Intentions - An Empirical Analysis of Homestays in the State of Uttarakhand, India","[58566, 76023, 76024]",536,"[71, 137, 28]",136,Applications of Knowledge and Technology,54,13,08,"Knowledge, Technology, and Innovation","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1020 [building - 202],"['Marketing', 'Strategic Planning and Management', 'Developing Countries']",WB-08
"In recent years, the transition from fossil fuels to clean energy technologies has accelerated. Although this transition might bring environmental and economic benefits, it requires a long-term strategic plan due to the large investment costs involved. In addition, the uncertainties in the technical and economic development of next generation clean energy technologies should be included in this plan. Our research focuses on optimizing strategic plans for the transition to next generation clean energy technologies using stochastic programming. As a case study, we examine the Machine Replacement Problem under Technological Advances. Although it is common to periodically replace machines that completed their economic lifetimes, the exact times of replacement should be optimally determined. For example, in the short-term, it might seem beneficial to replace a conventional machine with an existing clean technology whereas in the long-term, it might be more beneficial to wait and replace it with a future technology that is currently evolving. We formulate this problem using multistage stochastic programming by creating technology advancement scenarios. We analyze the underlying problem structure and the computational complexity of deterministic and stochastic versions. We also present how to solve large-scale problems more efficiently. Finally, we discuss potential applications of our approach to real-life problems such as clean energy transitions for fleets and campuses. ",Strategic Planning for Transition to Next Generation Clean Energy Technologies using Stochastic Programming ,"[74967, 76028]",247,"[93, 117, 111]",137,Planning problems in electrical energy systems,23,2,21,OR in Energy,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,49 [building - 116],"['OR in Energy', 'Programming, Stochastic', 'Programming, Mixed-Integer']",MA-21
"We study a general class of stochastic scheduling problems that include various machine scheduling and resource-constrained project scheduling problems under uncertainty as special cases. There are two general strategies to solve the addressed class of scheduling problems with stochastic activity durations - open-loop and closed-loop. Although closed-loop policy is theoretically advantageous over open-loop policy, as computing an optimal closed-loop policy requires solving the Bellman equation, which suffers the well-known “curse-of-dimensionalities” for large instances. In this paper, a Markov decision process [MDP] model built upon discrete-time Markov chain [DTMC] is developed for the addressed class of problems. To tackle the curse-of-dimensionalities of obtaining the exact closed-loop policy to the MDP model, we present a general approximate dynamic programming [ADP] framework that integrates simulation, optimization, and reinforcement learning, called Sim-Opt-RL, to provide quality and computationally tractable closed-loop policy. We implement the Sim-Opt-RL framework for a well-known and well-studied stochastic resource-constrained project scheduling [SRCPSP], with a custom designed genetic algorithm [GA], which outperforms the existing closed-loop algorithm, and is competitive with the state-of-the-art open-loop algorithms for the SRCPSP. ","Integrating Simulation, Optimization and Reinforcement Learning for a General Class of Scheduling Problems",[73533],932,"[118, 109, 129]",139,Machine Learning in Machine Scheduling,35,14,60,Project Management and Scheduling,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S09 [building - 101],"['Project Management and Scheduling', 'Programming, Integer', 'Scheduling']",WC-60
"Using powerful technique of stochastic time change, we introduce a new two-factor commodity price model, where one of the fundamental factors is the activity rate. This factor implicitly introduces stochastic volatility into the model. The model is developed under both physical and risk neutral probability measures, which allows for a wide range of applications
ranging from derivatives pricing to risk management.
We derive forward prices and forward curve evolution within the model’s framework and develop an ingenious calibration procedure, which allows us to filter out the activity rate from daily observed price data. We apply the  model to the rich dataset of daily crude oil and natural gas spot and futures prices and demonstrate its versatility and excellent fit to the historical forward curves",Commodity Forward Curves with Stochastic Time Change,[76025],146,"[135, 0]",142,Modelling commodity markets dynamics,74,2,57,Modern Decision Making in Finance and Insurance,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S06 [building - 101],['Stochastic Models'],MA-57
"This presentation will highlight how decision science can power decision
making on the journey from designing a global ocean transportation network serving 100000+ customers across 130 countries to the intricate orchestration of schedules for 700+ vessels to transport our customers goods and ensure availability of equipment to enhance our customers supply chains all the way. At every phase decision science can play a pivotal role in increasing consistency and quality of decision making. We will also delve into the orchestration of problem solving between the different phases.

The presentation will emphasize how the implementation of models and algorithms is really just ten cents a penny – successful implementation of decision science necessitates harmonizing with organizational dynamics, processes, and technological frameworks. We'll explore the evolution of decision science products at Maersk emphasizing the criticality of obsessing about customer and business outcomes through performance and process metrics needed to sustain the vitality and health of decision science products.",Charting the Course - How to power decision making in the Maersk transportation network with decision science,[76035],26,"[26, 65]",145,Berit Brouer,61,6,01,Plenaries,"Monday, 16:30-17:30",M,E,16:30,17:30,2024-07-01,Sportshallen [building - 101],"['Decision Support Systems', 'Logistics']",ME-01
"System dynamics [SD] is a methodology to generate qualitative and quantitative models. SD has two main concepts that are highly suitable to use with scenarios - feedback processes that define the structure of sociotechnical systems and accumulation processes that are responsible for the dynamic behaviour of systems over time. Kunc [2023] proposes framework that integrates methodologically scenarios and SD where the integration can take multiple forms depending on the use of SD for creating or supporting scenarios. Lane [2017] proposes a behavioural focus for SD in terms of its contributions related to mental models, encountering the world, building and implementing formal models, and content of formal models. This work evaluates how the behavioural focus of SD can support overcome some of the behavioural issues in scenario development process such as cognitive biases, effective group facilitation and overcoming business-as-usual thinking [Bryson et al, 2016]",Behavioural aspects of system dynamics use with scenarios,[61687],111,"[10, 133, 131]",147,Scenarios and foresight practices - Behavioural issues I,13,12,11,Behavioural OR,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,12 [building - 116],"['Behavioural OR', 'Soft OR', 'Simulation']",WA-11
"Harry M Markowitz [1927-2023] was a man who lived many lifetimes. An American Jew born in Chicago, his childhood ran through the turmoil of both the Great Depression and World War II. He went on to do much pioneering research, for which he won both the Nobel and John von Neumann prizes.

Within OR, he's recognised as the father of Modern Portfolio Theory for his work on mean-variance analysis. Not content to define just one field, he's also known as the grandfather of Behaviour Economics for his ideas on behaviour under risk. Including and beyond these, he was a prolific researcher across economics, mathematics, and computer science.

Markowitz' collaborators in that are a who's who of 20th century scientists - Dantzig, Friedman, and Sharpe among many others. His academic career took him to institutions the world over, with positions at City University of New York, London School of Economics, and the University of Tokyo. This intertwined with a robust career in industry, including stints at RAND, IBM, and CACI, where he had a significant impact. His life spanned a century of tremendous development, filled with incredible [and at times unbelievable] history and many valued contributions to the sciences.

In this talk we look over Markowitz' life, reviewing the discoveries he made along the way and the influence he had on the direction of so many fields.",The life and work of Harry M Markowitz,[75364],454,"[10, 5, 45]",148,Moments in the history of OR  2,27,14,20,Moments in the history of OR,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,45 [building - 116],"['Behavioural OR', 'Algorithms', 'Financial Modelling']",WC-20
"Oil spills are a constant threat to marine ecosystems and coastal communities. One of the critical aspects of oil spill response is the placement of oil booms. The decision of where to place oil booms is complex as it depends on several factors, including the location of the spill, the nature of the coastline and the potential for environmental and economic damage, especially to the local community. In this paper, a not-oriented slack-based data envelopment analysis [NO-SBM-DEA] approach is proposed to evaluate coastal sensitivity to oil spills. The NO-SBM-DEA approach considers the key aspects of coastal sensitivity - geomorphology, socio-economic and environmental aspects. The specifics of all variables are taken into account when calculating an overall performance score so that there is no need to switch between input and output orientation. The proposed approach provides a comprehensive and objective assessment of coastal sensitivity that can guide the placement of oil booms in the event of an oil spill. All data originate from studies carried out within the North Adriatic Incident Response System [NAMIRS] project. The proposed method was also applied to a case study in the northern Adriatic Sea to demonstrate its potential as a valuable tool for global oil spill mitigation efforts worldwide. The method provides an objective and transparent tool for assessing the sensitivity of coastal areas and can help ensure that oil booms are placed in the most effective locations.",Coastline Sensitivity Analysis for Oil Spill Mitigation - A Not-oriented Slack-based Data Envelopment Analysis Approach,"[54251, 53329]",941,"[24, 40, 143]",149,DEA applications in Environment and Sustainability II,89,9,48,Data Envelopment Analysis and its Application,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,60 [building - 324],"['Data Envelopment Analysis', 'Environmental Management', 'Transportation']",TC-48
"The 15-minute city model proposes a redesign of the urban system to provide access to daily needs or destinations [work, food, health, education, culture and leisure] within a 15-minute walk, cycle or public transport journey. One of the main objectives is to reduce car dependency and it's side effects. The 15-minute may be implemented by thinking a city as a possibility of choice leading to a Social Interaction Potential [SIP]. That is the possibility of easily reaching numerous and diverse people; a potential of jobs or any other amenities. The problem is that the high level of these potentials, which depend on density and transport speed, has been achieved for decades mainly thanks to the speed of the car. Cities are currently built on temporal proximity [by car] rather than physical proximity, which generates car dependency. In order to move towards a more sustainable mobility, it is imperative to return to more physical proximity, to a functioning based more on short distances, which is the idea of the 15-minute city. The question, then, is how to return to an urban functioning based on short distances without abandoning the essence of cities, in particular a high level of SIP. We have developed an optimization model, called OPTIDENS, which allows to find [if possible] the suitable location [or relocation] of any set activities in a city, in such way to respect a given SIP at the lowest travel speed. Some numerical results on data from real-life cities will be given.",OPTIDENS  - A Mathematical Programming Approach for the 15-minute city model,"[38759, 51155, 76044]",612,"[100, 64, 72]",151,Optimization of sustainable urban mobility,79,7,18,Sustainable Cities,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,42 [building - 116],"['OR in Sustainability', 'Location', 'Mathematical Programming']",TA-18
"We describe a novel application of the difference of convex function algorithm [DCA] to a variety of equilibrium problems using the mixed complementarity problem [MCP] format.  These problems involve bilinear constraints, i.e., complementarity and can be approximated iteratively via convex subproblems using DCA.  We develop the necessary theory to make this possible and showcase how it works on several MCPs in energy and other infrastructure.",Solving Equilibrium Problems in Infrastructure using the Difference of Convex Functions Algorithm,"[6412, 76045, 36601, 23956, 74595]",175,"[21, 93, 50]",152,"Energy sector coupling, optimization and equilibrium",23,3,21,OR in Energy,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,49 [building - 116],"['Convex Optimization', 'OR in Energy', 'Game Theory']",MB-21
"We study inventory planning under demand and supply yield uncertainty. We present new optimality conditions and explicit solutions for the associated single-period newsvendor'' model.  In general, the stochastic-optimal order policy is non-linear in the starting inventory level $x$. The literature has proposed Linear Inflation Rules [LIR] that inflate the classic order-up-to policy. We prove that LIR are only stochastically optimal in the degenerate setting where either demand or [not and] supply yield is uncertain.

Given the good performance of LIR we investigate their optimality under two robust formulations and provide bounds.
We prove that LIR are robustly optimal when only the support of the demand and supply yield distributions are known.  Yet we also provide the optimal distribution-free policy when the first two moments of those distributions are known, which again is non-linear. Both models provide novel, explicit order rules that may be useful for dynamic inventory control.",Inventory Planning with Supply Yield Uncertainty - On the Optimality of Linear Inflation Rules,"[35566, 48165]",162,"[61, 136, 127]",157,Stochastic Models in Logistics,50,8,39,Stochastic Modelling,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,35 [building - 306],"['Inventory', 'Stochastic Optimization', 'Robust Optimization']",TB-39
"Building resilient and sustainable supply chains of goods and energy between Europe and Asia is a key challenge because of long distance, political risks, economic sanctions, environmental impacts, weather conditions, geoeconomics, and etc. In spite of existing routes passing Russia [the Norther Corridor] or Iran [the Silk Road], the European Union [EU] and Turkiye are more interested in initiating a new route in the middle of the two, bypassing Russia and Iran, is so-called the Middle corridor. In the Middle corridor, Azerbaijan, located at the intersection of the North–South and East–West transport routes, is recognized as an important transport and logistics hub. Meanwhile, the establishment of the Zangezur corridor by ending the conflict of Armenia-Azerbaijan may enhance diversifying energy supplies to Europe and beneficial for the global trading. The aim of this study is conducting a route optimization [intelligent, agility, cost-effective] followed by a data-driven analysis of influential factors and explore the various instruments of geoeconomic competition, then design a sustainable supply chains network. ",Sustainable and optimal transport corridors from China [Asia] to Europe reflecting geopolitical tensions in the region,[36980],56,"[138, 25, 28]",158,Developing green and sustainable communities [EWG-ORD Workshop 1],67,9,18,OR for Development and Developing Countries,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,42 [building - 116],"['Supply Chain Management', 'Decision Analysis', 'Developing Countries']",TC-18
"In the ‘Model Munich’, Stadtwerke München has collected spatial data on buildings, parcels of land and building blocks. This digital twin of the City of Munich is primarily used to answer questions related to the transition of the heating sector. Therefore, building-specific heat demand, utilized energy sources and technical building properties, but also regionally resolved supply potentials for groundwater heat pumps, air heat pumps, heating networks, etc. are included. Moreover, the ‘Model Munich’ is connected to the bottom-up simulation model Invert/EE-Lab, which employs a nested logit approach for building-specific calculations of long-term scenarios regarding the change in heating systems and renovation activities. A specific application of the ‘Model Munich’ is municipal heat planning. In 2021, Stadtwerke München agreed with the City of Munich to utilize this model for municipal heat planning. Within this collaboration, numerous simulation runs were conducted using Invert/EE-Lab, from which valuable insights could be gained. This contribution provides an overview of the architecture of the ‘Model Munich’. In addition, results of the simulation runs are presented, aiming to achieve climate neutrality by no later than 2045 in accordance with the German Heat Planning Act.",Transition of the heating sector in a large city - simulation of scenarios with a digital twin,[15267],813,"[37, 94, 3]",159,Emissions and Heating Sector,80,13,53,Sustainable and Resilient Systems,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,8007 [building - 202],"['Energy Policy and Planning', 'OR in Environment and Climate change', 'Agent Systems']",WB-53
"Problems of modeling of uncertainty and imprecision for the analysis of the insufficient expert data [IED] is considered in the environment of interactive group multi-criteria decision making [MCDM]. Uncertainty index is presented by the fuzzy measure on alternatives set. Based on the informational entropy maximum principle, moments’ method for the IED is developed for the evaluation of associated probabilities class [APC] of Choquet’s second order capacity. The second pole of the IED - the data imprecision in the form of a fuzzy subset [image] on alternatives set is constructed.  Based on the Sugeno finite integral most typical value [MTV] as a prediction on possible alternatives set for the IED is constructed. In MTV the both poles of the IED [APC and image] are accumulated. The numerical example illustrates the possibilities of the approach, in the optimal selection of recommendation elements from MCDM alternatives. The sensitive and comparative analysis for this example is provided.",Associated Probabilities and Insufficient Expert Data in Multi-Criteria Decision Making,"[76048, 35266]",885,"[25, 49]",160,Fuzzy sets and systems,44,12,47,Multiple Criteria Decision Analysis,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,50 [building - 324],"['Decision Analysis', 'Fuzzy Sets and Systems']",WA-47
"We assume fuzziness in emergency logistic planning models for the disaster zone, such as the covering, location, transportation and routing problems. We consider a fuzzy multi-objective emergency shelters location and victims evacuation problem [FMOESLVEP] in the disaster-stricken zone. The model objectives include [1] maximizing the total selection reliability index of opened shelters; [2] minimizing total costs, including ﬁxed costs for opening shelters, victim transportation costs, and service costs, [3] minimizing of monotone expectation of total time for victims’ evacuation, and [4] minimizing the number of open shelters. FMOESLVEP models are developed for the different input fuzzy information. For the evaluation of selection reliability indexes of opened shelters the models take into account the interactions between decision making attributes by aggregating the extended Choquet integral. The realization scheme of the solution for the FMOESLVEP models is given.",Humanitarian Relief Logistics Fuzzy Planning Model for the Shelters’ Location and Victims’ Evacuation in the Disaster Zone,[35266],885,"[8, 25, 77]",161,Fuzzy sets and systems,44,12,47,Multiple Criteria Decision Analysis,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,50 [building - 324],"['Artificial Intelligence', 'Decision Analysis', 'Multi-Objective Decision Making']",WA-47
"Decision Support Systems [DSS] were born in the 70’s. Since this decade, we can find in the literature several tracks of evolution for these systems. We will draw the evolutions of DSS - describing the architecture, the usability, the functionalities, the programming technics as well as the way to use these 
systems. We will show how Artificial Intelligence and Machine Learning influenced the development of DSSs. Different projects will be presented related to DSSs.",Decision support systems - History and trends,[76052],42,"[26, 8]",163,Pascale Zarate,62,7,01,Keynotes,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,Sportshallen [building - 101],"['Decision Support Systems', 'Artificial Intelligence']",TA-01
"The present study approaches variational inequalities governed by the perturbed operator generated by the energy of the mathematical pendulum. It was proved recently Fan-hemicontinuity for the gradient of the norm defined on its scalarly-positive, closed convex subdomain of a Hilbert space. The aim of the present study is to establish whether or not the positive result obtained for Fan-hemicontinuity can be extended to an arbitrary reflexive Banach space. In this matter it is natural to consider the duality map and its topological properties. Based on weak-weak sequential continuity of the generalized duality map, the property above holds on the discrete spaces of sequences with power p summable and does not hold on the Lebsegue spaces of functions with power p integrable, [p not equal to 2], thus restricting the space setting where weak compactness results are applicable.",Fan-hemicontinuity for the gradient of the norm in several reflexive Banach spaces,[72512],262,"[21, 19, 52]",165,Recent advances on Variational Inequalities and Equilibrium Problems I,51,13,43,Variational Inequalities and Equilibrium Problems - From Theoretical Advances to Real World Applications,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,99 [building - 306],"['Convex Optimization', 'Continuous Optimization', 'Global Optimization']",WB-43
"We present the difference of convex functions algorithm [DCA] applied to a variety of non-convex optimization problems arising in energy sector coupling and equilibrium problems. Non-convexities emerge frequently in many energy models, such as though bilinear terms, integer constrained variables and non-convex quadratic constraints. Recently, DCA has proven effective in solving such problems by linearizing non-convex terms and converging to an optimal solution iteratively by solving convex subproblems. In our work, we use DCA to solve a wide range of non-convex problems, such as linear complementarity problems, mixed integer linear programs and quadratic programs with non-convex quadratic constraints. We demonstrate the effectiveness and versatility of DCA by applying it to a variety of energy models, such as the unit commitment problem, district heating networks and energy market equilibrium problems.",Addressing Non-Convexities in Sector Coupling Problems via the Difference of Convex Functions Algorithm,"[76045, 6412, 36601, 23956, 74595]",175,"[93, 84, 5]",169,"Energy sector coupling, optimization and equilibrium",23,3,21,OR in Energy,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,49 [building - 116],"['OR in Energy', 'Optimization Modeling', 'Algorithms']",MB-21
"A dynamic iterative auction-based approach is developed to solve the multi-agent scheduling problem with parallel machines. In the proposed approach, an auction procedure is established for dynamic jobs participating in a real-time auction. A straightforward and easy-to-implement bidding strategy without price is presented to reduce the complexity of the bid determination. An adaptive Hungarian algorithm is applied to solve the winner determination problem. An analysis is conducted to show that the proposed approach is individual rationality and that the myopic bidding strategy is a weakly dominant strategy for consumer to submit bids. Extensive simulation results show that the developed approach can get high-quality solutions and have considerable stability on large-scale problems. ",Multi-agent Parallel Machine Scheduling based on Auction,[49806],808,"[129, 9, 3]",170,Machine scheduling problems,32,14,49,"Lot Sizing, Lot Scheduling and Production Planning","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,M1 [building - 101],"['Scheduling', 'Auctions / Competitive Bidding', 'Agent Systems']",WC-49
"Judgmental forecasting is the domain of psychologists interested in forecasting and covers the areas of judgmental probability forecasting and the judgmental adjustment of time-series models, mostly at an individual-participant level. Closely linked is the Delphi method which is a group-based method. By contrast, scenario thinking was, until recently, the domain of practitioners interested in helping organisations make better decisions in the face of uncertainty. My paper discusses the role of subjective probability and outcome verification, and the focus on single point estimates as opposed to creating multiple, broad-brush anticipations of the future. Until recently, very little academic research has used experimental techniques to evaluate the quality of developed scenarios, but this position is changing. Additionally, recent work within the judgmental forecasting tradition has combined judgmental prediction with scenario storylines, a focus that has also become part of practice - for example, within the UK National Grid energy scenarios that are used for national policymaking. Clearly, scenario thinking is now becoming strongly established in practice - perhaps as a response to the World's lack of preparedness for Covid-19 pandemic. I discuss the emerging need to develop and apply yardsticks of the quality of both the scenario development process and of the resultant scenario content. I outline my thoughts on guidelines for such standard setting.","Using judgmental forecasting and scenario thinking for anticipating the future - what are the differences, the similarities, and the advantages of each?",[76062],569,"[133, 47, 25]",171,Scenarios and foresight practices - Behavioural issues II,13,13,11,Behavioural OR,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,12 [building - 116],"['Soft OR', 'Forecasting', 'Decision Analysis']",WB-11
"In order to meet customer needs efficiently, logistics plays a crucial role within the broader supply chain by managing the forward and reverse flow, as well as storage, of products, services, and associated data. This study addresses the supply chain network problem, focusing on determining the optimal allocation of orders from multiple plants, warehouses, and distributors. The goal is to minimize total transportation and inventory costs while simultaneously optimizing locations, flows, shipment composition, and shipment cycle times. The formulated problem is treated as a multi-objective programming challenge and is solved using the value function approach to obtain the optimal order allocation of products. A case study on ABC Company's supply chain is conducted to assess the effectiveness and applicability of the developed multi-objective model. The results indicate optimal transportation and inventory costs for ABC Company, along with an optimal distribution network within India. These findings hold significant relevance for the manufacturing sector, especially for those grappling with logistics challenges in their supply chain networks. The developed model equips researchers and managers to navigate various uncertainties and logistics risks associated with supply chain networks.",Managing Transportation and Inventory Simultaneously - A Dual-Objective Approach in Supply Chain Networks with Multiple Distribution Networks,"[72417, 76297]",68,"[65, 77, 84]",173,Decision Support for Sustainable Operations,45,10,45,Decision Support Systems,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,30 [building - 324],"['Logistics', 'Multi-Objective Decision Making', 'Optimization Modeling']",TD-45
"In this presentation, we consider a firm that may face sudden decreases in its revenue. Its revenue is modeled by a geometric Brownian motion, and the cumulative effect of negative shocks is modeled by a compound Poisson process. The firm has two options - either to exit the market or to adopt risk mitigation measures to reduce the impact of the revenue decrease. The firm's option value is modeled as an optimal stopping problem, which we analyze in this presentation. Furthermore, we examine the impact of protective strategies on the firm's option value.",Management of adverse events - risk mitigation or exiting?,[71590],71,"[135, 27, 45]",176,Robust decisions in finance and investments,74,7,57,Modern Decision Making in Finance and Insurance,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S06 [building - 101],"['Stochastic Models', 'Decision Theory', 'Financial Modelling']",TA-57
"In the present research, a decision support methodology for facility location selection problems is developed. The methodology is based on the TOPSIS [Technique for Order Performance by Similarity to Ideal Solution] approach under a picture fuzzy environment. We are focusing on a special case of facility location problem, namely location planning for service centers. A set of potential center sites, or alternatives, are typically involved in this kind of challenge, and they are assessed based on some weighted criteria. The alternative that best meets each criterion will be selected for implementation. To evaluate criteria our approach uses experts' assessments, that is in the proposed methodology the values of the criteria are expressed in picture fuzzy numbers, given by the group of experts. The case when the information on the criteria weights is completely unknown is considered. In the context of picture fuzzy sets, the criteria weights identification based on De Luca-Termini information entropy is presented. Hence, the proposed approach is based on a picture fuzzy TOPSIS decision-making model with entropy weights. The fuzzy positive-ideal solution [FPIS] and fuzzy negative-ideal solution [FNIS] are first defined using the TOPSIS algorithm. Then the ranking of alternatives is performed following the proximity of their distances to both FPIS and FNIS. For this purpose, the picture weighted Hamming distance is used. ",Facility Location Selection Problem based on Picture Fuzzy TOPSIS,"[76065, 35266]",885,"[8, 25, 49]",180,Fuzzy sets and systems,44,12,47,Multiple Criteria Decision Analysis,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,50 [building - 324],"['Artificial Intelligence', 'Decision Analysis', 'Fuzzy Sets and Systems']",WA-47
"This paper addresses contexts in manpower or in the progression of chronic diseases. It uses the notion of a cohort whose members advance through various states over time [e.g. employees or patients]. The aim is to steer the system towards a desired state, or set of states, through interventions made by recruitment or treatments which are evaluated on their capacity to achieve this goal in fixed or free time control settings. The presentation is based on a modelling framework, which blends DEA with Markov Chains in radial and additive models. The Markov process offers a set of equations that can be used to describe the movement of entities through time in a hierarchical system [e.g. illness states] and makes possible the investigation of interventions in order to guide the system towards a desired future structure. A given set of possible policies [e.g., new treatments used in a health system] are treated as DMUS at each stage and various models, including single or two-stage configurations with recruitment Decision Making Units [DMUs], are presented in single or multiple targeting environments. The paper concludes by deliberating on the merits and constraints inherent in these models. 

The research project was supported by the Hellenic Foundation for Research and Innovation [H.F.R.I] under the 2nd Call for H.F.R.I. Research Projects to support Faculty Members & Researchers [Project Number - 3154].
",Planning methods using Data Envelopment Analysis and Markov Systems,"[381, 664, 70818, 68715]",937,"[24, 135, 35]",181,DEA and stochastic models,89,4,48,Data Envelopment Analysis and its Application,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,60 [building - 324],"['Data Envelopment Analysis', 'Stochastic Models', 'Efficiency Analysis']",MC-48
"Today, much attention is paid to the engineering of dependable and resilient intelligent systems and models. This direction includes multi-criteria decision making [MCDM] models and systems in emergency situations. A new interactive MCDM approach based on evidence theory and fuzzy discrimination analysis is developed. MCDM’s utilities in the discrimination q-rung orthopair fuzzy values in the Dempster-Shaper’s believe structure environment are transformed. New aggregation operators – extensions of Dempster’s extremal expectations under discrimination q-rung orthopair fuzzy information are constructed. MCDM’s alternatives by new constructed operators are ranked. For the illustration of obtained results a numerical example is created from Emergency Psychiatric Diagnostics.",Evidence Theory and Fuzzy Discriminations in MCDM. Application in Emergency Psychiatric Diagnostics,"[50597, 35266]",885,"[8, 25, 49]",182,Fuzzy sets and systems,44,12,47,Multiple Criteria Decision Analysis,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,50 [building - 324],"['Artificial Intelligence', 'Decision Analysis', 'Fuzzy Sets and Systems']",WA-47
"In the recent literature, an extensive application of realized covariance has been documented, which plays a key role in optimal allocation of various assets within contemporary portfolio management. The advantage of realized covariance, against alternative measures, is utilizing intraday high-frequency data observed over short intervals. Using this approach unknown integrated covariance matrix of a multivariate diffusion process can be estimated. This research is focused on modeling realized covariance between the EUROSTOXX50 index of Eurozone and gold, aiming to explore the characteristics of gold as a safe haven during crisis periods [including the COVID-19 pandemic and the Ukrainian war]. In this context, the primary goal is to determine data properties, such as asynchronicity and the presence of microstructural noise and price jumps of return series. Subsequently, a suitable synchronization scheme and optimal sampling frequency will be chosen to eliminate these issues. To determine which estimator provides an unbiased and consistent estimation of realized covariance between the EUROSTOXX50 index and gold for investigating the safe-haven property, six covariance estimators will be compared - realizied covariance [RCOV], realized bipower covariance [RBPCOV], realized threshold coavraiance [RTCOV], realized outlyings weighte covariance [ROWCOV], robust realized two times sacled coavraiance [RRTSCOV] and Hayashi-Yoshida realized covariance [HYRCOV].",Utilizing realized covariance estimators in finding safe haven evidence,"[67175, 74849]",138,"[151, 45, 126]",183,Market dynamics and implications for portfolio decisions,74,9,57,Modern Decision Making in Finance and Insurance,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S06 [building - 101],"['Practice of OR', 'Financial Modelling', 'Risk Analysis and Management']",TC-57
"Motivated by Basel and Willemain’s [2001] work on estimating optimal tour lengths in the Traveling Salesman Problem [TSP], we identify an intrinsic relationship between the distribution of the feasible solution space and the optimal solution value in combinatorial optimization problems. Our goal is to use regression modeling to estimate optimal solution values. We begin with a model that uses standard deviation as a predictor and we explain why this makes sense. We study TSP instance sets, both Euclidean and non-Euclidean. Next, we add the mean and the minimum in order to obtain very high quality results. We extend our experiments to include more than 10,000 vehicle routing problem [VRP] instances. Again, we obtain excellent results. Finally, we focus on the more difficult Split Delivery Vehicle Routing Problem [SDVRP] by combining the mean and standard deviation predictors with two topological features. In testing on 95 diverse benchmark instances from the literature, our regression model successfully estimates the optimal SDVRP solution value to within about 3%, on average.",Estimating Optimal Split Delivery Vehicle Routing Problem Solution Values,"[76063, 9412, 1185]",242,"[14, 53, 145]",185,Exact Algorithms and Formulations for Network Optimization Problems,64,9,29,Combinatorial Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,157 [building - 208],"['Combinatorial Optimization', 'Graphs and Networks', 'Vehicle Routing']",TC-29
"Several scholars have utilized hierarchical network Data Envelopment Analysis modeling techniques to assess the performance of complex structures. However, there has been limited consideration given to the integration of a peer-appraisal setting within a self-evaluation hierarchical context. This aims to enhance discriminatory power and mitigate the issue of unrealistic weighting scheme. To this end, our study extends the single-stage hierarchical additive self-evaluation model of Kao [2015], by integrating the well-established cross-efficiency method. An original combination of a maxmin secondary goal model and the Criteria Importance Through Inter-criteria Correlation [CRITIC] method is proposed, to expand the basic hierarchical self-evaluation model. The maxmin model addresses the issue of the non-unique optimal multipliers obtained from the self-evaluation model, ensuring a more realistic weight scheme. The CRITIC method, that tackles the aggregation problem by objectively determining weights of criteria, rewards the minority and is conducive to a fairer evaluation. Results indicate that the proposed approach is more likely to obtain a unique efficiency and ranking score for the units under consideration. This study entails a numerical experimentation aimed at evaluating the efficiency of a set of 20 universities while validating the applicability of our proposed approach. ",Embracing fairness within a cross-efficiency hierarchical network DEA system,"[73367, 76116, 76117, 76118]",945,"[24, 35, 25]",187,DEA methodological developments I,89,14,48,Data Envelopment Analysis and its Application,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,60 [building - 324],"['Data Envelopment Analysis', 'Efficiency Analysis', 'Decision Analysis']",WC-48
"We all have to wait. Sometimes longer, sometimes shorter. Thanks to queuing theory, we know why this is the case. However, under the usual assumption of a continuous-time domain, we do not really know how long we have to wait. In the case where the Markov property does not hold for the inter-arrival-time distribution, only estimations for the waiting times exist. They are rather easy to calculate as they are solely based on the mean and variance of the distributions, everything else is neglected. But these estimates can be arbitrarily inaccurate and offer no guarantee of quality. One solution is to apply the discrete-time domain. Here, the entire distribution is known, exact parameters can be calculated. However, this approach is much more complex. Thus, an intermediate solution is required which is less complex but more precise. Therefore, the effect of other standardized moments must be analyzed and taken into consideration. First corresponding analyses especially regarding inter-arrival times have recently been carried out and have provided initial indications. In this paper, these results are reflected on and taken as basis. In addition, experiments specifically with regards to service-time distributions are presented and the results are compared. Corresponding findings and possible next steps are presented in this paper serving as a basis for discussion for future work as this is important to calculate correct buffer sizes.",Standardized moments of waiting,[67254],800,"[121, 65, 130]",189,Analysis of Stochastic Models II,50,15,39,Stochastic Modelling,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,35 [building - 306],"['Queuing Systems', 'Logistics', 'Service Operations']",WD-39
"The optimal creation of timetables under various aspects is a topic in many research and implementation projects. Various procedures and mathematical approaches are used as well as modelling, target functions and perspectives.
For example, timetables can be optimized from a railway undertaking’s perspective in terms of resource and circulation planning, from an infrastructure operator's perspective focussing capacity perspective, or from a transport association's perspective from traffic and stream considerations.
On European main railways, it became practice for a separated, independent company - usually infrastructure manager - to be responsible for coordinating and synchronizing the various requirements and wishes of the various parties involved and bringing them together into a common timetable.
The timetable processes are generally subject to predetermined agendas, phases and conditions, e.g. early route registrations, international traffic, route prices and priorities, framework agreements, etc.
As a practical report, the article presents an approach to making this timetable planning process available as a usable IT service through a service interface and gives a brief insight into the scenarios, usages and working methods as well as results of such service components and organizes them into the practical process and the framework conditions of practice-relevant systems automated timetable planning.
",Service-oriented approaches to automated timetabling,[59602],300,"[142, 134, 145]",190,Automated Timetabling,36,2,58,Automated Timetabling,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S07 [building - 101],"['Timetabling', 'Software', 'Vehicle Routing']",MA-58
"Classical queuing models assume that each incoming user is accepted into the system if there is at least one free space in the queue and is served in a certain order. However, frequently in real systems, users may be impatient and leave the system after some waiting time if their service has not begun. Another kind of practically important and interesting queues is so-called ticket queues. In such queues, each arriving user receives a numbered ticket and observes the number of the user being served, which is broadcast on a display panel. In a ticket queue, no physical queue is formed. Thus, based on information about the difference between his own number and the displayed number, the user decides whether to balk or wait for service. In contrast to the usual queues with balking users, the balking user in ticket queues leaves the system physically, but his number remains in the queue. 
We consider a ticket queueing system with a finite buffer of capacity. Arriving users obtain a ticket for service but may balk the system with the probability depending on the queue of tickets ahead of customers. Tickets issued to users remain in the ticket buffer whether they participate or not. The server is unaware about the presence of users in the system and spends some time for their service even the respective user already left the system. In this paper, good reviews of relevant research are presented and the stationary distribution of the considered system is analyzed.
",Modelling and Optimization for Service System with Ticket Queues,[75691],161,"[121, 136, 130]",192,Stochastic Models in Service Operations I,50,7,39,Stochastic Modelling,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,35 [building - 306],"['Queuing Systems', 'Stochastic Optimization', 'Service Operations']",TA-39
"Airlines occasionally either delay or completely cancel scheduled flights. Often the explicit reason is inclement weather, however, other factors, including loadfactor, might influence the delay/cancel decision. This paper presents an empirical analysis of US airlines flight cancellations considering loadfactor and weather at origin and destination. The data spans May 2022 to June 2023.","Airline Cancellation, Loadfactor, and Weather",[15633],697,"[124, 7, 4]",193,Pricing and Capacity Management,11,8,59,Pricing and Revenue Management,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S08 [building - 101],"['Revenue Management and Pricing', 'Analytics and Data Science', 'Airline Applications']",TB-59
"In presentative settings, suppliers sell substitutable products through retailers to the market with uncertain demand and market competition is in quantity. Structure B comprises a monopolistic supplier and a monopolistic retailer. With respect to structure B, structure U includes competition between suppliers, structure D includes competition between retailers, while structure SC includes competition between two supply chains. A retailer has access to a demand signal useful for updating the forecast of market uncertainty. A supplier has no signal access but can offer a payment to a retailer, who decides whether to accept the payment and disclose signal to the supplier, termed information sharing. Suppliers and retailers hold diverse beliefs about market conditions. Knowing each other’s market beliefs influences firms’ ex-post operations policies and ex-ante profit perception. We demonstrate that holding diverse beliefs about market uncertainty by firms facilitates information sharing except in the presence of only retailer competition, in which case it deters information sharing. Exchanging market beliefs about market uncertainty among firms strengthens their incentive to engage in information sharing, which exerts mixed effects on their actual profits. By comparison, suppliers and retailers are more likely to benefit from exchanging market beliefs about market size, though it is inconsequential to information sharing.","Holding diverse market beliefs by firms - Information flow, profit performances, and channel structure","[64909, 36064]",576,"[10, 50]",194,Behavioural operations and games ,13,13,07,Behavioural OR,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1019 [building - 202],"['Behavioural OR', 'Game Theory']",WB-07
"In the last decades, the impact of natural and manmade disasters has been increasing and so does the research in the field of humanitarian logistics. But what are the challenges that humanitarian logistics face in practice and where is the research going? Is there an alignment between research and practice or a gap?

By reviewing papers on humanitarian operations, problems raised by practitioner conferences and webinars, and conducting a survey of the author community on the topic, we link these problems with the literature developed in our field. We also identify characteristics of papers that close the gap between practice and research. We then go deeper into some research projects that have affected practice helping it to become efficient, effective, and sustainable.","Humanitarian operations - How can research help them become more efficient, effective and sustainable?",[16736],32,"[58, 0]",204,Maria Besiou,62,15,01,Keynotes,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,Sportshallen [building - 101],['Humanitarian Applications'],WD-01
"This research investigates the complex landscape of health and life insurance selection in India through the lens of multicriteria decision-making [MCDM]. With the burgeoning population and diverse healthcare needs, the insurance sector plays a pivotal role in safeguarding individuals against unforeseen health crises and securing the financial future of their dependents. The study aims to develop a robust decision-making framework that integrates multiple criteria to guide individuals in making informed choices regarding health and life insurance policies. The research methodology involves a thorough analysis of various factors influencing insurance decisions. A comprehensive literature review is conducted to understand consumers' preferences and challenges in the Indian insurance market. The MCDM approach is employed to synthesize the collected data and derive a hierarchical model of decision criteria. The weights assigned to each bar are determined through pairwise comparisons, reflecting the relative importance of factors influencing insurance choices. The research aims to provide a decision support system to guide individuals in selecting insurance plans. The findings of this study are expected to contribute to the existing body of knowledge on insurance decision-making in India, offering valuable insights for both consumers and insurance providers. The research aims to enhance the overall efficiency and effectiveness of India's health and life insurance industry.",Multicriteria Decision Making for Health and Life Insurance Selection in India - A Comprehensive Analysis,[73246],655,"[6, 25, 77]",206,Decision Support in the Public Sector and Policy Making,45,13,45,Decision Support Systems,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,30 [building - 324],"['Analytic Hierarchy Process', 'Decision Analysis', 'Multi-Objective Decision Making']",WB-45
"What does well-being have to do with operations? Well-being encompasses a lot - Are we happy as individuals? Are groups treated fairly? Is society sustainable? Operations management has many impacts on well-being at each of these levels, some more obvious than others. This talk will offer a wide-ranging exploration of linkages between operations and well-being. It organizes “operations” into five broad areas - pace and productivity, predictability and probability, process and prevention, performance and payment, and pollution and protection. For each of those, it explores what makes individuals [un]happy, what is fair, and what is sustainable. Operations cannot solve all societal problems but the links between quality of operations and quality of life are more numerous and nuanced than we usually realize.","The operations of well-being - How operations interacts with happiness, equity, and sustainability",[8509],25,"[139, 0]",209,Charles Corbett [IFORS Distinguished Lecture],61,16,01,Plenaries,"Wednesday, 16:30-17:30",W,E,16:30,17:30,2024-07-03,Sportshallen [building - 101],['Sustainable Development'],WE-01
"The ACOPF problem is a notoriously difficult nonconvex optimization problem arising in the operation of power grids.  To date, all numerically effective relaxations -- that is to say, both accurate and practicable, at scale -- require second-order constraints.  However, solvers have considerable difficulty tackling such constraints.
In this talk, we will describe an outer approximation linear cut scheme that yields relaxations which are both tight and can be solved quickly and robustly.  Additionally, we will describe a warm-start scheme that further improves on performance.  Finally, we will present a theoretical justification for, both, the success of the SOCP relaxations and their outer approximations.
Joint work with Matías Villagra [PhD student, Columbia University].",Solving large-scale ACOPF problems,"[80055, 58963]",176,"[113, 93, 5]",211,Topics in Mixed Integer Programming and Nonconvex Optimization,86,7,04,MINLP,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,1001 [building - 202],"['Programming, Nonlinear', 'OR in Energy', 'Algorithms']",TA-04
"In this presentation we consider dividing a quay of container terminal into berth segments so that quality of service for future ship arrivals is as good as possible. This problem will be referred to as a stochastic quay partitioning problem [SQPP]. SQPP is defined by a ship traffic model [STM], arrival intensity, quay length and a set of admissible berth lengths. Alternative solutions are evaluated on various arrival scenarios generated for certain arrival intensity from a stochastic traffic model [the STM]. Evaluation of an SQPP solution on one scenario is a problem of scheduling the arriving vessels on the berths, which is a classic berth allocation problem [BAP]. In SQPP the sizes of BAP instances which must be solved by far exceed capabilities of the methods presented in the existing literature. Therefore, tailored portfolios of algorithms capable of solving very large BAP instances under limited runtime are used. Features of SQPP solutions are studied experimentally. We demonstrate, that partitioning a quay into equal length berths is not always the best approach. A set of algorithms to partition a quay is proposed and evaluated.",Stochastic Quay Partitioning Problem,"[806, 50204, 19524, 66934, 61689]",227,"[14, 129, 70]",213,Combinatorial Optimization in Scheduling,64,13,26,Combinatorial Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,012 [building - 208],"['Combinatorial Optimization', 'Scheduling', 'Maritime applications']",WB-26
"The efficient allocation of limited resources along with profit optimization play a crucial role in large enterprises and organizations. Given the continuous fluctuations in data and the lack of historical data, managers seek an optimal dynamic strategy to maximize the profits and at the same time keep organizational efficiency at satisfying levels. In some business contexts a central decision-making unit might govern management decisions over multiple subordinate decision-making units and in this study, we investigate efficiency in a stochastic framework using a combined bilevel - DEA approach. We define discrete scenarios with some occurrence probabilities to assign values to uncertain parameters thus enabling decision readjustments upon the realization of each scenario, contributing to an enhanced overall strategy. The proposed approach involves an upper-level problem that handles organizational effectiveness [i.e. profit] and a lower-level problem that handles subDMUs' efficiency. Input and output bounds are also used in the same sense as assurance regions along with lower bounds on subDMUs' efficiencies. The presentation concludes with applications in the banking or healthcare sector.


The research project was supported by the Hellenic Foundation for Research and Innovation [H.F.R.I] under the 2nd Call for H.F.R.I. Research Projects to support Faculty Members & Researchers [Project Number - 3154].
",A hybrid bi-level DEA model under uncertainty conditions ,"[76099, 381, 68715, 69004, 19351, 76100, 664]",937,"[24, 84]",216,DEA and stochastic models,89,4,48,Data Envelopment Analysis and its Application,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,60 [building - 324],"['Data Envelopment Analysis', 'Optimization Modeling']",MC-48
"Firing shift is the shifting of artillery fire from one target
[registration] to another by the application of corrections
determined from the adjustment of the initial firing data
for the first target to the second [application]. In this paper,
a shift method based on simultaneous firing at two
different registration targets rather than a single one is
introduced. A method is presented for using the observed
data to explicitly solve for four major environmental parameters
that affect ballistic trajectories and subsequently
update them for the application mission. The distribution
of the remaining errors is determined and compared with
that of other correcting algorithms. Numerical examples
show the potential of the proposed method.",Artillery Firing Shift with Two Registration Targets,[50374],980,"[75, 0]",218,"Military, Defense, and International Security II",65,7,20,"Military, Defense, and International Security","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,45 [building - 116],['Military Operations Research'],TA-20
"While policymakers regularly add capacity to reduce congestion in public services like roads and hospitals, empirical studies show that adding resources does not always reduce congestion and delays. To clarify this phenomenon, we propose an explanatory model of rational delay-risk sensitive users who are less inclined to access services when delays are more variable. Using a rational queueing approach, we show that there are largely two opposite effects of increased resource capacity.  The public interest benefits through improved throughput and social welfare, while individual users can experience degraded service in terms of expected delays. We offer as an explanation that adding resource capacity reduces delay variability, so that delay-risk sensitive users will more readily join. 
In addition, we show that resource capacity not only influences the volume of demand but also its nature - increased service capacity attracts users who are less sensitive to expected delays and more sensitive to delay variability, which
implies that large-capacity systems predominantly attract users who prefer reliability over efficiency.",Why rational queueing of delay risk-sensitive users undermines the benefits of added resources,"[54027, 76101, 67228]",161,"[121, 135]",219,Stochastic Models in Service Operations I,50,7,39,Stochastic Modelling,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,35 [building - 306],"['Queuing Systems', 'Stochastic Models']",TA-39
"Many uncontrollable factors will affect company operations. In addition, the collected data is often snapshot observations, and if collected earlier or later, the values may differ. A company's inputs and outputs over time are thus stochastic variables. Several approaches have been proposed to deal with stochastic observations in efficiency measurement, including stochastic frontier analysis and chance-constrained programming. Conventionally, each DMU's input and output variables are assumed to be independent. This assumption is unrealistic because DEA requires isotonicity, which implies that the outputs correlate with the inputs. Ignoring this correlation will produce misleading results. In a network system, the merit of efficiency decomposition is that a relationship between the system and division efficiencies is obtained, based on which the division affecting the entire system's performance the most can be identified. When the observations are stochastic variables with multivariate normal distributions, they can be transformed into a variable with a standard normal distribution. In addition, only looking at a single time efficiency may lose the general picture of the overall performance for a multi-period. This paper aims to develop a model to measure the efficiency of DMUs with several divisions performing specific operations over a time period. This model is able to handle multi-period stochastic data and decompose the system's efficiency into division efficiencies.",Stochastic multi-period and multi-stage efficiency measures,[18930],937,"[24, 135, 25]",221,DEA and stochastic models,89,4,48,Data Envelopment Analysis and its Application,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,60 [building - 324],"['Data Envelopment Analysis', 'Stochastic Models', 'Decision Analysis']",MC-48
"The surprising fact that EURO24, the 33rd EURO conference, is the first hosted by Denmark, provides the opportunity to give a personal reflection on the Danish contribution to OR history.  There will be an emphasis on four individuals who have played a significant role in the development of OR theory and professional infrastructure.  Three of these individuals will be revealed at the lecture.  For now, it is sufficient to say that the lecture is in honour of my dear friend Jakob Krarup, who passed away last year.  He was greatly looking forward to the conference being held in his own city.  

From this description it will be obvious that this will not be a standard keynote address.  However, there may be one, or three, equations included.  That is a clue to one of the other three individuals whose contribution will be noted.
","To be OR - key Danish contributors to the history of OR, IFORS and EURO",[5870],40,"[88, 106]",222,Graham Rand,62,2,01,Keynotes,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,Sportshallen [building - 101],"['OR History', 'Profession of OR']",MA-01
"This paper proposes a new merged LSTM-MLP deep learning approach for option pricing. The model has the advantage of combining time series data with static data. On the one hand, it learns from historical asset returns, as well as considers the option characteristics at the pricing date. This configuration does not need an estimation for an explicit volatility measure, improving upon existing methods. We test it on S&P 500 European call options from 2015 to 2022. The model excels in accuracy and risk-adjusted returns, showcasing its usefulness in real-life applications.",Merged LSTM-MLP for option pricing,"[49143, 76103, 76149, 76129, 42303, 76181]",690,"[45, 8]",224,Risk management and valuation of financial contracts,74,8,57,Modern Decision Making in Finance and Insurance,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S06 [building - 101],"['Financial Modelling', 'Artificial Intelligence']",TB-57
"Leveraging large-scale data sources to employ personalized dynamic pricing has been a prominent practice in the retail industry. However, the outcome of this pricing strategy could be mixed. Therefore, this study aims to empirically investigate the impact of personalized dynamic pricing on store performance and individual consumer behavior under different conditions. We obtain unique access to a large data set from a leading on-demand delivery platform that has launched a personalized dynamic pricing strategy through price discounts in some of its grocery stores. Employing a quasi-experimental setting, we utilize the difference-in-differences method to estimate the causal impact of personalized dynamic pricing. First, we conduct store-level analyses and find that personalized dynamic pricing decreases store revenue. Then, to understand the underlying mechanism, we proceed with individual-level analyses and find that personalized dynamic pricing increases the individual transaction amount and frequency while increasing consumer churn. Finally, we investigate the moderating effects of an essential feature of personalized dynamic pricing - the price fluctuation over time. We find that a bit more fluctuation can mitigate the negative consequences of personalized dynamic pricing by reducing consumer churn and increasing consumer transaction activities. However, too much price fluctuation intensifies the negative consequences of personalized dynamic pricing.",Personalized dynamic pricing - pearls or perils?,"[76082, 65130, 62875]",185,"[32, 130, 7]",225,Experimental economics and game theory 1,73,13,40,Experimental economics and game theory,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,96 [building - 306],"['E-Commerce', 'Service Operations', 'Analytics and Data Science']",WB-40
"Coordinated decision making in manufacturing sector is crucial for business development. In this paper, a multi-objective robotic assembly line balancing problem is considered, which has the following features:1] it involves a two-stage decision making process where the first stage is to design the robotic assembly line in terms of the task and robot assignment, and the second stage is to determine how much to produce for each product model; 2] the demand for each product is uncertain; 3] two objectives are considered including the number of workstations and carbon emissions. A reinforcement learning [Q-learning] method is devised to solve such problem. Computational studies are performed to validate the performance of the proposed algorithm.",Robotic Assembly Line Balancing Problem Considering Uncertain Demand by Q-learning method ,[76107],222,"[14, 105, 117]",227,Applications to Logistics and Transportation,64,7,26,Combinatorial Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,012 [building - 208],"['Combinatorial Optimization', 'Production and Inventory Systems', 'Programming, Stochastic']",TA-26
"To decarbonize the aviation sector, the supply of sustainable aviation fuels [SAF] needs to be substantially increased. However, the production of SAF is associated with substantial risks, like demand risk, technological maturity risk, production cost risk or regulatory risk. To share these risks in SAF production, we propose risk-sharing collaborations between SAF suppliers and buyers. This approach fills a gap in existing research, which mostly focuses on technological aspects of SAF like its scalability or regulatory incentives. We create an optimization model that stochastically models SAF market prices and production costs, includes contract parameters like reservation fees or contract duration and models the heterogeneity in the SAF producer market through parameters like production efficiency, feedstock prices, penalties or geography. Firstly, a centralized scenario in which the whole supply chain’s profit is maximized is evaluated. Secondly, a decentralized scenario using a Stackelberg game is evaluated. The model determines the collaboration model one risk-averse buyer and multiple suppliers would agree on to maximize their profits. The results indicate the best suitable risk-sharing collaboration model for both SAF buyers and suppliers. ",Risk-sharing collaboration models for the production of Sustainable Aviation Fuels - an optimization model,"[75009, 35962]",545,"[136, 50, 100]",228,Clean Energy Supply Chains,19,4,24,Sustainable Supply Chains,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,83 [building - 116],"['Stochastic Optimization', 'Game Theory', 'OR in Sustainability']",MC-24
"We study convex optimization problems with a Quasi-Self-Concordant smooth components. Our problem class naturally interpolates between classic Self-Concordant functions and functions with Lipschitz continuous Hessian. Previously, the best complexity bounds for this problem class were associated with trust-region schemes and implementations of a ball-minimization oracle. In this talk, we show that for minimizing Quasi-Self-Concordant functions we can use instead the basic Newton Method with Gradient Regularization. For unconstrained minimization, it only involves a simple matrix inversion operation [solving a linear system] at each step. We prove a fast global linear rate for this algorithm, matching the complexity bound of the trust-region scheme, while our method remains especially simple to implement. Then, we introduce the Dual Newton Method, and based on it, develop the corresponding Accelerated Newton Scheme for this problem class, which further improves the complexity factor of the basic method. As a direct consequence of our results, we establish fast global linear rates of simple variants of the Newton Method applied to several practical problems, including Logistic Regression, Soft Maximum, and Matrix Scaling, without requiring additional assumptions on strong or uniform convexity for the target objective.",Minimizing Quasi-Self-Concordant Functions by Gradient Regularization of Newton Method,[63248],258,"[5, 21, 113]",231,Advances in Complexity of Convex and Nonconvex Problems,84,4,32,Advances in large scale nonlinear optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,41 [building - 303A],"['Algorithms', 'Convex Optimization', 'Programming, Nonlinear']",MC-32
"Game-based approaches are increasingly used in participatory decision-making. Yet, rigorous evaluation of their effectiveness is often lacking. Typical challenges include small sample sizes, no control treatment, and no control for the potentially confounding factors. Sometimes, claims about potential benefits of the game are unaligned with the evaluation. I will present a framework for promoting a more robust evaluation of game-based approaches, based on an illustrative case.
The illustrative case focuses on evaluating the effectiveness of playing a card game in a workshop designed to generate decision objectives for sustainable wastewater management. Specifically designed for non- experts, such as municipality council members, the card game targets the diverging phase of generating objectives and incentivize participants “collect” diverse criteria. 
Guided by the call for complementary data types in BOR and principles from the educational science, we concurrently developed a comprehensive evaluation process and the card game-based workshop. Our pre-test of the evaluation process with 10 students highlighted the complementary nature of quantitative and qualitative data types. Yet, challenges such as sample size and confounding factors remained. 
We suggest collaborative efforts among researchers to address these issues. The framework and insights from our illustrative case contribute to developing robust approaches to evaluate participatory interventions.",Playing a card game in a workshop to generate decision objectives - pretesting the evaluation,[50821],568,"[10, 55, 149]",232,Behavior in group decision-making ,13,9,11,Behavioural OR,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,12 [building - 116],"['Behavioural OR', 'Group Decision Making and Negotiation', 'Problem Structuring']",TC-11
"With the rapid evolution of technology and the constant changes in the business environment, many enterprises assign different tasks as projects. Therefore, project management performance is critical to the success of the enterprise. There are usually multiple projects executed concurrently in the organizations. Project portfolio management significantly influences project management performance due to the limited available resources and the interdependency between projects. Project portfolio selection is one of the most critical issues in the project portfolio management process. Allocating available resources to an optimal set of projects will result in the most benefits. This study considers a project portfolio selection problem that discusses selecting an optimal subset of projects from a set of candidate projects for maximizing benefits with an effective and efficient use of limited resources. The original mathematical model of the project portfolio selection problem is a nonlinear integer programming problem that is hard to solve for finding a globally optimal solution. This study utilizes a novel linearization approach that efficiently transforms the project portfolio selection problem with cross-product terms as a mixed-integer linear program solvable for finding a globally optimal solution. Compared with current methods, the proposed method uses much fewer continuous variables to linearly reformulate the problem. Numerical examples are presented to demonstrate that t",An efficient linear reformulation for project portfolio optimization problems,"[76112, 50970]",959,"[118, 113, 72]",235,"Projects, risk and law",35,8,60,Project Management and Scheduling,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S09 [building - 101],"['Project Management and Scheduling', 'Programming, Nonlinear', 'Mathematical Programming']",TB-60
"This is a personal sharing of lessons learned from someone who was deeply involved in Operations Research – in a corporate setting, as a national society organizer, an international OR group leader, a consultant in supply chain and a pro-bono government consultant. There were many lessons learned along the way that the speaker hopes will benefit others.",Lessons learned on the OR way,[30804],19,"[88, 151, 106]",237,Moments in the history of OR  1,27,13,20,Moments in the history of OR,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,45 [building - 116],"['OR History', 'Practice of OR', 'Profession of OR']",WB-20
"In recent years, sustainable development has become a core issue in enterprise management and value creation. In order to ensure the performance of the implementation of the sustainable development goals, the enterprise must evaluate sustainable performance appropriately. Due to the characteristics of high uncertainty and high complexity, it is an important issue to apply fuzzy sets theory with multiple experts to construct a sustainable performance evaluation model. It can be regarded as a group multiple-criteria decision-making problem. The opinions of experts always are fuzziness and uncertainty, it is reasonable for experts to use the linguistic variables to express their subjective opinions in the evaluation process.
In addition, the TOPSIS [Technique for Order Preference by Similarity to an Ideal Solution] is one of famous methods to deal with the decision-making problems. Therefore, this paper will combine entropy method with TOPSIS based on linguistic variables to construct a systematic evaluation model of sustainable performance. And then, this paper will propose a linguistic entropy TOPSIS algorithm [LE-TOPSIS], which will use entropy method to calculate the weights of criteria and to determine the ranking order of all enterprises by using TOPSIS. Finally, an example will present in accordance with the LE-TOPSIS algorithm to illustrate the effectiveness of the proposed method.
",Evaluation of Sustainable Performance Based on Linguistic Entropy and TOPSIS Methods,"[46295, 76113]",308,"[25, 49, 55]",238,MCDA applications,44,8,44,Multiple Criteria Decision Analysis,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,20 [building - 324],"['Decision Analysis', 'Fuzzy Sets and Systems', 'Group Decision Making and Negotiation']",TB-44
"In this paper we design cost allocation rules for inventory games with private information. First, we design incentive-compatible cost allocation rules for inventory games with private information via Vickrey-Clarke-Groves [VCG] rules. Then, we propose incentive-compatible and approximate budget-balanced cost allocations via polynomial approximations such as the Chebyshev
approximation and the Taylor approximation. In addition, we propose an incentive-compatible cost allocation with individual rationality. Finally, we conduct numerical experiments to compare the performance of the proposed cost allocations.",Incentive-Compatible Cost Allocations for Inventory Games with Private Information,[76115],896,"[50, 61, 138]",239,"Game Theory, Solutions and Structures X",88,13,36,"Game Theory, Solutions and Structures","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,32 [building - 306],"['Game Theory', 'Inventory', 'Supply Chain Management']",WB-36
"This study aims to implement the Combined Compromise Solution [CoCoSo] method to assist decision-makers in selecting a device for Continuous Glucose Monitoring [CGM] that enables effective management of people with diabetes. The CoCoso method is a Multi-Criteria Decision Analysis [MCDA] approach for selecting the best alternative among the available ones, considering the relative importance of criteria. In the current study, seven different types of CGM devices are examined, considering criteria such as accuracy, cost, number of compatible platforms, number of additional features, and setup time. The Preference Selection Index [PSI] method was used to determine the relative importance of these criteria. Sensitivity tests were conducted by adjusting the value of the parameter λ between 0.25 and 0.75. The results showed significant consistency, as evidenced by the Spearman correlation coefficients, which remained above 0.96. This high level of correlation indicates that, despite variations in the value of λ, the rankings of alternatives remained stable, reinforcing the reliability and validity of the model used in the research. The study concluded that the FreeStyle Libre 2 device is the highest-ranked alternative.",Selection of Glucose Monitoring Devices Using the CoCoSo Method - An Innovative Approach to Diabetes Management,"[76120, 50392, 201]",979,"[77, 25]",241,Medical decision making,3,5,17,OR in Health Services [ORAHS],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,40 [building - 116],"['Multi-Objective Decision Making', 'Decision Analysis']",MD-17
"This paper proposes a novel graph pointer network [GPN] model to learn branching rules for branch-and-bound [B&B] algorithms in combinatorial optimization. B&B is a widely used exact method that can solve various NP-hard problems, such as set covering, facility location, and maximum independent set. However, B&B relies on manually designed heuristics to select the branching variables, which are often suboptimal and inefficient. The GPN model aims to automatically learn these heuristics from data, by imitating the expert strong branching rule. The GPN model consists of two components - a graph neural network that encodes the graph structure of the problem state, and a pointer mechanism that outputs the branching probabilities based on the global and historical features. The model is trained by minimizing the Kullback-Leibler divergence between the predicted and the expert distributions. Experiments on three benchmark problems show that the GPN model outperforms both the classic and the state-of-the-art machine learning-based B&B methods in terms of solving speed and search tree size. The GPN model also demonstrates remarkable generalization abilities, adapting to unseen and larger instances. This paper presents an innovative approach to enhancing the performance of B&B algorithms using deep learning techniques.",Learning to Branch in Combinatorial Optimization with Graph Pointer Networks,"[76121, 58086, 76174]",731,"[11, 66, 5]",242,	[Deep] Reinforcement Learning for Combinatorial Optimization 2,14,5,03,Data Science Meets Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,1005 [building - 202],"['Branch and Cut', 'Machine Learning', 'Algorithms']",MD-03
"Flight tests play a critical role in the research and development of new aircraft as they help verify the airworthiness and capabilities and expose design and manufacturing defects. During the test course, a large number of test tasks need to be scheduled appropriately so that the flight tests can be completed with minimum cost and high efficiency. Therefore, there is a strong need for developing an efficient method that can generate high-quality test schedules. In this paper, we study the flight test scheduling problem to minimize the number of required test flights, thereby decreasing the cost and time required during the entire test course. We establish a mixed-integer programming model to formally describe the problem, propose several computationally efficient lower bounds to help verify the quality of obtained solutions, and develop a large neighborhood search algorithm for generating a high-quality solution of the flight test scheduling problem effectively. Comprehensive computational experiments are performed to demonstrate the efficiency of our proposed algorithm. We report some general managerial insights based on the obtained computational results.",Flight test scheduling - An integer programming model and a large neighborhood search algorithm,"[76119, 76125, 10607]",347,"[129, 109, 74]",244,Advanced heuristics for machine scheduling,35,9,60,Project Management and Scheduling,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S09 [building - 101],"['Scheduling', 'Programming, Integer', 'Metaheuristics']",TC-60
"Shipping alliance has become a prevalent cooperation form among major container shipping companies in recent decades. By forming an alliance, shipping companies can better satisfy the shipping demand and improve capacity utilization and profitability. At the same time, the shipping industry has become an important source of global greenhouse gas emissions. Faced with the emerging worldwide maritime carbon emission trading system, the shipping alliance needs to redesign its fleet to control carbon emissions and reduce its negative effects on the profit of the alliance members. This paper considers the fleet deployment and slot exchange problem in shipping alliance with the consideration of carbon emission trading system. A mixed integer nonlinear programming model is first proposed to simultaneously optimize the fleet deployment, the ship sailing speed and schedule for each shipping service, the slot allocation and exchange for each alliance member, and the container routing, with the objective of maximizing the alliance total profit [freight revenue minus the ship operating cost and emission trading cost] and minimize the variation of the profit rate among alliance members. A tailored spatial branch-and-bound algorithm is proposed subsequently considering the structure of the problem to obtain the global optimal solution. Numerical experiments also demonstrate the effectiveness of the model in emission reduction and profit improvement for a shipping alliance.",Fleet deployment and slot exchange in liner shipping alliance with carbon emission trading system,"[64273, 76126, 75745]",173,"[52, 70, 100]",246,Energy Management in Ports and Shipping I,52,8,62,OR in Port Operations,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S12 [building - 101],"['Global Optimization', 'Maritime applications', 'OR in Sustainability']",TB-62
"Understanding the mobility of ground freight transport is critical in urban planning and for developing public policies. Literature shows that most of the previous studies on this topic rely on surveys and limited data, hindering the accuracy of the analysis. To cope with this drawback, in this paper, we present an innovative methodology for characterizing last-mile freight transport that uses a novel data source - mobile phone data, which offers the advantage of a broader coverage compared to data sources used in previous contributions. To the best of our knowledge, no prior articles have employed mobile phone data sources to study freight transport. To bridge this gap, we propose a methodology that calibrates supervised machine learning models to determine which cell phones correspond to truck drivers. To do so, we construct input variables that seek to track the daily movement patterns of mobile phones, including traveled distances, interactions with highway networks, and land use variables. We test our approach by conducting a case study in Santiago, Chile, for which we analyze mobility patterns and logistic indicators disaggregated by day, hour, and zoning. Our results show high activity in Santiago's outskirts, especially in the northwest and southwest areas, with many interactions due to logistic centers and industrial zones. Similarly, central and eastern Santiago also show high stop density, reflecting intense commercial activity.","Characterizing last-mile freight transport using mobile phone data - The case of Santiago, Chile",[68604],503,"[143, 66, 65]",247,Last mile delivery modeling,6,2,56,Transportation,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S04 [building - 101],"['Transportation', 'Machine Learning', 'Logistics']",MA-56
"This study addresses how the government can ensure the cost-effective
procurement of vaccines from private manufacturers. We combine optimization and game theoretic techniques to address cost-effective immunization in the United States. Our optimization model from the government’s perspective minimizes negotiated government costs, while ensuring adequate national vaccination levels, linking dynamics in public and private sectors and incorporating competitive manufacturer behavior. The optimization
model embeds an extant game theoretic price model to capture competitive interactions among manufacturers in the private sector. The model is applied to two different case studies of the US pediatric vaccine market and the COVID-19 vaccine market. The results indicate that markets are at lower risk when high capacity manufacturers have moderate target profits, especially in cases of high demand and asymmetry. Our model can help restabilize a market that experiences a vaccine shortage We also underline scenarios in which the government may be able to prevent monopolies through financial incentives to manufacturers. The results support a paradigm shift from annual contracts to ongoing negotiations, which would enable the government to exercise control over high-risk markets. Our study demonstrates an analytical approach for managerial government officials to influence vaccine prices via the procurement of public-sector goods.
",Balancing Cost and Profit in the Public Sector Vaccine Market,[56949],611,"[56, 50]",248,Machine learning and game theory in healthcare,3,2,15,OR in Health Services [ORAHS],"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,18 [building - 116],"['Health Care', 'Game Theory']",MA-15
"The p-center problem is one of the classical location analysis problems. It involves locating one or more facilities [in the plane or on the network] among multiple demand points while minimizing the largest distance between a facility and a demand point. Recent papers present novel solution techniques that allow for solving large-scale planar p-center problems. However, p-center models involving obnoxious facilities have yet to be investigated. Obnoxious facilities emit or inflict nuisance, such as noise or pollution, on the demand points. This nuisance propagates by air, so it can be reduced by locating the facilities further away from the demand points. However, this new property makes p-center models nonconvex and more challenging to solve. 
We formulate the planar obnoxious p-center problem as a general non-linear program and reformulate it as a nonconvex quadratically constrained optimization problem. We present two solution techniques - an optimal [within tolerance] one involving the linearization of non-linear constraints and one involving a multi-start approach with a local solver [without the optimality guarantee]. The effectiveness and efficiency of our techniques are demonstrated on both generated and commonly used instances of the p-center problem. In addition, we investigate the impact of the obnoxious property on the planar p-center problem difficulty and solution quality.
",The Planar Obnoxious Facilities p-Center Problem,"[70185, 70267]",766,"[64, 19, 113]",249,Nonlinear Location Optimization,29,8,61,Locational Analysis,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S10 [building - 101],"['Location', 'Continuous Optimization', 'Programming, Nonlinear']",TB-61
"Sparse optimization is a popular research topic in applied mathematics and optimization, and nonconvex sparse regularization problems have been extensively studied to ameliorate the statistical bias and enjoy robust sparsity promotion capability in vast applications. However, puzzled by the nonconvex and nonsmooth structure in nonconvex regularization problems, the convergence theory of their optimization algorithms is still far from completion - only the convergence to a stationary point was established in the literature, while there is still no theoretical evidence to guarantee the convergence to a global minimum or a true sparse solution.
This talk aims to find an approximate global solution or true sparse solution of an under-determined linear system. We will propose two types of iterative thresholding algorithms with the continuation technique and the truncation technique respectively. We introduce a notion of limited shrinkage thresholding operator and apply it, together with the restricted isometry property, to show that the proposed algorithms converge to an approximate global solution or true sparse solution within a tolerance relevant to the noise level and the limited shrinkage magnitude. Applying the obtained results to nonconvex regularization problems with SCAD, MCP and Lp penalty and utilizing the recovery bound theory, we establish the convergence of their proximal gradient algorithms to an approximate global solution of nonconvex regularization problems. ",On Convergence of Iterative Thresholding Algorithms to Global Solution for Nonconvex Sparse Optimization,[41560],249,"[72, 52, 81]",252,Lower-order composite optimization problems,70,10,41,Nonsmooth Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,97 [building - 306],"['Mathematical Programming', 'Global Optimization', 'Non-smooth Optimization']",TD-41
"The use of optimization techniques for the optimal design of roads and railways has increased in recent years. The environmental impact of a layout is usually given in terms of the land use where it runs [avoiding some ecologically protected areas], without considering air pollution [in these or other sensitive areas] due to vehicular traffic on the road. 
This work addresses this issue and proposes an automatic method for obtaining a specific corridor [optimal in terms of air pollution], where the economically optimized road must be designed in a later stage. 
Combining a 1D traffic simulation model with a 2D air pollution model and using classical techniques for optimal control of partial differential equations, the problem is formulated and solved in the framework of Mixed Integer Nonlinear Programming. 
The usefulness of this approach is shown in a real case study posed in a region that suffers from serious episodes of environmental pollution, the Guadalajara Metropolitan Area [Mexico].",Design of an ecological corridor surrounding restricted urban areas - An optimal control approach,"[40020, 76133, 40021, 40005]",846,"[82, 139, 40]",258,Heterogeneity in optimal control problems,90,7,33,Optimal Control Theory and Applications,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,42 [building - 303A],"['Optimal Control', 'Sustainable Development', 'Environmental Management']",TA-33
"We propose a novel mathematical methodology to accomplish the optimal performance of a raceway - an open-channel pond where circulating wastewater is used for the cultivation of algae that will be employed as source in bioenergy production. 
Algal productivity maximization is addressed here by means of optimal control techniques for partial differential equations. 
So, we first introduce a rigorously detailed mathematical formulation of the control problem, based on a coupling of the hydrodynamical equations with the system for algal concentration. Then, we suggest a full numerical algorithm for its computational resolution, and finally we show some preliminary results related to the numerical optimization of the problem.",Maximizing algal productivity in raceway ponds for bioenergy generation - A mathematical perspective,"[40021, 40020, 46887, 40005]",373,"[82, 37, 139]",259,Optimal control in environmental economics,90,8,33,Optimal Control Theory and Applications,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,42 [building - 303A],"['Optimal Control', 'Energy Policy and Planning', 'Sustainable Development']",TB-33
"In this work, we develop a zeroth-order [derivative-free] implementation of the Cubically regularized Newton method for solving general non-convex optimization problems. Specifically, we consider the class of unconstrained problems whose the objective function is twice continuously differentiable with Lipschitz continuous Hessians. The proposed method employs finite difference approximations of gradient vectors and Hessian matrices. We use a special adaptive search procedure in our algorithms, which simultaneously fits both the regularization parameters of the models and the stepsizes of the finite difference approximations. Thanks to this procedure our scheme do not require the knowlege of the actual Lipschitz constants. Additionally, we equip our algorithm with the lazy Hessian update, meaning that the method reuse a previously computed Hessian approximation matrix for several iterations. In terms of worst-case complexity, we prove that the proposed method is able to find an $\epsilon$-approximate stationary point using at most $\mathcal{O}[ n^{3/2} \epsilon^{-3/2} ]$ function evaluations, where $n$ is the dimension of the problem. This complexity bound improve existing bounds in terms of the joint dependence on $n$ and $\epsilon$ for derivative-free methods applied to the minimization of functions with Lipschitz continuous Hessians. Illustrative numerical results confirm our theoretical findings.",Zeroth-order implementation of the regularized Newton method with lazy approximated Hessians,[72508],337,"[19, 16]",260,Algorithms for machine learning and inverse problems - zeroth-order optimisation,84,9,32,Advances in large scale nonlinear optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,41 [building - 303A],"['Continuous Optimization', 'Complexity and Approximation']",TC-32
"Effective capacity allocation methods play crucial roles in Revenue Management. Yet, current methods for determining optimal capacity controls under uncertainties, such as stochastic optimization, often assume a known distribution for unknown parameters. This assumption may degrade the model’s performance when faced with unexpected data patterns. This paper explores a novel approach through robust optimization formulations for addressing stochastic resource allocation problems. We introduce heuristics based on these robust formulations to derive actionable results. Through extensive simulations focused on seat allocation problems within the revenue management domain, our proposed formulations demonstrate significantly improved worst-case performances. Notably, even under favorable scenarios, our solutions remain comparable to existing methods in the revenue management literature.",Robust optimization models for single-leg and network airline revenue management,"[697, 76134, 76135, 38335]",222,"[4, 110, 127]",261,Applications to Logistics and Transportation,64,7,26,Combinatorial Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,012 [building - 208],"['Airline Applications', 'Programming, Linear', 'Robust Optimization']",TA-26
"We consider response to environmental taxation by competing heterogenous firms producing a commodity good with a polluting by-product [e.g., CO2]. Firms are heterogeneous with respect to production efficiency and have a choice of finite types of pollution control technologies. Firms are faced with fixed plus linear production costs. Cournot competition is assumed. The key research question is the extent to which environmental taxes can be used to induce green equilibrium ' i.e., an equilibrium where all firms select the most environmentally efficient technology.  We show that, in general, neither existence nor uniqueness of equilibria are guaranteed and characterize market demand functions under which existence does hold. However, even in this case, the equilibrium may include firms using less environmentally efficient technology choice at any level of environmental taxes.  Thus, environmental taxation alone cannot reliably induce green technology choice. However, when coupled with fixed-cost subsidies, both existence and uniqueness of the green equilibrium are shown to hold. ",Technological and Market Response to Environmental Taxation by a Competitive Industry,"[23272, 20952]",560,"[139, 40, 94]",262,Game Theory in Sustainable Supply Chains,19,14,24,Sustainable Supply Chains,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,83 [building - 116],"['Sustainable Development', 'Environmental Management', 'OR in Environment and Climate change']",WC-24
"This presentation highlights the interplay between optimization and machine learning, examining two pivotal examples.
First, we will explore how mathematical optimization can significantly enhance the interpretability of machine learning models, with a focus on classification problems. Interpretability is a crucial requirement for the practical deployment of machine learning models across diverse fields. Optimal decision trees are fundamental tools for building robust and interpretable classification models. However, the challenge lies in their inability to ensure optimality for large datasets. Recently some efficient approaches have been introduced for building optimal decision trees that require all the features to be binary. We will describe an efficient optimization-based approach for deriving a supervised discretization of the original dataset. Thanks to the use of optimization, the proposed technique allows to control the importance of the features extracted, and hence the granularity of the discretization. This novel technique not only facilitates the training of a robust and interpretable optimal decision tree but also allows to address the complexities associated with larger datasets.
Finally, we'll transition to a pragmatic application in process engineering, showcasing the successful integration of optimization and machine learning to solve an important real-world problem.",Optimizing machine learning - Enhancing interpretability and performance through mathematical optimization,[10022],39,"[66, 84]",265,Veronica Piccialli,62,10,01,Keynotes,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,Sportshallen [building - 101],"['Machine Learning', 'Optimization Modeling']",TD-01
"Double Roman domination [DRD] is an optimization problem posed on an undirected graph. It is formally described as assigning weights to graph vertices according to certain rules. Informally, the problem can be interpreted as allocating some service providers [e.g., military formations] to chosen locations within a geographical area. Thereby the whole service [i.e., response to enemy attacks] should be established in a least expensive way, while still assuring a required “double Roman” level of robustness and reliability. The standard version of DRD treated in the literature identifies the total expense of service with the total number of needed providers. 

In this work a new “minimum-cost” version of DRD is considered, which is more general and more realistic than the standard version. It assumes that the actual expenses of providing service can differ from one location to another. Thus, formally speaking, each vertex in our graph is given a cost that is interpreted as the cost of keeping one service provider at the corresponding location.

First, it is noted that the considered minimum-cost DRD problem is NP-hard, not only for general graphs but also for many special graph classes. Next, a dynamic programming algorithm is constructed, which shows that the problem can still be solved in linear time if the involved graph is a tree. Finally, as a solution for general graphs, a heuristic is proposed based on greedy approach and local search. ",On a minimum-cost version of double Roman domination,"[2474, 56684]",875,"[53, 14, 5]",267,Optimization issues on graphs I [Contributed],64,14,29,Combinatorial Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,157 [building - 208],"['Graphs and Networks', 'Combinatorial Optimization', 'Algorithms']",WC-29
"Many practical multi-criteria decision-making problems are formulated using Multi-Objective Mixed Integer Programming [MOMIP]. Among many methods of multi-criteria decision-making, interactive ones are considered the most rational. In interactive methods, Chebyshev scalarization and MIP solvers can be used to derive efficient outcomes. However, for a large-scale instance of the MOMIP problem, even commercial solvers may not provide the optimal solution of its scalarization within a time limit, and its existence is justified in practice. In such a case, the applicability of interactive methods becomes questionable because this interactivity is lost when the time limit is on the order of many minutes. In this work, for large-scale instances of the MOMIP problem, a hybrid decision-making support method is proposed. It combines features of interactive and a posteriori methods. Before the decision-making process begins, a finite two-sided Pareto front approximation is derived with the help of a MIP solver. It is used to calculate the so-called interval representations of Pareto optimal outcomes designated, directly or not, by weights of Chebyshev scalarization. Decision-makers can use them to reveal their preferences without additional optimization calculations. The use of the proposed method is demonstrated with selected large-scale instances of the MOMIP problem with two and three objective functions. Its limitations and possible enhancements are also discussed.",A hybrid decision-making support method for large-scale multi-objective mixed integer programming problems,[42628],653,"[112, 111, 77]",269,Integer Programming for Decision Support,45,9,45,Decision Support Systems,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,30 [building - 324],"['Programming, Multi-Objective', 'Programming, Mixed-Integer', 'Multi-Objective Decision Making']",TC-45
"Competitive facility location models involve locating one or more new facilities on the market where competing facilities already exist. The goal is for the added facilities to capture the maximum possible market share. Recent extensions to the classic competitive facility location problem introduce multi-purpose shopping for complementary or substitute goods. The idea comes from the observation that customers often shop for different goods and visit multiple locations during a single trip. The facilities [called clusters] offering these other goods exist but do not compete with our chain. The multi-purpose, multi-stop models existing in the literature thus far are, in fact, two-purpose, which means that they assume a customer makes either a single stop or two stops before returning to the point of origin. 

We formulate an extension to a two-purpose continuous competitive facility location model, introducing the possibility of more than two stops when shopping for complementary goods or services. Our problem is non-linear and non-convex, thus difficult to solve. We provide a solution technique and run experiments on instances incorporating 100 to 20,000 demand points and up to 20 facilities. We compare the solutions obtained by our model to the single-purpose and the two-purpose models. We show that introducing the possibility of more than two stops increases the market share captured by our chain.",Extensions to Competitive Facility Location with Multipurpose Trips,[71525],767,"[64, 19, 113]",270,Location with Multiple Actors,29,9,61,Locational Analysis,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S10 [building - 101],"['Location', 'Continuous Optimization', 'Programming, Nonlinear']",TC-61
"Compensation in multiple criteria decision aggregation procedures is commonly understood as allowing a gain in one criterion to offset a loss in another criterion. The compensatory behaviour depends on the selected multicriteria approach but can for some approaches also be steered by their specific design in terms of preference modelling and choice of aggregation logic. 
In domains such as environmental or public policy decision-making compensation is not always desired. There may be impacts that pose a loss too serious to be counterbalanced by any kind of good performance on other criteria. Furthermore, compensation may overshadow the existence of strong minority views. It may therefore be necessary to limit the extent to which an aggregation procedure can allow for compensation or explicitly acknowledge it.
The PROMETHEE methods are increasingly applied in environmental and public policy decision making due to their comprehensiveness and explainability. However, there are different statements in the literature about their compensatory properties. Furthermore, guidelines for applicants and analysts which explain how to control and steer the degree of compensation are apparently rare and opaque. We analyse the compensatory behaviour of outranking approaches with particular emphasis on the PROMETHEE methods and compare it to that of multiple attribute value theory approaches with the aim of providing guidelines for application and preventing unwanted compensation behaviour.",Analysing the compensatory properties of the outranking approach PROMETHEE,"[69695, 24622]",112,"[25, 40, 10]",273,BOR in public policy and environmental decisions,13,10,11,Behavioural OR,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,12 [building - 116],"['Decision Analysis', 'Environmental Management', 'Behavioural OR']",TD-11
"In urban settings, e-grocery delivery significantly impacts energy use and air pollution. Traditional routing solutions, while addressing general transportation, fall short in the specific area of fresh food delivery, which demands a focus on sustainability. Our research introduces a dynamic route planning framework that integrates graph neural networks for traffic flow forecasting with traditional optimization techniques. This framework is designed to balance environmental impact, food quality, and operational costs, adapting to changing urban logistics factors such as food demand, fleet size, and traffic conditions. The framework also incorporates advanced information-sharing strategies, enhancing its adaptability and efficiency in real-time scenarios. We emphasize the importance of sustainable and resilient solutions in the context of urban e-grocery logistics. The proposed solution, scalable for large-scale agent-based modeling, aims to establish more resilient and sustainable urban delivery systems, with a future focus on a specific city case study to demonstrate its practical effectiveness. The novelty in our approach is the inclusion of real-time information-sharing strategies and a focus on sustainability and resilience in route scheduling, which sets it apart from traditional systems. This fusion of advanced predictive analytics with practical optimization offers a new pathway to tackle the environmental and logistical challenges of urban e-grocery delivery.",Sustainable Dynamic Routing Framework for Efficient E-Grocery Delivery,"[76152, 76153]",658,"[32, 145, 131]",276,Decision Support for Operations Management,45,12,45,Decision Support Systems,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,30 [building - 324],"['E-Commerce', 'Vehicle Routing', 'Simulation']",WA-45
"This talk presents an algorithm for solving deterministic derivative-free optimization problems with thousands of decision variables. We employ a randomized dimension reduction technique based on Johnson-Lindenstrauss transforms and the trust-region framework of Cartis and Roberts. A key ingredient in our work is to extend the affine space within which the algorithm operates to include information learned in the course of the optimization. By adapting this space and judiciously using information gained in the course of the optimization, we obtain significant performance benefits on a class of large-scale test problems without sacrificing convergence guarantees.
",Accelerating Randomized Adaptive Subspace Trust-Region Derivative-Free Algorithms,"[74972, 74971, 76154]",337,"[72, 19, 113]",277,Algorithms for machine learning and inverse problems - zeroth-order optimisation,84,9,32,Advances in large scale nonlinear optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,41 [building - 303A],"['Mathematical Programming', 'Continuous Optimization', 'Programming, Nonlinear']",TC-32
"Vector Optimization problems have a wide range of applications in various fields, like economics, decision theory, game theory, information theory and optimal control theory. In this paper, unlike the general scalarization techniques, a sub gradient method without a scalarization approach is proposed for minimizing a non-differentiable invex function which works directly with vector valued functions. The proposed method includes regularization and the interior points variants of Newton's Method. The proposed algorithm generates a sequence of efficient points in the interior of epigraph of objective function which satisfy Karush-Kuhn-Tucker conditions.",A Subgradient Methods for general invex non-smooth vector optimization problems.,[76158],357,"[21, 77, 112]",282,Subgradient-based methods,70,9,41,Nonsmooth Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,97 [building - 306],"['Convex Optimization', 'Multi-Objective Decision Making', 'Programming, Multi-Objective']",TC-41
"This paper examines how a learning organization affects product quality performance from a behavioral operations perspective. Drawing on social information processing theory and social capital theory, we develop a theoretical model proposing [1] how a learning organization positively affects product quality performance through internal integration and [2] how employees’ supply chain management competencies amplify this relationship. We test our model using a longitudinal survey with three waves of data collection from employees in leadership positions in operations, manufacturing, and production functions located in the United Kingdom. Our findings support the proposed relationships. These results highlight the importance of aligning organizational processes and employees’ competency development in driving cooperative work across functions and enhancing product quality performance.",Learning organization and product quality performance - A moderated mediation model ,"[75778, 76828, 76829]",183,"[10, 57]",283,Behavior in supply chain collaboration,13,14,07,Behavioural OR,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,1019 [building - 202],"['Behavioural OR', 'Human Resources Management']",WC-07
"At previous EURO conferences, we presented, considered and contributed to the Kerkenes Eco-Village Project on Turkish rural countryside. At EURO 2024 2024 we will inform about new aspects and related developments and view this project also from the overall and national aspects of urbanization, energy supply and education in Turkey. 

Kerkenes Eco-Center, established by G. and F. Summers and supported by S.T Elias-Özkan, aims to promote sustainability through environmental studies. It pursues the following objectives:
- To advocate the use of renewable sources of energy;
- To act as a stimulus and a catalyst for environment-friendly building with appropriate materials and energy efficient designs;
- To act as a dynamic experimental base for testing designs, materials and activities suitable for viable and sustainable village life.
- To encourage village development and income generating activities that might halt and even reverse migration from rural areas to the cities.

We will relate the project with overall needs in Turkey, be it in the rural countryside or in the big cities, with regard to the demand for natural gas and for better primary school education in all social classes.",The Kerkenes eco-center in Anatolia - an interdisciplinary OR project,"[3524, 29929]",56,"[28, 90, 66]",286,Developing green and sustainable communities [EWG-ORD Workshop 1],67,9,18,OR for Development and Developing Countries,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,42 [building - 116],"['Developing Countries', 'OR in Development', 'Machine Learning']",TC-18
"This study introduces a market-based calibration approach for risk-based capital requirements under Basel III reforms. An advanced Merton Distance-to-Default model, modified for non-normal asset distributions, estimates the minimum leverage and risk-weighted capital ratios to maintain a default risk tolerance of 0.1\%. This market-oriented method externally evaluates asset uncertainty and default risk, overcoming the limitations of internal models, which often underrate systemic risk in banking. The paper also integrates a novel optimization model to align capital requirements with actual equity levels, ensuring global solution convergence by transforming the initial discontinuous, non-linear model into a smooth convex programming framework. This approach enhances the robustness and reliability of determining capital requirements in the banking sector.",A Market-based Calibration of Capital Requirements,"[1400, 76173, 76172]",470,"[44, 83, 131]",291,Applications to Economics and Finance,4,14,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S14 [building - 101],"['Finance and Banking', 'Optimization in Financial Mathematics', 'Simulation']",WC-63
"There are several optimization problems occurring in port terminals, and the berth allocation problem [BAP] is amongst the most important ones. The BAP is highly affected by uncertainty due to many factors like weather conditions and mechanical failures. Therefore, it is crucial to take uncertainty into account when approaching BAPs. Here, we study the BAP under a distributionally robust optimization [DRO] approach. We assume that each vessel has an associated deadline to finish its operations, but delays can occur. Therefore, we aim to minimize the worst-case of the expected sum of delays of vessels with respect to a set of possible probability distributions of the handling times. The proposed model is solved by an exact algorithm enhanced with several improvement strategies that drastically reduce the associated running time. The DRO model depends on a risk-parameter that makes it possible to obtain solutions with different degrees of protection against the risk.
Extensive computational results comparing DRO, stochastic programming, and robust optimization are also reported.",Distributionally robust optimization  for the berth allocation problem under uncertainty,"[51122, 19905]",82,"[70, 127, 136]",294,Seaside Planning III,52,7,62,OR in Port Operations,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S12 [building - 101],"['Maritime applications', 'Robust Optimization', 'Stochastic Optimization']",TA-62
"The Multi-Microgrid Network Structure Design Problem [MNSDP] is a binary matrix optimization problem that aims to minimize the cumulative length of preset power lines in a multi-microgrid system, while satisfying specific constraints. This problem is of great significance for improving the utilization rate of renewable energy sources and the stability and resilience of power systems in remote areas from the central grid. Due to the large-scale, sparse and nonlinear characteristics of this problem, traditional optimization methods are difficult to obtain satisfactory solutions within a reasonable time. This paper proposes a mathematical model of the MNSDP, incorporating three types of nodes with different reliability requirements. The paper also introduces a benchmark test suite, MNSDP-LIB, based on real-world scenarios, to evaluate the performance of algorithms. To solve the MNSDP, the paper develops a new matrix-based constrained differential evolution algorithm, LBMDE, which employs a binary-matrix-centric DE operator and an enhanced feasibility rule-based environmental selection strategy. The paper conducts extensive experiments to compare LBMDE with several state-of-the-art evolutionary algorithms and a commercial solver on MNSDP-LIB and other binary matrix optimization problems. The results demonstrate the superiority of LBMDE in terms of solution quality and computational efficiency, as well as its contribution to advancing renewable energy and microgrid technologies.",Large-Scale Binary Matrix Optimization for Multi-Microgrids Network Structure Design,"[58086, 76174, 76121]",856,"[5, 74, 93]",295,OR in Energy II,23,14,19,OR in Energy,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,44 [building - 116],"['Algorithms', 'Metaheuristics', 'OR in Energy']",WC-19
"We consider a manufacturer that produces in response to a stochastic demand and emits pollution during the production process. Industrial pollutants released into the air are characterized by spatial variability and heterogeneity, and the precision of instruments for measuring stochastic pollution stocks varies widely across pollutants. Consequently, commitment [open-loop] strategies to control production and associated emissions are considered more practical than contingent [feedback] approaches when manufacturers are concerned about environmental consequences. In this paper, we derive an optimal, open-loop control policy over an infinite time horizon and show a condition that ensures that such a policy induces asymptotic convergence of expected inventory stock and pollution stock trajectories to unique steady states. Moreover, we find that the variance in the pollution stock also converges asymptotically to a unique steady state, which is critical for prevention of irreversible environmental consequences. We show that environmental uncertainty affects inventories, leading in the long run to lower expected steady-state inventory stocks. We compare feedback and open-loop production policies and show that though there are losses associated with the inability to track inventory and pollution stocks accurately and thus applying the open-loop control instead of feedback, the expected long-term inventory and pollution stocks are not affected. ",Open-loop production control under stochastic conditions and environmental considerations,"[30492, 76176, 76177, 35882]",897,"[82, 135, 105]",296,Optimal control and resilience,90,9,33,Optimal Control Theory and Applications,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,42 [building - 303A],"['Optimal Control', 'Stochastic Models', 'Production and Inventory Systems']",TC-33
"Understanding the differentiability and regularity of the solution to a monotone inclusion problem is an important question with consequences for convex optimization, machine learning, signal processing, and beyond. Past attempts have been made either under very restrictive assumptions that ensure the solution is continuously differentiable or using mathematical tools that are incompatible with automatic differentiation. In this talk, we discuss how to leverage path differentiability and a recent result on nonsmooth implicit differentiation calculus to give sufficient conditions ensuring that the solution to a monotone inclusion problem will be path differentiable and provide formulas for computing its generalized gradient. Our approach is fully compatible with automatic differentiation and comes with assumptions which are easy to check, roughly speaking - semialgebraicity and strong monotonicity. We illustrate the scope of our results by considering three fundamental composite problem settings - strongly convex problems, dual solutions to convex minimization problems and primal-dual solutions to min-max problems.", Differentiating Nonsmooth Solutions to Parametric Monotone Inclusion Problems,[76178],338,"[81, 63, 66]",298,Algorithms for machine learning and inverse problems - optimisation for neural networks,84,10,32,Advances in large scale nonlinear optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,41 [building - 303A],"['Non-smooth Optimization', 'Large Scale Optimization', 'Machine Learning']",TD-32
"Long-term care has become one of the problems that must be addressed in today's society, and falls are one of the problems often encountered in long-term care. However, most previous studies have used accelerometers to detect falls in older adults. Since this sensor must be placed on the body of the elderly, its effect is quite limited in situations where the willingness to wear it is extremely low. This study uses deep image recognition technology to conveniently track the real-time behavior of the elderly and reduce the problems caused by accidents under the appropriate protection of the subjects' privacy. This research uses a Long Short-Term Memory Network [LSTM] to identify daily human movements and detect abnormal behaviors [such as falls]. 
In this study, two scenarios were tested. The first is to invite ten healthy people to perform in a laboratory that simulates long-term exposure. Each subject performed nine actions that might occur in the long-term care environment. The second scenario is to apply the database established in the first scenario to an actual long-term photo field. The experimental results show that under the simulated laboratory, the overall performance is quite good. In contrast, in the actual long-term photo field, the performance of motion recognition will be reduced due to the influence of occlusions",Using long short-term memory network for human body depth image motion recognition and abnormal behavior analysis in long-term care environment,[31517],416,"[8, 18, 7]",299,Recent Methodologies in Explainable AI [XAI] 1,71,2,04,Recent Advancements in AI ,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,1001 [building - 202],"['Artificial Intelligence', 'Computer Science/Applications', 'Analytics and Data Science']",MA-04
" The production process of fruit and vegetable often generates produce that does not conform to the aesthetic standards established by the retail sector. The so called ugly veg'' is a major contributors to global food waste. In this paper, We examine the capability of agri-food supply chains to incorporate ugly veg  to reduce food waste and financially empowering the growers who are often the weakest link. We model the interactions between the players in the chain and compare and contrast the common  spot contracts with the so called whole-crop contracts. We examine the players' incentives to up-cycling the ugly veg, and assess the potential to reducing waste and protecting growers against low wholesale prices.",Battling Waste and Empowering Growers with Ugly Veg - The Role of Contracts,"[75628, 65827, 67023]",537,"[138, 84, 50]",301,Agri-Food Supply Chains,19,9,24,Sustainable Supply Chains,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,83 [building - 116],"['Supply Chain Management', 'Optimization Modeling', 'Game Theory']",TC-24
"In this talk, we present the recent developments in the Cardinal Optimizer [COPT]. We discuss some key techniques that contributed to the performance improvements of our MIP solver and present performance numbers of the latest COPT release for all problem classes.",Recent advances in the Cardinal Optimizer,[50090],237,"[111, 14, 134]",303,MIP Solvers,76,2,30,Software for Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,53 [building - 208],"['Programming, Mixed-Integer', 'Combinatorial Optimization', 'Software']",MA-30
"Practical planning and decision-making problems are often better and more accurately formulated with multiple conflicting objectives rather than a single objective. This study investigates a situation relevant for Multiple Criteria Decision Making [MCDM] as well as Evolutionary Multi- objective Optimization [EMO], where the decision-maker needs to make a series of choices between nondominated options characterized by multiple objectives [criteria, attributes, dimensions]. The cognitive capacity of humans is limited, which leads to “cognitive burden” that influences human decision-makers’ decisions. We measure how different levels of decision diﬀiculty – the number of decision-making dimensions [attributes, criteria, objectives] – influence the cognitive burden in a laboratory study, and the impacts that this burden has on the decision-makers’ behavior and the quality of their decisions. We use psychophysiological, behavioral, and self-report methods. Our results suggest that a higher number of decision-making dimensions [i] increases cognitive burden significantly, [ii] leads to adopting satisficing strategies in which only a limited number of dimensions is considered, and [iii] decreases decision quality. ",Multiple conflicting objectives cause psychological burden on decision-makers and lowers decision quality,"[10953, 76182, 23949, 76183, 76184, 31731, 53935, 76185]",95,"[10, 25, 77]",304,Choice behavior,13,2,11,Behavioural OR,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,12 [building - 116],"['Behavioural OR', 'Decision Analysis', 'Multi-Objective Decision Making']",MA-11
"Decision makers are often faced with problems that are subject to uncertainty. Consider the problem of planning transport services for an upcoming season, determining optimal locations of new infrastructure, or establishing production plans and pricing strategies for a product. In this context, demand uncertainty is challenging to deal with, notably because it is decision-dependent. In this talk, we discuss data and modeling challenges associated with understanding and predicting demand. Focusing on the competitive facility location problem, we describe a methodology to deal with decision-dependent demand uncertainty without making strong distributional assumptions. We also provide a high-level overview of contextual stochastic optimization. Studied in the literature under a variety of names, contextual optimization refers to data-driven approaches to prescribe decisions by exploiting relevant side information. We position demand modeling for decision making in this context and outline future research directions on integrated learning and optimization.
 
",A Contextual Stochastic Optimization Perspective on Demand Prediction for Decision Making,[53470],36,"[136, 84, 143]",305,Emma Frejinger,62,8,01,Keynotes,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,Sportshallen [building - 101],"['Stochastic Optimization', 'Optimization Modeling', 'Transportation']",TB-01
"In real estate appraisal, the market value of a property is often identified based on the principle of price equilibrium; thus, the prices of similar, recently traded real estate assets are the basis for the assessment. Under this framework, a technique widely used by practitioners and appraisers goes under the name of sales comparison adjustment grid. The property to be assessed [the so-called subject] is compared to ten or so transactions [the so-called comparables]. Adjustments are made to the comparables’ prices to reflect their differences with respect to the subject. To that end, the properties’ main attributes - namely, the features the market players on the demand and supply side value the most - must be listed and weighted. The weighting of the attributes - usually based on the appraisers’ local real estate market knowledge - is among the most prominent weaknesses of the appraisal technique. We propose to improve it by deriving the weights from the data itself using a model combining the methodological setting of the Analytic Hierarchy Process [AHP] and nonlinear optimization. On the one hand, a ranking of the comparables is derived by the pairwise comparison of their prices. On the other hand, another ranking is derived by the pairwise comparison of their attributes. The weights of the attributes are then iteratively searched as those that minimize the distance between the two rankings.",AHP and Nonlinear Optimization to Improve Market-Value Appraisal of Real Estate Assets,"[76146, 76228, 76186]",122,"[6, 16]",308,Pairwise comparisons and preference relations 1,44,9,44,Multiple Criteria Decision Analysis,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,20 [building - 324],"['Analytic Hierarchy Process', 'Complexity and Approximation']",TC-44
"More and more individuals and families have been driven into the danger zone of going hunger recently. To rescue the situation, food sharing organisations [e.g. charities] have worked untiringly. One of the key challenges facing the food sharing organisations is the lack of decision support to help improve their operations efficiency and community of practice. This paper aims to investigate the role of peer learning in the creation of closed-loop social value networks, subsequently to support food sharing organisations and communities. 

A Systematic Literature and Practice Review [SLPR] has been undertaken. Based on the SLPR, a comprehensive peer learning framework for food sharing has been developed consisting of four core components - peer learning agents, peer learning modes, peer learning practice, and peer learning outcomes and impacts. The impacts of peer learning on the creation of closed-loop social value networks will be assessed via four stages of the network, including social value propositions, value beneficiaries, value delivery channels, and value capture and feedback. The peer learning framework contributes to new knowledge and practice in a number of aspects - [1] it defines relationships among the four peer learning components; [2] it establishes the main impacts of peer learning on four stages of social value networks; and [3] it provides recommendations to food sharing organisations in making efficient decisions.  
",Exploring the Role of Peer Learning in Supporting Decisions for Food Sharing Organisations,"[32247, 76187, 76188, 64957]",68,"[26, 138, 62]",309,Decision Support for Sustainable Operations,45,10,45,Decision Support Systems,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,30 [building - 324],"['Decision Support Systems', 'Supply Chain Management', 'Knowledge Engineering and Management']",TD-45
"This article provides an accelerated static hedge portfolio [SHP] method for evaluating American options based on stochastic volatility and double jump processes. Our proposed model is a generalization of the static hedge portfolio approach of Derman, Ergener, and Kani to evaluate American options based on the Richardson extrapolation. Numerical results show that the numerical efficiency of our accelerated static hedge portfolio approach is comparable to the least-squares Monte Carlo simulation method. Numerical results demonstrate that our proposed method is efficient and accuracy in evaluating American options with stochastic volatility and double jump processes.",Accelerated Static Hedging Method for Pricing American Options,[61223],571,"[45, 135, 131]",311,Methodology in asset allocation and banking,74,5,57,Modern Decision Making in Finance and Insurance,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S06 [building - 101],"['Financial Modelling', 'Stochastic Models', 'Simulation']",MD-57
"One of the main challenges that water companies face is to reduce carbon footprint in their transition to carbon neutrality. Past research assessing carbon performance of water companies have mostly ignored heterogeneity among the water companies evaluated. To overcome this limitation, this study employs a parametric metafrontier approach to assess and compare carbon performance of a sample of English and Welsh water companies, embracing water and sewerage companies [WaSCs] and water only companies [WoCs]. This method allows for a comparison of the carbon performance of these two types of companies and analyses the impact of environmental variables on their performance. It was evidenced that water treatment complexity, main source of raw water and population density significantly influence carbon performance of water companies. WaSCs were found to be more carbon efficient than WoCs. Moreover, on average WaSCs have improved carbon productivity across years [2011-2020] whereas WoCs have worsened it. The insights gained from this study are highly significant for policymakers focused on transitioning the water industry towards net-zero carbon emissions.",INTEGRATING GROUP HETEROGENEITY IN ASSESSING CARBON PERFORMANCE OF WATER COMPANIES - A CASE STUDY FOR ENGLAND AND WALES,"[76192, 4960, 2040, 76193, 76194]",812,"[94, 144, 147]",312,Carbon Taxing and Emissions Controls,80,15,53,Sustainable and Resilient Systems,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,8007 [building - 202],"['OR in Environment and Climate change', 'Utility Systems', 'Water Management']",WD-53
"This paper presents a framework for assessing digital merchandising on web and mobile equipment. Making a digital strategy to sell products online a success, is a great challenge. It involves evaluating various aspects related to how products are presented and sold online. Depending on the product, one can develop personalized actions for each type of consumer who visits the site, based on their personal interests and demographic data, to give greater relevance to the most recent articles that have been published on the website, or adapt your ecommerce to new trends and customer behavior. An empirical work has been done based on expert judgements for identifying dimensions’ aspects related to products presentation online sales in order to obtain cost-effectiveness. The study provides a basis for setting priorities formulating a Multicriteria decision making model.  ",A Multicriteria Framework for Assessing Digital Merchandising on Web and Mobile Equipment,"[57534, 76195, 76196, 58717]",894,"[32, 6, 77]",313,Pairwise comparisons and preference relations 4,44,13,44,Multiple Criteria Decision Analysis,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,20 [building - 324],"['E-Commerce', 'Analytic Hierarchy Process', 'Multi-Objective Decision Making']",WB-44
"In recent years, there has been a significant increase in the adoption of Boolean Satisfiability [SAT] models and algorithms to tackle various challenges in Engineering and Computer Science. This surge can be credited to the notable advancements in SAT algorithms during this period, allowing for the resolution of more complex problem instances across diverse application domains.

We present a highly specific modelization of SAT, with a resolution approach based on Quantum Approximate Optimization Algorithm [QAOA]. This particular methodology enables the solution of SAT problems with 96 clauses, currently representing the largest problem sizes solvable using quantum approaches.",SAT resolution with an Indirect Quantum based approach,"[25037, 76198]",375,"[5, 0]",314,Decomposition methods for Quantum Optimization,83,3,42,Quantum Computing Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,98 [building - 306],['Algorithms'],MB-42
"Classical Domain Adaptation methods acquire transferability by regularizing the overall distributional discrepancies between features in the source domain [labeled] and features in the target domain [unlabeled]. They often do not differentiate whether the domain differences come from the marginals or the dependence structures. In many business and financial applications, the labeling function usually has different sensitivities to the changes in the marginals versus changes in the dependence structures. Measuring the overall distributional differences will not be discriminative enough in acquiring transferability. Without the needed structural resolution, the learned transfer is less optimal. This paper proposes a new domain adaptation approach in which one can measure the differences in the internal dependence structure separately from those in the marginals. By optimizing the relative weights among them, the new regularization strategy greatly relaxes the rigidness of the existing approaches. It allows a learning machine to pay special attention to places where the differences matter the most. Experiments on three real-world datasets show that the improvements are quite notable and robust compared to various benchmark domain adaptation models.",Efficient forecast via transfer learning,[53121],73,"[7, 47, 126]",315,"Advancements of OR-analytics in statistics, machine learning and data science 2",16,3,28,"Advancements of OR-analytics in statistics, machine learning and data science","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,065 [building - 208],"['Analytics and Data Science', 'Forecasting', 'Risk Analysis and Management']",MB-28
"We investigate the influence of the newly implemented Carbon Border Adjustment Mechanism [CBAM] by the European Union on sustainable supply chain management. CBAM addresses the global disparity in carbon leakage due to divergent carbon taxation policies that vary across nations and regions. Given that CBAM is designed to prevent double taxation on carbon emissions, understanding its interaction with non-EU carbon regulation mechanisms, like cap and trade policies, is crucial. Our study primarily focuses on how the operational decisions of both EU and non-EU manufacturers are influenced by the interplay between the CBAM and cap and trade policy. Additionally, we aim to analyze how retailers within the EU determine their sourcing strategies, specifically, whether to source globally from non-EU manufacturers or opt for local sourcing within the EU. Our findings reveal that an increase in carbon emission permits allocated to non-EU manufacturers doesn't necessarily lead to a surge in profit. In fact, as additional permits are issued, the supply chain associated with non-EU manufacturers may encounter profitability dilemmas, impacting both the retailer and the non-EU manufacturer. We also find that when the CBAM fee is substantial, and the non-EU manufacturers are granted an large number of permits, retailers are more inclined towards local sourcing. ",Competitive retailers' sourcing strategies under Carbon Border Adjustment Mechanism,[76201],486,"[138, 50, 25]",316,Carbon Regulation,19,13,24,Sustainable Supply Chains,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,83 [building - 116],"['Supply Chain Management', 'Game Theory', 'Decision Analysis']",WB-24
"Online retailers throw out food that has not yet expired. This gives rise to the question whether online retailers generate more food waste than offline retailers, who typically throw out food only after it has expired. We focus on the food waste at the retailer which inherently ensues from the logistical set-up. We first provide a theoretical analysis to establish whether throwing out food before expiration indeed results in an increase in food waste, putting online retailers at a disadvantage compared to offline retailers. We show for well-behaved inventory control policies, including the optimal policy, that food waste increases when food is thrown out before expiration. Next, we compare the food waste of the online retailer with that of an offline retailer in numerical experiments. Note that the online retailer has some advantages over offline retailers as well. Online retailers benefit from full control of order picking, which is instead often done by the consumer in offline retail. Moreover, the online retailer often benefits from the pooling effect, as offline retailers might use multiple stores to satisfy the same demand volume as an online retailer from a single warehouse. Our numerical experiments with a base-stock policy suggests that online retail actually yields less food waste for many products, despite throwing out food before expiration.",Reduced food waste despite throwing out food before expiration - online vs. offline retail,"[76202, 76203, 27939, 27820]",853,"[61, 135, 139]",317,Reducing Food Waste,78,15,13,Secure & Sustainable Food Supply,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,15 [building - 116],"['Inventory', 'Stochastic Models', 'Sustainable Development']",WD-13
"In an era propelled by knowledge, our study pioneers a cutting-edge evaluation of teaching and research efficiency in Croatian universities, employing a sophisticated blend of AHP, DEA, and regression analysis. Our extensive literature review from the Scopus database, spanning scholarly discourse from inception to 2023, unveiled crucial focuses - teaching and research efficiency, and their convergence.
Distilling insights, we constructed a refined framework for inputs and outputs, utilizing AHP, and engaging diverse respondents—Ministry of Education, university rectors, and higher education policy advisors—in a structured comparison. They assigned relative importance to input and output hierarchies for teaching and research efficiency. The highest-ranked factors, determined by AHP, shaped the DEA analysis, revealing the efficiency frontier and enabling targeted improvement strategies for universities based on quantified efficiency scores.
In parallel, our study delves into the assessment of managerial ability within universities, a pivotal yet often overlooked aspect. Adopting methodologies proposed by Demerjian and Lev [2021] and Banker and Park [2021], we measure the efficacy of managerial decision-making in steering institutions toward relative teaching and research efficiency. By identifying determinants of managerial ability, our analysis contributes insights into the leadership dynamics shaping the overall efficiency of Croatian universities.","Croatian Universities Under the Microscope - Who Leads, Who Lags, and Why?","[76180, 76206, 76204]",938,"[35, 92, 24]",318,DEA applications in Education and Health I,89,5,48,Data Envelopment Analysis and its Application,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,60 [building - 324],"['Efficiency Analysis', 'OR in Education', 'Data Envelopment Analysis']",MD-48
"We study the design of optimal incentives in sequential processes.  An agent initiates a value-creating sequential process through costly investment with random success. If unsuccessful, the process stops. If successful, a new agent thereafter faces a similar investment decision, and so forth. For any realization of the process, the total value is distributed among the agents using a reward rule. Such rules induce a game among the agents. The domain of the rules is very rich, yet we are able to show equilibrium existence. We characterize rules that yield the highest welfare created in equilibrium. We also characterize rules that yield the highest possible payoff for the initiator in equilibrium. 
A canonical class of rules are those in which an agent's reward [apart from the initiator] is affected by her own success but not the success or failures of others. Our findings show that such simple reward rules invoking short-run individual incentives are sufficient to meet our long-run systemic objectives.",Successive incentives,"[57023, 70966, 29054, 76209]",642,"[27, 50, 137]",319,Market Design 2,87,12,43,Market Design,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,99 [building - 306],"['Decision Theory', 'Game Theory', 'Strategic Planning and Management']",WA-43
"A significant portion of the effort within the field of Operations Research is dedicated to the development of intricate ad-hoc algorithms and models tailored for addressing diverse optimization problems. Frequently, the research community exhibits a preference for complexity in the proposed methodologies. Notably, an esteemed characteristic sought in these methods is the incorporation of problem-specific knowledge, enabling the attainment of results that get as close as possible to optimality. 

In academic publishing, the pursuit of marginal gains in optimality gaps is a common goal, even at the expense of introducing additional complexity to models or algorithms. While there is consensus that such endeavors contribute to advancements in algorithms and propel the field of Operations Research forward, a frequently underestimated dimension is the practical applicability of these advancements in industrial settings.

Real industrial problems are often messy with imprecise [or numerous] objectives, soft constraints, preferences and a myriad of situations that demand pragmatic approaches. Additionally, these problems undergo rapid evolution with frequent model refinements, occurring often fortnightly. Here, the ongoing painful adaptation of intricate code bases stemming from ad-hoc methods is not the favored choice. In contrast, solvers offer greater flexibility, allowing faster implementation of new constraints through their APIs than altering tailored algorithms. In industrial contexts, the preference lies in flexibility, maintainability, robustness, and reduced operational load, where a modest optimality gap is deemed a minor trade-off.

Moreover, in the realm of large-scale real-world problems, mathematical solvers often prove impractical. It is commonplace to resort to simplifying the problem for solvability. This practice raises a critical question - Is it logical to insist on chasing optimality in solving a simplified problem with a reduced real-world fidelity? Furthermore, real problems entail vast datasets, often including forecasted or approximated input data. Does it make sense to go great lengths to optimally solve a problem when a portion of the input data involves approximations?

In this presentation, I advocate for relinquishing the pursuit of optimality and, instead, embracing heuristic solvers and simplified modeling approaches. Such strategies yield expedient, adaptable, maintainable, and easily implementable models. The discourse will draw upon various examples, spanning classical routing and scheduling problems, culminating in intricate real-world virtual machine placement bin packing problems encountered at Amazon Elastic Compute Cloud [EC2].
",Optimal is the enemy of good - Solving very large scale virtual machine placement problems at Amazon Elastic Compute Cloud,[781],41,"[74, 145, 129]",322,Ruben Ruiz,62,4,01,Keynotes,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,Sportshallen [building - 101],"['Metaheuristics', 'Vehicle Routing', 'Scheduling']",MC-01
"In the dynamic landscape of data-driven business operations, model drift poses a significant challenge, often leading to substantial economic repercussions. Traditional approaches to quantifying the impact of model drift have been rudimentary, frequently relying on simplistic, ad-hoc calculations. Our paper introduces the Economic Impact Estimation Metric, a pioneering method designed to assign a quantifiable fiat cost to the concept of model drift in a systematic and accessible manner.",Estimated economic impact of model drift,[76208],73,"[7, 18, 66]",323,"Advancements of OR-analytics in statistics, machine learning and data science 2",16,3,28,"Advancements of OR-analytics in statistics, machine learning and data science","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,065 [building - 208],"['Analytics and Data Science', 'Computer Science/Applications', 'Machine Learning']",MB-28
"Service Network Design Problems [SNDPs] are prevalent in the freight industry, holding significant practical value in their solutions. While the classic SNDP is often defined on a planning horizon discretized into a finite number of integral time units, the Continuous-Time Service Network Design Problem [CTSNDP] operates on a continuous-time planning horizon to avoid errors from discretization. Existing algorithms for CTSNDP and its variants primarily adopt a Dynamic Discretization Discovery [DDD] solution framework. This framework iteratively refines a finite set of integral time units, constructs a partially time-expanded network based on the discretization, and leverages the network to derive relaxations and feasible solutions for the problem. However, DDD algorithms face limitations in computational performance, especially when handling instances characterized by a high-cost ratio of vehicle-based costs over flow-based costs and high time flexibility. Consequently, a substantial portion of these instances remains unsolved. In this study, we enhance the DDD method for CTSNDP by introducing a new initial discretization, a novel exact method for deriving upper-bound solutions, and an improved refinement strategy. The computational study demonstrates that, with these innovative components, the enhanced DDD algorithm exhibits exceptional performance.",An Enhanced Dynamic Discretization Discovery Algorithm for the Continuous-Time Service Network Design Problem,"[76210, 64847, 64381]",779,"[143, 79, 109]",325,MILPs for Vehicle Routing 1,5,10,58,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S07 [building - 101],"['Transportation', 'Network Design', 'Programming, Integer']",TD-58
"This study aims to analyze corporate governance using Rough Set Theory [RST], highlighting its significance in both academic discourse and industrial practices. Corporate governance plays a pivotal role in ensuring trust and protection for investors, effective risk management, enhanced corporate performance, fulfillment of social responsibility, and adherence to legal and regulatory compliance. This study compares a bipolar model based on RST with a traditional multi-attribute decision model. The objective is to evaluate the effectiveness and distinguishing characteristics of the RST-based bipolar model compared to conventional multi-attribute decision models. The RST-based model introduces a unique perspective that deviates from the traditional approaches of multi-attribute decision-making. Through this comparative analysis, the study aims to provide valuable insights into the application of RST in decision-making frameworks, contributing to a deeper understanding of its practical implications in decision science. Through rigorous analysis, the research identified distinct characteristics and performance metrics of the RST-based bipolar model. It assessed its efficacy in decision-making contexts—key findings shedding light on the model's strengths and potential areas for improvement. The comparative analysis enhances our understanding of the practical implications of employing RST in decision science, providing valuable implications for academia and industry.",Comparing two MCDM Approaches to Evaluating Corporate Governance,[57550],122,"[6, 15, 25]",326,Pairwise comparisons and preference relations 1,44,9,44,Multiple Criteria Decision Analysis,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,20 [building - 324],"['Analytic Hierarchy Process', 'Complex Societal Problems', 'Decision Analysis']",TC-44
"Applying interactive multiobjective optimization [MOO] methods necessitates preference information from a decision maker [DM]. Because of the active role of the DM, comparison of interactive methods must be planned carefully. Previous comparisons have been rare and irreproducible. We have developed a new, reproducible experimental design to compare interactive MOO methods with human participants. Our design utilizes a novel questionnaire, which can measure the cognitive load experienced by DMs, their satisfaction with the solution process and the solution finally obtained. We can also measure the methods’ ability to reflect preferences and responsiveness to changes in the preferences. Moreover, we have developed a web-based user interface to allow participants to access and utilize interactive methods, and to answer the questionnaire interactively. Our design enables a transparent and reproducible comparison of interactive MOO methods.",Designing empirical studies for comparing interactive multiobjective optimization methods with human participants,"[71749, 67924, 79334, 23169, 6266, 2312]",95,"[77, 26, 10]",327,Choice behavior,13,2,11,Behavioural OR,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,12 [building - 116],"['Multi-Objective Decision Making', 'Decision Support Systems', 'Behavioural OR']",MA-11
"Stochastic dominance is a statistical tool developed for comparing the random variables among each other. In financial applications, these random variables usually represent random returns of the considered assets or portfolios. The paper focuses on portfolio selection problems with stochastic dominance constraints for various orders of stochastic dominance relations. Firstly, the tractable necessary and sufficient conditions for particular probability distributions are discussed. Secondly, these conditions are employed in the static and dynamic portfolio selection problems. Finally, the extensions for the case of vector comparisons are presented. The theoretical results are accompanied by empirical examples.",Stochastic dominance in decision making under uncertainty,[12024],38,"[136, 135]",330,Milos Kopa,62,9,01,Keynotes,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,Sportshallen [building - 101],"['Stochastic Optimization', 'Stochastic Models']",TC-01
"Counterfactual explanations play an important role in detecting bias and improving the explainability of data-driven classification models. A counterfactual explanation [CE] is a minimal perturbed data point for which the decision of the model changes. Most of the existing methods can only provide one CE, which may not be achievable for the user. In this work, we derive an iterative method to calculate robust CEs, i.e., CEs that remain valid even after the features are slightly perturbed. To this end, our method provides a whole region of CEs allowing the user to choose a suitable recourse to obtain a desired outcome. We use algorithmic ideas from robust optimization and prove convergence results for the most common machine learning methods including decision trees, tree ensembles, and neural networks. Our experiments show that our method can efficiently generate globally optimal robust CEs for a variety of common data sets and classification models.",Finding Regions of Counterfactual Explanations via Robust Optimization,[71481],120,"[66, 127, 111]",333,Mathematical Optimization for Counterfactual Explanations,15,5,27,Mathematical Optimization for XAI,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,047 [building - 208],"['Machine Learning', 'Robust Optimization', 'Programming, Mixed-Integer']",MD-27
"In this paper, a latency location routing problem with stochastic travel times is investigated. The problem is modeled as a two-stage stochastic program, by setting the location decisions at the first stage, and the routing decisions at the second one. A risk-averse decision-maker is assumed. An efficient and effective multi-start variable neighborhood search algorithm is proposed for tackling the problem. A sampling method is also presented for dealing with uncertainty captured by a continuous probability distribution. Several insights are provided based on the results of a series of computational tests performed to asses the methodological contributions provided by this work.",A risk-averse latency location-routing problem with stochastic travel times,"[9684, 62511, 710]",222,"[14, 143, 135]",336,Applications to Logistics and Transportation,64,7,26,Combinatorial Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,012 [building - 208],"['Combinatorial Optimization', 'Transportation', 'Stochastic Models']",TA-26
"This study presents a novel solution methodology to cover sustainability objectives by synchronizing risk and resilience impact estimations for multimodal freight transportation industry. Minimization of total cost, time, and carbon emissions by using augmented ϵ-constraint method are outlined under multi-objective mixed integer optimization model. Also, approach is shown as comparative analysis, after defining some pre-processing, symmetry breaking steps, valid inequalities, and logic cuts. After identifying fourteen different risk factors for multimodal transportation system, if the most critical risk factor occurs, there will almost 380% increase in total logistics cost and 210% increase in time, respectively. Decision makers can use alternative Pareto solution sets and information about potential impact of different risk factors over sustainability in their risk management systems. Therefore, some pro-active action plans can be proposed for dynamic environments and improve the logistics systems reliability under different supply disruptions. ",Design of Novel Resilience and Risk Management Model for Sustainable Supply Chains ,[31893],506,"[138, 143, 111]",337,Freight transportation and logistic I,6,8,55,Transportation,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S02 [building - 101],"['Supply Chain Management', 'Transportation', 'Programming, Mixed-Integer']",TB-55
"In this work we consider generalizations of the dual risk model with proportional gains, constant expense rate, and innovations that arise according to a renewal process. Among others, we consider the case where the proportional parameter can be a constant or a random variable, as well as cases where several independence assumptions among gain/innovation sizes and the interrarrival times of the gains/innovations are lifted. More precisely, we consider the case with causal dependence structure, as well as the case where the dependence is based on the generalized Farlie-Gumbel-Morgenstern copula. We also consider the case where the proportional parameter is randomly chosen, as well as the case where we may have two-sided jumps. The ruin probability and the distribution of the time to ruin are determined.",On dual risk models with dependence structure,[75613],15,"[126, 135, 121]",338,Advances in Stochastic Modelling and Applied Probability Ι,47,3,40,Advances in Stochastic Modelling and Learning Methods,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,96 [building - 306],"['Risk Analysis and Management', 'Stochastic Models', 'Queuing Systems']",MB-40
"Certainty equivalents are a core concept in Finance/Economics. Nevertheless, their experimental detection as outlined in papers like Holt/Laury [2002] and ensuing publications is not robust and not reliable enough given its role as a core concept - it consists of a one-off choice between a risky lottery and a riskless payoff and does not consider further conditions that might exert influence.
For that reason, this paper uses on the one hand the PEST procedure advocated by Luce. PEST analyzes repeated [and not just one-off] choices and determines certainty equivalents from the steady state, i.e., when decision makers no longer switch between the riskless payoff and the risky lottery. On the other hand, conditions are specified as - [i] expected value is given as additional in-formation next to the state-dependent outcomes of the risky lottery; [ii] different starting values of the riskless payoff in PEST. 
This paper finds:
[i] The condition “expected value” leads to higher average reaction times and requires more trials in the PEST procedure compared to the benchmark case “only state-dependent outcomes”.
[ii] The starting value of the riskless payoff in the PEST procedure exerts massive influence on certainty equivalents.
[iii] The number of fixation points [derived from eye tracking] and reactions times can [partially] explain why conditions influence certainty equivalents.
",Robust determination of certainty equivalents,[55580],94,"[10, 45]",339, Behaviour and decision processes ,13,12,07,Behavioural OR,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1019 [building - 202],"['Behavioural OR', 'Financial Modelling']",WA-07
"A non-cooperative model is formulated where a producer and a deforester generate polluting emissions over time. Given the benchmark provided by the cooperative solution, we investigate two scenarios i] whether the forest’s restoration should be done by the deforester alone [as the forest’s owner] or outsourced to the producer [the forest’s non-owner], and ii] whether non-cooperative strategies for either of these two scenarios can eliminate history dependency. ",SHOULD FORESTS BE RESTORED BY PRODUCERS OR DEFORESTERS?,[73457],373,"[139, 20, 50]",340,Optimal control in environmental economics,90,8,33,Optimal Control Theory and Applications,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,42 [building - 303A],"['Sustainable Development', 'Control Theory', 'Game Theory']",TB-33
"  We propose two new variants of the well-known Fan’s minimax theorem, weakening the assumptions of convexity and lower semi-continuity used in the convex-concave standard version. Namely, the convexity and lower semi-continuity are only required on a smaller set which does not have to be convex or compact. In this case, the bi-function may take extended values on the given pair of convex sets. A second variant is then given when the bi-function is finite on these sets, removing the lower semi-continuity condition.These results are next used to study the remoteness of sets and functions in Hilbert spaces.
Key words - Minimax theorem, convexity, remotal sets and functions.


",Partially non-convex minimax theorem and applications to  remoteness of sets and functions,"[76207, 39025]",294,"[21, 19]",343,Variational Analysis and Subdifferential techniques,82,9,42,Variational Analysis and Continuous Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,98 [building - 306],"['Convex Optimization', 'Continuous Optimization']",TC-42
"This paper introduces a methodology for prioritizing investment initiatives within a Regional Health Service. The absence of a structured methodology directly impacts the timely delivery of solutions in public health to citizens. Currently, investment priorities lack clear definition and justification, affecting the focus on crucial citizen issues that require prioritization in both the planning and execution of the investment portfolio.

To face this challenge, the Analytic Hierarchy Process [AHP] is employed. The prioritization relies on the judgments of experts and stakeholders engaged in the Health Service. Recognizing the constraints of limited financial resources, a method for scheduling projects over time is also incorporated.

The result of this model is a robust approach that enables the effective prioritization of public investment projects outlined in the portfolio. This, in turn, addresses the needs of citizens in public health with projects that materialize within more limited timelines and are based in criteria justifying the allocation of resources.
",DECISION MODEL FOR PRIORITIZING PUBLIC INVESTMENT PROJECTS IN A HEALTH SERVICE,"[58717, 76231, 57534, 58829]",961,"[6, 22, 118]",344,Project Management,35,7,60,Project Management and Scheduling,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S09 [building - 101],"['Analytic Hierarchy Process', 'Critical Decision Making', 'Project Management and Scheduling']",TA-60
"One major impediment to developing countries’ economic growth is the lack of access to affordable, sustainable, and reliable modern energy systems. Even today, hundreds of millions of people live in rural areas and do not have access to essential electricity services. In this study, we present a planar facility location–allocation problem for planning decentralized energy systems in rural development. We consider nano-grid and micro-grid systems to electrify rural households. While micro-grids serve multiple households with a common generation facility, nano-grids are small-scale systems serving individual consumers. The households served by micro-grids are connected to the generation facilities with low-voltage cables, for which we employ a distance limit constraint due to technical concerns, including power loss and allowable voltage levels. For this problem, we provide mixed-integer quadratically constrained problem formulations and propose model-based and clustering-based heuristic approaches.",A facility location–allocation problem for rural electrification,"[43460, 76232]",712,"[37, 0]",347,Empowering Energy Access,21,15,22,Energy Management,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,81 [building - 116],['Energy Policy and Planning'],WD-22
"Ensuring the efficacy of business process interventions is essential for informed decision-making and sustained improvements. This study focuses on a pragmatic approach that integrates process mining and the strengths of propensity score analysis to enhance precision and reliability in evaluating business process interventions. Drawing from our prior work, we present a practical methodology tailored for organizations seeking an effective solution.
We aim to construct a framework that utilizes both event logs and observed characteristics to address confounding factors, thereby enhancing the validity of conclusions in observational studies. Our work is complemented by a prototype tool designed to assist practitioners in selecting appropriate weighting methods, evaluating group balance, and assessing covariate balance. Through an applied case study on a simulated claims management process dataset, we illustrate how these techniques synergize to measure and enhance business process effectiveness and efficiency.
This study not only underscores the integration of process mining, business process management, and causal inference but also scrutinizes the inherent limitations and challenges. By providing a comprehensive view, our goal is to offer organizations actionable insights, implementation guidance, and an understanding of how this synergy can advance business process evaluation.",Advancing Business Process Evaluation - The Value of Integrating Process Mining and Propensity Score Analysis,"[66907, 77333, 77344]",66,"[7, 26, 25]",349,Emerging Trends in Decision Analysis,45,5,45,Decision Support Systems,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,30 [building - 324],"['Analytics and Data Science', 'Decision Support Systems', 'Decision Analysis']",MD-45
"As a bank, we handle millions of transactions a day. Almost all of these [>99.99%] are completely fine and legitimate transactions. However, every once in a while a sanctioned entity is either trying to send or receive money with a payment we process. We want, and are legally required, to stop such payments. Due to the high volume of transactions a day, algorithms play a crucial role in flagging potentially suspicious payments. If any of these algorithms judge there to be a match to a sanctioned entry, the payment is investigated manually. 
 
This poses a challenge though - the number of true positives is very low, so even the best algorithms will mostly flag false positives [>95%]. This means almost all of the payments an analyst goes through are fine, they just have an unlucky similarity to a sanctioned entity. To reduce this issue, the algorithmic thresholds have to be as tight as possible, while still finding all the true positives. 
 
In our presentation, we will show how this problem can be posed as a mixed-integer programming problem and the results it yields. We also discuss how to extend the formulation to allow for a risk-based approach, where a certain number of misses of true positives are allowed, in order to further reduce the number of false positives. This results in a fascinating pareto front, which is used as a basis of discussion around price of catching all true positives. 
",You Shall Not Pass - Minimizing false-positive ensemble classification through threshold optimization,"[76241, 76242, 36689]",571,"[44, 7, 111]",352,Methodology in asset allocation and banking,74,5,57,Modern Decision Making in Finance and Insurance,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S06 [building - 101],"['Finance and Banking', 'Analytics and Data Science', 'Programming, Mixed-Integer']",MD-57
"We study routing problems of a convoy in a graph, generalizing the shortest path problem [SPP], the travelling salesperson problem [TSP], and the Chinese postman problem [CPP] which are all well-studied in the classical [non-convoy] setting. We assume that each edge in the graph has a length and a speed at which it can be traversed and that our convoy has a given length. While the convoy moves through the graph, parts of it can be located on different edges. For safety requirements, at all time the whole convoy needs to travel at the same speed which is dictated by the slowest edge on which currently a part of the convoy is located. For Convoy-SPP, we give a strongly polynomial time exact algorithm using dynamic programming. For Convoy-TSP, we provide an O[log n]-approximation algorithm and an O[1]-approximation algorithm for trees. Both results carry over to Convoy- CPP which - maybe surprisingly - we prove to be NP-hard in the convoy setting. This contrasts the non-convoy setting in which the problem is polynomial time solvable.",Exact and approximation algorithms for routing a convoy through a graph,"[64423, 45060, 55353, 75270]",196,"[14, 16, 108]",353,"Discrete, continuous or stochastic optimization and control in networks, transportation and design I ",64,2,25,Combinatorial Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,011 [building - 208],"['Combinatorial Optimization', 'Complexity and Approximation', 'Programming, Dynamic']",MA-25
"Living donors are often incompatible with their intended recipients. Kidney exchange matches one patient and his or her incompatible donor with another pair in the same situation for an exchange. We studied novel immunological such as applet matching to optimize the Saudi National Kidney Exchange Program [SNKEP] involving a number of Saudi transplant centers and their pool of patients. Cross-hospital kidney exchanges allow an increase in the likelihood of finding the matching pairs, improve the qualities of the matches, and provide. We modeled the impact of new immunological matching strategies on overall graft survival. ",Novel immunological and optimization strategies to optimize Saudi Kidney Exchange,[61302],592,"[56, 0]",354,Kidney Exchange I,3,9,10,OR in Health Services [ORAHS],"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,11 [building - 116],['Health Care'],TC-10
"Recruiting candidates globally and across multiple sites in different geographic regions is necessary to speed up the enrollment of clinical trials. While patient enrollment can benefit from this globalization, initiating clinical trials has become much more complicated. In the start-up stage, the sites must be selected out of a set of potential candidates around the globe based on the specifics of those clinical trials, such as protocols, operational costs, and recruitment deadlines. Sites in one region can be very distinct from sites in another area. Yet, a common mistake in selecting sites is to rely on too little knowledge or subjective data. Poor selection decisions can lead to study delays and prolong the time to market for life-saving treatments. Thus, we propose a framework to aid the decision-making in global site selection problem. To ensure that our framework accurately captures the uncertainty in recruitment time, we adopt a risk-based constraint that accounts for random patient enrollment. The extensive computational studies help quantify significant time-cost trade-offs as a potential solution to control the costs of conducting a trial. ",Site selection problems for clinical trial supply chains,"[76243, 76244, 76245, 76246]",968,"[56, 84, 7]",356,Decision support in healthcare,3,2,17,OR in Health Services [ORAHS],"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,40 [building - 116],"['Health Care', 'Optimization Modeling', 'Analytics and Data Science']",MA-17
In this talk we present sufficient conditions for a set and the ordering cone of a space to ensure the density of the set of Henig proper efficient points in the efficient line. Our results are stablished in the context of arbitrary normed spaces and are free of convexity assumptions. We extend some previous results established under more restrictive conditions.,Some New Henig-type Existence and Density Results in Non-Convex Vector Optimization Problems,"[50283, 50285]",295,"[19, 81, 52]",357,Variational Methods in Vector Optimization,82,8,42,Variational Analysis and Continuous Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,98 [building - 306],"['Continuous Optimization', 'Non-smooth Optimization', 'Global Optimization']",TB-42
"Nowadays green loans have been widely promoted to support the capital-constrained supply chain under carbon regulation. But, the borrowers in the supply chain may use the preferential green loans for other purposes which are not related to green development. That phenomenon is mainly attributed to the lack of valid information investigation under traditional information technology. Blockchain, as an innovative technology, can provide valid financial information and identify liability in the use of green loans, which is an effective tool to restrict the misuse behavior. But will the high operating cost weaken the advantage of blockchain? To address this question, we consider a supply chain with a supplier and a capital-constrained manufacturer. The manufacturer may misuse part of the green loans to purchase primary components from the supplier. Two operational modes corresponding to no-blockchain adoption  and blockchain adoption  are designed, in which the supplier determines the wholesale price, and the manufacturer determines the order quantity under green loans. By comparing the optimal decisions of supply chain members, we put forward and differentiate three types of effects - the carbon regulation effect, the information verification effect, and the blockchain cost effect, which influence the adoption of blockchain technology in the supply chain. In addition, we show that blockchain can restrict the misused amount of green loans and improve the environmental performance.",Influences of Blockchain Technology on Restricting the Misuse of Green Loans  in a Supply Chain,"[58417, 68722]",560,"[138, 50]",358,Game Theory in Sustainable Supply Chains,19,14,24,Sustainable Supply Chains,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,83 [building - 116],"['Supply Chain Management', 'Game Theory']",WC-24
"Production scheduling seeks to identify effective job execution plans on machines to enhance overall operational efficiency. Prior studies in this area usually assume job processing rates [JPRs] to be constant. However, based on our analysis of real-world production data, multiple factors such as machine use, operator skills, material supplies, and the processing of prior jobs may influence the actual processing of a job in manufacturing operations. To catch the effect of production variabilities and improve scheduling solutions, we introduce a measure named context-aware processing rate [CAPR] in production scheduling. Based on this measure, a novel deep learning-empowered CAPR-guided scheduling approach is developed. Specifically, the CAPR is predicted for each processing context produced along with the pricing process. The CAPR is used to find beneficial allocation and sequencing of jobs to operators and machines with an efficient labelling algorithm. Experiments based on real-world data from our studied printing company show that the proposed method enables a substantial decrease in overall job completion time.",Context-aware processing rate guided production scheduling - a forecasting embedded branch-and-price heuristic method,"[75856, 76254, 76257]",408,"[69, 8, 13]",360,"Advancements of OR-analytics in statistics, machine learning and data science 4",16,5,28,"Advancements of OR-analytics in statistics, machine learning and data science","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,065 [building - 208],"['Manufacturing', 'Artificial Intelligence', 'Column Generation']",MD-28
"In this presentation, we will demonstrate that, given a separation property of cones, a $\mathcal{Q}$-minimal point in a normed space is the minimum of a specific sublinear function. This observation provides sufficient conditions, via scalarization, for several types of proper efficient points. Furthermore, we will explore necessary and sufficient conditions for approximate Benson and Henig proper efficient points expressed in terms of scalarization. The separation property we consider is a variant of a previously introduced property that had been applied in the setting of reflexive Banach spaces.  ",Scalarizations obtained through a  property of separation of cones,"[50285, 50283, 70738]",295,"[19, 21]",362,Variational Methods in Vector Optimization,82,8,42,Variational Analysis and Continuous Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,98 [building - 306],"['Continuous Optimization', 'Convex Optimization']",TB-42
"Traceability — the ability to track products from origin to consumption — is viewed as an essential first step in effective control and compliance relating to environmental and labour obligations along the supply chain. Despite such benefits, the widespread adoption of traceability technologies in supply chains is hindered by challenges including the high costs of adoption and the misalignment between entities incurring these costs and those reaping the benefits. We consider a network game-theoretic model of traceability technology adoption with possibly overlapping supply chains of multiple consumer-facing firms. We characterize the network-optimal adoption strategy with a tractable integer linear formulation. We then develop a cost sharing mechanism implementable via transfer payments to upstream suppliers. The cost sharing mechanism satisfies certain formal fairness properties, and, when efficient, supports the network optimal adoption strategy, and when inefficient, supports an equilibrium welfare which can be less than the network optimal welfare, and the ratio [a measure analogous to the price of anarchy for an associated non-cooperative game] is examined numerically using real-world supply chain structure data.  A central theme of our work is to advance cost sharing, implementable via simple upstream transfer payments, as a complementary solution to the seeding strategy to initiate the diffusion of traceability technology in supply chains.",Cooperative Adoption of Supply Chain Traceability,"[67079, 76255, 74719]",11,"[138, 139, 50]",364,Sustainable Supply Chain Management,19,12,24,Sustainable Supply Chains,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,83 [building - 116],"['Supply Chain Management', 'Sustainable Development', 'Game Theory']",WA-24
"This talk deals with a pairwise comparison matrix of the analytic hierarchy process. We show the convergence results of Newton's and secant methods for finding the maximum eigenvalue of the pairwise comparison matrix. In recent years, we have shown similar results for lower-order [3-rd and 4-th order] pairwise comparison matrices. The proof was based on our representation formula for characteristic polynomials. In this talk, we will report that we can extend the result to the general order of the pairwise comparison matrix. The proof relies intensely on the Gauss-Lucas’ theorem on the zeros of polynomials and their derivatives.
We also give computational simulation results for 4-th order pairwise comparison matrices. We randomly provide pairwise comparison matrices of Saaty's discrete scale and count the iteration numbers of Newton's, secant, and bisection methods, respectively.
The simulation gives insight into the superiority of Newton's method compared to the secant and bisection methods. ",Computation of the Maximum Eigenvalue of the Pairwise Comparison Matrix in the Analytic Hierarchy Process,"[76247, 76233]",122,"[6, 0]",365,Pairwise comparisons and preference relations 1,44,9,44,Multiple Criteria Decision Analysis,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,20 [building - 324],['Analytic Hierarchy Process'],TC-44
"The widespread adoption of the Internet and social networks has resulted in an explosion of data from diverse sources, often causing information overload. Consequently, a growing need for text summarization has emerged to assist users in quickly digesting information. Additionally, many practical scenarios require comparative opinions from multiple perspectives. However, most research tends to address issues from a single viewpoint. 
The aim of this study is to propose a graphical text summarization approach that integrates multiple perspectives through the utilization of text mining and goal-programming optimization methods. From the idea of a data warehouse schema, this study constructs three-layer heterogeneous graphs to represent entities, nouns, adjectives, and their interrelationships. Star and Snowflake diagrams are constructed to show relationships between nouns and adjectives. Additionally, a constellation diagram is introduced to visualize the relationships among different entities.
The proposed approach offers several significant advantages. It provides a straightforward and easily understandable method to summarize a large amount of text content on a single graph. Users can, therefore, significantly reduce the time spent individually browsing numerous documents or reviews. Decision-makers can compare the strengths and weaknesses of different entities from an integrated graph or understand the differences in user opinions from various perspectives.
",Applying Mathematical Programming to Visualize Text Summarization from Multiple Perspectives,[12083],726,"[7, 72, 53]",367,Optimization in Online Environments,14,3,03,Data Science Meets Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,1005 [building - 202],"['Analytics and Data Science', 'Mathematical Programming', 'Graphs and Networks']",MB-03
"The uneven allocation of water resources leads to hydropower instability. Therefore, Taiwanese policy-makers must find ways to more effectively reallocate water resources among hydroelectric reservoirs. Since hydroelectric reservoirs are managed by the centralized administration, Taiwanese policy-makers can pursue the overall performance of all hydroelectric reservoirs. Conventional centralized data envelopment analysis [DEA] models focus on the reallocation of each resource input based on the expansion of total outputs. However, these centralized models rarely consider that some resource inputs are closely related to certain outputs. To bridge this gap, this paper presents a centralized DEA model with non-separable. The proposed models aim to address the planning for reallocating water resources among hydroelectric reservoirs in Taiwan.",Water Resource Reallocation Planning with Integrated Hydroelectric Reservoirs in Taiwan - Constructing a Centralized Data Envelopment Analysis Model with Non-separable,[31411],940,"[24, 37, 78]",369,DEA applications in Environment and Sustainability I,89,8,48,Data Envelopment Analysis and its Application,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,60 [building - 324],"['Data Envelopment Analysis', 'Energy Policy and Planning', 'Natural Resources']",TB-48
"How can we arrange spheres in a way such that they fill out as much space as possible? This age-old mathematical question has a lot of applications, not only in mathematics, but also in physics and other areas of natural science. The problem has been studied in many interesting spaces, for example in finite fields, on the sphere and in the Euclidean space. In recent times, a lot of progress has been made, for example by the winner of the Fields medal Maryna Viazovska, who solved the problem in the eight dimensional Euclidean space.
In my talk, I want to consider this problem on spaces that are given by the Cartesian product of spheres. We will look at a family of dense packings and discuss how to verify their optimality using semidefinite programming, harmonic analysis and sum-of-squares techniques. Furthermore, we will relate this problem to other packing problems in different spaces.",The sphere packing problem on Tori,[75041],378,"[115, 0]",370,Recent advances in LP and SDP for discrete optimization problems,68,12,38,"Conic Optimization - Theory, Algorithms, and Applications","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,34 [building - 306],"['Programming, Semidefinite']",WA-38
"Improving human behavior requires first understanding it. Can mathematical models help provide such understanding? Yes. Cognitive Operations performs a systematic examination of two main approaches to modeling human decision making - optimization and simple heuristics. The approaches are discussed by focusing on general types of decisions—under risk, under uncertainty, strategic interaction and inventory control—and by drawing on economics, psychology and artificial intelligence. Cognitive Operations is committed to supporting academics and practitioners who seek to select a modeling approach that suits the operational decision at hand. It shows how to build models and employs clear criteria for assessing them. For a model to open the black box it must specify the cognitive processes that lead to the observed decisions. To predict decisions means to fix a model in one context and to test it in another. The comparative assessment of optimization and heuristic models serves scientific pluralism and leads to surprising insights. The studies in Cognitive Operations zoom in on bounded rationality, a field pioneered by Herb Simon, who was a founder of behavioral economics, cognitive psychology as well as artificial intelligence, and sometimes also wear the hat of an operations researcher. They are useful complements to empirical, experiential and conceptual studies of behavioral operations. It is possible to open the black box and predict our decisions.  
",Cognitive operations - Models that open the black box and predict our decisions  ,[3179],113,"[10, 25, 8]",371,Heuristics in BOR,13,15,11,Behavioural OR,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,12 [building - 116],"['Behavioural OR', 'Decision Analysis', 'Artificial Intelligence']",WD-11
"To model the physics of the used vehicles in trajectory planning problems, Newton's equations of motion must be incorporated, discretized by numerical solution schemes. Thus, the position of the vehicle is in the model known only at a discrete set of points. Due to this lack of information, corner cutting can occur, i.e., the vehicle can abbreviate through an obstacle if it remains outside at every evaluation point again. Increasing the number of discrete time steps cannot resolve this problem completely and leads to higher computation times. To obtain always realistic solutions, corner cutting must be restricted, yielding so called inter-sample avoidance methods. We present a new approach to ensure inter-sample avoidance for polyhedral obstacles. The hyperplanes describing the obstacle divide the mission space into a set of subareas which can be encoded by binary vectors. These are used to ensure feasible movements without requiring more evaluation points or additional variables. We give a procedure to generate an integer polytope containing the feasible movements and derive an upper bound on the number of necessary constraints to describe it. The new approach is qualitatively tested against other inter-sample avoidance methods from the literature and its computational efficiency is studied in numerical experiments, using a mixed-integer linear program for trajectory planning of unmanned aerial vehicles and the state-of-the-art solver Gurobi.",A new approach for inter-sample avoidance using an integer polytope formulation,"[62257, 16315]",196,"[111, 145, 143]",373,"Discrete, continuous or stochastic optimization and control in networks, transportation and design I ",64,2,25,Combinatorial Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,011 [building - 208],"['Programming, Mixed-Integer', 'Vehicle Routing', 'Transportation']",MA-25
"Dynamic pricing and quality are important strategic variables for firms. 
However, the effects of consumers' objective and subjective quality evaluations on dynamic pricing must be better understood. This article develops an optimal control model of a firm's dynamic pricing and objective quality investment decisions over time. Our research contributes to the literature on the price-quality relationship by providing a dynamic pricing rule and identifying the conditions below which more significant objective and subjective quality may lead to an increase or decrease in the price. 
Subjective quality impacts dynamic price but not objective quality. In addition, a negative price-quality relationship is possible with both objective and subjective quality. Consequently, the distinction between objective and subjective qualities yields more profitable marketing mix strategies.","Dynamic Pricing, Objective and Subjective Quality, and the Price-Quality Relationship","[39131, 76262, 76261]",847,"[71, 120, 20]",374,Optimal control in organizations,90,5,33,Optimal Control Theory and Applications,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,42 [building - 303A],"['Marketing', 'Quality Management', 'Control Theory']",MD-33
"Infectious diseases cause more than 25% of global deaths, but appropriate treatment can reduce the impact of epidemics. To account for treatment availability, we develop a networked metapopulation Susceptible-Exposed-Infected-Treatment-Recovered [SEITR] model that depends on supply chain inventory policies. The focus of this paper is on inventory policies during preparation [Prepositioning] and response [Reorder Point, Ordering, Inventory Rationing, and Delivery Scheduling] to the epidemic. Our approach involves Python-based simulations with customized Latin Hypercube sampling to sample parameter-policy combinations and Monte Carlo techniques to account for randomness in the model. Using the case of Uganda's three-tier medical supply chain, we analyze the impact of different parameter-policy combinations on epidemic outbreaks. The analysis quantifies this impact through key performance indicators [KPIs] from epidemics and supply chains, including final epidemic size, treatment ratio, outbreak duration, stockouts, service levels, and inventory left over. We will identify optimal policy-parameter combinations that improve responses to future epidemic outbreaks within specific categories of epidemics. We aim to provide actionable insights to enhance Uganda's national inventory policy for public health emergencies while contributing to the literature by providing insights about the most influential policies and parameters.",Influence of Inventory Policies on Epidemic Progression in Networked Metapopulations,"[76263, 17140]",554,"[58, 131, 61]",376,Infectious diseases and pandemics,38,13,21,OR in Humanitarian Operations [HOpe],"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,49 [building - 116],"['Humanitarian Applications', 'Simulation', 'Inventory']",WB-21
"Time series models face structural instability when applied to real data. Usually, studies yield to applications without the consideration of breaks leads to unreliable results. 
Progress has been achieved in the theory of identifying, estimation and testing of structural instabilities. Our experimental study shows who efficient the so-called bounds & likelihood heuristic can be. The subjects quickly identify breaks and are able to adapt their forecasts.

Time series models face structural instability when applied to real data. Usually, studies yield to applications without the consideration of breaks leads to unreliable results. 
Progress has been achieved in the theory of identifying, estimation and testing of structural instabilities by our new procedure. The experimental studies show how efficient the so-called bounds & likelihood heuristic can be. The subjects quickly identify breaks and are able to adapt their forecasts in a surprisingly good manner. The subjects can be well explained by the b&l model despite the occurrence of structural breaks.
The bounds & likelihood heuristic manages to model average [not individual] forecasts.

",Forcasting behaviour in instable environments,[281],185,"[10, 72, 16]",377,Experimental economics and game theory 1,73,13,40,Experimental economics and game theory,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,96 [building - 306],"['Behavioural OR', 'Mathematical Programming', 'Complexity and Approximation']",WB-40
"By studying the history of scientists respectively scientific discipline developments a series of background issues can be explained. 
Jakob Krarup is THE person who was able to connect his ideas with other disciplines as well as being the key to future development directions. Operations research as a quite young discipline, nevertheless full of contradictions, makes it mandatory to be overviewed by excellent connoisseur like Jakob had been. He was able to describe the evolution of OR in terms of developing applied methods of decision making in socio-technical systems, by using new methods explaining contemporary demand of army, society, industry, economics as well as politics. 
As a well known scientist, as an unique teacher and a really good friend the university of Graz will always remember him. 
The Austrian OR society ÖGOR is proud to list him as a honorary member since 2020.",Jakob Krarup as permanent guest professor at Karl Franzens University Graz in Austria,[281],983,"[88, 0]",378,Memorial Session for Jakob Krarup,66,5,65,Memorial Session for Jakob Krarup,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,R021 [building - 358],['OR History'],MD-65
"Smart ports offer a paradigm shift in the constantly evolving marine logistics industry, utilizing cutting-edge technology to enhance efficiency and sustainability. However, accurately measuring innovation performance in these complex and uncertain situations requires a creative and adaptable approach. This study presents a Neutrosophic Approach to measuring innovation success at smart ports while acknowledging and quantifying the inherent uncertainties. Our research incorporates neutrosophic set theory, which is a mathematical framework capable of handling imprecise and indeterminate data. This methodology enables the consideration of unpredictable factors that influence innovation in smart ports, such as technological disruptions, legislative changes, and market dynamics. By employing neutrosophic logic, the model captures the inherent ambiguity and vagueness associated with evaluating innovation performance in this rapidly evolving market. The study approach involves a comprehensive assessment of key performance indicators [KPIs] related to smart port innovation, accounting for both standard metrics and emerging indicators specific to the digitalization and automation of port operations. Neutrosophic language variables are utilized to measure the degree of truth, indeterminacy and falsehood associated with each KPI, facilitating a more nuanced evaluation by providing a comprehensive understanding of innovation success in smart ports amidst uncertainty. ",Measuring innovation performance in smart ports under uncertainty -  A neutrosophic approach,"[76090, 67314]",154,"[70, 49, 77]",380,Port Performance,52,15,62,OR in Port Operations,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S12 [building - 101],"['Maritime applications', 'Fuzzy Sets and Systems', 'Multi-Objective Decision Making']",WD-62
"In this paper, we study the assignment and routing of service professionals for serving customers of an on-demand home services platform considering the Triple Bottom Line [TBL] criteria for ensuring sustainability in operations. We characterise this as the Home Services Assignment and Routing Problem with the Triple Bottom Line [HSARP-TBL] and implement a Mixed Integer Linear Programming [MILP] model for solving it. We assign service professionals to customers based on their desired time slots and also transport modes for each customer visit by a professional, considering either combinations of public transport or a personal vehicle for each professional’s tour. The objective is to minimize costs due to time window violations and uncovered customers, catering to the economic pillar of the TBL. We incorporate additional constraints based on the TBL by - improving customer satisfaction based
on the ratings of assigned professionals to customers, with and without subscription [economic], controlling emissions due to transportation of professionals [environmental] and ensuring equity in service allocation
and net earnings between professionals [social]. For tackling large instances we implement a Hybrid Genetic Search [HGS] algorithm adapting it to our problem setting. We demonstrate that the HGS outperforms the
MILP model systematically for large instances in terms of solution value and computational time. ",A Triple Bottom Line optimization model for assignment and routing of on-demand home services,"[69609, 4914, 23193]",198,"[129, 100, 143]",382,Combinatorial optimization approaches for freight deliveries and home services,64,2,52,Combinatorial Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,8003 [building - 202],"['Scheduling', 'OR in Sustainability', 'Transportation']",MA-52
"In this work, we introduce a novel predict-and-optimize method for profit-driven churn prevention. We frame the task of targeting customers for a retention campaign as a regret minimization problem. The main objective is to leverage individual customer lifetime values [CLVs] to ensure that only the most valuable customers are targeted. In contrast, many profit-driven strategies focus on churn probabilities while considering average CLVs. This often results in significant information loss due to data aggregation. Our proposed model aligns with the guidelines of Predict-and-Optimize [PnO] frameworks and can be efficiently solved using stochastic gradient descent methods.",Profit-driven churn prevention through predict-and-optimize,"[70806, 18420, 57734]",392,"[66, 7]",383,Analytics for Decision Making,17,12,31,Analytics,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,54 [building - 208],"['Machine Learning', 'Analytics and Data Science']",WA-31
"We consider Benders' decomposition algorithms with affine optimality cuts for multistage stochastic mixed-integer programs. Exploiting extended spaces, we derive a hierarchy of convex polyhedral lower bounds for the expected cost to-go functions in the problem. These lower bounds, however, are not tight for mixed-integer state variables in general. That is why, we also introduce so-called scaled cuts. The advantage of these scaled cuts is that they allow for parametric non-linear feasibility cuts in later time stages, but that the optimality cuts for the expected cost to-go functions remain linear. We establish convergence by proving that the optimality cuts in the first stage recover the convex envelope of the first-stage expected cost to-go function.",Scaled Cuts for Multistage Stochastic Mixed-Integer Programs,[44993],301,"[117, 0]",385,Optimization under uncertainty - theory and solution algorithms,49,7,35,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,44 [building - 303A],"['Programming, Stochastic']",TA-35
"We developed portfolio selection models with heterogeneous ellipsoidal ambiguity sets of correlated returns and a performance ratio without a normality assumption and explained the equity home puzzle through the ambiguity channel. First, we develop a parsimonious model without risk or ambiguity aversion parameters and show it to be consistent with second stochastic dominance. We generalize the model for continuous ambiguity aversion. These models account for perceived ambiguity [beliefs] and ambiguity aversion [tastes] as potential puzzle explanations.

Taking the models to the data, we obtain optimal allocations matching those of international investors in 21 developed and 19 emerging markets. This is shown both for worst-case ambiguity aversion under perceived ambiguity well within the market estimat ambiguity sets or for mild ambiguity aversions given the market-estimated ambiguity sets. The global average ambiguity aversion is about 0.6, statistically different from the ambiguity-neutral 0.5. Our estimates of ambiguity aversion from the observed asset allocations for a large sample of countries closely match the scant experimental evidence from US, Dutch, and Chinese population samples. Our findings are robust to two fundamentally different methods for measuring ambiguity.

The worst-case model applied to a homogeneous ambiguity set predicts lower ambiguity with less diversified portfolios. We verify this on a large dataset of US household portfolios, and by running
",An explanation of under-diversification puzzles through ambiguity tastes and beliefs,"[676, 67869]",275,"[83, 45, 33]",389,Optimization Model for Novel Risks in Finance and Climate,4,5,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S14 [building - 101],"['Optimization in Financial Mathematics', 'Financial Modelling', 'Economic Modeling']",MD-63
"We consider manipulations in the context of coalitional games, where a coalition aims to increase the total payoff of its members. An allocation rule is immune to coalitional manipulation if no coalition can benefit from internal reallocation of worth on the level of its subcoalitions [reallocation-proofness], and if no coalition benefits from a lower worth while all else remains the same [weak coalitional monotonicity]. Replacing additivity in Shapley's original characterization by these requirements yields a new foundation of the Shapley value, i.e., it is the unique efficient and symmetric allocation rule that awards nothing to a null player and is immune to coalitional manipulations. We further find that for efficient allocation rules, reallocation-proofness is equivalent to constrained marginality, a weaker variant of Young's marginality axiom. Our second characterization improves upon Young's characterization by weakening the independence requirement intrinsic to marginality.",Coalitional Manipulations and Immunity of the Shapley Value,"[45947, 76280]",385,"[50, 66, 71]",394,"Game Theory, Solutions and Structures I",88,2,36,"Game Theory, Solutions and Structures","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,32 [building - 306],"['Game Theory', 'Machine Learning', 'Marketing']",MA-36
"Both sudden-onset and slow-onset disasters are causing disruptions to global trade, impacting the availability and affordability of commodities from agricultural to mineral ones. In this paper, we develop a multicommodity international trade network equilibrium model under disaster scenarios with distinct probabilities of the occurrence of the disasters and their impacts on the capacities associated with production, transportation, and consumption. The disaster scenarios can also affect the exchange rates. We state the governing equilibrium conditions and derive the variational inequality formulation in commodity path flow variables and Lagrange multipliers associated with the capacity constraints. For each disaster scenario, we construct an international trade network performance measure, an international trade network performance measure, followed by a unified performance measure that includes all the disasters and their probabilities. Robustness is then quantified as the difference between the network performance under no disruptions and the unified performance measure. An international trade network component performance indicator is also given to assess the impacts of the complete removal of trade network supply markets, demand markets, and/or transportation routes. The modeling framework is then illustrated through a series of numerical examples, motivated by Russia’s war on Ukraine. The work is of relevance to decision-makers and policy-makers.","Quantification of International Trade Network Performance Under Disruptions to Supply, Transportation, and Demand Capacity, and Exchange Rates in Disasters","[46334, 76283, 76284, 76285]",556,"[15, 58, 89]",397,Complex societal problems,38,10,21,OR in Humanitarian Operations [HOpe],"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,49 [building - 116],"['Complex Societal Problems', 'Humanitarian Applications', 'OR in Agriculture']",TD-21
"Classification has been widely used to categorize the existing instances [i.e., data points] and predict the new instances in various machine learning applications such as fraud detection in financial sector, fault and defect detection in manufacturing industry, and medical diagnosis, etc. However, most classification algorithms have been developed under the assumption that the data distribution of among classes is balanced even though unequal class distributions are quite common. So, the class imbalance problem may affect the performances of many machine learning tasks. Furthermore, the class overlap problem occurs when some instances are located in a certain common region in the data space and it also has a significant impact on the performance in imbalanced classification problems. Thus, in this study, we propose a new hybrid resampling algorithm to improve the performance of imbalanced classification problem by resolving both class imbalance and class overlap problems in imbalanced data simultaneously. In particular, we propose a class impurity measure based on k-nearest neighbor [k-NN] algorithm for adaptively oversampling and under-sampling. To demonstrate the effectiveness of the proposed method, comprehensive experiments are executed on forty imbalanced datasets with considering four classifiers and three classification performance measures.
[This work has been supported by the General Research Program funded by NSTC, Taiwan [Grant No. NSTC 110-2221-E-027-106-MY3]]",A hybrid resampling method for imbalanced classification problem,[29239],408,"[8, 25, 62]",402,"Advancements of OR-analytics in statistics, machine learning and data science 4",16,5,28,"Advancements of OR-analytics in statistics, machine learning and data science","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,065 [building - 208],"['Artificial Intelligence', 'Decision Analysis', 'Knowledge Engineering and Management']",MD-28
"Emergency response refers to the systematic response to an unexpected, disruptive occurrence such as a natural disaster. The response aims to mitigate the consequences of the occurrence by providing the affected region with the necessary supplies. A critical factor for a successful response is its timely execution, but the unpredictable nature of disasters often prevents quick reactionary measures. Preallocating the supplies before the disaster takes place allows for a faster response, but requires more overall resources because the time and place of the disaster are not yet known. This gives rise to a trade-off between how quickly a response plan is executed and how precisely it targets the affected areas. Aiming to capture the dynamics of this trade-off, we develop a $K$-adjustable robust model, which allows a maximum of $K$ second-stage decisions, i.e., response plans. This mitigates tractability issues and allows the decision-maker to seamlessly navigate the gap between the readiness of a proactive yet rigid response and the accuracy of a reactive yet highly adjustable one. The approaches we consider to solve the $K$-adaptable model are threefold - Approximately, via a partition-and-bound method, and optimally via a branch-and-bound method as well as a static robust reformulation in combination with a column-and-constraint generation algorithm. In a computational study, we compare and contrast the different solution approaches and assess their potential.",Planning effective emergency responses - K-adjustable robust optimisation for relief prepositioning,[57798],389,"[127, 72, 58]",404,"Disaster Response - Search and Rescue, Resource Allocation and Impactful Prepositioning",38,4,21,OR in Humanitarian Operations [HOpe],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,49 [building - 116],"['Robust Optimization', 'Mathematical Programming', 'Humanitarian Applications']",MC-21
"We study the problem of sharing the revenues raised from subscriptions to music streaming platforms among content providers. We provide direct, axiomatic and game-theoretical foundations for two focal [and somewhat polar] methods widely used in practice - pro-rata and user-centric. The former rewards artists proportionally to their number of total streams. With the latter, each user's subscription fee is proportionally divided among the artists streamed by that user. We also provide foundations for a family of methods offering a compromise among the previous two, which addresses the rising concern in the music industry to explore new streaming models that better align the interests of artists, fans and streaming services.",Revenue sharing at music streaming platforms,"[11527, 29054]",384,"[50, 33, 25]",406,"Game Theory, Solutions and Structures II",88,3,36,"Game Theory, Solutions and Structures","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,32 [building - 306],"['Game Theory', 'Economic Modeling', 'Decision Analysis']",MB-36
"When considering a convex composite minimization problem, it is well known that momentum allows first-order methods to be accelerated both theoretically and numerically. In particular, we know that for a suitable choice of parameters inertial methods ensure fast convergence rates under additional geometry assumptions such as strong convexity. However, the improved convergence results demonstrated in the literature hold only if the function to minimize has a unique minimizer. This extra assumption is limiting, since some common functions [such as the LASSO function or $L^1$ regularized functions in general] can satisfy some growth condition without having a unique minimizer. The question then arises - is this assumption necessary to prove fast convergence properties? 

We propose an approach that aims to avoid this hypothesis while still obtaining fast convergence rates. This strategy allows to extend several known convergence results in the continuous setting [i.e. for dynamical systems associated to inertial schemes]. We also provide fast convergence guarantees  for the iterates of FISTA [introduced by Beck and Teboulle] and V-FISTA [proposed by Beck] in a relaxed setting, showing that this uniqueness assumption is not required for inertial methods to be efficient.",Inertial methods beyond minimizer uniqueness,[76295],208,"[5, 0]",407,Algorithms for machine learning and inverse problems - adaptive strategies,84,5,32,Advances in large scale nonlinear optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,41 [building - 303A],['Algorithms'],MD-32
"The proactive and reactive resource-constrained project scheduling problem [PR-RCPSP], introduced by Davari and Demeulemeester [2019b], addresses uncertainties in real-world projects in a novel manner. A PR-RCPSP solution is a proactive and reactive policy [PR-policy] that includes the baseline schedule and foresees potential transitions [reactions] to other schedules. In their subsequent work, Davari and Demeulemeester [2019a] categorize reactions into selection-based and buffer-based. While both classes are crucial, buffer-based reactions hold greater significance. Recognizing the theoretical importance and managerial value of the buffer-based reactions, we solve the PR-RCPSP using a purely buffer-based approach. To achieve this goal, we propose a method for generating a sufficient selection, along with proactive and reactive procedures, among other novel heuristics. We construct schedule pools from which we obtain optimal PR-policies. Experimental results show that our solutions outperform the best existing alternatives, particularly when reactions are less preferred, all while significantly reducing computational time. Remarkably, our purely buffer-based approach exhibits exceptional performance in instances featured by high resource factors or low resource strengths, situations where solving their deterministic counterparts presents greater challenges. Our results offer valuable managerial insights for navigating projects under uncertainty.",A purely buffer-based approach to the proactive and reactive resource-constrained project scheduling problem,"[76300, 41246, 26508]",348,"[118, 5, 151]",415,Project scheduling under uncertainty,35,4,60,Project Management and Scheduling,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S09 [building - 101],"['Project Management and Scheduling', 'Algorithms', 'Practice of OR']",MC-60
"We consider the multichannel healthcare delivery problem during a pandemic and explore capacity allocation and pricing decisions for the in-person and telemedicine channels. We propose a model that explicitly considers patient choices between the channels, including the fear of contracting the disease in an in-person visit. The effect of contagion on the patient’s channel choice is endogenous. Choices drive in-person volume which in turn drives the contagion effect. We show that in equilibrium when the potential demand increases linearly, the in-person demand increases logarithmically due to the endogenous contagion effect. This explains the documented revenue losses of healthcare systems during the COVID-19 pandemic and underlines the tele-channel's importance as a leading revenue source. Under optimal pricing, we find a positive price gap between the channels that increases in contagion awareness of the patients, a behavioral parameter in the model. In comparison with a revenue-maximizing healthcare provider that prefers the higher-priced in-person channel, when physician infection is also considered, a social planner is shown to allocate more capacity to telemedicine thereby protecting physicians. Modulating channel choice by increasing patients' contagion awareness can be beneficial for both revenue and welfare. Even partial awareness can go a long way in compensating the revenue losses, despite the negative impact of contagion awareness on total demand in equilibrium.",Contagion-aware patient choice - managing multichannel healthcare delivery during a pandemic,[76205],595,"[56, 130, 10]",418,COVID-19,3,13,15,OR in Health Services [ORAHS],"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,18 [building - 116],"['Health Care', 'Service Operations', 'Behavioural OR']",WB-15
"We investigate two broad categories of resource allocation problems in which resources can be activated to fulfill the demand of one or multiple customers, respectively, with the objective of minimizing the total cost. Each resource incurs a fixed activation cost, and for each resource-customer pair, there is a variable allocation cost represented by separable convex functions. The traditional mixed-integer programming models for these problems are enhanced using the technique of perspective reformulation, which involves substituting each term in the separable objective function with its perspective term. In this work, we have developed a branch-and-Benders-cut algorithm to address the perspective reformulation models of these two general problem classes. Our approach incorporates two novel features. Firstly, a family of generalized Benders optimality cuts expressions suitable for perspective reformulation models, referred to as perspective Benders cuts, is derived, which are tighter than classic generalized Benders cuts. Secondly, we have developed a unified ad-hoc procedure, named reduce and recover, to efficiently separate perspective Benders cuts by solving the reduced quadratic subproblems. These features have led to a highly efficient solution method on these two general problem classes, which is verified through extensive computational experiments based on benchmark instances with four different classes of nonlinear allocation functions. ",Perspective Benders Cuts for Resource Allocation Problems with Separable Convex Costs,"[76305, 76119]",720,"[11, 111, 113]",419,Mixed Integer Nonlinear Programming and Nonconvex Optimization ,86,13,04,MINLP,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1001 [building - 202],"['Branch and Cut', 'Programming, Mixed-Integer', 'Programming, Nonlinear']",WB-04
"We consider the variable-size mixed-variable black-box optimization, which involves variable dimensions and optimizing a diverse and numerous sets of variable types through a black-box function. In this study, we extend the mixed-variable black-box optimization problem and use estimation methods to achieve more efficient optimal solution search. This approach can be applied to find the equipment configuration of a central air conditioning system that provides an optimal configuration that minimizes overall installation and operating costs.",Variable-size mixed-variable black-box optimization for central air conditioning configuration,"[76026, 76347, 2102]",74,"[18, 14, 19]",424,"Advancements of OR-analytics in statistics, machine learning and data science 3",16,4,28,"Advancements of OR-analytics in statistics, machine learning and data science","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,065 [building - 208],"['Computer Science/Applications', 'Combinatorial Optimization', 'Continuous Optimization']",MC-28
"Recent studies point to weather dependency of renewable electricity sources as a limitation to the system’s resilience in case that high shares of renewable electricity are reached. The impact of weather is, however, highly variable in space and time, making high-resolution resilience assessment a necessity. This study aims to test Swiss electricity system in 2035 against 25 years of historical weather conditions that affect hourly profiles of solar photovoltaic [PV], wind power, and hydropower, as well as electricity demand for heating. In particular, four strategies of locating new solar PV, wind plants, and heat pumps in Switzerland are investigated, based on [i] continuation of current trends, [ii] technical potential, [iii] population, and [iv] minimum system cost approach. The analysis is conducted using a technology-rich cost-optimization model EXPANSE that generates scenarios at a temporal resolution of three hours resolution and a spatial resolution of Swiss 2136 municipalities. Six electricity system resilience indicators are then calculated to compare the four strategies - equivalent availability factor, energy import dependency, diversification of energy supply, decentralization index, renewable electricity generation, and self-sufficient electricity supply.  ",Assessing weather resilience of the Swiss electricity system in 2035 via spatially-explicit modeling ,"[76091, 70894]",682,"[37, 84, 93]",425,Sustainable Energy,80,4,53,Sustainable and Resilient Systems,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,8007 [building - 202],"['Energy Policy and Planning', 'Optimization Modeling', 'OR in Energy']",MC-53
"The exponential cone is an absolute beast with respect to its usages in mathematical modeling, and has proven to be a computational stable and high-performant element of the MOSEK optimization core. In this talk we will explore some of its surprising use cases, e.g. in machine learning, before unveiling the successful story of the projection onto the exponential cone algorithm. The Moreau decomposition theorem, the KKT conditions, the convex analysis book by Rockafellar, unconstrained root finding and floating-point safeguards are all parts of this story.",The exponential cone - machine learning and the projection algorithm,[76306],163,"[72, 21, 18]",427,Applications of conic optimization,68,5,38,"Conic Optimization - Theory, Algorithms, and Applications","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,34 [building - 306],"['Mathematical Programming', 'Convex Optimization', 'Computer Science/Applications']",MD-38
"This talk introduces the global optimization capability within FICO Xpress Solver, which allows to solve general mixed-integer nonlinear problems to global optimality. We will discuss how existing features of the MILP and local NLP solvers are used and explain several different extensions for, e.g., spatial branching and convexification cuts, and their performance implications.",Solving MINLPs to Global Optimality with FICO Xpress Solver,[63850],238,"[134, 52, 113]",430,MINLP Solvers,76,3,30,Software for Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,53 [building - 208],"['Software', 'Global Optimization', 'Programming, Nonlinear']",MB-30
"Increasing demand for healthcare has necessitated the need to ration care resources by prioritization. While such prioritization is effectively implemented in many settings, their use in appointment workflows have been impossible since appointments are provided on a first-come-first-serve basis. We present a novel approach to enable rationing by waiting time in outpatient settings offering clinical appointments. Under the Time Windows Access Protocol [TWAP], scheduling agents are provided a distinct contiguous portion of the booking horizon to search for an available appointment upon an appointment request at from a priority class. Patients with requests from high priority classes [i.e., urgent, complex, high match with provider services] are uniformly incentivized to book and attend appointments with low wait offers, while sufficient dilution of requests from lower priority classes are induced with higher [but still, medically safe] levels of wait. We present a computationally efficient methodology to determine the optimal time windows for given delay-dependent appointment realization probabilities that reflect patients' delay tolerance [i.e., sensitivity to wait]. We demonstrate the potential use and effectiveness of TWAP for new patient appointment requests under different rationing schemes [i.e., the desired patient mix] using simulations as well as real-life transactional data from a leading academic medical center. ",Rationing by Waiting Time for Outpatient Clinical Appointment Workflows via Novel Patient Access Workflows,"[852, 66457, 76309, 51263]",596,"[56, 12, 73]",431,Appointment planning,3,8,15,OR in Health Services [ORAHS],"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,18 [building - 116],"['Health Care', 'Capacity Planning', 'Medical Applications']",TB-15
"OBJECTIVES - this communication aims to discuss how and in which context a random price generator for health care markets can be useful in comparison with other approaches for economic modeling and pricing of medical products and services
METHODS - a comparison of the different approaches used will be discussed with their statistical limitations - nested logit models, cumulative logit models, latent model of choices , shrinkable estimators with Hierarchical Bayesian approach, combination of robust pricing optimization with multinomial logit choice models, and choice models with random price generators and current model application of the BLP model on NGS sequencing.
RESULTS - random generators proved to be useful but do not fit any kind of structural conditions to adjust supply and demand of health re markets in more or less regulated health and economic systems. an approach by sets of alternatives and choice models allow to adjust for heterogeneity of demand but lack of price data in some markets make it useful to rely on shrinkable estimators with HB approaches. In duopoly cases, the use of BLP is currently tested and may help for investigation of antitrust policies.
CONCLUSIONS - the complexity of health care markets lead to consider various combinations of models and even participatory modeling to reach growing consensus with various layers of decision makers both on the payer side and the economic actors switching to data driven models for both drug and vaccines.",Comparison of HB Shrinkable Estimators  and DCE Simulators for Demand for Care ,[56343],957,"[33, 56, 131]",432,MCDA in medicine,44,14,47,Multiple Criteria Decision Analysis,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,50 [building - 324],"['Economic Modeling', 'Health Care', 'Simulation']",WC-47
"In 1982, Chan and Pang introduced the quasi-variational inequality, that is, a Stampacchia variational inequality where the constraints set is subject to modifications depending on the considered point. Nowadays, it provides a broad unifying setting for the study of several mathematical problems and real-world applications.
The classical way to get the existence of solutions consists of requiring that the constraints set-valued map is a self-map; in addition, suitable continuity assumptions and properties on its values are required. However, to deal with the challenges arising from the applications, sometimes it is needed to remove some of these requirements.
With this spirit, the talk aims to discuss some recent developments on the topic.",Quasi-variational inequalities and applications - new challenges on the constraints set-valued map,[66664],30,"[52, 0]",439,Tools and algorithms for equilibrium detection,63,10,40,"Interfaces Between Optimization, Hierarchical Problems and Equilibrium Detection with Applications","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,96 [building - 306],['Global Optimization'],TD-40
"This study examines an emerging type of online authentication platform for luxury products retailing, which has not been fully investigated due to its novel business models and innovative characteristics. While regular online retailing business delivers products directly from the sellers to consumers, such platforms first provide authentication services and then fulfill the orders. On the one hand, authentication builds trust between sellers and consumers to encourage authentic-product-selling behavior and expand demand [shine light], it also helps consumers avoid counterfeits by reducing counterfeit-selling behavior [harness shadows]. On the other hand, it is costly, time-consuming, and sometimes even misidentifies items. To understand whether this lengthy and expensive process benefits both sellers and consumers, we develop queueing models to characterize sellers-authenticator collaboration and its impacts on consumer welfare. The analysis reveals that a collaboration, which increases both the authenticator’s and the sellers’ profit, may hurt consumer welfare due to the authentication fee and waiting, especially at intermediate levels of honesty and market belief. The results are shown to be robust through extensive experiments. As a practical remedy, we recommend that authenticators implement seller management systems to increase the level of honesty, which benefits all stakeholders.",Shining light and harnessing shadows - The dual role of authentication platforms in online luxury retailing,"[76311, 67085]",183,"[130, 10, 121]",442,Behavior in supply chain collaboration,13,14,07,Behavioural OR,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,1019 [building - 202],"['Service Operations', 'Behavioural OR', 'Queuing Systems']",WC-07
"This study determined the performance assessment of the six [6] State Colleges and Universities of Northwestern Philippines vis-à-vis their performance along the following indicators - Graduation Outcomes, Research, Outreach and Inclusivity, and Teaching and Learning Outcomes for five [5] academic years 2016-2020. It utilized the Mamquist DEA Approach, a non-parametric measure. Furthermore, it determined the peer groups and weights of the DMUs [Decision Making Units – the different Colleges and Universities], the virtual inputs/outputs or potential improvements of the colleges/universities to be in the efficient frontier, the input and output slacks [input excesses and output shortfalls] needed in the different indicators and the best practices to be considered by the inefficient and weak efficient DMUs. The “best practice” in the frontier is the basis to calculate the adjustments necessary for the DMUs. Different indicators showed varied performance levels in the different academic years but there are best practices from the “efficient” DMUs which could be adapted by the “weak efficient” and “inefficient” ones. ",PERFORMANCE ASSESSMENT OF HIGHER EDUCATION INSTITUTIONS IN THE PHILIPPINES - A DEA APPROACH,[46614],914,"[24, 35, 29]",443,"OR Initiatives for Education, Sustainability, and Developing Countries",48,5,16,OR Education,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,19 [building - 116],"['Data Envelopment Analysis', 'Efficiency Analysis', 'Development']",MD-16
"The goal of this work is to identify the best Search and Routing Planning [SRP] for an Unmanned Aerial Vehicle [UAV] to maximize the total number of casualties detected in a disaster-affected area within a limited mission duration. First, a Mixed-Integer Non-linear Programming [MINLP] model is formulated. To linearize the model, continuous search time variables are discretized such that each can only take a finite number of possible values. Therefore, the resulting Mixed Integer Linear Programming [MILP] problem is an approximation of the original problem. This approximation is shown to be highly accurate; however, it can not solve problems with more than 10 search regions in less than three hours. Second, an exact solution approach is introduced, which is capable of solving problems of size 14 or smaller during the given computational time limit. Next, an efficient clustering heuristic is suggested to solve larger instances of the original problem with more than 200 search regions. Finally, a case study based on the 2023 Turkey-Syria earthquakes is presented. The results reveal, despite thousands of casualties remaining missing for weeks in the real-life scenario, the proposed solution methods can help detect 60-77% of casualties within a few hours in provinces under study.",UAV Search and Routing Planning in a Disaster Area,"[5144, 76313, 76314, 74595]",204,"[145, 30]",444,Vehicle routing I,64,4,29,Combinatorial Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,157 [building - 208],"['Vehicle Routing', 'Disaster and Crisis Management']",MC-29
"Because a product’s value decays over time, recovered items [cores] cannot regain their original value when remanufactured—even if kept in pristine condition. We consider a Remanufacturing Original Equipment Manufacturer who can remanufacture recovered cores to their original configuration or upgrade them to current technology. He sells new and remanufactured items concurrently. Each production period signifies an upgrade of the product’s features to a new technological generation. 

We use our novel modeling of Marginal Value of Time [MVT] to perform a rigorous structural decomposition over all potentially optimal values of costs, consumers’ valuations, core availability, and MVT to determine conditions under which cores should be restored to their original configuration and those conditions under which cores should be upgraded to current technology. We also show that for only a very small region of the parameter space [1.58% of remanufacturing instances] does concurrently offering both generations of remanufactured units provide greater profit than does remanufacturing to the optimal single generation. Furthermore, the average improvement in these instances is negligible [0.42%]. 
",How Marginal Value of Time Influences Optimality when Remanufacturing to Multiple Generations,[1001],924,"[125, 0]",445,Remanufacturing and refurbishing operations,18,10,23,"Circular Economy, Remanufacturing and Recycling ","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,82 [building - 116],['Reverse Logistics / Remanufacturing'],TD-23
"Dynamic energy tariffs and on-site energy generation offer manufacturers new opportunities to optimize their energy consumption. In addition to conventional goals such as minimizing makespan, manufacturers can focus on minimizing energy costs or emissions. However, scheduling processes in the face of uncertain future energy prices and emissions is a major challenge. Energy storage systems [ESS] can compensate for differences between predicted and actual energy costs and emissions. To make an investment decision for ESS, decision makers need to assess their impact on energy costs and emissions, as well as their return on investment. In the literature, the Green Flexible Job Shop Scheduling Problem [FJSP] is concerned with resource and environmental aspects in addition to economic objectives. However, existing approaches neglect the combination of a multi-criteria objective with an uncertain dynamic energy mix and the use of ESS. We propose a two-phase approach based on a memetic NSGA-III and mathematical programming with the goal of minimizing a schedule’s makespan, energy costs, and emissions, incorporating dynamic energy prices and emissions, on-site generation, and ESS. We evaluate the approach using FJSP benchmark instances from literature as part of a rolling horizon approach with real energy market data. We investigate the impact of ESS by presenting estimated Pareto fronts, showing potential savings in energy cost and carbon emissions.","Multi-Objective Energy-Aware Scheduling - A Memetic Two-Phase Evolutionary Approach incorporating Real-Time Energy Market, on-site Generation, and Storage Systems","[69619, 45137, 45027]",839,"[93, 74, 129]",450,The role of storage in energy problems,23,8,19,OR in Energy,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,44 [building - 116],"['OR in Energy', 'Metaheuristics', 'Scheduling']",TB-19
"Responsible policymakers and transportation planners must ensure safe transportation during large-scale events, especially in urban rail transit systems that are not designed to handle high demand, as overcrowding can lead to dangerous situations and even fatalities. This paper presents an approach to optimize the number of passengers allowed into urban rail transit systems, considering the limitations of trains and station platforms, while also ensuring that as many people as possible can be transported. To achieve this, we developed an optimization model embedded into an iterative heuristic that incorporates the predicted demands, real time-data and the capacity limitations of the entire system. To ensure the accuracy and robustness of our model, we evaluated our approach on a data set based on a comprehensive transportation demand simulation. Our results indicate the effectiveness of our approach. By iteratively optimizing the allowance, we can achieve a good balance between the capacity utilization and the number of people transported. Our system can assist policymakers and transportation planners to control the passenger flows and thereby prevent overcrowding and accidents.
",Coordinated Passenger Flow Control into Urban Rail Transit Systems to Mitigate Overcrowding during large-scale Events,[14588],975,"[65, 143, 30]",455,Transportation Network Modelling and Optimization II,6,3,55,Transportation,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S02 [building - 101],"['Logistics', 'Transportation', 'Disaster and Crisis Management']",MB-55
"The evolution of technology has changed the ways customers engage with retailers. Customers now utilize various sales channels in their buying processes. Consequently, retailers are challenged with customers who tend to switch between various sales channels. Omni-channel retailing allows customers to buy products from anywhere and return them anywhere, while allowing retailers to fulfill orders from anywhere. This flexibility enhances the customer experience by integrating all channels, allows retailers to achieve more availability and boosts the sales and traffic of the retailers. However, it also places significant pressure on retailers to streamline their operations efficiently and expedite product delivery to customers. In this study, we explore two omni-channel implementations, namely, ship-from-store and home delivery. Store customers can opt for in-store fulfillment or request home delivery, whereas online orders can be shipped from the fulfillment center or any other store location that maximizes the retailer’s overall profit. We further consider that both store and online customers can switch across channels. We construct a dynamic programming framework to examine optimal fulfillment decisions for both online and store orders, accounting for uncertainties in both demand and the cost of shipment to individual customers. We present our results through optimal fulfillment strategies and numerical experiments.",Channel Switching in Omni-Channel Retailing,"[23379, 56958]",528,"[61, 138, 108]",456,E-Commerce,30,14,50,Retail Operations,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,M2 [building - 101],"['Inventory', 'Supply Chain Management', 'Programming, Dynamic']",WC-50
"Conflict analysis is a research area that using mathematical models represents and studies the evolution of conflicts in a systematic way. The Graph Model for Conflict Resolution [GMCR] is a successful model in this area due to its flexibility and relative ease of use. In GMCR, many stability concepts try to capture different behavioral characteristics of decision makers [DMs] in conflicts. In this paper, a stability concept is proposed for the GMCR that is useful in applications where DMs do not have full knowledge about their opponents’ preferences. As in real conflicts there are factors that may make it difficult to fully obtain information about others’ preferences, to understand DMs' behavior in such a scenario is valuable. With our proposed stability concept, it is possible to carry out a stability analysis in the GMCR even if only part of the preference information is known. The Variable Horizon with Partially Known Preferences Stability works by anticipating what a given DM believes that final conflict state would be after any chosen number of moves and countermoves following its initial move in a bilateral conflict. The proposed solution concept generalizes two important stability notions, limited-move stability and maximin stability, which are particular cases when DMs have full knowledge and no knowledge at all, respectively. Finally, an application of a dispute among two Brazilian states about a certain geographical area is made with the proposed stability.",A Variable Horizon Stability Concept for Partially Known Preferences in Bilateral Conflicts,"[76287, 76321]",896,"[50, 55, 3]",458,"Game Theory, Solutions and Structures X",88,13,36,"Game Theory, Solutions and Structures","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,32 [building - 306],"['Game Theory', 'Group Decision Making and Negotiation', 'Agent Systems']",WB-36
"The hotel industry faces the uncertainty of reservations and cancellations. Although dynamic pricing provides appropriate discounts for last-minute cancellations, it is not always possible to fill vacancies unless there is sufficient demand. Therefore, it is important to know how the accuracy of cancellation forecasts affects the results of dynamic pricing in practice. In this study, we analyze the relationship between forecast accuracy and expected returns using actual data on room reservations and cancellations in a region in Japan. The results show that a 10% improvement in forecast accuracy results in a 2% improvement in revenue. We also show that this improvement simultaneously improves consumer surplus by lowering the average sales price and improving the load factor. Furthermore, the improvement is more effective during the peak season when the number of potential customers is higher.",Boosting Hotel Profits - The Power of Enhanced Cancellation Predictions,"[76315, 51011]",697,"[124, 7, 8]",460,Pricing and Capacity Management,11,8,59,Pricing and Revenue Management,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S08 [building - 101],"['Revenue Management and Pricing', 'Analytics and Data Science', 'Artificial Intelligence']",TB-59
"In long-term construction projects with complex work interfaces, disputes that arise during construction often leave contractors unable to complete the work within the contractually agreed timeframe. This results in varying degrees of loss, compelling the contractors to seek a resolution of responsibility through litigation to safeguard their rights. Construction project delays have been extensively studied, with researchers often delving into analyses of factors influencing project schedules. The current study examines legal documents from litigation cases to observe the relationship between factors influencing project schedules and the outcomes of disputes commonly seen in litigation. This involves consolidating Supreme Court rulings on project schedule extensions and utilizing text mining tools to analyze and explore the cause-and-effect relationships of schedule-influencing factors in construction dispute litigation cases. The aim is to provide insights into whether resorting to the courts is a valuable option during contract disputes in construction projects. ",Analyzing Delay Litigation Cases of Public Building Construction Using Text Mining Techniques,"[76323, 76324]",959,"[18, 118]",463,"Projects, risk and law",35,8,60,Project Management and Scheduling,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S09 [building - 101],"['Computer Science/Applications', 'Project Management and Scheduling']",TB-60
"We incorporate integrated assessment models [IAM] of climate change in stochastic debt sustainability analysis [DSA] and study the effects of transition risk on sovereign debt dynamics. Following recent literature on the effects of climate transition on default probabilities, we introduce a climate risk premium into standard DSA. We calibrate this climate transition-debt channel for several eurozone sovereigns. A calibration spanning the Paris Agreement documents a significant increase in transition risk following this agreement. We then use ensembles of IAMs to obtain forward-looking projections of the climate premium under the orderly and disorderly scenarios from the Network for the Greening of the Financial System. Using the combined IAM-DSA model, we identify adverse debt effects from the increasing transition risk across countries and all IAMs. The adverse effects materialize earlier under the orderly transition, but under disorderly transition, they are significantly more impactful. We find, with high confidence level, that modest extra fiscal effort can maintain debt sustainability under orderly transition, albeit this is not the case for disorderly transition. Our analysis also identifies significant quantitative differences across IAMs, although the general conclusions remain consistent across models. We identify significantly different effects among countries depending on their level of debt and climate exposure.",Climate transition risk to sovereign debt sustainability,"[76328, 67132, 676]",275,"[94, 45, 117]",467,Optimization Model for Novel Risks in Finance and Climate,4,5,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S14 [building - 101],"['OR in Environment and Climate change', 'Financial Modelling', 'Programming, Stochastic']",MD-63
"Energy poverty stands as a pressing issue across the European Union [EU], casting shadows over public health and hindering sustainable development efforts. Despite its prominence, scholarly literature reveals a dearth of studies quantifying the breadth of fuel poverty within the EU. Remarkably, none have ventured into applying methods from the ELECTRE family. In response to this research gap, our study undertakes the task of classifying energy poverty across EU member states in the year 2020 by employing the innovative ELECTRE-Tri-nC method. To further refine our analysis, we supplement this method with the Simos-Roy-Figueira [SRF] method, aiding in the assignment of weights to our selected criteria. Our model integrates nine distinct criteria to discern the energy poverty levels of 24 EU member states. Unveiling the findings of our comprehensive study, we observe a distinct trend - Eastern and Southern Europe grapple with elevated levels of energy poverty, while the Scandinavian nations and Central Europe demonstrate commendable low levels. Meanwhile, Western Europe and the Baltic countries fall within the moderate spectrum of energy poverty. By shedding light on these disparities, our study not only underscores the urgency of addressing energy poverty but also highlights regional variations that demand tailored interventions and policy considerations. In doing so, we endeavour to catalyse informed discourse and strategic action aimed at alleviating the burdens of energy pove",Mapping Energy Poverty - A Multi-Criteria Classification Framework for the European Union,"[53033, 76329, 24610, 60792, 76330]",308,"[25, 37, 26]",468,MCDA applications,44,8,44,Multiple Criteria Decision Analysis,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,20 [building - 324],"['Decision Analysis', 'Energy Policy and Planning', 'Decision Support Systems']",TB-44
"Floods affect more people every year than any other natural disaster. Compound flooding is when multiple flooding events exacerbate the impact and duration of a flood. This paper studies relief logistics for compound flooding through the case of floods in Sindh province of Pakistan in 2010. The problem is treated as time-varying flows of relief goods from relief warehouses through periodic relief caravans to relief camps or storage close to them; and flows of affectees from shelter sites closest to them to more accessible shelter sites. This study concludes that relocation of affectees may be as cost effective as prepositioning of relief goods depending on optimal placement of relief warehouses. Factors that reduce logistics cost and improve demand fulfillment include prepositioning relief goods near shelters, frequent relief deliveries, and smaller storage modules that can reflect relief demand between caravan arrivals more closely.",Relief Goods Prepositioning and Affectee Relocation Model For Compound Flooding,"[76264, 41523, 79763]",389,"[30, 79, 58]",469,"Disaster Response - Search and Rescue, Resource Allocation and Impactful Prepositioning",38,4,21,OR in Humanitarian Operations [HOpe],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,49 [building - 116],"['Disaster and Crisis Management', 'Network Design', 'Humanitarian Applications']",MC-21
"Bids in procurement auctions are typically required to remain valid for a certain period of time. If the underlying cost changes in time, as for example due to inflationary economic conditions or changes of the production input prices, the eventual profit when awarding takes place may be dramatically lower than anticipated. To address this risk, bidders can impose an upper bound constraint on the probability that the final mark-up stays below a pre-determined level. In this setting we derive the equilibrium bidding strategies when the cost change is realized continuously or in discrete steps. We determine the effect of the validity time to the expected bidder profit and show how bidders can profit if the risk is correctly managed. On the other hand, we demonstrate that the contracting authority always suffers for a prolonged validity time in terms of the expected procurement project cost and we develop methods to determine the optimal time within which bids are valid. We explore and compare methods of price adjustments which can partially or totally compensate against cost change. We conclude with a discussion of the findings and recommendations for both bidders and auctioneers on the efficient management of this kind of risk.",For how long should bids be valid in a procurement auction?,[66559],646,"[9, 124]",471,"Game Theory, Solutions and Structures VIII",88,10,36,"Game Theory, Solutions and Structures","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,32 [building - 306],"['Auctions / Competitive Bidding', 'Revenue Management and Pricing']",TD-36
"This paper considers the problem of assessing and hedging the risk carried by a portfolio of non-standard derivative products managed with a parametric model. We first formalise the problem and define the framework in which it can be solved by using a constrained simulation approach with respect to the model parameters. Our approach is suitable for an agent who may be agnostic with respect to a prior model and who wish to account for expert views on the range of possible models. Instead of breaking them into several subproblems, the proposed methodology has the important advantage of answering the risk measurement and hedging question in one step. Namely, the agent needs to run the simulation just once to get the desired answers. We present numerical results obtained with recent market data when applying the method to a portfolio of variance swaps and forward-start options valued with stochastic volatility models. In this project we apply classic Operational Research techniques, namely constrained simulation, in a novel manner to risk measurement and hedging of trading portfolios.",A constrained simulation approach to robust risk management of derivative products.,[51622],690,"[45, 126, 131]",472,Risk management and valuation of financial contracts,74,8,57,Modern Decision Making in Finance and Insurance,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S06 [building - 101],"['Financial Modelling', 'Risk Analysis and Management', 'Simulation']",TB-57
"Industrial parks play a crucial role in facilitating the rapid industrialization of many emerging economies countries. However, planning these parks is complex, and their success hinges on strategic location decisions. This study proposes a mixed-integer linear programming formulation to determine the optimal locations of industrial parks to minimize total costs. It integrates large-scale actual distance measurements and multi-mode transportation considerations, capturing geographical nuances more realistically. Recognizing the energy-intensive nature of industries, the model incorporates transportation costs associated with energy sources. Real-world data sourced from the Indonesian nickel industry lends practical relevance to the proposed model within limited facility location studies with real-world applications. The case study examines 81 potential park locations, 139 nickel mining, and 20 coal mining sites, resulting in over 13,000 variables. Leveraging OpenSolver, optimal solutions are obtained within 15 seconds. The analysis unveils two optimal industrial park locations alongside capacity, raw materials, and energy supply allocation to selected industrial parks. Insights from sensitivity analysis include the pronounced influence of land acquisition, sea transport costs, and the elevated total costs associated with single-location industrial parks. Future research will explore uncertainty parameters relevant to the context of emerging economies.",Deterministic Model for Optimal Industrial Parks Location -  The Case of Indonesia's Nickel Industry,"[75752, 79231, 69944]",769,"[64, 84, 63]",478,Applications of Location Methods,29,12,61,Locational Analysis,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S10 [building - 101],"['Location', 'Optimization Modeling', 'Large Scale Optimization']",WA-61
"The rapid rise in real estate prices around the world has attracted widespread attention. The reasons for the rise in real estate prices include insufficient land supply, population growth, economic development, investment demand, and other factors. Several surveys show that average house prices are rising faster than average wages, causing many families to face difficulties in house purchases.
In Taiwan, people generally feel the pressure of house purchases. High housing prices relative to average wages require residents to face longer repayment periods and higher loan interest rates during the process, exacerbating the difficulty of house purchase. In addition, unequal development among regions has also led to great different considerations for house environment in different regions. House purchase decisions of different age groups are affected by many variables. The number of years and amount of the loan directly affect the repayment pressure and quality of life, while the average transaction price per square meter of houses is an important factor affecting the choice of living area.
This paper takes Taiwan as an example. We focus on different districts in Taipei City of Taiwan. The views and difficulties faced by people on house purchase are discussed. Based on survey data, including housing prices and average salaries, the different decision-making models between old, middle-aged, and young generations of dual-income couples in house purchase are presented.",Decision-Making Models of House Purchase Among Different Age Groups,"[76340, 76343]",890,"[7, 0]",485,MCDA and urban planning 2,44,9,47,Multiple Criteria Decision Analysis,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,50 [building - 324],['Analytics and Data Science'],TC-47
"Detailed scheduling has traditionally been optimized for the reduction of makespan and manufacturing costs. However, growing awareness of environmental concerns and increasingly stringent regulations are pushing industry towards reducing the carbon footprint of its operations. Scope 2 emissions, which are the indirect emissions related to the production and consumption of grid energy, are in fact estimated to be responsible for more than one-third of the global GHG emissions. In this context, carbon-aware scheduling can serve as a powerful way to reduce manufacturing’s carbon footprint by considering the time-sensitive carbon content of grid energy and the availability of on-site renewable energy.

This study introduces a carbon-aware flow-shop scheduling model designed to reduce scope 2 emissions. The model is formulated as a mixed-integer linear problem, taking into account the forecasted grid energy generation mix and available on-site renewable energy, along with the set of jobs to be scheduled and their corresponding energy requirements. The objective is to find an optimal day-ahead schedule that minimizes scope 2 emissions. The problem is addressed  using a dedicated memetic algorithm, combining evolutionary strategy and local search.

Results from a comprehensive case study confirm that by considering the dynamic carbon content of grid energy and on-site renewable energy availability, significant reductions in carbon emissions can be achieved.",Reducing scope 2 emissions through carbon-aware flow-shop scheduling,"[71542, 59853, 59857, 4741, 35904]",920,"[94, 14, 129]",486,Scheduling for sustainability,18,5,23,"Circular Economy, Remanufacturing and Recycling ","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,82 [building - 116],"['OR in Environment and Climate change', 'Combinatorial Optimization', 'Scheduling']",MD-23
"In the classical Set Cover problem, one is given a finite ground set Q, a collection S of subsets of Q to choose from, each of which is associated with a nonnegative cost and the goal is to choose a minimum-cost sub-collection of S whose union covers the entire ground set Q. The Set Cover problem assumes that every element of the ground set is always present. In practice, however, this may not always be the case. In the context of locating emergency facilities, the ground set corresponds to [not precisely known] emergencies. The regions in which emergencies can occur are modeled as the elements. The subset of regions in which an emergency occurs is uncertain and modeled as a scenario from the uncertainty set. For each facility, there is a set of regions that can be reached within a guaranteed response time. Following a robust optimization approach, we consider a second collection U of subsets of Q. Each set in U represents a possible scenario of the emergencies. The goal now is to choose a minimum-cost sub-collection of S such that at least a certain number of scenarios from U are covered. This leads to the Partial Scenario Set Cover problem [PSSC] which generalizes the Partial Set Cover problem, which is itself a generalization of the classical Set Cover problem. We study the complexity of PSSC, provide approximation algorithms based on LP-rounding and generalizations of the classical greedy algorithm for Set Cover.  ",Partial Scenario Set Cover and its Application to Emergency Planning,"[75305, 19477]",688,"[14, 16]",487,Disaster & Emergency Management,80,5,53,Sustainable and Resilient Systems,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,8007 [building - 202],"['Combinatorial Optimization', 'Complexity and Approximation']",MD-53
"The development of a problem structuring sensibility in engineering has led to a processual turn in engineering practice. By adopting a strong process epistemology – everything can be modelled as a process – it is possible to model purposeful activity conceptually, even if it includes processes that will eventually become reified as objects in the world. In addition to this delayed reification, Hierarchical Process Modelling [HPM] has an inbuilt measurement schema for process performance that includes an explicit representation of uncertainty and the possible calculation of overall performance using necessity and sufficiency conditions. Modelling proceeds from an initially agreed purpose amongst stakeholders through a how/why questioning dialectic in group model building. Since a processual interpretation of engineering practice has been made evident using this modelling approach, it is possible to examine the implications for integrating hard/soft OR practice. These are discussed, supported by case examples.",Hierarchical Process Modelling [HPM] - integrating conceptual and quantitative modelling,[53737],129,"[149, 151, 133]",494,Approaches for Integrating Quantitative Modelling,26,4,13,Soft OR and Problem Structuring Methods,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,15 [building - 116],"['Problem Structuring', 'Practice of OR', 'Soft OR']",MC-13
"The paper is dedicated to  optimization of linear functions over the cone of doubly non-negative matrices and its dual, the   sum of semidefinite plus nonnegative cones. Several equivalent conditions of    strict complementarity and nongeneracy are given.  The findings   are illustrated with examples.",Conic optimization problems over semidefinite plus nonnegative  cone,"[17905, 23467]",164,"[21, 115]",496,Special Classes of  Convex Conic Optimization problems,68,7,38,"Conic Optimization - Theory, Algorithms, and Applications","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,34 [building - 306],"['Convex Optimization', 'Programming, Semidefinite']",TA-38
"We focus on freight transportation carriers that transport shipments that are small relative to vehicle capacity and incur transportation costs that exhibit economies of scale. To achieve profitability, such carriers route shipments through a network of hubs. The locations of a carrier's hubs dictate the customer markets it can profitably serve. We consider a carrier that seeks to increase its profitability by growing its customer base. To do so, it seeks to expand its network into new regions by merging with carriers that already operate in those regions. We focus on how, and by how much, the network that results from such a merger should be redesigned to maximize profitability. To study this issue we present deterministic and stochastic profit-maximizing hub location models that capture the interaction between economies of scale and profitability. The models differ in whether and how they model the recognition of uncertainty in shipment sizes in the decision-making process. We perform a case study based on operations from a multi-regional less-than-truckload freight transportation carrier to derive insights into the profitability of different redesign strategies.  We analyze the networks prescribed by solutions to these optimization models to derive insights into how a network that results from a merger should be redesigned. We also study how uncertainty impacts the structure of the redesigned networks.",Strategic Expansion of Freight Transportation Hub Networks under Uncertainty,"[23588, 76346, 67646, 45712]",582,"[64, 65]",498,Hub Location,29,4,61,Locational Analysis,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S10 [building - 101],"['Location', 'Logistics']",MC-61
"In most competitive sports, one of the main interests of the audience is to know who is truly strong. Various tournaments are held to determine the strongest champions, but the winners depend on the tournament regulations to no small degree. It is not always sure that the champion is the highest-performing team/player. The outcome of a competitive sports match, which wins, can be analogous to a pairwise comparison in decision-making. In many competitive professional sports leagues, all teams belonging to a league fight each other in a round-robin tournament, and the team with the most wins is honored as the champion. However, in Japan's professional sumo league, Ozumo, about 40 sumo wrestlers, rikishi, compete in a limited schedule of 15 days, so not all combinations of rikishi compete against each other. Using a method that handles incomplete pairwise comparisons, we propose a method for obtaining the values representing the actual performance of teams/players from the results of a semi-round-robin tournament such as Ozumo. For the construction of a pairwise comparison matrix, we consider match time.　We also provide applications to the actual data of Ozumo.",Performance Assessment of Sumo Wrestlers from Match Results as Incomplete Pairwise Comparison with Consideration of Match Time,"[76233, 76247]",892,"[6, 0]",499,Pairwise comparisons and preference relations 2,44,10,44,Multiple Criteria Decision Analysis,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,20 [building - 324],['Analytic Hierarchy Process'],TD-44
"In this presentation, we showcase a range of projects that integrate optimization and macroeconomic modeling to explore the circular economy in Norway.

The first project focuses on utilizing input-output modeling to evaluate the impacts of building renovation policies in Oslo, with a specific emphasis on energy efficiency measures. The findings of this work have been published in peer-reviewed journals and serve as a foundation for future analyses that combine detailed technical information with macroeconomic analysis.

The second project, Synagri, employs supply-use modeling to assess the effects of biotechnology advancements in the agricultural sector and their benefits for farmers and other stakeholders near the beginning of the value chain. In particular, the project considers and models the circularity and reuse of byproducts of the animal rearing and food processing.

Lastly, the European project Treasource employs value-chain optimization in the context of recycling plastics, batteries, and bio-based streams in Northern Europe. The developed model focuses on waste generated from industrial processes and consumption, exploring its recycling potential.

All of the aforementioned projects prioritize data-driven circular economic analysis and cover diverse topics, each presenting their own unique challenges and requirements. Some of them have concluded, while others are ongoing work.",Optimisation and Macroeconomic models in Circular Economy analyses,"[31054, 61758, 61801]",923,"[33, 89, 100]",500,Optimization for the Circular Economy,18,9,23,"Circular Economy, Remanufacturing and Recycling ","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,82 [building - 116],"['Economic Modeling', 'OR in Agriculture', 'OR in Sustainability']",TC-23
"This paper introduces a novel approach for detecting sea-based drug transfers through the utilization of Automatic Identification System [AIS] data. We propose a model for the detection of the ’drop-off’ method, a prevalent technique for contraband smuggling at sea. Unlike the prevailing focus on unsupervised methods in existing maritime anomaly detection research, our paper introduces a supervised model tailored to the ‘drop-off’ method, particularly in the context of fishing vessels. We have developed a machine learning algorithm, employing a Long Short-Term Memory [LSTM] model capable of identifying ‘drop-offs’. This model holds the potential for integration into real-time surveillance systems. Furthermore, our model demonstrates generalizability, thereby enhancing maritime security efforts and providing invaluable assistance in countering drug traffic on a global scale.",Detecting Drug Transfers via the Drop-off Method - A Supervised Model Approach using AIS Data,"[76350, 76351]",74,"[66, 0]",502,"Advancements of OR-analytics in statistics, machine learning and data science 3",16,4,28,"Advancements of OR-analytics in statistics, machine learning and data science","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,065 [building - 208],['Machine Learning'],MC-28
"Our world is bedeviled with uncertainty and complexity. Markov models originated by A.A. Markov and further extended R.A. Howard and later, many other contributors, formulate, analyze, and evaluate simple and advanced models to solve the world's dynamic and complex problems ranging from genetics to robotic engineering and maintenance problems and sustainable developments. This work surveys recent developments in Markov decision processes [MDP] as well as provides insight into how MDP and Semi-Markov Decision Processes can be applied to a variety of problems. The finding shows that any practical problem on complexity and uncertainty can at least be modeled as a Markov model, provided we can break the problem into states, establish the transition probabilities, the rewards structure, and the optimization criteria are known.",Recent development on the Markov decision and semi-Markov decision processes,[29092],55,"[108, 139, 90]",507,Sustainable Economics in Developing Countries  ,67,13,18,OR for Development and Developing Countries,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,42 [building - 116],"['Programming, Dynamic', 'Sustainable Development', 'OR in Development']",WB-18
"The Belgian gas transport network is divided into two separate networks - the low calorific gas network from the Netherlands and the high calorific network with gas arriving from other producers. The Netherlands has announced the gradual cessation of the delivery of Dutch gas to Belgium. It is necessary to reorganize the transport of gas. Compared to the existing model for the Belgian network, two important changes must be modeled - on the one hand, the disappearance of the historic supplier will imply significant changes in gas flows in the single network merger of the two old ones. This involves possibly resizing of gas pipelines, reversing the direction of use of compression stations or creating new ones.

The other important change in the model is the objective function. Historically, a single company purchased the gas and transported it. Its main objective was therefore to minimize its gas purchasing costs. This no longer corresponds to the current situation where the commercial function is separated from the transport function. We take into account the new situation - namely the minimization of gas transport costs. 

We present real data of the new Belgian network and the new formulation to provide a case study for all future developments in the field of gas network operations, an activity particularly subject to variations in recent years in Europe.",The reorganization of the natural gas transport network in Belgium following the cessation of gas deliveries from the Netherlands.,[29305],838,"[38, 150, 93]",514,OR in Gas Networks ,23,7,19,OR in Energy,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,44 [building - 116],"['Engineering Optimization', 'Network Flows', 'OR in Energy']",TA-19
"In this work, we deal with batch Bayesian Optimization problems over a box and we propose a novel bi-objective optimization [BOO] acquisition strategy to sample points where to evaluate the objective function. The BOO problem involves the Gaussian Process posterior mean and variance functions, which, in most of the acquisition strategies from the literature, are generally used in combination, frequently through scalarization. However, such scalarization could compromise the Bayes-Opt process performance, as getting the desired trade-off between exploration and exploitation is not trivial in most cases. We instead aim to reconstruct the Pareto front of the BOO problem based on optimizing both the posterior mean as well as the variance, thus generating multiple trade-offs without any a priori knowledge. We then propose two possible methodologies to select potentially optimal points from the Pareto front. Our methodology is finally tested with well-known acquisition strategies from the literature, in order to highlight its effectiveness on different settings.",A Bi-Objective Optimization Based Acquisition Strategy for Batch Bayesian Global Optimization,"[67459, 76359, 76360, 10045]",87,"[19, 77]",515,Advances in Continuous Multiobjective Optimization,34,8,37,Multiobjective Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,33 [building - 306],"['Continuous Optimization', 'Multi-Objective Decision Making']",TB-37
"A vehicle starts its route from a depot and visits N unordered customers. New goods are delivered to the customers and expired goods are collected from them. The demands of the customers are stochastic with known distributions. The actual demands are revealed when the vehicle arrives at each customer’s site. During its route, the vehicle may return to the depot in order to unload expired goods and to restock with new goods. The cost structure includes travel costs between customers and travel costs between each customer and the depot. After the first visit at each customer’s site two decisions must be made. The first decision is to choose the next customer that the vehicle will visit. The second decision is to choose how the vehicle will go to the next customer. The problem is to find [i] the minimum total expected cost for servicing all customers and [ii] the optimal decisions that must be made after the first visit at each customer’s site. A suitable stochastic dynamic programming algorithm is developed for the solution of the above problem.",A Pick-Up and Delivery Vehicle Routing Problem With Stochastic Demands and Unordered Customers,"[26039, 49511, 74121]",782,"[108, 65]",516,Vehicle Routing Under Uncertainty 2,5,7,64,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S16 [building - 101],"['Programming, Dynamic', 'Logistics']",TA-64
"On-demand delivery platforms are characterised by uncertain demand with spatiotemporal patterns and short delivery time targets. Platform operators manage a courier fleet aiming to deliver the orders as quickly as possible and in a cost-efficient way. A core component of the platform is the courier assignment algorithm assigning incoming orders to couriers in real-time and determining a sequence of pickups and deliveries for each courier. Column generation is commonly used to solve these types of problems in practice - First, a set of feasible columns is generated via sequencing algorithms. Then, a set-partitioning problem is solved with the generated set of columns. As the size of the problem increases with the number of columns included in the problem, it is essential to include only the columns that are likely to be optimal in the problem. Our study aims to learn the optimal columns via machine learning utilising the characteristics of the problem instance and the feasible decision space. We then improve the efficiency and the solution quality of the algorithm by favouring the columns that are more likely to be part of the optimal solution. We conduct computational studies on a set of historical instances utilising the optimality predictions generated by the trained machine learning model and evaluate in terms of solution quality and run times.
",Learning optimal courier assignments in on-demand delivery platforms,"[76066, 42722]",320,"[66, 14, 65]",519,Machine Learning in Applied Optimization,14,8,03,Data Science Meets Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,1005 [building - 202],"['Machine Learning', 'Combinatorial Optimization', 'Logistics']",TB-03
"In recent years, the rapid proliferation of economic activities in India has been accompanied by a surge in the demand for electric vehicles [EVs]. Following the Paris Agreement, India has set ambitious targets for reducing carbon emissions, including the 30@30 goal.  However, with varying economic and demographic characteristics in India, EV adoption presents unique challenges and hinges on the development of a robust, equitable, and strategically planned charging infrastructure. This is because, while the global average indicates that we need to have at least one charger per 10 EVs, the current EV to public charging infrastructure in India is 181:1.In this study, we analyse the growth of EV ecosystem across Indian states. Based on the literature, we first identify the various factors that affect the EV ecosystem. Using Principal Component Analysis [PCA], we construct a comprehensive measure of these factors by grouping them into similar themes and then evaluate the performance of the Indian states using a modified Data Envelopment Analysis [DEA] approach. This comprehensive approach will not only provide valuable insights for policymakers but also pave the way for targeted interventions aimed at accelerating the transition towards a sustainable electric mobility future. Additionally, we suggest targeted interventions for states with lower scores, focusing on policy enhancement and infrastructure investments to accelerate their progress in the electric vehicle ecosystem.",Electric Vehicle and Charging Infrastructure Growth in Indian States - Assessment using a modified PCA- DEA Application,"[71902, 51414]",941,"[24, 35, 100]",521,DEA applications in Environment and Sustainability II,89,9,48,Data Envelopment Analysis and its Application,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,60 [building - 324],"['Data Envelopment Analysis', 'Efficiency Analysis', 'OR in Sustainability']",TC-48
"For the set partitioning problem, the existence of a sequence of integer solutions with non-increasing costs produced by simplex pivots, permitting to reach optimality was proved by Balas, Padberg 1972. Finding the following term in the sequence can be very time-consuming due to degeneracy. When there is no entering variable that leads to a better integer solution, we use a linear programming sub-problem to find a group of variables to enter the basis to obtain an improved solution which is integer most of the time. When this solution is not integer, we solve a small integer programming problem to obtain a better integer solution. 
We first, present results for large-scale problems [with up to 500 000 variables] for which optimal integer solutions are obtained, most of the time, without any branching. We also, present the integration of this new approach with column generation and parallelism to solve large-scale bus driver and airline pilot problems. Problems with many thousand tasks are solved 5 to 10 faster than with branch and price based on CPLEX
",Integral Simplex for the Set Partitioning Problem,"[10966, 76897, 51646, 24885]",395,"[109, 4, 13]",523,Large Scale Optimization in Air Transportation,64,13,29,Combinatorial Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,157 [building - 208],"['Programming, Integer', 'Airline Applications', 'Column Generation']",WB-29
"We study continuous set covering on networks. The covering condition has a representation as a piece-wise linear concave constraint, which we formulate as a disjunctive system. We propose three MILP reformulations based on indicator constraint, big-M, and disjunctive programming techniques to formulate the disjunctive system. We classify new and old formulations and conduct experiments to compare them.",Modelling of piece-wise linear concave constraints in continous covering problems,"[68511, 22410]",194,"[72, 111, 113]",524,Recent Advances in MINLP,86,9,04,MINLP,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,1001 [building - 202],"['Mathematical Programming', 'Programming, Mixed-Integer', 'Programming, Nonlinear']",TC-04
"Many transit authorities are switching from diesel buses to electric ones to address concerns over air pollution, greenhouse gas emissions, and the growing demand for energy. While there are many existing scheduling models for diesel buses, they would not fit electric buses [EBs] because EBs have a shorter range and take longer to charge. This has led to new research, including this study, which looks at both how to schedule EBs and where to place fast-charging stations with the objective of minimizing the total system costs. These costs include purchasing and operating of EBs and installing the charging stations. In our research, we developed two types of mathematical models to find the best schedules and charging station locations. We created a Mixed-Integer Linear Programming [MILP] for an arc-based model and an Integer Linear Programming [ILP] model for a path-based model. To solve these models, we employed a Cplex solver and a Branch-and-Price algorithm. After testing these methods with various scenarios, we found that the Branch-and-Price algorithm worked faster than the Cplex solver. Lastly, we did a sensitivity analysis to find out which type of electric bus is most cost-effective, taking into account the actual features of different buses. We also looked at how changes in the buses’ battery sizes and travel ranges would affect the optimum solution.",Joint Optimization of Electric Bus Scheduling and Fast Charging Infrastructure Location Planning,"[76364, 51646]",815,"[143, 13, 129]",527,Electric Busses,85,8,51,Public Transport Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,M5 [building - 101],"['Transportation', 'Column Generation', 'Scheduling']",TB-51
"Consumer behavior in terms of waste separation and recycling [WSR] is extremely important to achieve the goal of “zero-waste city” and circular economy. However, there is a discrepancy between consumers’ willingness to participate in waste separation and their actual behavior in China. The government's persuasion strategies have limited impact on changing WSR behavior. We argue that the strategies need to collectively change WSR behavior through their influence on the motivation and ability [M/A] of all consumers to achieve extensive public participation. Therefore, we propose a novel agent-based approach that integrates a psychological behavioral model and a simulation model to analyze WSR behavior and the effectiveness of persuasion strategies. A behavioral model based on the Fogg behavioral model is developed to investigate the M/A elements that influence WSR behavior. A multi-agent model is developed to analyze WSR behavior by considering the interactions between individual consumers and the environmental and peer pressures. By simulating the changes in WSR behavior under different strategy implementation scenarios, we find that consumers’ M/A are constantly changing due to their interactivity, and the public participation rate is significantly higher when diverse strategies are implemented that encompass all M/A elements. Our proposed approach and empirical results contribute to the research and implementation of waste management and low-carbon consumer behavior.",An agent-based approach to exploring consumer behavior and the effectiveness of persuasion strategies in waste separation and recycling in China,"[76139, 76148]",926,"[139, 3, 131]",531,Recycling,18,13,23,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,82 [building - 116],"['Sustainable Development', 'Agent Systems', 'Simulation']",WB-23
"The Distance Geometry Problem [DGP] is as follows - draw a weighted graph in a K-dimensional Euclidean space [K given] so that vertices are points and edges are segments with the same length as their weights. In many applications the input is not quite a graph, but simply a list of distance values without their adjacencies. Thus, the assignment of distance values to graph edges must be carried out concurrently to the realization in the K-dimensional space. This is known as the unassigned DGP [uDGP]. There is a natural MINLP formulation for the uDGP, which is very hard to solve even for tiny instances. We present alternative formulations as well as relaxations, restrictions, and other approximating reformulations.",Distance geometry without the graph,[8446],176,"[72, 17, 111]",532,Topics in Mixed Integer Programming and Nonconvex Optimization,86,7,04,MINLP,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,1001 [building - 202],"['Mathematical Programming', 'Computational Biology, Bioinformatics and Medicine', 'Programming, Mixed-Integer']",TA-04
" Recent Advances on the Variational Inequality approach to Network Games

Network Games, [NG], are a class of games where each individual [player] is identified with a node of a graph and the players that can interact directly are connected through links of the graph. The specificity of these games is the central role played by the graph structure in the description of the patterns of interactions, and in the final social or economic outcome.
The mathematical consequence of this description is that some interesting results depend on quantities such as the spectral radius of the adjacency matrix, its minimum eigenvalue, and its powers.
The theory of Variational Inequalities has not yet been exploited in the context of NG, with only a few exceptions. In this talk we outline some recent results as well as some research perspectives within this framework.",Recent Advances on the Variational Ineqaulity approach to Network Games,[19716],549,"[50, 132, 53]",534,Recent advances on Variational Inequalities and Equilibrium Problems II,51,14,43,Variational Inequalities and Equilibrium Problems - From Theoretical Advances to Real World Applications,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,99 [building - 306],"['Game Theory', 'Social Networks', 'Graphs and Networks']",WC-43
"By monitoring the financial markets, we can discover a huge number of potential assets to be invested in. It is very complicated and time-consuming to analyze all assets and then use all of them in optimization according to the preferred portfolio strategy. For this reason, it is potentially useful for portfolio managers to focus on a specific sample of assets identified by the filtering process. This paper proposes an efficient approach for asset preselection based on multidimensional non-dominated sorting of selected asset statistics. In contrast to previous research, we designed an innovative application on statistics obtained from approximated return series using nonparametric regression and principal component analysis [PCA]. Additionally, we study its impact on mean–variance and recently proposed complex mean–trend risk large-scale portfolio selection strategies. In particular, this process examines the efficient frontier of portfolios on the basis of differing return and risk perspectives. In the empirical part applied to the US stock market data, we present an ex-post and ex-ante analysis with results for 40 portfolio strategies. The results obtained strongly indicate that for most risk-averse investors, the mean–trend risk strategies with the preselection outperform the same strategies without the preselection, as well as the mean–variance strategies.",Mean–trend risk portfolio selection with non-dominated sorting asset preselection,"[67822, 74165, 13349]",413,"[84, 126, 44]",536,Portfolio risk management,9,5,51,Risk management in finance,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,M5 [building - 101],"['Optimization Modeling', 'Risk Analysis and Management', 'Finance and Banking']",MD-51
"We examine a renewable energy source [RES] generator engaged in short-term electricity market trading. Traditionally, variable RES generation has been exempted from balancing responsibilities; however, this exemption is no longer prevalent in many markets due to increasing penetration. We consider a generator participating in both a day-ahead market and a subsequent balancing market. We seek the RES generator's optimal day-ahead offerings in terms of energy volumes, considering the impact of balancing market settlements. To this end, we implement and analyze the accuracy of state-of-the-art Machine Learning [ML] models in forecasting imbalance signs, prices, and other market outcomes, assessing the potential to enhance RES's profitability. Additionally, we evaluate the impact of employing storage techniques to mitigate the intermittency associated with RES capacity. We develop a two-stage stochastic optimization model. The first stage involves day-ahead decisions that anticipate scenario-dependent balancing settlements and real-time battery operation in the second stage. The model uses improved forecasting scenarios generated by an ensemble of ML models, considering contextual and past historical market outcomes. We conduct an extensive set of out-of-sample simulations using real data from the Spanish market, and under the two main financial settlement mechanisms available for balancing markets - single and dual pricing, each with distinct implications for the RES generator.",ML-Driven Day-ahead Offering for RES Generators in Uncertain Balancing Markets,"[76367, 55098]",405,"[93, 136, 66]",537,Stochastic Optimization for Energy Transition,49,3,35,"Stochastic, Robust and Distributionally Robust Optimization","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,44 [building - 303A],"['OR in Energy', 'Stochastic Optimization', 'Machine Learning']",MB-35
"The awareness of climate change and the corresponding need for climate protection have become omnipresent for companies nowadays. In 2005, the European Commission established the European Union Emissions Trading Scheme [EU ETS], in order to mitigate the effects of anthropogenic climate change and render them more predictable. From 2027, as a result of the so-called Green Deal, companies will no longer receive subsidies and will responsible for paying the full costs of greenhouse gas emissions. The ramifications for companies are still uncertain.
Starting from this, the aim of the research endeavors to delve deeper into the examination of the impact of greenhouse gas emission costs on German companies. It is assumed that the effects will be most significant in carbon intensive productions. Taking this into consideration, this research will model and analyse an average company in the steel industry. 
Along with the implementation of the EU ETS, companies are faced with the challenge of making decisions on the purchase of European Emission Allowances [EUA’s] or the investment in lower- emission technology. The study aims to provide an overview of the range of strategies a company is faced with. Subsequently, a selection of possible scenarios are analysed e.g. the optimum time of purchase, considering capital commitment costs. Furthermore, the uncertainty caused by fluctuations in EUA‘s and fluctuating raw material prices are considered.",Analysing the optimal time of purchase in case of increasing carbon dioxide prices,[69583],3,"[1, 40]",538,OR in Accounting - Performance and ESG,7,14,59,OR in Financial and Management Accounting,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S08 [building - 101],"['Accounting', 'Environmental Management']",WC-59
"In many applications nonlinear data is measured, for example when considering rotations, bases of sub spaces or even sub spaces themselves. While this can be modelled as linear data with constraints, this induce more complicated algorithms.
If the constraints on the other hand form a Riemannian manifpold, for example the Special Orthogonal group, the Stiefel or the Grassmann manifold, one can employ classical unconstraint algorithms, while staying inherently feasible.
In the recent years these algorithms were extended to non-smooth objectives as well as constrained optimization on manifolds.

In this talk, we present a framework and several algorithms that are available in Manopt.jl, which together with Manifolds.jl forms a family of Julia packages to efficiently solve optimization problems on Manifolds in Julia. ",Nonsmooth Optimization on Riemannian Manifolds in Manopt.jl,[61690],845,"[81, 113, 134]",540,Optimization Frameworks,76,14,30,Software for Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,53 [building - 208],"['Non-smooth Optimization', 'Programming, Nonlinear', 'Software']",WC-30
"Mitigating climate impacts from greenhouse gases [GHG] is a complicated issue given the increasing energy demand. Utilizing environmentally sustainable vehicles and eco-friendly transport modes appears to be a highly effective strategy to tackle this problem. The European Union recognizes the crucial role of short-sea shipping [SSS] in the decarbonization efforts across the transport sector. The policy strategically aims to improve the share of SSS by encouraging a transition from road transportation to maritime alternatives. 
As part of a European project named AEGIS, this study seeks to use greener fuels in vessels by implementing a hybrid propulsion system that integrates methanol and batteries. The study explores the synergy of this propulsion system with autonomous vessels, intending to improve the competitive standing of SSS. The research undertakes a comprehensive examination of this innovative transportation on sustainability, implementing a cost-benefit analysis [CBA] that includes economic and environmental. In this research, a case study focused on the transportation route between the port of Hitra in Norway and the port of Rotterdam in the Netherlands, utilizing the concept of mother and daughter vessels. The results suggest that autonomous ships could potentially enhance the maritime sector’s competitive edge. This could result in an increase in cargo transportation via this mode, thereby improving environmental sustainability.",Transformation in the Maritime Sector via Autonomous Vessels and Eco-Friendly Fuels,"[71693, 17213]",536,"[143, 70, 139]",541,Applications of Knowledge and Technology,54,13,08,"Knowledge, Technology, and Innovation","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1020 [building - 202],"['Transportation', 'Maritime applications', 'Sustainable Development']",WB-08
"While electricity storage is pivotal in the move away from fossil fuel-based generation to renewables, its roll-out might have undesirable side-effects. We develop a system dynamics simulation model to investigate the impact of introducing large-scale, economically viable storage, on security of supply during this transition period.
As efficient storage options partly mitigate the intermittent nature of renewables such as wind and photovoltaics, their capacity factor will go up, resulting in an increased derated capacity margin, and thus a reduced need for future investments. However, the speed of the technological evolution that will enable large-scale viable storage, and the implementation rate, are hard to predict. Incorrect anticipation of future capacity factors could lead to a complicated transition period.
If investors are over-optimistic regarding the speed of this evolution, they are likely to overestimate the efficiency of existing renewable generation and be reluctant to invest, leading to capacity adequacy problems and a risk of shortages, if not blackouts. On the contrary, underestimating the speed of this evolution would lead to costly over-capacity.
The model allows us to investigate different scenarios based on investors’ anticipations, initial share of renewables, etc.  We observe capacity shortages in some scenarios, while others indeed generate significant excess capacity. Furthermore, we argue how the regulator might mitigate some of these issues.",Security of supply in the transition to economically viable large-scale electricity storage,"[29044, 28505]",839,"[36, 37, 131]",542,The role of storage in energy problems,23,8,19,OR in Energy,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,44 [building - 116],"['Electricity Markets', 'Energy Policy and Planning', 'Simulation']",TB-19
"This talk introduces Tran2SP, a novel framework for solving two-stage stochastic programming [2SP] problems, which are characterized by making decisions under uncertainty. Traditional methods, while effective, often suffer from computational inefficiencies and the necessity to re-solve for new or altered instances. Tran2SP overcomes these challenges by employing the Transformer architecture to approximate the value function with piecewise linear functions, enhancing scalability and reducing computational demands. Unlike previous approaches that require extensive data or re-learning for modifications in the problem, Tran2SP adapts to changes efficiently, offering high-quality, robust solutions across various problem instances without the need to start from scratch. This advancement represents a significant improvement in solving 2SP problems, highlighting Tran2SP's potential to address the dynamic nature of uncertainties in optimization tasks. We further validate this approach through comprehensive experiments, confirming its effectiveness and robustness in practical applications.",Tran2SP - Transformer-based Two-Stage Stochastic Programming,[23405],436,"[84, 136, 66]",543,Novel Optimization Models in Finance,4,12,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S14 [building - 101],"['Optimization Modeling', 'Stochastic Optimization', 'Machine Learning']",WA-63
"This paper examines the tourism industry in Mexico, focusing specifically on the performance of each of its 32 states. To facilitate the development of an appropriate performance measurement tool, we begin with the data envelopment analysis [DEA] methodology, an optimization tool used for evaluating the efficiencies of a set of decision-making units [DMUs]. While it is usually assumed that each DMU functions independently of other units, having its own set of inputs and outputs, there are situations where the DMUs can be interdependent due to the presence of shared factors. Thus, a DMU can share an input or output with other DMUs, thereby generating sub-groups of DMUs. In such a setting, like the one considered herein, one must view a projection to the frontier as involving all sub-group members simultaneously. As well, in some cases the shared factor cannot be split up and assigned to each sub-group member separately. This situation can become even more complex in a two-stage setting where a DMU can share a factor with some other DMUs and the DMU groupings are different in one stage than in another. Finally, in the study herein we consider the presence of different attributes, i.e. different types of tourism.",Measuring efficiency in tourism - a problem of shared factors and multiple attributes in DEA,"[76370, 68246, 6373, 53928]",943,"[24, 113, 85]",544,DEA applications in Policy Making and Planning II,89,12,48,Data Envelopment Analysis and its Application,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,60 [building - 324],"['Data Envelopment Analysis', 'Programming, Nonlinear', 'OR and the Arts\xa0']",WA-48
"In this paper, we utilize the internal structure of the bus transportation system's to assess its production inefficiencies. Specifically, we exploit information about linkages of resources across departments and identify inefficient sub-activities within the production system. Our proposed framework employs a two-stage network DEA framework to assess the overall production efficiency of the bus transportation system. Since departments of the bus transportation system share limited resources, game-theoretical approach has been employed for decomposing production efficiency to estimate optimal efficiency estimates for each division. To demonstrate the practical application of our approach, we implement it in the context of the Indian bus transportation system. Finally, we conclude by discussing the policy implications of our research and offering recommendations for managers to enhance overall production performance.",A novel two-stage network DEA approach to estimate production efficiency of the bus transportation system.,"[64610, 71486]",944,"[24, 143, 77]",546,DEA applications in transportation,89,13,48,Data Envelopment Analysis and its Application,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,60 [building - 324],"['Data Envelopment Analysis', 'Transportation', 'Multi-Objective Decision Making']",WB-48
"The desired transition towards a hydrogen economy requires hydrogen costs to come down, through optimal choices of infrastructure design and operation.
In this talk, we present an approach based on multistage stochastic optimization mixing design choices with operational decisions taken on an hourly basis. 
We consider an elementary hydrogen infrastructure which consists of an electrolyzer, a compressor and a storage to serve a transportation demand.
This infrastructure is powered by three sources of energy [on site photovoltaics, renewable electricity through power purchase agreement, power grid].
The modelling of the electrolyser covers its different functioning modes and the nonlinear relation between the production of hydrogen and the electricity consumption. 
The optimization problem is to minimize the operational costs over a week [while emphasizing the use of renewable sources], by making hourly decisions in an uncertain context. 
We consider uncertainties affecting on site photovoltaics production and hydrogen demand. We formulate a multistage stochastic optimization problem, and we develop suitable algorithms based on dynamic programming. We present numerical results for a given infrastructure design. Then, we consider various combinations of infrastructure design and their subsequent optimal operation. 
With this, we discuss the optimal sizing of equipment, especially the sensitivity of electrolyzer and storage designs to the uncertainties.",Multistage stochastic optimization of an elementary hydrogen infrastructure,"[76371, 12378, 67932, 19141, 78742]",234,"[138, 136]",547,Production Optimization and Supply Chain Management of Green Hydrogen under Uncertainties,22,13,09,Energy Markets,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,10 [building - 116],"['Supply Chain Management', 'Stochastic Optimization']",WB-09
"The standard quadratic optimization problem [StQP] consists of minimizing a quadratic form over the standard simplex. If the quadratic form is neither convex nor concave, the StQP is NP-hard. This problem has many relevant real-world applications ranging from portfolio optimization to machine learning.
To accommodate uncertainty in the data matrix, we present a model with known distribution of the data matrix where both the StQP after realization, and the here-and-now problem are indefinite. Similarly to some robust formulations of uncertain StQPs, it turns out that a chance-constrained epigraphic StQP is in fact equivalent to a deterministic [possibly non-convex] StQP. We test the performance of a chance-constrained epigraphic StQP to the uncertain StQP by simulation.",Uncertain standard quadratic optimization under distributional assumptions - a chance-constrained epigraphic approach,"[76362, 8503]",301,"[117, 114]",548,Optimization under uncertainty - theory and solution algorithms,49,7,35,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,44 [building - 303A],"['Programming, Stochastic', 'Programming, Quadratic']",TA-35
"Even if energy-based, the transition towards decarbonization is a multi-dimensional process evolving over time and space and related to the broader social, techno-economic, environmental, and geopolitical aspects. While facing the uptake of new technologies or strategies in the energy system framework, it is important to address this complexity, to support policymakers in strategic planning. 
In this regard, the work aims to elaborate a structured science-based decision-making approach, through the exploitation of different instruments and tools, i.e. multi-criteria decision methods, spatial analyses, modelling of energy scenarios. A three-step procedure is developed, based on the evaluation of three concepts - [i] predisposition, through the application of a Multi-criteria Analysis; [ii] multi-dimensional suitability, exploiting a Multi-criteria Spatial Decision Support System; [iii] competitiveness, by modelling energy scenarios. 
Elaborated to be scalable and replicable for different strategic energy planning processes, the approach is exploited to assess if and how [i] North African countries are predisposed to green hydrogen production, [ii] North African areas are suitable for green hydrogen production, [iii] the trade to Europe can be competitive in the long-term. This approach allows to investigate the multi-dimensional patterns of the transition and its key players like hydrogen, through heterogeneous instruments able to express the heterogeneity of the process.
",Elaboration of a Multi-Dimensional Decision Support System for Strategic Energy Planning - Application to Production and Trade of Green Hydrogen,"[68006, 71398, 76373, 48051]",905,"[37, 26, 137]",550,"How to support complex decisions. Negotiating the trade-off between Social, Environmental and Economic values 2",44,5,47,Multiple Criteria Decision Analysis,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,50 [building - 324],"['Energy Policy and Planning', 'Decision Support Systems', 'Strategic Planning and Management']",MD-47
"Our research addresses a production network design problem involving strategic decisions related to selecting production site locations and their capacities, choosing suppliers, and sizing an internal fleet of vehicles and/or engaging external transportation providers. The problem also considers tactical decisions regarding flows between origin-destination pairs, produced quantities, and inventory levels. These decisions are made within a dynamic environment considering uncertainty in suppliers and production capacities, as well as the availability of transportation vehicles. The problem's structure allows us to model it as a multi-stage stochastic model. However, we approximate it as a two-stage model where strategic decisions are made at the first stage, while tactical ones are set as second-stage decisions. To solve various instances of our problem, we assess the efficiency of three resolution methods - Cplex's default branch-and-cut algorithm, Cplex's Benders decomposition, and partial Benders decomposition. Next, we demonstrate the relevance of integrating the three strategic decisions instead of making them separately and study the impact of different degrees of uncertainty on these decisions. Our findings justify that partial Benders decomposition produces high-quality solutions while achieving computational efficiency, highlighting the suitability of the stochastic model and the resolution approach for dealing with practical supply chain applications.",Production Network Design under Supply Uncertainty,"[73821, 73584, 73820, 32566]",445,"[138, 79, 136]",551,Supply Chain Network Optimization,6,15,55,Transportation,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S02 [building - 101],"['Supply Chain Management', 'Network Design', 'Stochastic Optimization']",WD-55
"The collective motion of social agents arising from their interactions has been the subject of intense scientific research. Understanding the collective dynamics of human crowds is crucial for improving pedestrian traffic flow, ensuring crowd safety, effective urban planning, and preventing crowd disasters. From the perspective of crowd management, it is crucial to understand the relationship between crowd density and velocity, known as the fundamental diagram [FD]. This relationship helps to study the capacity of spaces where traffic moves, such as roads for vehicles and sidewalks for pedestrians. Constructing a realistic FD requires the utilization of an effective method for density estimation. While existing literature offers several methods, determining the 'best' method remains unresolved and may depend on the crowd situation. Most research has focused on situations where the moving crowd is constrained within a physical boundary, such as a corridor or sidewalk. However, there is no well-defined method of density estimation for groups in an unbounded space. I wish to present our recently developed voronoi-cell-based density estimation method that can estimate crowd density in a wide variety of situations, irrespective of the presence of spatial constraints. Our proposed method of pedestrian density estimation could facilitate the construction of FDs from the point of view of individual pedestrians, along their trajectories.",Density estimation for a moving crowd without a physical boundary,[76375],505,"[143, 30, 5]",552,Traffic flow modeling ,6,4,56,Transportation,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S04 [building - 101],"['Transportation', 'Disaster and Crisis Management', 'Algorithms']",MC-56
"A key factor to consider in the development of new technologies in a number of fields is the use of unmanned aerial vehicles [UAV]. This rapidly developing technology is used in military, communication, health, mapping, agriculture and transportation fields. The importance of cargo transportation has grown due to the growth of e-commerce. Currently, with advancing technology, and the effect of the pandemic, purchases are increasingly made over the internet. For cargo transporters, this situation leads to an increase in the number of destination points, in distances traveled, and in the delivery frequency, and a decrease in the package sizes. As a result, the planning of transportation has become increasingly complex. One solution is to make greater use of UAVs in this sector, and to reduce reliance on trucks through appropriate planning. This involves two aspects - the UAV delivering to a point, while the cargo truck delivers to a separate point. In this study, we consider a multiple traveling salesperson problem simultaneously using multiple trucks and UAVs for package delivery. We develop a general variable neighborhood search algorithm, and compare the results with the existing studies in the literature. Computational experiments show that our approach is able to find highly satisfactory solutions in reasonable time, and outperforms the existing methods in terms of best solution, average solution and solution time in majority of the instances.",Multiple Traveling Salesperson Problem With Drones - General Variable Neighborhood Search Approach,"[22492, 68192, 4848]",781,"[65, 143, 14]",554,Routing Unmanned Aerial Vehicles 2,5,4,64,VeRoLog - Vehicle Routing and Logistics,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S16 [building - 101],"['Logistics', 'Transportation', 'Combinatorial Optimization']",MC-64
"Recent developments in Europe have disrupted gas supplies and reignited the urgency of moving towards electricity production from winds. But, the narrow predictability of wind and the intermittency of electricity generation can complicate
decisions within trading desks. To alleviate the issue at hand, an attempt in this paper is to provide an optimal training mechanism by accounting seasonalities in wind productions as electricity sells in the exchange. We formulate the problem from a standpoint of an institutional investor concerned with ESG factors. For the trader, we define his reward at each
future price after a number of trades. We introduce a parametrized approximate value function using deep convolutional neural network in which we specify the weights of the Q-network at each iteration. We derive some key implications for the trader as to the best time to buy wind power futures and at what price to do it.
",A reinforcement learning mechanism for trading wind power futures,"[74483, 76377]",57,"[66, 40, 45]",555,AI and ESG for the small economy SDG agenda [EWG-ORD Workshop 2],67,10,18,OR for Development and Developing Countries,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,42 [building - 116],"['Machine Learning', 'Environmental Management', 'Financial Modelling']",TD-18
"The paper introduces the three-index assignment problem [3IAP] consists of assigning n tasks to n machines in n factories provided that a task is exactly allocated to a machine in a factory so that the total cost of allocation is minimal. The 3IAP, extending the classic two-dimensional assignment problem, is established as NP-hard. Due to its computational complexity, innovative heuristics are designed for near-optimal solutions. Applying Genetic Algorithms [GA] to 3IAP evolves a population, seeking optimal or near-optimal solutions over generations. This investigation advocates enhancing their quality by integrating a local search method.
The study proposes a hybrid genetic algorithm, integrating GA with a particular local search based on permutations, yielding high-quality solutions in a notably reduced computational timeframe compared to deterministic methods. The efficacy of this hybrid approach is demonstrated through experimental results and comparisons with other optimisation methodologies.
",Hybrid Genetic Algorithm for Solving The Three-index Assignment Problem,[58545],877,"[5, 14, 109]",558,Heuristic Algorithms for Combinatorial Optimization Problems I [Contributed],64,14,52,Combinatorial Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,8003 [building - 202],"['Algorithms', 'Combinatorial Optimization', 'Programming, Integer']",WC-52
"We consider the valve placement and control problem, which consists in simultaneously determining the location and settings of new pressure control valves in water distribution networks for the optimization of performance [here, leakage reduction] at minimum cost. We present a hybrid method combining the complementary advantages of non-linear programming solvers and evolutionary algorithms to solve the resulting non-convex bi-objective mixed-integer non-linear program. The performance of the proposed algorithm is first evaluated on small case study networks from the literature. Our results show that the proposed hybrid method converges quickly to a good approximation of the Pareto front of the corresponding valve placement and control problems. Next, the hybrid method is applied to a large-scale operational network from the UK and obtained solutions are compared to the locations of existing pressure control valves [determined based on engineering judgement]. ",Bi-objective valve placement and control in an operational water distribution network using a hybrid evolutionary-deterministic method,[74398],52,"[112, 111, 113]",559,Multiobjective Mixed-Integer Nonlinear Optimization,34,9,37,Multiobjective Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,33 [building - 306],"['Programming, Multi-Objective', 'Programming, Mixed-Integer', 'Programming, Nonlinear']",TC-37
"Optimization over trained machine learning models has applications including verification, minimizing neural acquisition functions, and integrating a trained surrogate into a larger decision-making problem. We formulate and solve optimization problems constrained by trained graph neural networks [GNNs]. For the classical GNN architectures considered in this talk, optimizing over a GNN with a fixed graph is equivalent to optimizing over a dense neural network. Thus, we study the case where the input graph is not fixed, implying that each edge is a decision variable, and develop two mixed-integer optimization formulations. To circumvent the symmetry issue caused by graph isomorphism, we propose two types of symmetry-breaking constraints, and provide a proof that adding these constraints will not remove all symmetric solutions. As a byproduct, a graph indexing algorithm is constructed to yield an indexing satisfying these constraints. To test our symmetry-breaking strategies and optimization formulations, we consider an application in optimal molecular design.","Optimization over trained graph neural networks - mixed-integer formulations, symmetry breaking, and optimal molecular design",[76271],176,"[14, 72, 111]",560,Topics in Mixed Integer Programming and Nonconvex Optimization,86,7,04,MINLP,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,1001 [building - 202],"['Combinatorial Optimization', 'Mathematical Programming', 'Programming, Mixed-Integer']",TA-04
"This presentation deals with the distribution of products in the context of an energy company. This problem can be modeled as a Vehicle Routing Problem [VRP] with several complexities like - time windows, different vehicles, several products, among others. The urgency of optimizing energy transportation cannot be overstated, given the rising global demand for energy resources. Tabu Search is a powerful meta-heuristi algorithm commonly used for solving combinatorial optimization problems. Its adaptability and efficiency make it well-suited for a variety of applications, as the VRP, for identifying good vehicle routes. However, like any optimization algorithm, Tabu Search faces challenges, and one of the main issues is dealing with local optima where the algorithm might get stuck. In scenarios with numerous local optima, Tabu Search may struggle to escape these regions of attraction efficiently, leading to slow convergence and potentially sub-optima solutions.		 In response to this challenge, we propose a novel double neighborhood strategy employing opposite optimization directions [minimization and maximization]. The maximization component is strategically employed to facilitate the escape from local minima. We showcase the application of this approach to a challenging and intricate VRP problem. Our results demonstrate the effectiveness of the double neighborhood strategy in enhancing the performance of Tabu Search, particularly in scenarios where traditional implementations ma",Solving a VRP in the energy sector through tabu search with a double neighborhood,"[7234, 76386, 76385]",858,"[14, 74, 145]",564,"Discrete, continuous or stochastic optimization and control in networks, transportation and design II",64,3,25,Combinatorial Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,011 [building - 208],"['Combinatorial Optimization', 'Metaheuristics', 'Vehicle Routing']",MB-25
"This paper proposes a novel horizontal collaborative framework for grape transportation within a cooperative that supplies grapes for pisco production. Nowadays, each cooperative member collects and delivers grapes to previously assigned processing plants without coordinating with the other members. In order to evaluate the potential impact of shared transport resources, we model collaboration among farmers as a cooperative game with transferable cost, where a mixed integer programming formulation that models transport activities is repeatedly solved for computing the characteristic function. Although the grand coalition is theoretically better than any other coalition structure, our approach limits the number of farmers per coalition to assess incremental collaboration's benefits. By conducting a case study involving sixty farmers, we show that a collaborative approach leads to a 13.4% reduction in the cooperative's transportation costs. However, we observe that the benefits of collaboration show diminishing returns. Particularly, we show that a coalition structure with a maximum number of three players per coalition captures almost 94% of the maximum potential savings. ",A horizontal collaboration approach for grape transportation in a pisco cooperative,[53496],589,"[143, 50, 89]",565,Agrifood supply chain decision problems,20,4,12,OR in Agriculture and Forestry ,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,13 [building - 116],"['Transportation', 'Game Theory', 'OR in Agriculture']",MC-12
"Over the past few there have been so many disasters around the globe mostly due to climate change and pandemics. To deal with natural and man-made disasters efficient humanitarian supply chains have attracted significant attention. To benefit from efficient humanitarian supply chains there has been focus on efficiency measurement using novel systems, frameworks and models. In spite of the significance of measuring efficiency in humanitarian supply chains, there are still some key barriers  with the current  efficiency measurement using novel systems, frameworks and models. The goal of the current study is developing a new network data envelopment analysis [NDEA] model that contains a number of distinctive advantages. In this paper the divisional and overall efficiency of  humanitarian supply chains  are measured using our novel model correctly. Moreover, our developed model can address all types of data in the process of measuring the efficiency of humanitarian supply chains. We presented a case for our developed model along with some managerial insights. ",Measuring the efficiency of humanitarian supply chains in face of disasters using a network data envelopment analysis,[76504],941,"[94, 138, 126]",570,DEA applications in Environment and Sustainability II,89,9,48,Data Envelopment Analysis and its Application,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,60 [building - 324],"['OR in Environment and Climate change', 'Supply Chain Management', 'Risk Analysis and Management']",TC-48
"In the analytical context of addressing humanitarian issues, huge-scale facility location problems play an important role. Whether it comes to finding locations for schools, hospitals, or water wells, these problems often involve many demand locations and many potential facility locations. Solving such problems using traditional methods quickly becomes intractable, or even impossible due to large memory requirements. 
We propose a decomposition approach to efficiently solve huge-scale problems in an exact way by clustering the demand points, solving subproblems for each cluster independently, and combining solutions of subproblems by solving a integer programming problem. Clustering takes advantage of the spatial separation of demand locations to address subproblems independently, guaranteeing optimality. Optionally, heuristic subclustering can be used to further divide clusters. When subclustering is used, bounds on optimality gaps are provided, offering quality guarantees about obtained solutions. 
We demonstrate the effectiveness of the proposed approach with a case study, which is done in collaboration with the 510 team of The Dutch Red Cross, involving the placement of water wells in West Darfur [Sudan]. Solutions for capacitated and uncapacitated problems are obtained within reasonable computation times while providing optimality guarantees where appropriate.",A method for huge scale maximum covering facility location problems with an application to water well placement in West Darfur,[72951],543,"[63, 130, 58]",573,Facilities Routing and Planning in Developing Countries,67,14,18,OR for Development and Developing Countries,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,42 [building - 116],"['Large Scale Optimization', 'Service Operations', 'Humanitarian Applications']",WC-18
"Feature selection across different versions of a product, to create a sufficient level of differentiation and yet satisfy customers’ expectations, is one of the complex decisions that companies need to make frequently. In this research, using Mixed-Integer Linear Programming, we decide on the levels of each feature in the various versions of the product line, in the presence of discount on the purchase price of initial components as a function of the purchased quantity. We study the trade-off between the cost of the products, and customers’ utility based on a First Choice model, to maximize the firms’ profit. Since the problem can easily get intractable with an increase in its size, we propose various families of valid cuts to strengthen the model and accelerate solving. Conducting extensive numerical experiments, we provide managerial insights to help manufacturers’ designing their products. We also analyze the impact of varying the purchasing discount rate of components, the degree of market competitiveness, and the degree of heterogeneity of customers’ preferences on the optimal design choices. The impact of these variations is measured in terms of manufacturer’s profit, intensity of economies of scale, and the level of homogeneity vs. heterogeneity of the different versions of the
product line.",Feature selection for new product line design with economies of scale,"[69397, 4914, 56897]",835,"[105, 69, 111]",575,Mathematical programming for machine scheduling,32,15,49,"Lot Sizing, Lot Scheduling and Production Planning","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,M1 [building - 101],"['Production and Inventory Systems', 'Manufacturing', 'Programming, Mixed-Integer']",WD-49
"In this paper, we study the relief logistics problem that emerges in the response phase after a disaster. We consider a case in which a set of demand nodes must be supplied with some urgent commodity over a planning horizon. The supply for this relief item is irregular and time-dependent and often not enough for the demand since, for instance, it may depend on voluntary donations. As observed in previous disasters, donations lead to a high level of uncertainty in the distribution network as the amount depends on the choice of volunteers. We assume that supply information reveals at the beginning of each period, upon processing the collected in-kind donations. This uncertainty poses a major challenge when it comes to fairly distributing the available amount.	
The problem of interest can be cast as a multi-period location-inventory problem and formulated using multi-stage stochastic programming. Unlike most of the existing literature, location decisions take place in each stage of the planning horizon as the information gradually reveals. 
	The mathematical model is tested using instances built from real data available in the literature, using a set of scenario trees with different supply levels. Computational tests are made on these benchmark instances with various sizes of network and scenario trees. 
	The results show that the new modeling framework can be extremely useful when it comes to finding fair solutions for supplying a relief item with uncertain availability.",Relief Supply Chain Planning - Fair allocation of in-kind donations in post-disaster phase,"[66173, 1379, 56337, 9684]",775,"[30, 58, 136]",576,"Efficiency, equity and fairness in humanitarian operations",38,9,21,OR in Humanitarian Operations [HOpe],"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,49 [building - 116],"['Disaster and Crisis Management', 'Humanitarian Applications', 'Stochastic Optimization']",TC-21
"In some ML communities, researchers claim that obtaining local solutions of optimality criteria is often sufficient to provide a meaningful and accurate data model in real-world analytics. However, this is simply incorrect and sometimes dangerously misleading, particularly when it comes to highly structured problems involving non-convexity such as discrete decisions [binary variables]. This talk will advocate the necessity of research efforts in the quest for global solutions and strong rigorous bounds for quality guarantees, showcased on one of the nowadays most popular domains -- cardinality-constrained models. These models try to achieve fairness, transparency and explainability in AI applications, ranging from Math.Finance/Economics to social and life sciences.

From a computational viewpoint, it may be tempting to replace the zero-norm [number of nonzero variables] with surrogates, for the benefit of tractability. We argue that these relaxations come too early. Instead, we propose to incorporate the true zero-norm into the base model and treat this either by MILP relaxations or else by lifting to tractable conic optimization models. Both in practice and in theory, these have proved to achieve much stronger bounds than the usual LP-based ones, and therefore they may, more reliably and based upon exact arguments, assess the quality of proposals coming from other techniques in a more precise way. With some effort invested in the theory [aka later relaxations], the resulting models are still scalable and would guarantee computational performance closer to reality and/or optimality.",Need to relax - but perhaps later?,[8503],34,"[52, 66, 111]",577,Immanuel M. Bomze,62,3,01,Keynotes,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,Sportshallen [building - 101],"['Global Optimization', 'Machine Learning', 'Programming, Mixed-Integer']",MB-01
"In recent years, with the increase in global production and demand, transportation problems have become a widely studied area, and studies focus on providing high-quality service at the lowest cost. This study considers a bi-objective shipment consolidation and dispatching problem with the objectives of minimizing the total cost and the total distance. In order to create a non-dominated solution set, a multi-objective mixed integer linear programming model is developed and augmented epsilon constraint method is used to generate the non-dominated frontier. However, this approach is not capable of finding the non-dominated solution set in a reasonable time, even for small-sized instances, and therefore, we propose a multi-objective variable neighborhood search heuristic. To measure the performance of the proposed approach, a computational experiment is conducted on randomly generated instances available in the literature. The experimental results indicate that the multi-objective variable neighborhood search heuristic performs efficiently in reasonable time.",Multi-Objective Shipment Consolidation and Dispatching Problem,"[4848, 59088, 22492]",786,"[143, 145, 77]",578,Heuristics for Vehicle Routing 3,5,3,58,VeRoLog - Vehicle Routing and Logistics,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S07 [building - 101],"['Transportation', 'Vehicle Routing', 'Multi-Objective Decision Making']",MB-58
"We consider the complex cut polytope - the convex hull of Hermitian rank-one matrices xx*, where the elements of the n-dimensional vector x are complex m-th unit roots. These polytopes find applications in MAX-3-CUT, digital communication, and more generally, complex quadratic programming. For m = 2, the complex cut polytope corresponds to the well-known real cut polytope.  We provide an exact description of the complex cut polytope for m = n = 3 and investigate second order semidefinite liftings of the complex cut polytope. For such second order liftings, we show a method for reducing the size of the matrix, without weakening the approximation. We support our theoretical findings with numerical experiments.
",Semidefinite liftings for the complex cut polytope,"[76276, 71382, 3287]",170,"[115, 103, 141]",582,Advances in Complex and Real Semidefinite Programming,68,8,38,"Conic Optimization - Theory, Algorithms, and Applications","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,34 [building - 306],"['Programming, Semidefinite', 'Polyhedral Combinatorics', 'Telecommunications']",TB-38
"
This paper explores the application of simulation in the redesign of dispatch processes to address delivery fulfillment and sustainable issues. The study focuses on a company that processes and supplies food to several employee cafeterias in a metropolitan city. The main objective of this company's logistics dispatch is the efficient delivery of final products using a fleet of refrigerated electric vans to ensure the sustainability.  The challenge identified centers on intermittent delivery discrepancies, which lead to customer losses and product waste. By employing simulation and business process management [BPM], the administrative processing time has been reduced by 10 to 15 minutes. This time saving is crucial to food durability and to avoid delays associated with the unloading of incorrect or modified shipments. In addition, real-time fleet tracking has been established using specialized software. This tracking capability allows the company to analyze data continuously, with the goal of minimizing losses, reducing costs and promptly addressing delivery delays with up-to-date information.

",Logistical Support for the shipment of food products ,"[76265, 57534, 58717]",637,"[65, 139, 143]",583,Retail Distribution I,30,12,50,Retail Operations,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,M2 [building - 101],"['Logistics', 'Sustainable Development', 'Transportation']",WA-50
"We consider the block relocation problem [BRP], a combinatorial optimization problem that may arise in storage systems where items are organized in stacks. The objective is to retrieve all items in a predefined order with a minimal number of relocations. It can be distinguished between a restricted and an unrestricted version of the BRP. While in the restricted BRP [R-BRP] only relocations of items located above the item to be retrieved next are permitted, in the unrestricted BRP [U-BRP] all possible relocations are allowed. Existing exact methods concerning the BRP are frequently search-based methods which appear to be very effective. Nevertheless, recent literature concerning the R-BRP has shown that model-based methods can be competitive and therefore should also be taken into consideration. In this paper, we propose a new model-based approach for the U-BRP. It eliminates the fact that the number of variables is increasing with the number of necessary relocations; a disadvantage most mathematical models for the U-BRP have in common.",A new modeling approach for the unrestricted block relocation problem,"[59745, 14715]",174,"[111, 105, 65]",584,Container Stacking and Yard Planning I,52,3,62,OR in Port Operations,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S12 [building - 101],"['Programming, Mixed-Integer', 'Production and Inventory Systems', 'Logistics']",MB-62
"We investigate patterns in the optimal timing of the firm's strategic decision for growth internally or by acquisition. Using tools from spectral analysis, and based upon the Fourier series, we devise an algorithm that monitors the timing of key firm policy decisions. We apply this
algorithm to the firm's decision to invest in three intangible assets related to the firm's organization capital for the purpose of generating financial performance. We assess it using five measures and through the construction of three hypotheses tied to the time horizon for the return
on each investment, its effect on the capacity in the industry and economy, and the degree to which each form of investment is dependent upon the capital market for its mode of financing. Our results imply that there is variation in the timing of each form of investment and interpret
that this variation owes to it taking longer for managers to realize gains when investments are made through mergers and acquisitions relative to internal growth. Managers experience an effect delay in this realization owing to delays in the procurement and application of intangible
investments obtained from mergers and acquisitions that require transfer of technical expertise whereas the smaller effect delay for internal growth reflects the faster ability of knowledge-based firms to turn the knowledge already contained inhouse into financial performance outcomes. Suggestions for future research are provided.
",The role of timing in the firm's decision to grow internally or by acquisition,[73587],60,"[44, 45, 62]",588,Applications of knowledge and innovation in finance,53,4,08,AI & Innovation in Sustainable Finance,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,1020 [building - 202],"['Finance and Banking', 'Financial Modelling', 'Knowledge Engineering and Management']",MC-08
"Hypertension is a major public health issue and the most important modifiable risk factor for cardiovascular disease. Effective hypertension management is hindered by [i] the noise in blood pressure [BP] measurements [technology factor] and [ii] the physicians’ judgment bias [human factor]. The latter refers to the physician’s error in inferring the patient’s true BP from the measurements. This study investigates the role of these two factors in hypertension management from the perspective of the clinical value of information. 

We present an analytical framework with two main modules - a learning module that models how clinical judgments about the patient’s underlying BP are made [with and without bias] and an optimization module that models how optimal treatment decisions are made under various learning strategies. 

Our results suggest that the value of information concerning the patients’ underlying BP depends on [i] the physician's judgment bias and decision flexibility, cardiovascular risk as well as the short-term and long-term BP variability of the patient, and the measurement noise of the device. In addition, among the two types of judgment bias [i.e., under-estimation and over-estimation], the value of information is much higher under the under-estimation bias, where the clinician undervalues the significance of the observed BP in inferring the true underlying BP.
",Measurement Noise and Judgment Bias in  Hypertension Management,"[76610, 76613, 18827]",608,"[22, 56]",592,Healthcare Analytics,3,4,15,OR in Health Services [ORAHS],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,18 [building - 116],"['Critical Decision Making', 'Health Care']",MC-15
"Flexible mechanisms, in terms of both hardware and market rules, are required to ensure the economic, technical, and environmental sustainability of renewable-dominated electric energy systems. Short-term markets such as intraday markets are very important for wind and solar photovoltaic producers since the prediction of their power generation is more trustable just a few hours ahead of the actual energy supply. In this work, we tackle the problem of a wind power generator that aims at deciding the offers to be made in the continuous intraday market taking into account the possibility of participating in consecutive trading floors, such as in the auction-based intraday market and the balancing market. The model is formulated as a multi-stage stochastic programming problem considering the uncertainty of the wind power availability, the electricity prices in each trading floor, and the probability of acceptance of the offers in the continuous market. The CVaR is introduced in the formulation to control the risk level of the solution. The model is tested with real data from the Spanish power system. The scenarios of wind power at each decision stage are generated considering the evolution of the forecast error distribution according to the time spanning between the offering and the delivery time. Numerical results of different sensitivity analyses will be presented.",Optimal participation of wind producers in the continuous intraday market - a multi-stage stochastic approach,"[30692, 18498, 12583]",844,"[36, 136, 47]",595,OR in Electricity Markets,23,3,19,OR in Energy,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,44 [building - 116],"['Electricity Markets', 'Stochastic Optimization', 'Forecasting']",MB-19
"An energy community is a legal figure, recently coined by the European Union, that creates a framework to encourage active participation of citizens and local entities in the energy transition to net-zero. In this work, we study the optimal participation of energy communities in day-ahead, reserve, and intraday electricity markets.

The motivation to do so is that there are time periods where energy communities cannot meet their internal demand, and periods where they generate excess electricity. This is because most of the electricity they generate comes from variable renewable resources like solar and wind. Electricity market participation is a natural way to ensure they meet their internal demand at all times, and, simultaneously, make the most of the excess electricity.

We propose a multi-stage stochastic programming model that captures variable renewable and electricity price uncertainty. The multi-stage aspect models the different times at which variable renewable generation is considered to be known and electricity prices from different markets are revealed. This results in a very large scenario tree with 34 stages, and hence a very large optimization problem. Scenario reduction techniques are applied to make the problem tractable. Case studies with real data are discussed, considering different energy community configurations, to analyse proposed regulatory frameworks in Europe. The added value of considering stochasticity in this problem is also analysed.",Optimal Participation of Energy Communities in Electricity Markets under Uncertainty. A Multi-Stage Stochastic Programming Approach.,"[73010, 51173, 71912, 5708]",533,"[117, 93, 136]",597,Stochastic Optimization - Advanced Applications,49,13,34,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,43 [building - 303A],"['Programming, Stochastic', 'OR in Energy', 'Stochastic Optimization']",WB-34
"We develop universal gradient methods for Stochastic Convex Optimization [SCO]. Our algorithms automatically adapt not only to the oracle's noise but also to the Hölder smoothness of the objective function without a priori knowledge of the particular setting. The key ingredient is a novel strategy for adjusting step-size coefficients in the Stochastic Gradient Method [SGD]. Unlike AdaGrad, which accumulates gradient norms, our Universal Gradient Method accumulates appropriate combinations of gradient- and iterate differences. The resulting algorithm has state-of-the-art worst-case convergence rate guarantees for the entire Hölder class including, in particular, both nonsmooth functions and those with Lipschitz continuous gradient. We also present the Universal Fast Gradient Method for SCO enjoying optimal efficiency estimates.",Universal Gradient Methods for Stochastic Convex Optimization,[63242],258,"[21, 136]",599,Advances in Complexity of Convex and Nonconvex Problems,84,4,32,Advances in large scale nonlinear optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,41 [building - 303A],"['Convex Optimization', 'Stochastic Optimization']",MC-32
"In the construction industry, lattice joists [referred to as an item in this study] are precast structures that combine a steel truss with a concrete base. Their production is based on a client's order that specifies the quantity, length, and deadline for delivery. A process manager uses this information to create a weekly production plan that determines the number and type of items to produce in each period. This study considers a real-world production planning problem in a Brazilian lattice slab factory. The primary concerns of the factory include minimizing the waste of steel trusses and optimizing setups and inventory. Decisions regarding purchasing steel trusses [objects] and managing stocks in different sizes are also considered. We propose two alternative mathematical formulations for this integrated lot sizing and cutting stock problem. Additionally, solution strategies are proposed to address the resulting mixed-integer programming models. Computational tests are conducted to evaluate the effectiveness of the proposed model and the solution approaches.",Integrated lot-sizing and cutting stock problem applied to lattice slab production,"[9828, 68186, 23765]",804,"[105, 23, 13]",603,Integrated lot-sizing problems,32,3,49,"Lot Sizing, Lot Scheduling and Production Planning","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,M1 [building - 101],"['Production and Inventory Systems', 'Cutting and Packing', 'Column Generation']",MB-49
"The guillotine rectangular cutting problem deals with a single rectangular plate of raw material and a collection of rectangular items to be cut from the plate. Each item is associated with a profit and a demand. The problem searches for a feasible layout of a subset of items on the plate so as to maximize the total profit of the selected items. The guillotine constraint restricts feasible layouts to those that can be obtained via guillotine edge-to-edge cuts that run parallel to an edge of the plate. We propose a novel constraint programming model that is suitable for guillotine cutting with an arbitrary number of stages of alternating horizontal and vertical guillotine cuts. This is an assignment-based model that models guillotine cuts using a constant number of rectangular regions, with some regions allocated to items. It treats the entire plate as a primary region and decides on the guillotine cuts required to split the regions recursively till they produce space for the items. To speed the search, the model explores the strength of cumulative scheduling relaxations of the cutting problem. Our model is a successful alternative to more traditional mixed-integer linear programming [MIP] models. It outperforms a number of state-of-the-art MIPs on a set of small- and moderate-sized benchmark instances and proves optimality for several instances that remain challenging for these MIPs.",A Constraint Programming Solution to the Guillotine Rectangular Cutting Problem,"[58363, 64873]",89,"[23, 107, 129]",604,Cutting and Packing 1 - 2D rectangular,81,2,07,Cutting and Packing [ESICUP],"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,1019 [building - 202],"['Cutting and Packing', 'Programming, Constraint', 'Scheduling']",MA-07
"In today's data-driven world, efficient collection of information is crucial. We generalize this on a graph and propose a model that can be applied to a wide range of information collection problems. To solve an information collection problem on a graph where nodes contain retrievable information within a specific range, we introduce a two-phase approach. In the first phase, we pre-process the graph to determine the information that can be collected by moving through edges of the graph. In the second phase, we use a mixed-integer linear programming [MILP] model to maximize the amount of valuable information collected by traversing the edges of the graph within a given time limit. We provide extensions that can accommodate additional assumptions such as varying speed, information degradation, and threat or detection avoidance. Our model has practical applications for any problem where an agent, sensor, or decision maker is collecting information from a space.",On a Generalization of the Information Collection Problem,"[76314, 76313, 76620, 5144]",787,"[111, 53, 145]",605,MILPs for Vehicle Routing 2,5,12,58,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S07 [building - 101],"['Programming, Mixed-Integer', 'Graphs and Networks', 'Vehicle Routing']",WA-58
"Explainable AI [XAI] aims to enhance the transparency and interpretability of AI systems, a crucial need in decision analytics where understanding AI decision-making processes can significantly influence trust and operational adoption. Traditional XAI models often focus on one-to-one mappings between inputs and outputs, providing localized explanations for individual predictions. This approach, while useful, falls short in capturing the complex, interconnected relationships inherent in many real-world data sets. It particularly struggles with dynamic environments where feedback loops and interactions among variables play critical roles in shaping outcomes. These limitations highlight the need for more sophisticated methods that can unravel the intricate web of relationships and their causal effects on predictions. Causal machine learning offers a promising avenue for advancing explainability by focusing on the identification and understanding of causal relationships rather than mere correlations. Unlike traditional XAI models, causal approaches seek to model the entire system of variables and their interactions, including feedback loops. This enables a more comprehensive view of how changes in one part of the system can ripple through and affect other parts, providing insights into the mechanisms driving predictions. This enables insights into the mechanisms driving predictions.",Causal Machine Learning - A Methodological Shift in Explainable AI for Decision Making,[76614],111,"[26, 7, 53]",607,Scenarios and foresight practices - Behavioural issues I,13,12,11,Behavioural OR,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,12 [building - 116],"['Decision Support Systems', 'Analytics and Data Science', 'Graphs and Networks']",WA-11
"Making accurate predictions of the true production frontier is critical for reliable efficiency analysis. However, traditional deterministic non-parametric methods like Free Disposal Hull [FDH] or Data Envelopment Analysis [DEA] provide approximations of the production frontier that suffer from overfitting, systematically underestimating firms' inefficiency and yielding inaccurate predictions of the output. In this work, we propose a new approach that, following the machine learning paradigm, provides a more accurate prediction of the underlying true production frontier by adapting the Gradient Tree Boosting algorithm to the production context. We prove that the new estimator satisfies certain required regulatory conditions such as envelopment of data and monotonicity. The performance of the new models is evaluated through a computation experience that shows the outperformance of the new approach in terms of mean squared error and bias in relation to the standard techniques. Moreover, we show how to calculate different efficiency measures using the estimator determined through the new algorithm. Nevertheless, from a computational point of view, the new approach presents thousands of decision variables, making it computationally complex to solve. To tackle this problem, we also propose and check a heuristic approximation for the exact measures.",Improving the estimation of production functions through machine learning - a gradient boosting approach,"[76624, 14048]",936,"[24, 66]",613,DEA and Machine Learning,89,3,48,Data Envelopment Analysis and its Application,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,60 [building - 324],"['Data Envelopment Analysis', 'Machine Learning']",MB-48
"This work addresses the challenge of decreasing vessel turnaround time in Roll-on Roll-off [RoRo] shipping as a means to achieve emission reduction by enabling time savings that allow for slower sailing. RoRo shipping has received comparatively less attention than container shipping by research, yet it is of high relevance, especially in Europe with its long shorelines. The study investigates the potential of stevedoring optimization by extending the Dual Cycling Problem from the literature, where on- and off-loading operations happen simultaneously. To that end, we model a continuous time horizon, differentiate between driver-handled cargo and stevedore-handled cargo with heterogeneous processing times, and solve the assignment and sequencing of operations for the stevedores. We propose a mixed integer linear programming model to solve the resulting problem. Given high computational effort for instances of realistic size, we further propose a biased random-key genetic algorithm, a random construction heuristic, and rule-based heuristics to find good solutions in short runtime. We present computational results demonstrating the effectiveness of the proposed approaches under varying conditions. Additionally, managerial insights clarify the superiority of stevedoring with a dual cycling strategy over single cycling, the impact of different cargo-handling policies, and the effects of different stowage policies on the turnaround time.",Optimizing RoRo Stevedoring Operations - Dual Cycling for Vessel Turnaround Efficiency,"[72606, 59653, 19297, 13086]",669,"[70, 129, 111]",615,Seaside Planning I,52,2,62,OR in Port Operations,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S12 [building - 101],"['Maritime applications', 'Scheduling', 'Programming, Mixed-Integer']",MA-62
"This paper considers the problem of determining lines and frequencies in a public transport system. In comparison to existing approaches, we explicitly consider congestion and assume that passengers may choose different routes to reduce discomfort due to crowding. Our solution approach targets at finding a system-optimal solution by generating passenger routes in a dynamic fashion, whilst also adding cutting planes to deal with the non-linearity introduced by the congestion terms. Since passengers may deviate from system-optimal routes, line plans are evaluated by computing a user-equilibrium routing based on Wardrop's principle. Case studies demonstrate that incorporating congestion leads to fundamentally different line plans, that achieve a lower perceived travel time both for the system-optimal routing and for the user-equilibrium. ",Row and column generation for line planning under congestion,[53556],283,"[143, 119, 13]",616,Network Design and Line Planning for Public Transportation 1,85,9,51,Public Transport Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,M5 [building - 101],"['Transportation', 'Public Local Transportation Systems', 'Column Generation']",TC-51
"We consider a multi-product stochastic inventory problem in which the retailer faces cash constraints but it can leverages business overdraft to deal with unexpected cash shortage. We propose a stochastic programming formulation for this problem and apply stochastic dual dynamic programming [SDDP] to solve it. Some of the improvements of SDDP are considered and developed to make the algorithm more computational efficient. Finally, numerical experiments are conducted to evaluate the performance of SDDP and obtain managerial insights. ",Multi-product stochastic inventory problem with business overdraft,[76619],795,"[136, 61, 72]",618,Inventory Models,50,9,39,Stochastic Modelling,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,35 [building - 306],"['Stochastic Optimization', 'Inventory', 'Mathematical Programming']",TC-39
"Data-driven stochastic optimization has empowered decision-making by providing an effective way to exploit data to tackle complex problems in many domains. However, there is still a lack of theory and methods to quantify the sensitivity of the decision cost to the natural input to stochastic optimization, namely, the data set. In this talk we show that Distributionally Robust Optimization [DRO] offers a natural framework to perform this sensitivity analysis by establishing the notion of marginal value of the quality of a data set. We discuss the ability of the Wasserstein metric to encode data quality and then introduce a Wasserstein DRO formulation to compute the marginal value of data sets from multiple sources or providers that may refer to the same uncertain input parameter and/or to different ones. ",Defining and Quantifying the Marginal Value of a Data Set via Distributionally Robust Optimization,"[39196, 76631, 76633]",430,"[117, 26, 136]",620,Data Valuation from Data-driven Optimization,49,12,35,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,44 [building - 303A],"['Programming, Stochastic', 'Decision Support Systems', 'Stochastic Optimization']",WA-35
"Renewable energy generation in microgrids can have an important role in the decarbonization of electricity networks. Microgrids can also provide ancillary services to the main grid during different operational circumstances. This paper explores the use of microgrids during cold-load pickup [CLPU] phenomena, which are typical after an outage in distribution networks with high share of thermostatically controlled load [TCL]. As load loses diversity after an outage CLPU phenomena is a drastic increase after re-connection. We present a mixed-integer linear programming [MILP] formulation for scheduling service restoration [SR]. The proposed model computes the optimal re-connection sequence, generation and storage levels for the microgrids as well as power supply level from the main grid. Unlike existing literature this paper uses a network with multiple microgrids. It also allows for different conditions at the microgrids - with some operating in islanded mode and having diversified load, while others being in outage and having to do service restoration with CLPU. The model is tested on different modifications of the IEEE-13 feeder network. The proposed approach is suitable for practical SR scheduling and to test the CLPU capabilities of a distribution network during the planning process. The model formulation has the flexibility to allow for additional improvements in precision and complexity in estimating its key components. ",Optimal Integration of Microgrids in Smart Grids During Cold-Load Pick-Up,"[73376, 3287]",857,"[93, 111, 37]",622,OR in Energy III,23,15,19,OR in Energy,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,44 [building - 116],"['OR in Energy', 'Programming, Mixed-Integer', 'Energy Policy and Planning']",WD-19
"Decision trees are one of the most famous methods for solving classification problems, mainly because of their good interpretability properties. Moreover, due to advances in recent years in mixed-integer optimization, several models have been proposed to formulate the problem of computing optimal classification trees. The goal is, given a set of labeled points, to split the feature space with hyperplanes and assign a class to each partition. In certain scenarios, however, labels are exclusively accessible for a subset of the given points. Additionally, this subset may be non-representative, such as in the case of self-selection in a survey. Semi-supervised decision trees tackle the setting of labeled and unlabeled data and often contribute to enhancing the reliability of the results. Furthermore, undisclosed sources may provide extra information about the size of the classes. We propose a mixed-integer linear optimization model for computing semi-supervised optimal classification trees that cover the setting of labeled and unlabeled data points as well as the overall number of points in each class for a binary classification. Our numerical results show that our approach leads to a better accuracy and a better Matthews correlation coefficient for biased samples compared to other optimal classification trees, even if only few labeled points are available.",Mixed-integer linear optimization for semi-supervised optimal classification trees,"[75780, 55237, 23956]",73,"[66, 111]",624,"Advancements of OR-analytics in statistics, machine learning and data science 2",16,3,28,"Advancements of OR-analytics in statistics, machine learning and data science","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,065 [building - 208],"['Machine Learning', 'Programming, Mixed-Integer']",MB-28
"When using machine learning for automated prediction, it is important to account for fairness in the prediction. Fairness in machine learning aims to ensure that biases in the data and model inaccuracies do not lead to discriminatory decisions. E.g., predictions from fair machine learning models should not discriminate against sensitive variables such as sexual orientation and ethnicity.

A fundamental assumption in machine learning is the independence of observations. However, this assumption often doesn't hold true for data describing social phenomena, where data points are often clustered based. Hence, if the machine learning models do not account for the cluster correlations, the results may be biased. Especially high is the bias in cases where the cluster assignment is correlated to the variable of interest. 

We present a fair mixed effects support vector machine algorithm that can handle both problems simultaneously. With a reproducible simulation study we demonstrate the impact of clustered data on the quality of fair machine learning predictions.",Fair mixed effects support vector machine,"[76641, 55237]",235,"[66, 7, 114]",625,Fairness and responsible AI,16,7,28,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,065 [building - 208],"['Machine Learning', 'Analytics and Data Science', 'Programming, Quadratic']",TA-28
"
Leveraging Social Return on Investment [SROI] for Enhanced Social Impact offers a ground breaking approach to boosting the effectiveness of social initiatives through efficiency analysis, sustainable development, and decision support systems. This method quantifies the social, economic, and environmental value of projects, providing a monetary perspective on their impact. By integrating SROI into decision-making processes, organizations can pinpoint the most efficient and impactful ways to allocate resources, aligning closely with sustainable development objectives.

The paper highlights how incorporating SROI metrics into decision support systems aids in the systematic evaluation of social investments, facilitating a data-driven approach to choosing projects that yield the highest social returns. This not only enhances the strategic allocation of funds but also improves transparency and accountability, enabling organizations to communicate their achievements more effectively.

In essence, using SROI as a tool for enhanced social impact fosters a culture of evidence-based decision-making. It ensures that investments are directed towards initiatives with the greatest potential for positive social change, thereby contributing significantly to sustainable development efforts. This concise approach underscores the importance of SROI in optimizing social impact and advancing the global agenda for a more sustainable and equitable future.",Leveraging SROI for Enhanced Social Impact,"[76642, 77858]",274,"[139, 26, 35]",626," Enhancement of circularity, inclusivity, and smartness in cities I",79,4,18,Sustainable Cities,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,42 [building - 116],"['Sustainable Development', 'Decision Support Systems', 'Efficiency Analysis']",MC-18
"For market relations, a controlled differential model with supply-demand functions and with delay parameter in control  is constructed. We call the difference between supply and demand functions the disbalance index. It is assumed that consumer demand at any point in time will be satisfied through pre-ordering. To describe the development of the market equilibrium process over time, a dynamic model with an integral disbalance index is considered. For the proposed model an optimal control problem is studied. Namely, the existence theorem of an optimal control and the necessary conditions for optimality are formulated  on the basis of results obtained in [1, 2]. In the case of a linear function of supply and demand, all controls that are doubtful of optimality are detected. An example is discussed at the end. Another case  of optimal control of market relations was studied in [3].

References:

 [1] Kharatishvili G. Tadumadze T. Formulas for the variation of a solution and optimal control problems for differential equations with retarded arguments. J. Math. Sci., 140 [1] [2007], 1-175.

[2] Tadumadze T. Variation formulas  of  solutions for functional differential equations with  several  constant delays and  their applications in optimal control problems.   Mem . Differ. Equ.  Math . Phys., 70 [2017], 7-97.

[3] Dvalishvili Ph. Tadumadze T. Optimization of market relation model with delay.  Journal of Modern Technology and Engineering 4[1] [2019] 5-10 ",Optimal control of one economic differential model of market relations,"[76361, 40696, 76653]",339,"[82, 33, 20]",628,Optimal control in supply chain management,90,2,33,Optimal Control Theory and Applications,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,42 [building - 303A],"['Optimal Control', 'Economic Modeling', 'Control Theory']",MA-33
"We consider joint inventory and pricing decisions of substitutable products  in an assortment that are differentiated by some primary and secondary attributes captured by a nested logit consumer choice.  The choice model we consider is characterized by nests having different price elasticity parameters and dissimilarity indices, which generalizes recent work.  On the supply side, we consider a newsvendor-type setting under Normal demand which is generated from an approximation to a Poisson arrival process.  Motivated by results in the literature on the riskless case, which ignores inventory costs, we assume that the prices of all products in the assortment can be written as a function of a single decision variable. We verify that this simplifying assumption yield near-optimal results based on a detailed numerical study.  We then analyze the properties of the expected profit at optimal inventory levels as a function of the pricing decision variable. ",Pricing and Inventory Decisions for an Assortment under a Generalized Nested Logit Choice,"[36103, 76644]",481,"[71, 124, 130]",629,Assortment Management,30,9,50,Retail Operations,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,M2 [building - 101],"['Marketing', 'Revenue Management and Pricing', 'Service Operations']",TC-50
"When drones are to be used in last-mile delivery, the limited drone flight range poses a major challenge. To mitigate this drawback, we allow the drone to be launched and recovered from moving trucks. Further, we allow for variable drone velocities, as the energy consumption and therefore the drone flight range is heavily dependent on its velocity. We consider one truck that launches the drone, and another one that recovers it. Both trucks drive along streets, and the launching and recovery can take place on any position along these streets. To synchronize the trucks with the drone, the departure times of both trucks are determined as well. Further, we incorporate obstacles that the drone must avoid, which may represent buildings, mountains, no-fly zones, or areas with a high probability of the drone being captured. We take on a geometric approach where the drone operates in the Euclidean space, and to the best of our knowledge, such a geometric viewpoint has not yet been adopted in a MILP for truck-drone last-mile delivery. Our MILP determines a collision-free trajectory for the drone in an environment with obstacles where the drone is launched and recovered by moving vehicles, such that the parcel is delivered as early as possible. The results of our extensive computational study, including the evaluation of three valid inequalities, prove the usefulness of our model, as even large instances with up to 300 obstacles can be solved within reasonable computation time.",Collision-free Trajectory Planning for Drones with Velocity Dependent Energy Consumption and Moving Piggyback Vehicles,"[75121, 14715]",977,"[143, 65]",630,Last mile delivery with drones,6,15,56,Transportation,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S04 [building - 101],"['Transportation', 'Logistics']",WD-56
"Carbon tax is one of the widely used emission policies to encourage companies to decrease their emissions. Although numerous attempts have been made to investigate the effects of a carbon tax policy on the traditional selling business model, the effects of this policy on the servitization business model, in which the firms sell the use or the outcome of a product, remain as an important research gap. Motivated by this fact, this study proposes Stackelberg Game models to investigate the economic, environmental, and social effects of a carbon tax policy on the servitizing firms. Equilibrium results reveal that carbon tax yields an increase in the usage fee, and thus aggregate usage in the market decreases as a result of working under a carbon tax policy. This decrease in aggregate usage, on the one hand, yields a decrease in the environmental impact, and on the other hand, deteriorates the servitizing firm’s profit.",Should the Governments Charge a Carbon Tax from Servitizing Firms?,[43356],485,"[138, 100, 50]",631,Sustainable Food and Health Care Logistics,19,10,24,Sustainable Supply Chains,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,83 [building - 116],"['Supply Chain Management', 'OR in Sustainability', 'Game Theory']",TD-24
"In Vehicle Routing Problem with Backhauls [VRPB] is a special type of vehicle routing problem in an effort to reduce the overall cost of shipment from/to a depot. In VRPB, customers are divided into two categories as linehaul customers and backhaul customers. A vehicle that starts from depot first delivers goods to linehaul customers, then bring goods from backhaul customers to the depot on the way back. In this study, we tackle the VRPB problem with electric vehicles. Electric vehicles require special attention in route planning as they also require stops for charging. To this end, we developed a two-stage optimization procedure which constructs the routes for delivery and backhauls and determine the location of EV charging stations by taking into account both constructed routes and charging needs of the vehicles. The method is presented on an example dataset received from a manufacturing company located in Eskisehir, Türkiye.",Vehicle Routing Problem with Backhauls - An Application with Electric Vehicles,[49764],488,"[145, 65, 64]",632,Sustainability in Vehicle Routing,19,7,24,Sustainable Supply Chains,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,83 [building - 116],"['Vehicle Routing', 'Logistics', 'Location']",TA-24
"In the last years, tools from Dynamical Systems have been fruitfully applied to study existing accelerated optimization methods and to develop new ones. Typically, continuous-time models for accelerated methods are mechanical systems with damping.
In this talk, we present an optimization method based on a conservative mechanical system, where the objective function plays the role of the potential energy.
Due to the absence of damping, the convergence of this method completely relies on the adaptive restart strategy - a] the initial velocity is set equal to 0; b] by the conservation of the mechanical energy, part of the initial potential energy is transformed into kinetic energy; c] when a proper restart condition is met, the velocity is reset to zero and the kinetic energy at the restart time is instantly dissipated.
We prove the convergence result both for the continuous-time method and for the discrete-time version. Finally, we discuss some possible extensions to the nonsmooth case, with particular focus on l1-regularization.

[1] A. Scagliotti, P. Colli Franzone. A piecewise conservative method for unconstrained convex optimization, COAP [2022].
[2] A. Scagliotti, P. Colli Franzone. A subgradient method with constant step-size for l1-composite optimization, BUMI [2023].",Adaptive restart of conservative dynamics for convex optimization,[76648],208,"[21, 5]",635,Algorithms for machine learning and inverse problems - adaptive strategies,84,5,32,Advances in large scale nonlinear optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,41 [building - 303A],"['Convex Optimization', 'Algorithms']",MD-32
"Some components used to assemble a final product are assembled in-house in separate assembly lines. Subcomponents are used to assemble components. Some subcomponents are purchased outside or produced outside the plant and delivered. We want to decide the order quantity of subcomponent which is used in the production of component. 
As an example, in many auto manufacturing plants, vehicle engine [component] is assembled in-house and used in vehicle [final product] assembly line. Major components [subcomponents] of engine are engine block, crank shaft, piston…. Some auto manufacturers assemble engine in-house and make engine block outside the vehicle manufacturing plant and deliver it. We want to find the optimal delivery batch size [order size] of engine block to vehicle manufacturing plant.
We develop a mathematical model for the problem and obtain a simple optimal poicy.",Optimal Order Quantity for Subcomponent - An Extension of EOQ Model,[1845],837,"[61, 105]",638,Production planning problems,32,10,49,"Lot Sizing, Lot Scheduling and Production Planning","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,M1 [building - 101],"['Inventory', 'Production and Inventory Systems']",TD-49
"This paper develops a new robust optimization model for production and distribution planning considering competition among supply chains. The model maximizes the profit of a production and distribution corporation providing deteriorating products. The corporation's market share is enhanced by concentrating on freshness, some encouraging policies, and competitive characteristics. The model considers price, distance, and service level the most critical competitive characteristics. Regarding these characteristics, Huff's gravity model is developed to assess retailers' behavior better and compute market share rightly. Therefore, a mixture of compensatory and non-compensatory approaches considering the comparative attractiveness associated with each characteristic is devised to estimate the final utility of the regarded corporation for the retailers. Also, a robust optimization method is used to cope with the uncertainty. Due to the NP-hardness of the problem, an adaptive large neighborhood search algorithm is suggested. The model's performance is analyzed by solving a real-world industrial case study. Based on the computational examination, employing this study can seriously increase the corporation's profit by improving its market share.",A novel competitive model for production and distribution planning of deteriorating products under uncertainty - a case study,"[76109, 43561]",425,"[105, 138, 84]",641,Retail Inventory Management I,30,3,50,Retail Operations,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,M2 [building - 101],"['Production and Inventory Systems', 'Supply Chain Management', 'Optimization Modeling']",MB-50
"In this talk we present two Fenchel-type dual problems for a DC [differ-
ence of convex functions] optimization primal problem. These dual problems are built by means of the c-conjugation scheme, a pattern of conjugation which has been shown to be suitable for evenly convex functions. In particular, we study characterizations of weak, strong and stable strong duality for both pairs of primal-dual problems. Finally, we also give conditions which relate the existence of strong and stable strong duality for both pairs.",On Fenchel c-conjugate dual problems for DC optimization,"[71272, 26649]",266,"[21, 84, 72]",644,Infinite Optimization - stability and duality,82,12,42,Variational Analysis and Continuous Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,98 [building - 306],"['Convex Optimization', 'Optimization Modeling', 'Mathematical Programming']",WA-42
"In this study, we address the integrated production planning, berth allocation, and quay crane scheduling problem. The problem is formulated as a Mixed-Integer Linear Programming [MILP] model with the objective of maximizing total revenue while considering demurrage costs. The computational results are derived from both real-life instances and generated instances associated with the OCP Safi complex; an industrial fertilizer site operated by the OCP Group. The results show that we achieve near-optimal solutions in less than one hour using the CPLEX MIP solver.","A MILP formulation for Integrated Production Planning, Berth Allocation, and Quay Crane Scheduling Problem in tidal ports with multiple quays","[73820, 74112, 73821, 76654, 74114]",82,"[111, 138, 129]",645,Seaside Planning III,52,7,62,OR in Port Operations,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S12 [building - 101],"['Programming, Mixed-Integer', 'Supply Chain Management', 'Scheduling']",TA-62
"Uncertain optimization problems with decision-dependent information discovery allow the decision maker to control the timing of information discovery, in contrast to the classic multistage setting where uncertain parameters are revealed sequentially based on a  prescribed filtration. This problem class is useful in a wide range of applications, however, its assimilation is partly limited by the lack of efficient solution schemes. In this work, we study two-stage robust optimization problems with decision-dependent information discovery where uncertainty appears in the objective function. The paper's contributions are twofold - we develop an exact solution scheme based on a nested decomposition algorithm, and we improve upon the existing K-adaptability approximation by strengthening its formulation using techniques from the integer programming literature. We use the orienteering problem as our working example, a challenging problem from the logistics literature which naturally fits within this framework. The complex structure of the routing recourse problem forms a challenging test bed for the proposed solution schemes, in which we show that the exact solution method outperforms at times the K-adaptability approximation, however, the strengthened K-adaptability formulation can provide good quality solutions in larger instances while significantly outperforming existing approximation schemes even in the decision independent information discovery setting. ",Exact and Approximate Schemes for Robust Optimization Problems with Decision Dependent Information Discovery,"[51181, 76660, 51100, 42148]",272,"[127, 109, 143]",647,Stochastic Optimization with Decision-Dependent Uncertainty,49,9,35,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,44 [building - 303A],"['Robust Optimization', 'Programming, Integer', 'Transportation']",TC-35
"The challenges posed by urban growth and development are indeed significant world-wide. As cities expand, they grapple with issues related to space, resources, and facilities. The migration from rural to urban areas exacerbates these challenges, leading to increased demand for services such as energy, sanitation, and mobility. Pollution and other environ-mental problems also arise due to rapid population growth in urban centers. Traditionally, decision-making in municipalities followed top-down approaches, often overlooking the perspectives and desires of citizens. However, a paradigm shift toward Smart Cities [SC] emphasizes multi-stakeholder involvement and shared visions. To address this, a novel decision-making methodology is proposed, integrating the needs of various stakeholders, including citizens. The stages of the methodology are a] SC Paradigm - Which emphasizes sustainable development, green growth, and collaboration among stakeholders; b] Survey-Based Approach - The process begins with a survey to capture community needs. This data collection phase is crucial for understanding the diverse perspectives within the community; c] ELECTRE - a decision-making technique, is employed to formalize and prioritize these needs. It balances different interests and helps optimize project design. This methodology was applied to a neighborhood in the city of Santiago de Chile, generating solutions to the main problems faced by residents of the sector.",Enhancing Infrastructure Decision-Making in Smart Cities - A Support Methodology,"[58829, 76661, 58717, 57534, 76662]",911,"[22, 100, 139]",648,"Assessment Methods for Shaping the Green, Inclusive, and Digital Cities II",79,3,18,Sustainable Cities,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,42 [building - 116],"['Critical Decision Making', 'OR in Sustainability', 'Sustainable Development']",MB-18
"The European Green Deal envisages ensuring climate neutrality by 2050. This includes a modern and competitive economy based on a clean environment and neutralized climate. Companies are playing a significant role in this process. The Green Deal initiative encourages other countries to move towards a green economy, so all countries can use the concept of the Green Deal as a symbol of sustainability. The purpose is to evaluate the financial and innovation performance of the largest European, North American and Asian-Oceanian companies, revealing how companies contribute to the implementation of the Green Deal. Systematization of scientific literature, selection of input and output variables based on the results of scientific literature, analysis of statistical relationships, and panel regression models as the main research methods are applied to achieve this purpose. The results show that companies from all three regions [European, North American and Asian-Oceanian] significantly concur with the implementation of the Green Deal. Return on assets and leverage as financial performance indicators significantly influence the results of environmental performance. Research and experimental development expenditure as an indicator of innovation performance has a significant impact on environmental performance as well. Companies should find more opportunities to pursue financial, innovation and environmental goals together to achieve long-term climate goals.","The Impact of Companies' Performance on the Implementation of the Green Deal in the regions of Europe, North America and Asia-Oceania","[76607, 46721]",3,"[45, 94, 100]",649,OR in Accounting - Performance and ESG,7,14,59,OR in Financial and Management Accounting,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S08 [building - 101],"['Financial Modelling', 'OR in Environment and Climate change', 'OR in Sustainability']",WC-59
"As a natural monopoly transportation approach, the railway service has gained increasing attention from the European Union. Hence, a comprehensive performance analysis system is needed to urge infrastructure managers to reduce costs. Two-stage network [2SN] Data Envelopment Analysis [DEA] is widely used as a productive efficiency analysis technique in railway industries’ managerial decision-making processes. However, the conventional 2SN DEA models are vulnerable to biased outcomes when the generated virtual projection point is an exterior point. Specifically, given the inputs-outputs Production Possibility Set [PPS] in a 2SN system is unknown, the conventional approach identifies the virtual exterior projection point by integrating the inputs-intermediates and intermediates-output PPSs. In this paper, unified non-radial graph models in both the slack-based scheme and the Russell-efficiency scheme are proposed to solve the issue of exterior projection points. We also apply the dual transformation to discuss the shadow price of inputs, outputs, and intermediates through the multiplier model. We apply the proposed novel approach to the dataset regarding European Railway industries. Our approach provides a comprehensive efficiency analysis from both Investment and Operations perspective. The adjusted projection points in our DEA model also provide an applicable benchmark to support the decision-making process. ",The pitfall of virtual exterior projection points in unified Non-Radial Two-Stage Network DEA and its duality - An Empirical Study on Railway industry in Europe,"[76070, 5889]",944,"[24, 143, 35]",657,DEA applications in transportation,89,13,48,Data Envelopment Analysis and its Application,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,60 [building - 324],"['Data Envelopment Analysis', 'Transportation', 'Efficiency Analysis']",WB-48
"This work focuses on tanks’ wine transfers in a winery that are part of the production process of the wine before being bottled. During peak production periods, the winery may face a shortage of available tanks, resulting in unnecessary transfers and increased water usage for tank cleaning as well as higher energy consumption. To address this issue, a mixed integer optimization model has been developed to reduce water and energy consumption by making only the necessary movements, using the minimum number of tanks, and covering the shortest distance. The model is based on three critical elements - the production planning, the tanks, and the route for the wine production. The production planning defines what and when must be produced and the initial mixtures for each class of wine. The ordered sequence of stages to produce each type of wine defines a route. Each stage requires tanks to keep the source and the resulting product. The model ensures each product follows the defined order, using only the necessary tanks and covering the shortest distance. The non-overlapping of tanks is guaranteed, which can be reused once they are empty and clean. The duration of each operation is considered, including transfer time and cleaning after each use. Furthermore, the model ensures that tanks are filled up following wine requirements at each stage to guarantee the quality of the final product. The application of the model is demonstrated in a variety of realistic examples.",Optimizing Winery Production Transfers using a Mixed-Integer Model to Reduce Water and Energy Consumption,[50736],4,"[111, 118, 63]",660,Optimization in Agriculture,20,2,12,OR in Agriculture and Forestry ,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,13 [building - 116],"['Programming, Mixed-Integer', 'Project Management and Scheduling', 'Large Scale Optimization']",MA-12
"We develop and implement methods for determining whether relaxing sparsity con- straints on portfolios improves the investment opportunity set for risk-averse investors. We formulate a new estimation procedure for sparse second-order stochastic spanning based on a greedy algorithm and Linear Programming. We show the optimal recovery of the sparse solution asymptotically whether spanning holds or not. From large equity datasets, we estimate the expected utility loss due to possible under-diversification, and find that there is no benefit from expanding a sparse opportunity set beyond 45 assets. The optimal sparse portfolio invests in 10 industry sectors and cuts tail risk when compared to a sparse mean-variance portfolio. On a rolling-window basis, the number of assets shrinks to 25 assets in crisis periods, while standard factor models cannot explain the performance of the sparse portfolios.",Sparse spanning portfolios and under-diversification with second-order stochastic dominance,"[76675, 76676, 76677]",436,"[110, 45]",661,Novel Optimization Models in Finance,4,12,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S14 [building - 101],"['Programming, Linear', 'Financial Modelling']",WA-63
"Funded by the Grantham Centre for Sustainable Futures, this research explores the development of a dynamic system for urban last-mile delivery, aimed at enhancing efficiency, resilience, and sustainability in urban logistics. Focusing on integrating real-time traffic data and predictive analytics, the study extends a previous work on a consolidation-based multi-modal delivery model, grounded in Life Cycle Assessment methodology. 

Central to this research is the incorporation of real-time traffic data from platforms like Waze. This data is crucial for dynamically reconfiguring delivery routes in response to changing traffic conditions, thereby maintaining operational efficiency and resilience. Alongside, predictive analytics, utilising machine learning algorithms and demand forecasting models, plays a significant role. Focused on Sheffield area, the study also leverages data from platforms like UK Open Data for informed and automated territory reconfiguration processes. We further use Global Mapper software and clustering algorithms for dynamic adjustment of delivery territories.

The implementation strategy included a comprehensive simulation phase using AnyLogic. This phase was instrumental in modelling urban traffic and delivery scenarios, allowing for an effective comparison of the dynamic system against traditional static models. Key performance indicators such as operational costs, delivery times, and environmental impacts were analysed, providing valuable insights.",Enhancing Efficiency and Sustainability through Reconfigurable Delivery Systems,"[74086, 74076]",864,"[65, 66, 131]",667,Sustainability in Distribution and Transportation,64,9,26,Combinatorial Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,012 [building - 208],"['Logistics', 'Machine Learning', 'Simulation']",TC-26
"Fairness in Machine Learning is a prominent research area that, however has received little attention in the context of spatial or geographical data. This work is motivated by a real urban planning database that shows lack of equity in terms of access to green areas. On the basis of the geographically weighted regression [GWR], this work presents a regression model for predicting  spatial responses in fair way. The parameters of the model, namely, the bandwidth parameter and the regression coefficients are estimated in two stages. First, the bandwidth is obtained from a jack-knife cross-validation approach. Second, the regression coefficients are the solutions of a quadratic optimization problem with linear inequality constraints modeling the unfairness of predicted responses. In order to deal with large datasets, an alternating block coordinate descent algorithm is suggested. The performance of the method is illustrated using an assortment of real datasets.",An in-processing and optimization-based method for a fair geographical regression,[68040],123,"[66, 114, 100]",668,Unraveling the Black Box - Advances in Model Explainability,15,13,27,Mathematical Optimization for XAI,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,047 [building - 208],"['Machine Learning', 'Programming, Quadratic', 'OR in Sustainability']",WB-27
"Our talk concerns an optimal control problem [P] related to a generalized Fokker-Planck [FP] equation. First we establish some properties of the solutions to the generalized FP equation using a semigroup approach in an appropriate Sobolev space. Problem [P] is proven to be deeply related to a stochastic optimal control problem [PS] for a McKean-Vlasov equation. We prove next the existence of an optimal control for the deterministic problem [P] and the existence of an optimal control for an approximating optimal control problem [Ph] related to a backward Euler approximation of the generalized FP equation [with a constant discretization step h]. Finally, we show that under additional hypotheses [Ph] converges to [P] in a certain sense. First order necessary optimality conditions for [Ph] are derived as well.
",Optimal control of a generalized Fokker-Planck equation,[76687],898,"[82, 20]",669,Optimal control theory,90,10,33,Optimal Control Theory and Applications,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,42 [building - 303A],"['Optimal Control', 'Control Theory']",TD-33
"Two-stage stochastic programming is a popular framework for optimization under uncertainty, where decision variables are split between first-stage decisions, and second-stage [or recourse] decisions, where the latter can be adjusted after uncertainty is realized. These problems are often formulated using Sample Average Approximation [SAA], where uncertainty is modeled as a finite set of scenarios, resulting in a large deterministic problem, i.e., where the model is repeated for each scenario. The resulting problems can be challenging to solve, and several decomposition approaches have been proposed. More recently, Patel et al. [2022] approximated the expected second-stage objective value for a set of scenarios using a neural network, which can then be embedded in the first-stage problem to produce good heuristic solutions. In this work, we propose approximating the second-stage objective value with a quantile neural network, which can capture uncertainty and is not limited to expected-value optimization, e.g., to optimize the Conditional Value at Risk [CVaR]. We discuss optimization formulations for embedding the quantile neural network and demonstrate the effectiveness of the proposed framework using several computational case studies including mixed-integer and nonlinear problems. Particularly, we test this novel methodology in relevant power systems’ operation problems with high renewable energy penetration.",A Quantile Neural Network Framework for Two-stage Stochastic Optimization - Application to Power Systems Operation,"[76685, 55098, 67082]",147,"[42, 111, 136]",670,Learning-assisted Optimization in Energy Problems,23,2,19,OR in Energy,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,44 [building - 116],"['Expert Systems and Neural Networks', 'Programming, Mixed-Integer', 'Stochastic Optimization']",MA-19
"The two-dimensional non-oriented bin packing problem with due dates consists in packing a set of rectangular items, which may be rotated by 90 degrees, into identical rectangular bins. The bins have equal processing times. The problem searches for a feasible layout of items so as to minimize the maximum lateness, with the lateness of each item being the difference between its due date and the completion time of its assigned bin. This minimax problem is computationally challenging - It has a huge solution space with many alternative solutions whose packing configurations are either symmetric or lead to the same solution cost.
		
We propose two exact approaches. First, we solve the problem as a mixed-integer program [MIP], which we strengthen with a set of feasibility and symmetry-breaking constraints. Our second solution employs constraint programming [CP], and thus benefits from the strength of cumulative scheduling relaxations of the packing problem. Our extensive computational experiments show that MIP, which stands as a more traditional modelling technique within the cutting and packing community, is only successful on small-sized benchmark instances with as few as 60 items. It is outperformed by CP that can solve medium and large instances with up to 100 items in a reasonable time.",Exact Mixed-Integer and Constraint Programming solutions to the two-dimensional bin packing problem with due dates,"[76684, 58363]",89,"[23, 111, 107]",675,Cutting and Packing 1 - 2D rectangular,81,2,07,Cutting and Packing [ESICUP],"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,1019 [building - 202],"['Cutting and Packing', 'Programming, Mixed-Integer', 'Programming, Constraint']",MA-07
"This research introduces the Breakpoint Exact Algorithm with Capacities [BEAC] and the Breakpoint Heuristic Algorithm [BHA], both of which offer substantial advancements in solving the choice-based pricing problem [CPP] with and without capacity constraints. The BEAC, enhancing the Breakpoint Exact Algorithm [BEA] with a capacity management strategy, outperforms the state-of-the-art mixed-integer linear programming [MILP] approach by 20 times in computational speed. The BHA, employing a coordinate descent method, excels in high-dimensional scenarios, showing remarkable efficiency in both capacitated and uncapacitated cases. Notably, it outpaces the MILP by a factor of 100 to 5000 for the capacitated case, and the state-of-the-art Branch and Benders Decomposition approach by several orders of magnitude for the uncapacitated case, while maintaining an average optimality gap of less than 0.2%. The dynamic line search extension of the BHA succeeds in identifying the global optimum in all tested instances, albeit with a significant speed reduction. For future research, other extensions of the BHA to escape local optima should be considered.",Fast Algorithms for Capacitated Continuous Pricing with Discrete Choice Demand Models,"[76694, 75787, 26236]",695,"[10, 5, 124]",676,Customer behaviour,11,5,59,Pricing and Revenue Management,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S08 [building - 101],"['Behavioural OR', 'Algorithms', 'Revenue Management and Pricing']",MD-59
"Customer segmentation plays a key role in improving supply chain management by implementing appropriate marketing strategies. The objectives of this research are to design and validate a multicriteria model to support decision making for sustainable customer segmentation in a business to business context. First, the model based on the transactional customer behaviour is extended by a hierarchy with three main criteria - Recency, Frequency and Monetary [RFM], customer collaboration and growth rates. Customer collaboration includes quota compliance, variety of products and customer commitment to sustainability [reverse logistics and shared information]. Second, the Global Local Net Flow Sorting [GLNF sorting] algorithm is implemented and validated using real company data to classify 8,157 customers of a multinational healthcare company. Third, the SILS quality indicator has been implemented and validated to assess the quality of preference-ordered customer groups. The results are also compared with an alternative model based on data mining [K-means]. The multicriteria system proposed allows to segment thousands of customers in ordered categories by preferences according to company strategies. The segments generated are more homogeneous, robust and understandable by managers than those from alternative methods. These advantages represent a relevant contribution to automating supply chain management while providing detailed analysis tools for decision making.",A Multicriteria Model for Sustainable Customer Segmentation using a Sorting Outranking Method,"[783, 70922, 31644, 19630]",100,"[138, 77, 100]",677,MCDA applications in Engineering and Management 1,44,2,47,Multiple Criteria Decision Analysis,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,50 [building - 324],"['Supply Chain Management', 'Multi-Objective Decision Making', 'OR in Sustainability']",MA-47
"Hospital beds are an important medical resource, and the dynamic admission scheduling of elective patients can help to avoid bed shortages. However, such elective patients scheduling problem is challenging due to the high level of uncertainty behind the bed availability and its impact on admission rates. In fact, Meng et al. [2015] proposed a robust optimization approach to determine the elective admission quotas based on budgeted uncertainty sets to minimize the expected bed shortages. A key challenge to be addressed by this approach is the uncertainty behind the stochastic patient arrival and length of stay [LoS]. However, the budget of the uncertainty set is not easy to determine, which in contrast can be avoided in the robust satisficing approach proposed by Long et al. [2022]. Robust satisficing favors solutions where the risk-aware objective function best achieves an acceptable goal even when the actual probability distribution deviates significantly from the empirical distribution, and it can tolerate greater uncertainty than robust optimization. In this paper, we propose a robust satisficing method for determining quotas for elective admissions, which will best avoid bed shortages under extreme uncertainty. We do numerical experiments for the proposed robust satisficing model based on the MIMIC-IV dataset and compare it with the results generated by robust optimization methods. The results show that the robust satisficing model can withstand greater uncertainty.",Data-Driven Robust Scheduling of Elective Patients,"[76699, 62300]",610,"[127, 56, 129]",681,Admission and discharge,3,15,10,OR in Health Services [ORAHS],"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,11 [building - 116],"['Robust Optimization', 'Health Care', 'Scheduling']",WD-10
"In a typical knockout [KO] pool, each entrant must select a winning team every week of the regular season. If they fail to do so, they no longer have a chance of winning that pool since that loss [or tie], by their chosen team, has removed them from the list of entrants who are still alive in that pool. Also, each entrant is allowed to select a team just once during that season. If this style of pool is extended to a single elimination tournament [SET], like Wimbledon, the World Cup, NCAA’s March Madness, etc., then different players/teams must be taken in each round. Strategies for outlasting every other entrant in such a KO pool will be considered in this talk, and results from recent March Madness tournaments will also be examined. These strategies are quite different from those that may typically be utilized during a regular season KO pool as now one hopes to survive and advance past each round [where significantly fewer choices are available after each round, which is not the case in a weekly KO pool]. Therefore, outlasting all other entrants [who have not already been knocked out] is much harder to achieve. [Many of these SET KO pools require 2 or more choices per round – at least until the semi-finals have been reached.]",Strategies for surviving single elimination tournament knockout pools,[46000],664,"[99, 74]",684,Sports scheduling and optimization,37,7,16,OR in Sports,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,19 [building - 116],"['OR in Sports', 'Metaheuristics']",TA-16
"We examine the queueing performance of a real-time capacity-balancing [CB] algorithm for routing patients from the Emergency Department [ED] to an Observation Unit [OU] or to an inpatient ward. We study the significance of relative bed capacities between the ward and the OU. OUs are designed for specific patient types, with shorter expected hospital stays. By redirecting patients and reserving OU capacity for those who benefit most, overall hospital stay durations are reduced. However, high ward utilization may cause longer ED waiting times. To prevent ED crowding, implementing a CB algorithm requires careful analysis of ward and OU long-run utilization.",Capacity Balancing Algorithm for Placement of Observation Patients - Assessing the Effect on ED Crowding,"[27310, 3024, 76704, 76705]",594,"[56, 121, 131]",685,ED logistics,3,12,10,OR in Health Services [ORAHS],"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,11 [building - 116],"['Health Care', 'Queuing Systems', 'Simulation']",WA-10
"This work presents a case study focusing on the importance of Assets and Liabilities Management [ALM] in a complex economic scenario, of a Brazilian fintech company with a significant cash flow of 27 billion BRL per year. Applying methods from modern portfolio theory, multivariate analysis and data mining, we identify the classifying variables that affect credit risk and liquidity for this company and demonstrate how through a data-oriented risk profile one can optimize profitability with strategies that nonetheless comply with United Nations' Sustainable Development Goals [SDGs]. Specifically, we approach the adequacy of the company's ALM strategy to SDG 8 [Decent Work and Economic Growth], which is of particular importance in the Brazilian context and Latin America in general.",Exploring aspects of risk and sustainability in portfolio composition and Assets and Liabilities Management [ALM] for a Brazilian fintech,"[76336, 76706, 76707, 76708]",930,"[7, 135, 126]",686,Corporate finance risk management,9,7,51,Risk management in finance,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,M5 [building - 101],"['Analytics and Data Science', 'Stochastic Models', 'Risk Analysis and Management']",TA-51
"State Road Transport Undertakings [STUs] in India are state government-operated transport services that connect various landscapes and isolated mountainous regions across the nation, serving as a crucial catalyst for improving public transit. These organizations are driven by a strong sense of social responsibility. As an illustration, they function in locations
with poor income generation to cater to the public. Hence, it is necessary to regularly assess their performance and consistently make attempts to make necessary adjustments. Road transport has a significant role in the economy, but it also contributes significantly to energy consumption and environmental damage, particularly through air pollution. STUs, which run
on high-speed diesel, emit air pollution as a byproduct of fuel combustion. This has sparked worries over its adverse effects on climate change and the long-term viability of ecosystems. The current body of research on performance evaluation of passenger transportation systems fails to consider the fundamental structures of the transportation process and instead approaches it as an opaque entity. As a result, it is unable to accurately determine the exact reason for inefficiencies, which makes it challenging to adjust in the complex transportation process. Prior research on STUs has not considered the possible negative impacts on the environment. ",Evaluating eco-efficiency under dynamic network setting for passenger road transportation - A case study of State Road Transport Undertakings in India,"[23679, 75800]",944,"[24, 35, 143]",688,DEA applications in transportation,89,13,48,Data Envelopment Analysis and its Application,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,60 [building - 324],"['Data Envelopment Analysis', 'Efficiency Analysis', 'Transportation']",WB-48
"The production and assessment of research outputs are pivotal aspects of the university knowledge generation process, influencing spin-offs and collaborations arising from industry-university research endeavors. While numerous studies have utilized research outputs to gauge university performance, many have primarily concentrated on quantitative variables such as the sheer number of publications and patents. This emphasis on quantitative indicators alone may lead to an overestimation and bias in assessing research performance in specific instances. Recognizing this limitation, our study aims to address this gap by employing a two-stage network data envelopment analysis [DEA]. In addition to quantitative variables, our focus extends to incorporating quality attributes like publication and patent citations alongside the number of research projects. Our dataset spans 57 research-active South Korean private universities from 2010 to 2019. Empirical findings reveal that integrating quality attributes into an appropriate production model enhances the accuracy and reliability of efficiency measurement results. Furthermore, the use of the two-stage network DEA allows for a nuanced observation of the disparity between the quantity and quality stages of university research outputs, presenting a valuable advancement beyond traditional black-box DEA models.",Evaluation of the efficiency of research outputs of Korean private universities - a two-stage network DEA approach,[76711],938,"[24, 35, 33]",690,DEA applications in Education and Health I,89,5,48,Data Envelopment Analysis and its Application,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,60 [building - 324],"['Data Envelopment Analysis', 'Efficiency Analysis', 'Economic Modeling']",MD-48
"Influence maximization [IM] has been widely studied in recent decades, aiming to maximize the spread of influence over networks. Despite many works for static networks, fewer research studies have been dedicated to the IM problem for dynamic
networks, which creates many challenges. An IM method for such an environment, should consider its dynamics and perform well under different network structures. To fulfill this objective, more computations are required. Hence, an IM approach
should be efficient enough to be applicable for the ever-changing structure of a network. In this research, an IM method for dynamic networks has been proposed which uses a deep Q-learning [DQL] approach. To learn dynamic features from the
network and retain previously learned information, incremental and transfer learning methods have been applied. Experiments substantiate the good performance of the DQL methods and their superiority over compared methods on larger sizes
of tested synthetic and real-world networks. These experiments illustrate better performance for incremental and transfer learning methods on real-world networks.",Influence Maximization in Dynamic Networks Using Reinforcement Learning,"[76712, 75612]",14,"[53, 66]",691,Reinforcement Learning - Methods and Applications ,47,8,40,Advances in Stochastic Modelling and Learning Methods,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,96 [building - 306],"['Graphs and Networks', 'Machine Learning']",TB-40
"Traditional inventory control approaches typically begin by estimating the demand distribution within a predefined family of distributions based on historical observations.
The traditional inventory models use these estimated distributions to find the optimal order policy. However, these approaches often rely on insufficient information, for example, only the mean and standard deviation of demand, which may not adequately capture demand changes over time. By contrast with these approaches, we utilize a Light GBM model to predict the daily demand. By rebuilding the LightGBM model at each time period we track changes over time. Consequently, we propose an innovative approach that integrates the LightGBM model within a two-stage stochastic optimisation framework executed on a daily basis. In particular, by employing this integrated predictive model, we achieve a more responsive tracking of changes in demand and make the replenishment decision through continuous reoptimisation of stochastic programming on a daily basis. The effectiveness of our approach is demonstrated in our case study, a retail company dataset, Where the new approach results in reducing inventory management costs without significantly affecting customer service levels, distinguishing it from traditional approaches.
",Stochastic reoptimisation model for inventory management,"[76678, 76713, 64873]",518,"[61, 47, 136]",693,"Advancements of OR-analytics in statistics, machine learning and data science 9",16,13,28,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,065 [building - 208],"['Inventory', 'Forecasting', 'Stochastic Optimization']",WB-28
"Concerns about a price bubble within the European Union Emissions Trading System [EU ETS] emerged during the third trading period. We argue that bubble tests based on costs for switching from cheap, polluting to costly, clean energy sources is restricted to situations of market certainty. This limitation is unrealistic, considering the ongoing CO2 reduction measures. Additionally, establishing fundamental value through switching costs lacks a singular approach, leading to inconclusive findings. We propose a robust approach to infer bubbles in the EU ETS. Empirical findings do not support the presence of a bubble in the third or fourth trading period.",EU ETS Market Expectations and Rational Bubbles,[76710],190,"[7, 45, 139]",695,Natural Resource Management and Commodity Markets,4,4,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S14 [building - 101],"['Analytics and Data Science', 'Financial Modelling', 'Sustainable Development']",MC-63
"In this talk, we address the solution of optimal transport problems using Interior Point Methods [IPM]. We propose two approaches which exploit the expected sparsity of the optimal solution to enhance the efficiency of the numerical linear algebra required to solve the Newton system. 
The first approach deals with classical optimal transport over dense bipartite graphs, enforcing sparsity of the intermediate approximations by means of a column-generation-like approach. The numerical experiments show that this method can achieve scalable results for very large problems. 
The second approach deals with optimal transport over sparse graphs, using a proximal stabilized interior point method. The induced primal-dual regularization allows to use sparsified versions of the normal equations to inexpensively generate IPM search directions. The proposed method, despite a potential high level of inexactness in the Newton direction, retains the polynomial complexity of standard IPMs, for suitable choices of the sparsification parameters. Numerical results show that this approach is efficient and robust for large  scale problems, and can outperform classical graph algorithms like network simplex and cost scaling.",A regularized interior point method for sparse optimal transport on graphs,"[68595, 9890, 68601]",192,"[60, 21, 150]",696,Modern techniques for network optimization,68,9,38,"Conic Optimization - Theory, Algorithms, and Applications","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,34 [building - 306],"['Interior Point Methods', 'Convex Optimization', 'Network Flows']",TC-38
"Analytical models of innovation diffusion need to reduce real-world  complexity in order to be numerically tractable. The seminal Bass model, for example, assumes [i] that consumers are fully connected to each other and thus are correctly informed about the share of adopters in the population at each point in time and [ii] that consumers are identical in their innovativeness and their propensity to conform with others. Both assumptions are counterfactual with respect to real consumers, who can only oversee the behavior of a limited set of peers and have individual traits [e.g., some are innovators in adopting new products and do not care much about the share of adopters among their immediate peers, while others are laggards]. In our research, we investigate whether accounting for consumer heterogeneity makes a difference with respect to a better fitting of computed sales to real-world sales [with data taken from Bass's work].

As a means of introducing consumer heterogeneity in an innovation diffusion model, we exploit the strengths of agent-based modeling, which allows us to connect consumers through a realistic social network and assign them individual attitudes [drawn from a normal distribution around the population's mean values provided by Bass]. Our talk details the setup of four simulation experiments in which we vary consumer heterogeneity in the two above-mentioned dimensions, and discuss our findings.
",The heterogeneity effect in modeling innovation diffusion - Findings from an agent-based simulation experiment,"[74882, 4357]",565,"[131, 3, 132]",697,Simulation in innovation,77,7,43,"Agent-based Models in Management, Economic and Organisation Sciences","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,99 [building - 306],"['Simulation', 'Agent Systems', 'Social Networks']",TA-43
"We consider the problem of generating a set of counterfactual explanations for a group of instances, with the one-for-many allocation rule, where one explanation is allocated to a subgroup of the instances. For the first time, we solve the problem of minimizing the number of explanations needed to explain all the instances, while considering sparsity by limiting the number of features allowed to be changed collectively in each explanation. A novel column generation framework is developed to efficiently search for the explanations. Our framework can be applied to any black-box classifier, like neural networks. Compared with a simple adaptation of a mixed-integer programming formulation from the literature, the column generation framework dominates in terms of scalability, computational performance and quality of the solutions.",One-for-many Counterfactual Explanations by Column Generation,"[67604, 662]",120,"[66, 13, 109]",698,Mathematical Optimization for Counterfactual Explanations,15,5,27,Mathematical Optimization for XAI,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,047 [building - 208],"['Machine Learning', 'Column Generation', 'Programming, Integer']",MD-27
"The phenomenon of momentum in sports has been explored in various studies; however, there is little evidence regarding the interplay between offence and defence. Using play-by-play data from NBA seasons 2015/16 to 2018/19, we investigate potential team-level momentum effects. Within state-space models, we first analyse offensive and defensive performances independently and, afterwards, integrate them into a joint framework. Our findings reveal the absence of significant momentum effects in the separated state processes. In contrast, we obtain a positive and significant momentum effect in the combined model. These results indicate that a successful defence performance affects the offence and vice versa. As such, it underlines the necessity of including characteristics such as the interaction between offence and defence for an accurate analysis of momentum in team sports.",Momentum Effects in Team Sports - Analysing the Interaction between Offence and Defence in the NBA,"[69797, 76715]",949,"[99, 7]",699,Sports analytics,37,13,16,OR in Sports,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,19 [building - 116],"['OR in Sports', 'Analytics and Data Science']",WB-16
"Horizontal collaboration, alongside with vertical collaboration, is one of the essential elements of synchromodal transport. Collaboration requires information sharing, among all involved organizations, even if they are competitors. Yet, implementing collaborative systems encounters significant challenges, such as lack of trust among potential participants and concerns about losing competitiveness. To address this challenge, we develop a simulation model of a synchromodal network to study the interactions between different players in the network as well as the impact of horizontal collaboration on individual logistic service providers. The results show that while collaboration yields benefit for the overall system, these advantages are not evenly distributed among all players, and certain entities may even experience losses under specific circumstances. Our model visualizes these disparities and can be used to design compensation schemes that encourage logistic service providers to participate in collaborative systems. Ultimately, this model provides insights into the advantages and obstacles of horizontal collaboration in synchromodal and in general logistics networks and can be useful for designing more equitable and sustainable logistics systems.",Examining the Impact of Horizontal Collaboration on Individual Logistics Service Providers within Synchromodal Networks,"[71639, 76718, 10790, 76717]",506,"[131, 143, 3]",700,Freight transportation and logistic I,6,8,55,Transportation,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S02 [building - 101],"['Simulation', 'Transportation', 'Agent Systems']",TB-55
"The hosting capacity determines the operational threshold for each integrated energy unit. During hosting capacity allocation, mixed load/generation profiles and intrinsic non-convex network models pose significant computational challenges. This paper introduces an alternative approach for optimal capacity allocation, drawing inspiration from constraint conversion and geometric programming. This approach promises to achieve superior computational speed and a high-quality optimal solution. By initially assessing the multidimensional operational feasible region for involved units, the model simplifies by excluding voltage and current variables, thus reducing complexity. Through model reformulation guided by geometric programming, the model convexity will be established, thus further improving both the speed and solution quality of this solving process. 
Relevant case studies reveal that the computation time could be reduced at most by 88.3% by adopting the proposed approach. Due to model convexity, even a higher-quality solution can be achieved accordingly, which yields a better objective function value compared to that using the Gurobi benchmark solver. These findings underscore the penitential of this approach on computation performance enhancement when determining optimal hosting capacity allocation in electricity networks.
",Geometric Programming for Optimal Hosting Capacity Allocation in Distribution Grids,"[76345, 78756, 78787]",857,"[12, 21, 37]",701,OR in Energy III,23,15,19,OR in Energy,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,44 [building - 116],"['Capacity Planning', 'Convex Optimization', 'Energy Policy and Planning']",WD-19
"In the Atacama Desert, solar radiation deviates from the global standard, boasting high irradiation levels with intense ultraviolet content. Given the spectral dependence of photovoltaic [PV] technologies, optimizing PV devices mandates consideration of local conditions and technology type. Solar cells are typically optimized for standard conditions, but our study focused on adapting them for Atacama's unique spectrum. We optimized an n-type bifacial passivated emitter and rear totally diffused solar cell [n-PERT] by adjusting geometrical and doping parameters using a hybrid genetic algorithm. Six parameters—cell and emitter thicknesses, back surface field thickness, and doping concentrations—were optimized for both Atacama spectrum [AM 1.08] and standard conditions [AM 1.5]. Validating our model, we found the computed and experimental efficiencies to differ by less than 1% under standard conditions, affirming accuracy. Our optimization revealed the necessity of tailored configurations and doping for Atacama deployment. Reducing layer thicknesses and increasing doping led to a 5.4% efficiency boost under AM 1.08. Finally, we emphasize the potential impact of metallization and the feasibility of thinning the emitter and back surface field, opening avenues for enhanced PV technology in extreme environments.",Enhancing N-PERT Solar Cells under the Atacama Desert Solar Spectrum,"[73652, 76722, 76723, 20674]",842,"[93, 52, 38]",704,OR in Energy,23,13,19,OR in Energy,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,44 [building - 116],"['OR in Energy', 'Global Optimization', 'Engineering Optimization']",WB-19
"Maintenance activities are inevitable and costly in integrated mining operations. Conducting maintenance may require the whole system, or sub-units of the system, to be shut down temporarily. These maintenance activities not only disrupt the unit being shut down, but they also have consequences for inventory levels and product flow downstream. In this paper, we consider an interconnected mining system in which there are complicated maintenance relationships and stock accumulation at intermediate nodes. We propose a time-indexed mixed-integer linear programming formulation to optimize the long-term integrated maintenance plan and maximize the total throughput. We also devise an algorithm, which combines Benders decomposition and Lagrangian relaxation, to accelerate the computational speed. To validate our mathematical model, we perform simulations for a real-world case study in the iron ore industry. The results show that our method can yield better solutions than CPLEX optimization solver alone in faster time.  ",Long-term maintenance optimization for integrated mining operations,[76273],310,"[97, 111, 84]",708,Industrial Optimization,14,2,03,Data Science Meets Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,1005 [building - 202],"['OR in Mining', 'Programming, Mixed-Integer', 'Optimization Modeling']",MA-03
"In retail, sales promotion plays a key role in gaining a competitive advantage. Products are often sold by combining multiple sales-promotion types, e.g., regular sales, discounts, and coupons. Therefore, inventory control for multiple sales-promotion types has been attracting interest. Each promotion type for a product differs in its demands’ distribution and target service level. The inventory-policy variables for each promotion type are typically optimized individually to replenish the inventory separately. However, this separation method risks shortage and excess stock since the inventory is not shared among the promotion types. An integrated inventory-control method of the [s, S] policy for multiple sales-promotion types is proposed. The policy variables are calculated as follows. The required inventory is first calculated for each inventory type such as safe stock, pipeline stock and cycle stock. The policy variables are then calculated by summing the required inventory of each inventory type for all promotion types. This enables the sharing of inventory among promotion types and satisfies the target service level of each type, reducing shortages. By omitting the cycle stock while replenishing for promotion types with large uncertainty in demand, the excess stock due to forecast error is reduced. The results of numerical experiments indicate that the proposed method reduces shortage and excess stock compared with the conventional separation method.",Integrated inventory-control method for multiple sales-promotion types,"[76333, 69531, 76728, 76729]",634,"[61, 5, 138]",710,Retail Optimization,30,15,50,Retail Operations,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,M2 [building - 101],"['Inventory', 'Algorithms', 'Supply Chain Management']",WD-50
"Bed census predictions play a key role in hospital capacity management decisions, such as ward dimensioning, staffing decisions, patient-to-bed assignment and the development surgery schedules. Currently, predictions are typically solely based on the doctor’s estimate of the Expected Discharge Date [EDD]. In this presentation, we propose two probabilistic models to combine the EDD with the LOS distributions retrieved from ERP data. Using the Poisson Binomial distribution and probabilistic convolution, we obtain the full census distribution. We apply our method on real world hospital data, and it turns out to be very accurate.",Predicting next week’s bed census - combining medical expertise with data.,"[70511, 68161, 25223, 2667, 63476]",610,"[56, 47, 26]",711,Admission and discharge,3,15,10,OR in Health Services [ORAHS],"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,11 [building - 116],"['Health Care', 'Forecasting', 'Decision Support Systems']",WD-10
"Green hydrogen production via water electrolysis powered by renewable energy is essential to decarbonize key industries. However, it poses challenges such as ensuring hydrogen demand despite the uncertain availability of renewable energy sources. This work focuses on short-term electrolytic hydrogen production planning, considering a real case with a production site linked to a wind farm and the electricity grid. The wind farm provides fluctuating renewable electricity at negligible cost, while grid electricity incurs higher cost. The purchase of electricity from the grid has to be planned and declared a day ahead of production, i.e. before the exact availability of wind power is known. On-site hydrogen storage offers some flexibility for production planning, as it allows to produce hydrogen in advance.

The future availability of the wind power source is informed by forecasts subject to error, leading to a production overcost. A cohesive framework is introduced to handle this case study. First, a two-stage stochastic programming approach is presented to model this problem. Then, a probabilistic neural network is used to estimate the conditional wind power uncertainty and generate scenarios from the knowledge of past forecast errors. Finally, a time-efficient Benders decomposition approach is proposed, in which special features of our problem are exploited to speed up the resolution. A realistic simulation demonstrates the benefits of the presented approach.",Day-ahead lot-sizing under uncertainty - An application to green hydrogen production,"[75796, 29574, 22050, 75819]",234,"[93, 136, 66]",712,Production Optimization and Supply Chain Management of Green Hydrogen under Uncertainties,22,13,09,Energy Markets,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,10 [building - 116],"['OR in Energy', 'Stochastic Optimization', 'Machine Learning']",WB-09
"The Binary Polynomial Optimization [BPO] problem is defined as the problem of minimizing a given polynomial function over all binary points. The main contribution of this paper is to draw a novel connection between BPO and the problem of performing model counting over the [max, +] semiring on Boolean functions. This connection allows us to give a strongly polynomial algorithm that solves BPO with a hypergraph that is either beta-acyclic or with bounded incidence treewidth. This result unifies and significantly extends the known tractable classes of BPO. The generality of our technique allows us to deal also with extensions of BPO, where we enforce extended cardinality constraints on the set of binary points, and where we seek k best feasible solutions. We also extend our results to the significantly more general problem where variables are replaced by literals. Preliminary computational re- sults show that the resulting algorithms can be significantly faster than current state-of-the-art.
",A Knowledge Compilation Take on Binary Polynomial Optimization,[75781],194,"[109, 0]",713,Recent Advances in MINLP,86,9,04,MINLP,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,1001 [building - 202],"['Programming, Integer']",TC-04
"We investigate the effect of the recent Inflation Reduction Act [IRA] - introduced in August 2022 -
on equity raised in the US healthcare macro-sector. For the data analysis, we employ a panel approach, in
which a subset of entries of a matrix of new equity raised values is observed, and one aims to predict
counterfactual values for the missing entries [i.e., the equity raised values in the US healthcare macro-
sector that would have been obtained after August 2022, in the absence of the IRA]. Rows of the data
matrix refer to specific pairs having the form region/macro-sector [for a total of 3 x 5=15 rows], and their
columns to specific quarters of the year [for a total of 2 x 4=8 quarters, 4 before the adoption of the policy,
and 4 after that event]. Such predictions are obtained by employing a supervised machine-learning method
named matrix completion, whose use for causal panel data analysis has been recently advocated in a
celebrated work by Athey et al. in 2021. According to the results of our analysis, the effect of the IRA was to
increase equity raised values in the US healthcare macro-sector during the treatment period, as compared
to their predicted counterfactual values.",Effect of the Inflation Reduction Act on Equity Raised Values in the US Healthcare Macro-Sector -  A Matrix-Completion Approach,"[75774, 12557, 76731, 76732]",188,"[44, 5, 33]",714,Applications in Finance and Economics,4,2,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S14 [building - 101],"['Finance and Banking', 'Algorithms', 'Economic Modeling']",MA-63
"This paper provides accurate estimations of the impact of sentiment analysis on forecasting trends in stock prices. We use FinBERT, a pre-trained and fine-tuned NLP model to label and to score a large web-scrapped data [tweets posts and Reddit discussions] of 10 selected energy stock daily returns and the S&P Energy Index spanning from 2018 to 2023. Our approach confirms - i] the accuracy of stock price predictions going up top 85\%. ii] Results are highly sensitive to the test period and confirm the link between risk aversion and negatif or positif score. Also, a novel contribution is provided and consist on employing time series model to capture the temporal dependencies and autocorrelation structures within both the sentiment scores and stock returns data.  Our study reinforces the concept of market efficiency and offers empirical evidence regarding the delayed influence of emotional states on stock returns.",Impact of Sentiment analysis on Energy Sector Stock Prices  - A FinBERT Approach,"[51172, 76733, 76734]",188,"[8, 47, 45]",715,Applications in Finance and Economics,4,2,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S14 [building - 101],"['Artificial Intelligence', 'Forecasting', 'Financial Modelling']",MA-63
"This study conducts a comprehensive comparative analysis of blockchain technology's impact on traditional automotive and electric vehicle [EV] manufacturing sectors, focusing on transparency, security, efficiency, trust, compliance, sustainability, innovation, and customer satisfaction. It utilizes Multiple Criteria Decision Making [MCDM] methods such as AHP and TOPSIS, along with TISM and MICMAC methodologies to develop the conceptual framework and relationship. In traditional automotive manufacturing, blockchain enables enhanced supply chain transparency, ensuring traceability of components and materials, thus improving compliance and fostering trust among stakeholders. This transparency enhances collaboration and streamlines operations. For EV manufacturing, blockchain facilitates lifecycle management of critical components like batteries, optimizing usage, monitoring performance, and ensuring responsible disposal. It also integrates renewable energy sources, promoting sustainability and clean energy innovation. The analysis provides guidance for decision-makers in both sectors to tailor blockchain strategies, unlocking benefits such as transparency, security, efficiency, stakeholder collaboration, and sustainability throughout the supply chain. Embracing blockchain technology empowers manufacturers to navigate challenges and capitalize on opportunities, fostering a more resilient, efficient, and sustainable automotive industry.",Comparative Insights - Untangling Blockchain Enablers in Traditional vs. Electric Vehicle Manufacturer,"[76358, 77373]",894,"[138, 69, 6]",716,Pairwise comparisons and preference relations 4,44,13,44,Multiple Criteria Decision Analysis,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,20 [building - 324],"['Supply Chain Management', 'Manufacturing', 'Analytic Hierarchy Process']",WB-44
"The strong Slater condition plays a significant role in the stability analysis of linear semi-infinite inequality systems. In this work, we deal with the stability of the intersection of a given evenly convex set with the solution set of a linear system whose coefficients can be arbitrarily perturbed. More specifically, we analyze the set of strong Slater points associated to a given linear inequality system with an evenly convex constraint set X. Such sets become the solution sets of linear systems containing weak inequalities, strict inequalities and strong Slater type inequalities. For this type of systems we characterize the existence of solutions by means of dual conditions in terms of the system data, extending some results given by other authors in the field of stability of semi-infinite linear systems with solutions in a certain closed convex set.",On the strong Slater condition of linear systems with an evenly convex constraint set,[4976],266,"[19, 0]",717,Infinite Optimization - stability and duality,82,12,42,Variational Analysis and Continuous Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,98 [building - 306],['Continuous Optimization'],WA-42
"A common approach to solving MINLP problems is to convert them to a MILP using piecewise-linear [PWL] models to approximate the nonlinearities. However, finding a PWL approximation that respects an error tolerance with the minimum number of linear pieces is a challenging task. Moreover, available methods are complicated and difficult to implement. This paper addresses finding error-bounded PWL approximations for continuous and smooth bivariate functions. The method proposed is an easy-to-implement heuristic applied for the case of a triangulation generated over rectangular grids. On each iteration, a nonlinear programming problem is solved to adjust the placement of the linear pieces to minimize the approximation error. Then, we increase the number of breakpoints for the variable with maximum error reduction potential. For an industrial use case from the steelmaking industry, the ladle dispatching problem with refractory temperature control, we show how applying this heuristic can reduce the complexity of the resulting MILP while respecting a strict approximation error requirement. The heuristic achieved an average reduction of 67% in the solution time of the MILP with an average 34% reduction in the approximation error.",A heuristic for fitting piecewise-linear models of bivariate functions with simplices and its application to an industrial use case,"[76563, 76738, 66084, 76737]",856,"[59, 129, 111]",723,OR in Energy II,23,14,19,OR in Energy,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,44 [building - 116],"['Industrial Optimization', 'Scheduling', 'Programming, Mixed-Integer']",WC-19
"In this world of such a volatile, uncertain, and changing environment and markets, especially in a post-pandemic ‘new-normal’ context, it is very important to be ahead of competitors so as to maintain one’s market share. This is true for all markets, but more than anything for B2B markets that are affected immediately form supply chain disruptions and need to employ multiple channels to survive and thrive in turbulent times. Products and services can be sold profitably only if customer needs are satisfied at acceptable [low-ish] prices. Many factors are involved in making a product cost efficient, one of which is forecasting. Forecasting is one of the most essential parts of almost every supply chain in the world. This paper contributes towards the improvements in the forecasting methods currently being used by forecasters in B2B markets, a key area in the early days of Industrial Marketing Management [70s and 80s] that has been revived recently due to the revolution of big data predictive and prescriptive analytics. This paper is focused on forecasting the competing product life cycles using Lotka-Volterra predator-prey equations. The objective of this paper is to find a better version of Lotka-Volterra equations suited for general market products. Various regression analysis techniques have been applied in order to make an attempt to find the best way of forecasting using Lotka-Volterra equations.",Forecasting B2B markets with machine learning and predator-prey models - empirical evidence from global Aerospace companies,"[76630, 76843, 71578, 76817]",652,"[47, 66, 4]",725,Artificial Intelligence and Machine Learning for Decision Support,45,8,45,Decision Support Systems,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,30 [building - 324],"['Forecasting', 'Machine Learning', 'Airline Applications']",TB-45
"The global population is battling with the urgent need to transition towards sustainable and efficient energy systems. With 15% of the total emissions in Germany, the heating sector is one of the largest greenhouse gas-emitting sectors. Subsidies are key policy instruments governments employ to incentivize the adoption of such technologies. However little research has been done on optimizing these subsidies to achieve the best emission reduction on a long-term horizon. This challenge is compounded by the colliding interests of the German government, which aims to minimize total emissions through subsidy allocation, and property owners, who seek to minimize costs through investments in heating technologies.
While certain heating technologies may offer short-term sustainability benefits, others hold greater potential for long-term emission savings. To address this issue, we propose a bi-level optimization model utilizing a modified grid search at the upper level and linear programming at the lower level to optimize subsidy distribution decisions.
Preliminary findings indicate that subsidies targeting both heating pumps and biomass heating show promise for maximizing emission reduction. Nonetheless, it is observed that additional subsidies for biomass heating entail substantial costs relative to their marginal impact. This underscores the importance of careful subsidy allocation strategies to achieve optimal outcomes in emission reduction efforts.
",Reducing Greenhouse Gas Emissions through Strategic Subsidy Allocation for Building Heat Technologies - A bi-level Optimization Approach,"[72013, 72344]",840,"[37, 93, 94]",726,OR in Heating Systems,23,9,19,OR in Energy,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,44 [building - 116],"['Energy Policy and Planning', 'OR in Energy', 'OR in Environment and Climate change']",TC-19
"This paper studies the effect of sovereign debt on corporate borrowing cost. We develop a theoretical model of a global financial intermediary sector which is tested using data on global syndicated loans for a large panel of firms, banks and countries. Our evidence shows that for government debt around 75 per cent of GDP, any additional percentage increase burdens firms borrowing cost by 11.73 basis points. The effect becomes more pronounced for firms and banks located in the same country, while the degree varies depending on the firm’s size and profitability. Overall, results underline that the cost of loans is a relevant mechanism through which government debt affects real economic activity.",Does Sovereign Debt Consolidation Shape Firms’ Credit Cost?,[76739],470,"[33, 44, 45]",728,Applications to Economics and Finance,4,14,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S14 [building - 101],"['Economic Modeling', 'Finance and Banking', 'Financial Modelling']",WC-63
"Centrality measures are quantitative metrics used to evaluate the prominence of nodes within a network. They provide insights into the relative significance of nodes based on their positions, connections or spreading power within the network. These measures help identify key nodes that play pivotal roles in information flow, communication, or influence propagation within various types of networks, including social networks, transportation networks, and biological networks. Centrality analysis is applied in diverse fields to understand network structures, identify central actors or locations, and optimize network efficiency or resilience.

We introduce the Selected Target Rank [STR], a novel centrality measure derived from addressing the NP-hard Target Set Selection Problem [TSS], a renowned challenge in graph theory. This problem focuses on identifying the smallest subset of nodes within a network whose activation can effectively influence the entire network. We use strategies and solutions to the TSS problem to provide the nodes with a rank. This measure aims to quantify the importance of each node based on its membership in groups that are able to fully influence the network. With this new perspective on centrality, we can quantify the indispensability, necessity or relevance of nodes to ensure an efficient full diffusion.

We test STR against standard centrality measures.

[Supported by AEI grant MICINN PID2020-112581GB-C21]",Selected target rank as centrality measure for influence spreading,"[70255, 36990]",209,"[53, 132, 74]",730,Analytics and the link with stochastic dynamics I,17,7,31,Analytics,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,54 [building - 208],"['Graphs and Networks', 'Social Networks', 'Metaheuristics']",TA-31
We study the perpetual American options optimal stopping problem in two-dimensional diffusion models with linear and multiplicative payoff structure. It is assumed that the risky asset prices are modelled as geometric Brownian motions driven by constantly correlated standard Brownian motions. We find closed formulas for the value functions expressed in terms of the optimal stopping boundaries which in turn are shown to be unique solutions to nonlinear Fredholm integral equations. A key argument in the existence proof is played by pointwise maximisations of the expression obtained by the change-of-measure arguments. These provide tight bounds on the optimal stopping boundaries as well as describes its shape and asymptotic behaviour for small or large coordinate values of the risky asset prices. This is a joint work with Goran Peskir [Manchester].,Perpetual American Options in Two-Dimensional Diffusion Models,[62060],292,"[136, 83, 82]",731,Dynamics of the Firm I,90,3,33,Optimal Control Theory and Applications,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,42 [building - 303A],"['Stochastic Optimization', 'Optimization in Financial Mathematics', 'Optimal Control']",MB-33
"In multi-hospital networks [MHNs], a critical decision revolves around how to allocate clinical services across different hospitals with limited capacity. This decision typically falls between two extremes - a fully generalized configuration where all services are offered in every hospital, and a fully specialized configuration where each hospital exclusively provides one service. However, research and practical evidence indicate that neither fully specialized nor fully generalized configurations are optimal. In this paper, we present a mathematical model along with an exact analytical solution to explore the optimal service configuration within an MHN comprising two hospitals of equal fixed capacity and two services. Our model addresses uncertainty by incorporating queueing theory and economies of scale, crucial factors in service configuration problems. Through extensive numerical experiments, we demonstrate that, in most scenarios, a semi-specialized configuration emerges as optimal, maintaining one hospital as general and the other as specialized. Nonetheless, there are specific circumstances where alternative configurations, such as fully specialized or fully generalized, may prove optimal.",Generalized or Specialized Hospitals - An Analytical Queuing Approach to Explore Clinical Service Configurations,"[76744, 74244, 25706]",103,"[56, 121, 113]",732,Healthcare services,3,5,15,OR in Health Services [ORAHS],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,18 [building - 116],"['Health Care', 'Queuing Systems', 'Programming, Nonlinear']",MD-15
"We propose a novel learning procedure to assist in the solution of a well-known computationally difficult optimization problem in power systems - The Direct Current Optimal Transmission Switching [DC-OTS]. This model consists in finding the configuration of the power network that results in the cheapest dispatch of the power generating units. The DC-OTS problem takes the form of a mixed-integer program, which is NP-hard in general. The proposed approach leverages known solutions to past instances of the DC-OTS problem to speed up the mixed-integer optimization of a new unseen model. Although it does not offer optimality guarantees, a series of numerical experiments run on a real-life power system dataset show that it features a very high success rate in identifying the optimal grid topology [especially when compared to alternative competing heuristics], while rendering remarkable speed-up factors.",Learning-Assisted Optimization for Transmission Switching,"[18518, 39196, 43618]",147,"[93, 111, 66]",734,Learning-assisted Optimization in Energy Problems,23,2,19,OR in Energy,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,44 [building - 116],"['OR in Energy', 'Programming, Mixed-Integer', 'Machine Learning']",MA-19
"In recent decades, the adoption of outsourcing to offshore vendors across various domains, such as software engineering and IT support, has grown significantly. Managing these vendors effectively, particularly in scenarios involving multiple cost centers, presents a range of challenges including issues of trust between vendors and clients, effective knowledge gathering and retention, cultural differences, the impact of client organizational structures on team dynamics, and complex revenue considerations across cost centers. This paper extends the well known Team Formation Problem [TFP] that is focused on the formation of efficient teams based on skills and experience, by introducing a tailored variant for managing offshore vendors in multi-cost centre environments. We introduce a new mathematical model that accounts for multiple projects and clients, the varying expertise levels of workers and flexible definitions of team structures. The proposed model aims to optimize resource utilization and maximize throughput. Additionally, our research includes an experimental study employing synthetic datasets with varying sizes and characteristics to validate the applicability of our model in real-world scenarios.",Optimizing Team Formation in Offshore Vendor Centers,"[76727, 23636, 76922]",220,"[14, 5, 72]",735,Advanced Topics in Combinatorial Optimization,64,8,26,Combinatorial Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,012 [building - 208],"['Combinatorial Optimization', 'Algorithms', 'Mathematical Programming']",TB-26
"In this paper, we introduce a new cooperative game theory model that we call production-distribution game to address a major open problem for operations research in forestry, raised by R{\o}nnqvist et al. in 2015, namely, that of modelling and proposing efficient sharing principles for practical collaboration in transportation in this sector. The originality of our model lies in the fact that the value/strength of a player does not only depend on the individual cost or benefit of the objects she owns but also depends on her market shares [customers demand]. We show however that the production-distribution game is an interesting special case of a market game introduced by Shapley and Shubik in 1969. As such it exhibits the nice property of having a non-empty core. We then prove that we can compute both the nucleolus and the Shapley value efficiently, in a nontrivial and interesting special case. We in particular provide two different algorithms to compute the nucleolus - a simple separation algorithm and a fast primal-dual algorithm. Our results can be used to tackle more general versions of the problem and we believe that our contribution paves the way towards solving the challenging open problem herein.
",Horizontal collaboration in forestry - game theory models and algorithms for trading demands,"[76749, 43244, 3671]",621,"[14, 50, 103]",738,OR in Forestry II,20,8,12,OR in Agriculture and Forestry ,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,13 [building - 116],"['Combinatorial Optimization', 'Game Theory', 'Polyhedral Combinatorics']",TB-12
"Out-of-hospital cardiac arrest [OHCA] is a significant public health problem with notably low survival rates. Early defibrillation is crucial for survival, highlighting the importance of nearby automated external defibrillators [AEDs].  Current AED placement strategies often rely on historical OHCA data, which are limited in availability.  Publicly available demographic/socioeconomic data are often easily available and shown to have correlations with OHCA risk. This study aims to 1] estimate spatial cardiac arrest risk using demographic/socioeconomic data alone 2] compare AED location models based solely on estimated risk with those incorporating historical OHCA data to inform demand. Machine learning techniques were applied to a comprehensive dataset spanning multiple municipalities. Predicted OHCA incidence of each district were used to optimize AED locations, alongside AED optimization models that used smoothed out historical cardiac arrest data as demand. Results on several municipalities underscore the value of an OHCA registry. Nonetheless, in its absence, machine learning models leveraging demographic and socioeconomic data offer a viable means to substantially enhance coverage. ",Unlocking the Value of Extensive Data - Estimating spatial cardiac arrest risk to guide resource allocation decisions,"[52961, 66853, 72675, 72674]",336,"[56, 66, 64]",739,Analytics for Combinatorial Problems from Health Care to the Food Industry,17,10,31,Analytics,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,54 [building - 208],"['Health Care', 'Machine Learning', 'Location']",TD-31
"Policy makers are formulating offshore energy in-
frastructure plans, including wind turbines, electrolyzers, and
HVDC transmission lines. An effective market design is crucial
to guide cost-efficient investments and dispatch decisions. In this talk, we jointly discuss the impact of offshore market design
choices on the investment in offshore electrolyzers and HVDC
transmission capacity. We present a bilevel model that incorpo-
rates investments in offshore energy infrastructure, day-ahead
market dispatch, and potential redispatch actions near real-time
to ensure transmission constraints are respected. Our findings
demonstrate that full nodal pricing, i.e., nodal pricing both
onshore and offshore, outperforms the onshore zonal combined
with offshore nodal pricing or offshore zonal layouts. While
combining onshore zonal with offshore nodal pricing can be
considered as a second-best option, it generally diminishes the
profitability of offshore wind farms. However, if investment costs
of offshore electrolyzers are relatively low, they can serve as
catalysts to increase the revenues of the offshore wind farms.
This work contributes to the understanding of market designs
for highly interconnected offshore power systems, offering in-
sights into the impact of congestion pricing methodologies on
investment decisions. ",Evaluating Offshore Electricity Market Design Considering Endogenous Infrastructure Investments - Zonal or Nodal?,[62333],244,"[36, 0]",740,Modelling European market coupling ,22,5,14,Energy Markets,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,16 [building - 116],['Electricity Markets'],MD-14
"In urban logistics, the integration of drones with traditional delivery methods presents a promising avenue for improving efficiency and sustainability. This study addresses the optimization of last-mile deliveries by a single drone-truck system. Similar to the vehicle routing problem [VRP], the objective is to minimize travel cost of the travel time while serving all clients exactly once. The vehicle routing problem with drones [VRPD] presents the drone as an additional delivery vehicle that the truck can pick up and transport. This problem involves a network composed of clients, a depot, and dock hubs. The drone can take off from the truck at any vertex, visit at most as many clients as the number of its compartments, and must land at a dock hub, where the truck can pick it up again. Moreover, the drone's battery capacity must be considered on each trip. In this project, we will increase the drone's autonomy by allowing it to visit dock hubs as battery swapping points during a trip [increasing its driving range] and by including cross-docking. The cross-docking process allows the truck to leave products at the dock hubs, where the drone will visit, pick up the products, and start a new trip. We propose an exact approach to solving the problem and present preliminary results.",An exact method for the Single Drone-Truck Routing Problem,"[76643, 24079, 27820]",977,"[145, 143, 14]",743,Last mile delivery with drones,6,15,56,Transportation,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S04 [building - 101],"['Vehicle Routing', 'Transportation', 'Combinatorial Optimization']",WD-56
"Addressing the inefficiency of traditional manual reviewer assignments in the peer-review process, this study introduces a decision support system that automates the assignment task. Utilizing information retrieval, natural language processing [NLP] and optimization techniques, it tackles the reviewer assignment problem through a structured, three-stage approach. Initially, it gathers diverse information from various sources to build a comprehensive database of proposals and reviewers. Then, it applies word embedding techniques to convert multilingual proposal and reviewer texts into vector representations, and it uses the cosine similarity metric to determine content similarity between each proposal-reviewer pair. Concurrently, it assesses reviewer competency by analyzing their past evaluation performance and areas of expertise through predefined knowledge rules. In the final stage, a multi-objective integer linear programming model assigns reviewers to proposals, optimizing proposal-reviewer similarity and reviewer competency while avoiding conflicts of interest. Furthermore, it explores a max-min approach to improve outcomes for the least-advantaged proposals. This model is enhanced by two additional models to ensure balanced reviewer workloads. The system's efficiency is tested with a real-world dataset from the project proposal evaluation process of a regional development agency. The results show that the proposed system significantly outperforms traditional methods.",A DECISION SUPPORT FRAMEWORK FOR AUTOMATED REVIEWER ASSIGNMENT USING NLP AND OPTIMIZATION,"[76691, 20135, 76752]",320,"[112, 66, 26]",745,Machine Learning in Applied Optimization,14,8,03,Data Science Meets Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,1005 [building - 202],"['Programming, Multi-Objective', 'Machine Learning', 'Decision Support Systems']",TB-03
"With the so-called EU-taxonomy regulation in 2020, the European Union established a uniform classification system to determine environmentally sustainable economic activities. According to the EU regulation, economic activities are environmentally sustainable if they contribute substantially the achievement of certain environmental objectives, e.g. climate change mitigation or pollution prevention and control. Insofar as they do not explicitly contribute the environmental objectives, economic activities should not significantly harm the environmental objectives. The EU regulation includes a range of measures to achieve the defined environmental objectives. Incentives should be provided to invest in sustainable and climate friendly economic activities. In this context, the question arises which tax incentives exist to facilitate sustainable investments and how these incentives affect the investment decision. I implemented different tax incentive measures, like levying an environmental tax or tax exemptions for sustainable investments, and investigated their effect on the investment decision made on basis of the net present value criterion. This is compared with the case that the decision is only made on basis of the tax burden.",Tax incentives to facilitate sustainable investments,[64922],255,"[1, 100]",749,"OR in Accounting - Planning, Taxation, and Reporting",7,13,59,OR in Financial and Management Accounting,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S08 [building - 101],"['Accounting', 'OR in Sustainability']",WB-59
"Current dynamic layout models only consider changes in the placement of temporary facilities, like workstations and material storages, without considering changes in the spatial structure of the layout itself. While they are practical to account for repositioning of the facilities and the resulting changing material flows, these models assume a static layout structure over time. To address this limitation, we present a novel optimization model introducing 'dynamic spaces'. They accommodate the changing availability of rooms or areas representing the layout over time. This innovation finds application in many cases, such as optimizing deconstruction projects where parts of a building are taken down and can’t be used as locations anymore midway through the project or when safety protocols require certain areas of a factory to be sealed off temporarily during remodeling. Additionally, it can also be used to represent expanding layouts e.g., if surrounding areas are temporarily used during a construction project. In this contribution we analyze the difference in the optimization process compared to classical formulation and show the strength of the new model in numerical case studies.",Dynamic spaces in layout planning - a new concept to introduce time-dependent layouts,"[70301, 47858, 2675]",768,"[43, 64, 65]",752,Location in Logistics and Supply Chain Management,29,10,61,Locational Analysis,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S10 [building - 101],"['Facilities Planning and Design', 'Location', 'Logistics']",TD-61
"We consider an investment model in which a firm decides to invest in the market, taking into account its future revenue and the possible occurrence of adverse events that may impact its reputation. The firm can buy an insurance contract at the investment time to mitigate reputation risk. The firm decides when to enter the market and the insurance strategy that maximizes its value. We consider three types of insurance contracts and different premium principles. We provide analytical conditions for the optimum and study several numerical examples. Results show that the firm's optimal strategy depends on the risk size, the firm's risk aversion, and the insurance premium. ",Reputation risk mitigation in investment strategies,"[72633, 71590]",930,"[126, 0]",753,Corporate finance risk management,9,7,51,Risk management in finance,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,M5 [building - 101],['Risk Analysis and Management'],TA-51
"An open-source implementation of a hybrid genetic search is currently among the very best heuristics for the capacitated vehicle routing problem. While being highly efficient and effective, the code is also relatively simple. It is therefore an interesting challenge to improve the performance of the heuristic while making as small changes to the code as possible. Two such improvements can be found by analyzing, respectively, a cross-over operator and a splitting procedure. An improved cross-over operator is shown to reduce gaps to best-known solutions by 4.2%, and an improved splitting procedure is shown to reduce gaps by an additional 3.9%. In both cases, changes to the implementation is in the order of tens of lines of code.",Improving a Hybrid Genetic Search for the Capacitated Vehicle Routing Problem,[18335],754,"[145, 74, 14]",756,Heuristics for Vehicle Routing 1,5,14,64,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S16 [building - 101],"['Vehicle Routing', 'Metaheuristics', 'Combinatorial Optimization']",WC-64
"Human operations, whether in business, at home, or otherwise, are causing a transgression of the boundaries of a safe and just operating space for planet Earth and humankind. Developments in operations that have steadily grown over the long course of history, and which have especially gained momentum since the uptake of fossil fuel powered machines in the recent and on-going industrial revolutions, now threaten to cause irreversible damage to ecosystems and society. The present situation calls for new perspectives and understanding of operations and operations research that enable to change the course of development and for operations to provide sustainable solutions for the planet and humankind.

In this talk we firstly reflect on the history of operations while explicitly reflecting on its environmental and social sustainability and on the corresponding development of operations research, roughly from the Big Bang onwards. Next we reflect on the [un]sustainability of today's operations and reflect on operations research contributions [being] made. 

The on going 4th industrial revolution, which depends so extensively on analytics and data science, promises to change the way humankind lives and works, the way we operate, once again. Hence it provides a window of opportunity for operations researchers to learn from the past and aim to contribute to creating sustainable operations for humankind and planet Earth. We review the state of the art on contributions of the fourth industrial revolution for sustainable operations and highlight priorities for future sustainable operations research.
","The sustainability of operations - past, present, future",[59035],19,"[100, 88, 139]",758,Moments in the history of OR  1,27,13,20,Moments in the history of OR,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,45 [building - 116],"['OR in Sustainability', 'OR History', 'Sustainable Development']",WB-20
"The sigma-mu efficiency methodology, derived from Stochastic Multi-Attribute Acceptability analysis [SMAA], addresses uncertainty in decision alternatives’ performance by constructing Pareto-Koopmans efficiency frontiers [Greco et al.,2019]. These frontiers evaluate alternatives based on expected performance μ and variability σ, across diverse criteria weights.
In this paper, we assess alternatives’ performance by synthesizing the distribution of composite indicator values, incorporating additional parameters beyond μ and σ, namely, skewness and kurtosis. These parameters offer valuable insights into the shape characteristics of the probability distribution of composite indicators, i.e. tailedness and symmetry.
Therefore, in this study, we propose revisiting the SMAA model by adopting
the versatile Dirichlet distribution to the weights of the criteria [Saint-Hilary, et al.,2017; Jia et al.,1998]. The Dirichlet distribution, thanks to its flexibility in representing shapes, incorporating prior knowledge, and supporting the simplex, is deemed suitable for modeling weights’ uncertainty without expert elicitation and capturing skewness and kurtosis based on shape parameters α and β.
The proposed approach is employed to evaluate the European Small and Middle-Sized Enterprises' [SMEs] performance from 2018-2022
using financial, qualitative, and ESG criteria, creating a comprehensive composite indicator aligned with the sustainable goals of Agenda 2030.

",An Enhanced Simulation-Based Approach for Multicriteria Evaluation Problems,"[76681, 3435, 2127, 2128]",889,"[25, 0]",760,MCDA applications in Engineering and Management 2,44,3,47,Multiple Criteria Decision Analysis,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,50 [building - 324],['Decision Analysis'],MB-47
"Multiobjective optimization problems [MOPs] present unique challenges, characterized by optimizing several conflicting objectives simultaneously and the, often exponential, number of Pareto-optimal solutions. Classical approaches typically involve scalarization to single objective problems obtaining at most one Pareto-optimal solution per single objective problem. The advent of quantum computing [QC], with its properties like superposition and entanglement, opens new frontiers in addressing the complexity of MOPs. So far, application-oriented problems, like MOPs, have barely been touched by quantum algorithms.
In this research, we explore the application of QC to discrete MOPs, capitalizing on its inherent capabilities to process vast and complex search spaces. We investigate how existing quantum optimization algorithms can be adapted and extended to the multiobjective domain. Based on previous work, we focus on variational algorithms and Grover Adaptive search and augment these with scalarization techniques and bound sets, among others. To demonstrate the efficacy of our approach, we apply these QC algorithms to an unconstrained multiobjective combinatorial optimization problem.
Our results showcase that, while the computational advantage over classical computing is currently tested, QC adds value when tackling conceptually hard problems. Furthermore, we are paving the way for the practical applicability of quantum computing in solving real-world optimization problems.",Harnessing the Power of Quantum Computing for Multiobjective Optimization Algorithms,"[57906, 76760]",602,"[77, 14, 5]",762,Theory of Multiobjective Optimization,34,14,37,Multiobjective Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,33 [building - 306],"['Multi-Objective Decision Making', 'Combinatorial Optimization', 'Algorithms']",WC-37
"Home healthcare [HHC] nurses in the Flemish context face a significant challenge - the irregularity of work rosters in successive rostering periods, which disrupts their ability to plan personal activities. This issue affects their job satisfaction and contributes to high turnover rates within the industry.

Existing literature predominantly addresses nurse satisfaction in HHC through single-objective models using a weighted sum, often failing to capture the complex trade-offs involved. In addition, decision-makers struggle to assign appropriate weights to each factor in this function.
 
This study proposes a bi-objective model to balance costs and care worker satisfaction, defined by roster regularity in successive planning periods. Our novel approach utilizes a multi-directional local search [MDLS] framework with an embedded matheuristic to approximate the Pareto frontier between the conflicting objectives [i.e., costs and care worker satisfaction]. This matheuristic constructs multi-day planning in an integrated manner by rostering nurses, assigning patients to nurses, scheduling patient visits and constructing routes. Our model offers decision-makers valuable insights for optimizing rostering practices by presenting multiple solutions that highlight trade-offs. Finally, the effectiveness and performance of our solution approach will be demonstrated by discussing the results of some empirical experiments.
",Quantifying the trade-off between costs and nurse satisfaction in home healthcare planning decisions,"[67332, 23971, 36613, 9128]",598,"[56, 77, 74]",763,Home Health Care and Operating Room Scheduling,3,12,15,OR in Health Services [ORAHS],"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,18 [building - 116],"['Health Care', 'Multi-Objective Decision Making', 'Metaheuristics']",WA-15
"This paper constructs a multicommodity spatial price equilibrium framework designed to allow for the quantification of the impacts of congestion and transportation capacities on trade flows and product prices while capturing the degradation of fresh produce quality through explicit quality deterioration formulas. We present the multicommodity fresh produce trade network model under equilibrium conditions both without and with minimum quality standards. Alternative variational inequality formulations of the governing equilibrium conditions are derived, with one of them exhibiting particularly favorable features for computational purposes. The numerical examples focus on the banana trade, since bananas are the most widely traded fresh produce commodity globally,  with leading exporters being Ecuador and Costa Rica and major importers, United States and the European Union. Through various scenarios, we simulate congestion and time delays in the Panama Canal and report on the impacts of ongoing disruptions,  on shipment times, equilibrium shipment volumes, the supply and demand prices, quality levels at the supply markets and at the demand markets, and transportation costs. Our baseline numerical example results closely align with real-life data on export volumes of bananas, the associated supply and demand prices, and transportation costs.  The comprehensive numerical results reveal that reductions in transportation capacity and prolonged shipment times, as in the Panama Canal.",Multicommodity Fresh Produce Trade Networks with Quality Deterioration Under Congestion and Transportation Capacities,"[76761, 46334]",589,"[50, 53, 89]",764,Agrifood supply chain decision problems,20,4,12,OR in Agriculture and Forestry ,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,13 [building - 116],"['Game Theory', 'Graphs and Networks', 'OR in Agriculture']",MC-12
"Variable splitting has traditionally been employed in [first order]
Lagrangian decomposition approaches and, more recently, in the Alternating
Direction Method of Multipliers [ADMM]. In this presentation we will show how variable splitting can be efficiently used within [second order] interior point methods for solving large support vector machine [SVM] problems, a binary classification technique extensively used in machine learning. Briefly, by replicating variables, the SVM problem can be decomposed into smaller SVMs with additional linking constraints that equate the values of the different copies. The resulting problem's structure can be exploited by specialized interior point methods that compute the Newton direction with a combination of direct and iterative solvers [i.e., Cholesky factorizations and preconditioned conjugate gradients]. This new approach is compared with state-of-the-art solvers for SVMs, which are based on either interior point algorithms [such as SVM-OOPS] or specific algorithms developed by the machine learning community [such as LIBSVM].

Reference:

J. Castro, New interior-point approach for one- and two-class linear
support vector machines using multiple variable splitting, Journal of
Optimization Theory and Applications, [2022], https://doi.org/10.1007/s10957-022-02103-1.",Specialized interior point method for support vector machines based on variable splitting,[23970],702,"[60, 66, 134]",766,Specialized Optimization Algorithms,76,13,30,Software for Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,53 [building - 208],"['Interior Point Methods', 'Machine Learning', 'Software']",WB-30
"Nowadays, the countries of the European Union contemplate an energy mix that includes a high percentage of renewable energies, favorable for the charging of electric vehicles and for the reduction of polluting gases. Unfortunately, the energy used to charge an electric vehicle in Mexico comes from a highly polluting energy mix that primarily uses fossil fuels, which generates 18% of CO2 emissions. Multiple authors suggest that electric vehicle charging should be done using renewable energy sources. In Mexico, there are no sustainable alternative energy sources that allow for the charging of electric vehicles, compromising the sustainability of this technology and its development. This research focuses on the optimal location, through an optimization-simulation model, of a charging station that transforms energy obtained from clean sources, which will allow meeting the demand for charging electric vehicles, leveraging pet waste in the context of parks in Mexico City. First, the Factor Method was used to calculate the optimal location of charging for electric vehicles. Second, the optimal location was implemented based on an agent-based simulation approach using AnylogicTM software to analyze the dynamics of electric vehicles and pets. Third, simulation scenarios were built considering an increase on demand. The results obtained support decision-making regarding the implementation of the charger in parks, demonstrating the utilization of organic waste generated by pets.",An optimization-simulation model for locating an electric vehicle charging station using clean energy sources..,"[76762, 37306, 76763]",685,"[131, 26, 139]",767,Charging Infrastructure toward Sustainable Transport,80,7,53,Sustainable and Resilient Systems,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,8007 [building - 202],"['Simulation', 'Decision Support Systems', 'Sustainable Development']",TA-53
"Container terminals are crucial links in the global supply chain. These facilities operate in a highly dynamic environment. In this context, a main logistic challenge is scheduling the container handling equipment to execute the container transport and handling operations efficiently and effectively. We research the objectives and optimization strategies to deploy in scheduling and dispatching container handling equipment, in automated container terminals. In this effort, we review literature on scheduling and dispatching in container terminals, model the container terminal scheduling problem and address different solution methods. We decompose the problem and focus on the Automated Stacking Cranes [ASC] scheduling problem. This problem addresses trade-offs such as delivering outbound containers to the transfer zones just-in-time versus delivering the containers early to avoid extra handling in the yard. The choices made on when to schedule container moves have a direct impact on the space utilization in transfer zones and on the minimization of unnecessary travel by the ASCs. In this study, we formulate the ASC Scheduling problem and discuss an online scheduling method. ",Scheduling Automated Stacking Cranes in Container Terminals,[26507],409,"[129, 65, 70]",769,Manufacturing scheduling with sustainability considerations,35,12,60,Project Management and Scheduling,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S09 [building - 101],"['Scheduling', 'Logistics', 'Maritime applications']",WA-60
"The Multi-Period Cutting Stock Problem [MPCSP] involves cutting large stock objects into small items to satisfy demands for each period while allowing for inventory. The MPCSP is a variation that integrates the well-known Cutting Stock Problem [CSP] and the Lot-Sizing Problem. Practical applications are found in the furniture and paper industries.

In CSP, the primary goal is to minimize material costs. However, real production involves auxiliary costs, like switching between cutting patterns, causing interruptions and setup costs. A desirable cutting plan has fewer patterns. Literature on pattern setups in MPCSP is limited, and when capacity constraints are considered, the problem becomes even more challenging.

Pattern-based models and column generation are common approaches for MPCSP along with another heuristics. A large number of columns in practical problems is a challenge for pattern-based models, making techniques like evolutionary algorithms an alternative approach.

This work proposes a Biased Random Key Genetic Algorithm [BRKGA] optimized by Q-Learning [BRKGA-QL] to solve MPCSP with capacity constraints and pattern setups.

The BRKGA-QL is compared with a hybrid column generation approach from the literature using benchmark instances. Results show BRKGA-QL consistently outperforms in terms of integer values across all instances.",Hybrid Biased Random Key Genetic Algorithm For The Capacited Multi-Period Cutting Stock Problem with Setups,"[76766, 9828, 41397, 23765]",804,"[14, 74, 66]",771,Integrated lot-sizing problems,32,3,49,"Lot Sizing, Lot Scheduling and Production Planning","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,M1 [building - 101],"['Combinatorial Optimization', 'Metaheuristics', 'Machine Learning']",MB-49
"This article aims to analyse two distinct facets of last-mile delivery, namely drones and common carriers. Urban delivery is witnessing a surge in popularity of drones due to their ability to outpace traffic and exhibit minimal carbon emissions. Nevertheless, there are situations where a logistic provider may need to subcontract certain clients to a third party, commonly referred to as a common carrier, due to many reasons, including a faster turnaround time or a desire to reduce carbon emissions. The private fleet consists of multiple trucks and drones, and they serve customers independently from the depot. The common carrier informs the logistic carrier beforehand of the estimated delivery time and carbon emissions for all clients. The problem attempts to investigate the relationship between the two objectives, 'makespan' and 'carbon emissions' of the whole network. A bi-objective Mixed Integer Linear Programming [MILP] formulation is proposed. An iterative algorithm is designed for small to medium-sized problems that can sequentially generate pareto-optimal solutions. In addition, the results of a large-scale problem will be evaluated using a heuristic based on neighbourhood local search. Performing a sensitivity analysis on the parameters of the common carrier yielded some intriguing insights for the managers.","Vehicle Routing Problem With Drones, Private Fleet and Common Carrier","[73392, 76767, 38681]",781,"[145, 65, 100]",772,Routing Unmanned Aerial Vehicles 2,5,4,64,VeRoLog - Vehicle Routing and Logistics,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S16 [building - 101],"['Vehicle Routing', 'Logistics', 'OR in Sustainability']",MC-64
"We investigate a new multi-hospitals collaborative operating room scheduling problem with consideration of the downstream recovery beds to make integrated decisions from tactical and operational levels. For solving this challenging problem, we propose an effective multi-operator driven iterated tabu search [MOITS] algorithm to achieve a good search balance between intensification and diversification. The greedy initial solution construction procedure employs a priority scoring rule to sequence patients to obtain a high-quality initial schedule. The multi-operator driven tabu search procedure employs four move operators to manipulate surgeries in different OR time blocks, two move operators to change recovery hospitals of the scheduled surgeries, and one move operator to change specialties of the OR time blocks. New evaluation functions are designed for the recovery beds related move operators to allow capacity violations during the search. The elite set guided adaptive perturbation procedure uses historical information from a pool of best found solutions to adjust the OR time blocks assigned to each specialty. Experimental results indicate that our proposed MOITS algorithm is capable of finding much better solutions with an order-of-magnitude time reduction than Gurobi across problems of different sizes.",Multi-hospitals collaborative operating room scheduling with downstream capacity constraints,"[40508, 76772, 65572, 29337, 62321]",226,"[56, 14, 74]",774,Applications of combinatorial optimisation in industry and services I,64,7,29,Combinatorial Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,157 [building - 208],"['Health Care', 'Combinatorial Optimization', 'Metaheuristics']",TA-29
"In this paper, we study the problem of liquidation strategies in financial networks over two periods. Financial contagion arises when substantial overlapping illiquid assets are sold at significantly depressed prices. We present a novel strategic liquidation model for a bank in financial networks, allowing for preemptive liquidation before maturity, where the uncertainty of other banks’ two-period liquidation decisions poses a challenge to risk evaluation. To address the issue, we propose a robust quantification for the liquidation income and develop the optimal liquidation strategy for maximizing the bank’s cash after liquidation under the worst-case scenario, which we refer to as the ‘maximin strategy.’ We prove that the maxmin strategy is unique and remains invariant in the presence of interbank liabilities. We further demonstrate that with the information on all banks’ liquidation strategies available, the strategic liquidation problem becomes a liquidation game whose unique Nash equilibrium is that all banks adopt their maxmin strategy. In addition, with only aggregate information on interbank liabilities, we provide a modified near-optimal strategy for weakly interconnected financial networks. Our results provide guidelines for developing robust liquidation strategies that alleviate losses from financial contagion and build insights into evaluating liquidation effects in the financial network for decision-makers.",Robust Asset Liquidation Strategies in Financial Systems,"[76769, 64199]",142,"[126, 127, 83]",775,Risk management in finance,9,2,51,Risk management in finance,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,M5 [building - 101],"['Risk Analysis and Management', 'Robust Optimization', 'Optimization in Financial Mathematics']",MA-51
"Decision makers for adopting sustainability in their operations did not conventionally realize the environmental and socio-economic advantages of returns management. However, studies have shown sustainable investments in returns produce ecological improvements that can have considerable financial advantages [Win-win situation]. However, comprehensive research is still required to achieve this sustainability to achieve economic and environmental paradigms. Research must focus on providing resilient models that demonstrate the competitive advantage companies can build by extracting value from product returns. It has also been realized that to make Enterprise Systems [ES] more sustainable, responsive, and competitive, considerable research effort is required to demonstrate performance improvement forward [i.e. flow of materials, products, and information from producer to consumer] as well as in reverse flows [referring to both recovery and return of products and information from consumer to producer]. The tradeoffs between the performance objectives of both flows necessitate the development of enough process capabilities that can improve performance simultaneously. Flexibility can be seen as one such capability to enhance the resilience of value recovery flow systems. This is an effort to study and demonstrate economic performance through a resilient product recovery system under various scenarios, which is the primary subject of the paper.
",Flexibility Focused Sustainable and Resilient Product Recovery Network Models,"[76771, 76819, 23679]",689,"[25, 40, 150]",776,Sustainability in Consumer Systems & Industry,80,9,53,Sustainable and Resilient Systems,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,8007 [building - 202],"['Decision Analysis', 'Environmental Management', 'Network Flows']",TC-53
"In this talk we deal with set-valued functions with values in the power set of a separated locally convex space where a nontrivial pointed cone induces a partial order relation. We present a conjugation pattern which allows a biconjugation theorem when the set-valued function is proper and evenly convex, which means that its epigraph is the intersection of an arbitrary family of open half-spaces. Duality theory is then developed via perturbational approach and the first results are also presented.",C-conjugacy for set-valued functions and first results on duality theory,[26649],266,"[21, 0]",780,Infinite Optimization - stability and duality,82,12,42,Variational Analysis and Continuous Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,98 [building - 306],['Convex Optimization'],WA-42
"Forecasts accuracy is definitively a crucial topic for industrial companies. Indeed, its impacts are huge especially for finance and production departments. It can occur high costs for the company if the forecasts are not accurate, due to stock-outs or excesses of inventory, for example.
Therefore, the purpose of this study is to optimize accessories forecasting for a medium-sized Swiss enterprise. To do that, different forecasting techniques are tested and a comparison is made between statistical methods and machine learning algorithms. The results have been adjusted thanks to the key account managers [KAM] expertise. 
In this talk, a comparison between exponential smoothing, seasonal autoregressive integrated moving average [SARIMA], SARIMAX [SARIMA with exogenous regressors] and Machine Learning algorithms such as k-nearest neighbors [k-NN], LASSO regression, linear regression and even random forest is presented.
To compare these different methods, two measures of statistical dispersion are computed - mean absolute error [MAE] and root mean squared error [RMSE]. These results have been standardized for a better comparison. It results that for our dataset SARIMAX [with the KAM’s expertise as exogenous variable] gives better results that all the machine learning algorithms tested.",How to improve accessories sales forecasting of a medium-sized Swiss enterprise?  A comparison between statistical methods and machine learning algorithms,"[64256, 76776, 156]",514,"[47, 59, 138]",782,"Advancements of OR-analytics in statistics, machine learning and data science 5",16,8,28,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,065 [building - 208],"['Forecasting', 'Industrial Optimization', 'Supply Chain Management']",TB-28
"Recent works have shown that line search methods can speed up Stochastic Gradient Descent [SGD] and Adam in modern over-parameterized settings. However,existing line searches may take steps that are smaller than necessary since they require a monotone decrease of the [mini-]batch objective function. We explore nonmonotone line search methods to relax this condition and possibly accept larger step sizes. Despite the lack of a monotonic decrease, we prove the same fast rates of convergence as in the monotone case. Our experiments show that nonmonotone methods improve the speed of convergence and generalization properties of SGD/Adam even beyond the previous monotone line searches. We propose a POlyak NOnmonotone Stochastic [PoNoS] method, obtained by combining a nonmonotone line search with a Polyak initial step size. Furthermore, we develop a new resetting technique that in the majority of the iterations reduces the amount of backtracks to zero while still maintaining a large initial step size. To the best of our knowledge, a first runtime comparison shows that the epoch-wise advantage of line-search-based methods gets reflected in the overall computational time.",Don’t be so Monotone - Relaxing Stochastic Line Search in Over-Parameterized Models,[76777],305,"[63, 136, 66]",784,Algorithmic Advances in Large Scale Nonconvex Optimization,84,3,32,Advances in large scale nonlinear optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,41 [building - 303A],"['Large Scale Optimization', 'Stochastic Optimization', 'Machine Learning']",MB-32
"Biclustering, also called co-clustering, block clustering, or two-way clustering, involves the simultaneous clustering of both the rows and columns of a data matrix into distinct groups, such that the rows and columns within a group display similar patterns. Focusing on the densest k-disjoint-biclique problem as a biclustering model, the objective is to identify a set of k disjoint bicliques within a weighted complete bipartite graph such that the sum of the densities of the complete subgraphs induced by these bicliques is maximized. A branch-and-cut method is proposed to achieve global optimality. The upper bound routine involves solving a semidefinite programming relaxation, employing a cutting-plane approach to strengthen the bound. For the lower bound, a maximum weight matching rounding procedure is designed, leveraging the solution of the relaxation solved at each node. Computational results on both synthetic and real-world instances show that the proposed algorithm is capable of solving instances with a size ten times larger than general-purpose solvers.",A branch-and-cut algorithm for biclustering via semidefinite programming,[69969],285,"[52, 11, 115]",785,Large Scale Constrained Optimization - Algorithms and Applications,84,2,32,Advances in large scale nonlinear optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,41 [building - 303A],"['Global Optimization', 'Branch and Cut', 'Programming, Semidefinite']",MA-32
"The present study introduces a novel mathematical model developed to address a challenge posed at the 174th European Study Group with Industry, by a prominent footwear company. The objective is to design a model that optimizes the utilization of shoe injection machines, machines used to produce a diverse range of shoe models and colors of shoes across up to 15 different sizes. The primary goal is to enhance production efficiency throughout the manufacturing process, including the preparation and finishing phases. To tackle this complex optimization problem, an integer programming model has been developed.  This model is designed to minimize setup times associated with changes in color, model, and size, thereby streamlining the production process. Moreover, considering the substantial investment involved, the model also takes into consideration the limited availability of molds.  In this work, some preliminary findings derived from both academic and real-world instances will be presented. Additionally, ongoing efforts and future enhancements aimed at refining the efficacy of the proposed model will be outlined.
This work has been partially supported by national funds through FCT - Fundação para a Ciência e Tecnologia, under the project UIDB/04728/2020.
",A Mathematical Model for the Optimization of a Shoe Injection Machine ,"[43794, 24649, 76796, 9554, 76793]",805,"[151, 109, 129]",786,Lot-sizing with industrial applications I,32,4,49,"Lot Sizing, Lot Scheduling and Production Planning","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,M1 [building - 101],"['Practice of OR', 'Programming, Integer', 'Scheduling']",MC-49
"In supply chain network design [SCND], facility location decisions play a pivotal role. Yet, the individual role of and interdependence between facilities constituting an optimal solution remain inadequately understood. This lack of contextual information hinders decision-makers' confidence in these models, resulting in a gap between academic research and practical implementation.
Our study evaluates optimal and near-optimal solutions for a given instance, introducing concise measures to characterize commonalities in the decisions and the criticality and interdependence between individual location decisions. We relate these characteristics to the spatial distribution of candidates and customers and show that contrasting academic perception, the underlying spatial patterns yield little explanatory value for the stability of location decisions. We propose a pattern-recognition-based approach to show that well-performing solutions to an instance often share an implied division of the facility customer space, which not only explains the criticality and relationship between individual facilities but also allows anticipating how minor perturbations will affect the network. Our research offers a deeper understanding of facility location dynamics within SCND. This may empower decision-makers to trust and implement these models in real-world settings.",From isolated core facilities to interrelated service regions - understanding dependencies in supply chain network design,"[59703, 5078]",768,"[64, 138]",787,Location in Logistics and Supply Chain Management,29,10,61,Locational Analysis,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S10 [building - 101],"['Location', 'Supply Chain Management']",TD-61
"We quantify the bullwhip effect [which measures how the variance of replenishment orders is amplified as the orders move up the supply chain] when demands constitute a first-order auto-regressive random process and lead times constitute a possibly temporally correlated stationary sequence of random variables. We assume future demands are predicted with the minimum mean squared error method and random lead times are estimated using any method. Under these general assumptions we derive a formula 
for the bullwhip effect measure as the ratio of the replenishment orders variance and demands variance. 
Using this formula we analyse the impact of auto-correlated demands and auto-correlated lead times on the bullwhip effect. Our investigation of the impact of the lead time auto-correlation on bullwhip appears to be unique in the literature. Our analysis focuses on using the naive forecasting method, the moving average method and the minimum mean squared error method for forecasting the lead times. We show how the bullwhip effect is influenced by demand auto-correlation, lead time auto-correlation, and number of periods in the moving average forecast of the lead times. We reveal that there exists  minima and maxima in bullwhip effect as a function of those parameters. For the moving average forecasting method of lead times and their negative auto-correlation we observe an even-odd phenomenon. Our theoretical results are confirmed by Monte Carlo simulation.",The bullwhip effect with correlated lead times and auto-correlated demand,"[76780, 76783, 76784]",162,"[138, 135, 131]",789,Stochastic Models in Logistics,50,8,39,Stochastic Modelling,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,35 [building - 306],"['Supply Chain Management', 'Stochastic Models', 'Simulation']",TB-39
"We present a multi-stage stochastic optimization model for a waste collection routing problem. The problem is modeled as an inventory routing problem where decisions are related to the selection of  bins to be visited and the corresponding visiting sequence in a predefined time horizon. The aim is the maximization of the total expected waste collection at lowest transportation cost, considering uncertainty in the waste accumulation rate in the network bins. Stochasticity in waste accumulation is modeled through scenario trees generated via conditional density estimation and dynamic stochastic approximation techniques. The model is solved through a rolling horizon approach, providing a worst-case analysis on its performance. Computational experiments are carried out on instances based on real data of a large Portuguese waste collection company. The impact of stochasticity on waste generation is examined through stochastic measures, and the performance of the rolling horizon approach is evaluated. Some managerial insights are finally discussed.",A rolling horizon heuristic approach for a multi-stage stochastic waste collection problem,"[70429, 24015, 59022, 48740, 24902]",281,"[145, 136, 61]",792,Robust and Stochastic Routing Problems,49,2,35,"Stochastic, Robust and Distributionally Robust Optimization","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,44 [building - 303A],"['Vehicle Routing', 'Stochastic Optimization', 'Inventory']",MA-35
"We study the procurement of physician-preferred implants where both the hospital and patient surplus are considered and optimized. For better flexibility, we introduce the use of alternate devices instead of preferred ones. Considering alternate implants allows for improving the hospital surplus through larger revenues and volume discounts, although it implies an additional cost to cross train physicians who may need to use implants and associated surgical procedures they are less familiar with. The problem is NP-hard. We propose two heuristic procedures for quickly finding nearly optimal solutions and for estimating the optimality bound. We demonstrated results of a series of experiments where we considered different scenarios corresponding to several patient demand distributions across patient-types and emphasized the benefits of alternate implants and physician cross-training. Using patient-reported costs and total hip replacement outcomes from the medical literature in the experiments, we show that the increased flexibility physicians acquire from cross-training - [1] Improves patient outcomes; [2] Improves the hospital’s financial performance by increasing hospital revenues and cost savings from volume discounts when larger orders of certain medical devices are placed; [3] Improves patient and physician satisfaction; [4] Reduces the negative impact of demand uncertainty on a hospital’s financial performance.  

",A Patient-Centered Hospital Procurement of Physician Preferred Implants ,[28804],968,"[56, 77, 73]",794,Decision support in healthcare,3,2,17,OR in Health Services [ORAHS],"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,40 [building - 116],"['Health Care', 'Multi-Objective Decision Making', 'Medical Applications']",MA-17
"The measurement of the relative efficiency of the Decision-Making Units [DMUs] in a fuzzy environment is conducted through the employment of the fuzzy data envelopment analysis [FDEA]. In FDEA, one of the primary concerns is to identify the benchmarking for inefficient DMUs, in order to enhance the efficiency of these particular units. Additionally, the complete ranking of the DMUs is also an important aspect to consider within the realm of FDEA. To address these issues, this article proposes the utilization of a reference set and complete ranking technique for the DMUs in a neutrosophic environment, which is an extension of the fuzzy environment that effectively represents uncertainty. In order to evaluate the relative efficiency of the DMUs, the Neutrosophic DEA model is solved using the possibility mean approach. Furthermore, a reference set based on the possibility mean function is suggested to identify peers for the inefficient DMUs. Moreover, the Neutrosophic super efficiency model, is employed to assess the relative super efficiency score of the DMUs. This super efficiency score is then utilized to achieve a complete ranking of the DMUs. To demonstrate the effectiveness and applicability of the proposed approach, a numerical example is considered.",Complete Ranking and Benchmarking Methods for Improved Efficiency Assessment in Neutrosophic Environments,"[76768, 76788, 76746, 79745]",945,"[24, 35, 49]",795,DEA methodological developments I,89,14,48,Data Envelopment Analysis and its Application,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,60 [building - 324],"['Data Envelopment Analysis', 'Efficiency Analysis', 'Fuzzy Sets and Systems']",WC-48
"I will review the existing links between pairwise comparisons and gauge theory in physics. These links already exist in portfolio management since 1999 but are highlighted only recently in pairwise comparisons, even if the mathematical structures are the same. After a indroduction in which the necessary notions of connexion, holonomy and gauge are refreshed for the audience, I will give an in-depth description of the mathematical analogy between priority vectors with Yang-Mills fields, both in a deterministic and a stochastic way. In this picture, the notion of inconsistency indicator will have a plurality of natural meanings in terms of mathematical concepts. At the end of this short talk, I hope to have the time to discuss perspectives both for decision theory from physic, in particular with the controversal use of the theory of Lie groups but also with a minimal setting for pairwise comparisons in the framework of categories and also with quantization procedures, and for physics from decision theory, in particular with inconsistency reduction for scalar gauge fields [e.g. electromagnetic fields]. ",Pairwise Comparisons - Gauge Theoretical Aspects and Beyond,[76790],892,"[6, 0]",796,Pairwise comparisons and preference relations 2,44,10,44,Multiple Criteria Decision Analysis,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,20 [building - 324],['Analytic Hierarchy Process'],TD-44
"The traditional additive two-stage network DEA model under constant returns to scale has non-increasing decomposition weights [i.e. the weight assigned to the first stage is not less than the weight assigned to the second stage], which also has a direct impact on the stage efficiencies. Previous research has revealed that adding external input/s in the second stage provides the necessary conditions under which there can be a reversal in the decomposition weights for the two stages. In this paper we are investigating the case of an external public input in the second stage, which is shared across DMUs but not allocatable, and its effect on the decomposition weights. The empirical investigation is undertaken using a dataset of Japanese Regional Banks. Bank of Japan has a long tradition with quantitative easing as a monetary policy instrument that could help boost economic activity, avoid deflation, and overcome the problems of the liquidity trap. Given that quantitative easing provides all banks with liquidity, we model it as a constant input in the second stage of the model. This generalised structure of the model with an external single constant input in the second stage tend to produce results which do not suffer from the issues under investigation.",Including a Public Input in the Additive Two-Stage Network DEA Model - The Case of Quantitative Easing in Japan,"[72127, 13122, 76794]",942,"[24, 44]",798,DEA applications in Policy Making and Planning I,89,10,48,Data Envelopment Analysis and its Application,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,60 [building - 324],"['Data Envelopment Analysis', 'Finance and Banking']",TD-48
"For the cost-efficient supply of stores, it is common in retail practice to rely on third-party logistics providers [3PL] instead of their an fleet. A 3PL carries out the retailers' delivery tours and commonly bills according to a zone-based tariff. This tariff assigns each store to one zone that reflects the travel distances for the delivery. Stores further from the depot are assigned to higher zones. The cost of a tour depends on the furthest zone visited and the volume, subject to discounts. Additionally, 3PLs limit the detour of a tour to ensure economic feasibility. The detour limitation prevents excessive driving within the zones visited. While using 3PLs and their zone tariffs reduces the complexity for retailers, the question arises of how retailers can plan cost-minimal tours that are economical for 3PLs. We address this issue and formalize the problem as a Capacitated Vehicle Routing Problem with a Zone Tariff. The nonlinear tariff and the non-monotonically increasing detour drive the complexity. We provide the first mixed-integer formulation [MIP] for the problem, which we strengthen with valid inequalities. We develop a logic-based Benders decomposition algorithm [LBBD] to solve larger instances and improve it with problem-specific acceleration techniques. In our numerical experiments, we solve a real-world case and adapted instances from the literature. The LBBD approach reduces the runtime compared to the MIP by up to one order of magnitude.",Logic-Based Benders Decomposition for a Capacitated Vehicle Routing Problem With a Zone Tariff,"[76795, 22691, 49049]",779,"[143, 145, 72]",800,MILPs for Vehicle Routing 1,5,10,58,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S07 [building - 101],"['Transportation', 'Vehicle Routing', 'Mathematical Programming']",TD-58
"The increase of greenhouse gas [GEE] levels alters the energy balance between the atmosphere and the Earth surface, leading to temperature changes that modifies the atmosphere chemical composition. Then, the air pollution represents one important index to be analyzed, as it is directly related to climate changes mitigation goals. The development of a public tool that allows future predictions of potential GEE environmental impacts are essential to the economy, environment and social and health aspects. In this sense, the development of systems for monitoring, forecasting, and controlling emissions plays an important role. The main objective of this research is to apply trainable and non-trainable combination methods for air pollution forecasting in Brazil. Trainable ensembles based on Artificial Neural Networks [ANN] and linear regression are compared with non-trainable combinations, single ANN, and linear statistical approaches. Different models are considered so far, including Autoregressive Model, Autoregressive and Moving Average Model, Infinite Impulse Response Filters, Multilayer Perceptron, Radial Basis Function Networks, Extreme Learning Machines, and Echo State Networks. The use of trainable ensembles led to a better performance. The use of robust tools is paramount to help governments in managing air pollution issues like hospital collapse during adverse air quality situations.",Greenhouse gases levels prediction using artificial neural networks,"[27997, 76803, 76801, 76802, 76800]",520,"[8, 47, 66]",805,"Advancements of OR-analytics in statistics, machine learning and data science 10",16,14,28,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,065 [building - 208],"['Artificial Intelligence', 'Forecasting', 'Machine Learning']",WC-28
"In the full cycle of the Operations Research methodology, artificial intelligence is most prominently utilized in the algorithmic aspect. Meanwhile, modeling is traditionally considered an art exclusive to human being experts. What if we could simply tell the computer the problem and, in return, receive an optimization model and its solution? We propose a new class of Decision Support Systems [DSS], termed Conversational Decision Support Systems [C-DSS]. These systems, empowered by agents based on large language models, perform the art of modeling and elevate human-machine interaction to the interface of problem definition and mathematical modeling, using natural language as the primary mode of communication. In our concept, a decision-maker interacts with the C-DSS by discussing the problem to obtain decision proposals.

First, we analyze how general-purpose language models handle problems that can be modeled as mixed-integer problems, requiring various modeling techniques such as linearization of convex/concave functions, logical conditions, and indicator constraints, among others. Additionally, we enhance the general-purpose language model with specialized knowledge using the Retrieval Augmented Generation method. The generated mathematical programming models require verification, which can also be facilitated by another language model-based agent. Therefore, we propose an architecture of C-DSS powered by several specialized language model-based agents.
",Conversational Decision Support Systems - a telling-to-modeling AI-enhanced approach,[19983],652,"[26, 8, 72]",806,Artificial Intelligence and Machine Learning for Decision Support,45,8,45,Decision Support Systems,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,30 [building - 324],"['Decision Support Systems', 'Artificial Intelligence', 'Mathematical Programming']",TB-45
"The technology of reflection coefficient inversion aims to broaden the spectrum and increase the dominant frequency based on the effective collection of data. Reflection coefficient inversion can be achieved through conventional physics-based optimization modeling. In recent years, machine learning has seen tremendous development in the field of inverse problems. Deep learning has made breakthrough progress in fields such as image processing and natural language processing [NLP]. Following the convolutional neural network [CNN] and recurrent neural network [RNN], the transformer network architecture, comprised only of self-attention mechanisms and feedforward neural networks, has also become a research hotspot in recent years. Our latest work has realized blind seismic reflection coefficient inversion using a transformer network. By setting up appropriate encoders and decoders and incorporating physical mechanism constraints, the transformer has demonstrated powerful seismic inversion capabilities. Experimental data indicates that the new method based on the transformer network architecture does not require any a priori assumptions about the seismic records’ reflection coefficient series, seismic wavelets, or Q value models; it possesses strong noise resistance and resolution; after the model training is completed, it has relatively good generalization performance, and the model prediction process is more efficient compared to traditional methods.",Seismic optimizing inversion with multi-head attention mechanism and transformer,[74171],443,"[84, 59, 66]",807,Nonlinear optimization algorithms and applications,82,14,42,Variational Analysis and Continuous Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,98 [building - 306],"['Optimization Modeling', 'Industrial Optimization', 'Machine Learning']",WC-42
"In this talk, we give a new derivative-free method by introducing an improved under-determined quadratic interpolation model. We analyze the least norm type under-determined quadratic interpolation model proposed by Conn and Toint from the perspective of the property of trust-region iteration. We found the Karush-Kuhn-Tucker multiplier's non-determinacy when constructing a quadratic model considering the trust-region iteration in the case where the current iteration point is on the boundary of the trust region. The lack of the quadratic model's uniqueness caused by the Karush–Kuhn–Tucker multiplier's non-determinacy leads us to propose a new model by selectively treating the last obtained under-determined quadratic model as a quadratic model or a linear one. We propose the theoretical motivation, computational details, and the quadratic model's formula derived from the Karush-Kuhn-Tucker conditions. The formula is implementation-friendly for the existing model-based derivative-free  methods. The numerical results with released codes support the advantages of our quadratic model in the derivative-free optimization methods. To the best of our knowledge, this is the first work considering the 
property of trust-region iteration and the model's optimality when constructing the under-determined quadratic model for derivative-free trust-region methods.",A New Derivative-free Method Using an Improved Under-determined Quadratic Interpolation Model,[26230],443,"[72, 19, 113]",808,Nonlinear optimization algorithms and applications,82,14,42,Variational Analysis and Continuous Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,98 [building - 306],"['Mathematical Programming', 'Continuous Optimization', 'Programming, Nonlinear']",WC-42
"Integrating intelligent screening technologies into educational admissions systems fundamentally transforms the role of educators in selecting candidates. By employing advanced AI-based techniques, this system enhances the decision-making capabilities of educators in the candidate selection process. The system assists educators in identifying promising candidates through the analysis of applicant data, including academic achievements, extracurricular activities, and personal statements. The theoretical foundation is rooted in predictive analytics, which assesses the content presented in applicants' documents and their compatibility with the program's development to estimate candidates' suitability and potential success. This adjustment more accurately reflects the review and evaluation of student data in the university admissions process, particularly regarding the alignment of students' demonstrated content with the objectives of their chosen department.",Educational Admissions Transformed - Navigating the Future with Intelligent Screening,[46952],791,"[8, 34]",810,OR Education I,48,2,16,OR Education,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,19 [building - 116],"['Artificial Intelligence', 'Education and Distance Learning']",MA-16
"There is an increasing trend of combining Operations Research [OR] with Machine Learning [ML] techniques. Both disciplines are already very beneficial when used independently. Combining both techniques will even enhance the benefits. The author starts with a brief historical overview on Operations Research [OR] and Machine Learning [ML], tracing the important milestones in each field. Then the author talks about the different categories and techniques used in Operations Research and Machine Learning, highlighting the diverse methodologies such as linear programming, mixed integer programming and network optimization in OR, together with supervised, unsupervised learning and reinforcement learning in ML. 
Next, the author talks about the practice of Operations Research and Machine Learning, including the educational background and job roles or functions of practitioners in the field, the software tools and the most common techniques or algorithms used. The author also discusses the different applications in various industries. The author will highlight the benefits and limitations of Operations Research [OR] and Machine Learning [ML] when used independently. Then, the author explores the ways both techniques are integrated based on the literature such using ML then OR, using ML in OR, and using OR in ML. Finally, the author will mention some case studies from the literature how companies leveraged on both techniques to enhance their operational decision-making process. 
",Integration of Operations Research [OR] and Machine Learning [ML] – A Literature Review,[66158],317,"[66, 151, 7]",812,Data Science and Optimization,14,12,03,Data Science Meets Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1005 [building - 202],"['Machine Learning', 'Practice of OR', 'Analytics and Data Science']",WA-03
"This study addresses the escalating concern of zombie firms in the Indian context, employing a unique methodology to identify and analyze their characteristics. Utilizing a dataset encompassing 45,867 firm-year observations from 2009-2021, our approach identifies 1,199 zombie firms, comprising 29.35% of the sample. Regression analysis reveals a positive link between high reliance on debt capital and the likelihood of zombification. These firms exhibit characteristics such as being relatively new, possessing small asset sizes, higher leverage, and poor performance. Additionally, they maintain lower cash holdings, higher leverage ratios, and lower capital intensity. Our findings indicate that zombie firms adopt a conservative cash policy and aggressive financing, coupled with a conservative investment approach. This study contributes to the existing literature, shedding light on the implications of zombification for corporate policies in the Indian business landscape. The research emphasizes the potential misallocation of funds, posing a financial burden for both companies and the economy. As the first of its kind in the Indian context, this study holds critical implications for corporates, creditors, and policymakers, providing valuable insights for enhancing economic well-being and long-term viability.",Firm Zombification & Corporate Policy - Empirical examination  of Indian Firms,"[76754, 77514]",142,"[44, 1]",813,Risk management in finance,9,2,51,Risk management in finance,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,M5 [building - 101],"['Finance and Banking', 'Accounting']",MA-51
"In this paper, we study the surgery scheduling problem in the operating room theatre. The problem considers the sequencing of patients and calculation of their start times with splitting of surgeries into resource phases to facilitate the efficient use of different types of resources. We propose a dedicated two-layer heuristic to compose an operational patient and resource schedule. The first optimisation layer applies an evolutionary heuristic to devise patient schedules while considering the scheduling of the operating surgeons and rooms. This step employs a machine-learning mechanism predicting the feasibility of chromosomes, which improves the algorithm's efficiency and effectiveness, and relies on novel local search operators to find high-quality solutions. The second layer devises the schedule of the other resources using a decomposition-based heuristic. Computational experiments are conducted to show the performance of the proposed two-layer heuristic and validate its design choices. We benchmark the proposed algorithm with other optimisation procedures and show the contribution of considering multiple resource phases for real-life decision-making.",A two-layer heuristic for patient sequencing in the operating room theatre considering multiple resource phases,"[61085, 19342]",598,"[56, 129, 74]",814,Home Health Care and Operating Room Scheduling,3,12,15,OR in Health Services [ORAHS],"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,18 [building - 116],"['Health Care', 'Scheduling', 'Metaheuristics']",WA-15
"Recognizing the critical role that ports play in promoting trade and economic expansion worldwide, port authorities are actively seeking strategies to reduce inefficiencies and improve port operations. The seaports along the northern Adriatic are an essential part of the European transport network. An important approach to reduce inefficiencies in the ports is to assess and eliminate port congestion in order to minimize operational bottlenecks. Data Envelopment Analysis [DEA], a widely used technique for assessing decision making units [DMUs] in different sectors, is also suitable for measuring congestion in individual units. In this study, a one-step DEA model is used to determine the efficiency scores and congestion of ports in the northern Adriatic in the years 2020-2022, allowing for a simultaneous assessment of inefficiency and congestion. The results show which ports are confronted with the problem of inefficiency and congestion during the period under investigation. The analysis provides these ports with insights for more precise and effective resource allocation and coordination.",Input congestion assessment in the North Adriatic ports,"[76781, 76809]",944,"[24, 143, 70]",818,DEA applications in transportation,89,13,48,Data Envelopment Analysis and its Application,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,60 [building - 324],"['Data Envelopment Analysis', 'Transportation', 'Maritime applications']",WB-48
"Abstract - Reconfigurable intelligent surface [RIS] is a hot candidate technoloty for 6G. RIS aided multiple input multiple output interference channel is considered. The precoding beamforming matrices and RIS parameters are jointly designed for sum rate maximization. The approximation explores the upper bound of the sum rate, and the optimization problem is reformulated as a minimax saddle point problem through the approximated Lagrangian function. The single-loop primal dual method is designed for the saddle point problem, where the primal variable is updated through one projected gradient step and the dual variable is solved through quadratic interpolation. It turns out that the proposed method performs well numerically, which achieves promising sum rate with very little computational cost.",Single-loop primal dual method for complex-field phase optimization problem in wireless communication,[50094],443,"[19, 113]",819,Nonlinear optimization algorithms and applications,82,14,42,Variational Analysis and Continuous Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,98 [building - 306],"['Continuous Optimization', 'Programming, Nonlinear']",WC-42
"For a road traffic network with uncertain service, a learning-based optimization is presented. In order to appropriately address spatial evolution of traffic congestion inside road links, a time-varying traffic model with uncertain capacity at links downstream is considered. Accounting for road users’ behavior, a learning-based bilevel program can be proposed. A learning-based optimal signal settings can be determined at the upper level via reinforcement learning and users’ equilibrium traffic flow can be decided at the lower level. In order to effectively solve the proposed bilevel program, a learning-based optimization can be decomposed into two sub-problems. A Quasi-Newton update is introduced to find solution with globally asymptotical convergence. In order to ensure feasibility of solution found against high-consequence realization of stochastic capacity, a learning-based robust model is proposed. Numerical experiments are performed at a moderate traffic grid. As compared to conventional approach, obtained results obviously showed that the proposed model can exhibit sufficient gain of achieving effectiveness while attenuating time-varying congestion in the presence of stochastic capacity at links downstream.",An active learning-based optimization for road traffic networks with uncertain service,[1558],151,"[66, 5, 72]",820,Transportation Network Modelling and Optimization I,6,2,55,Transportation,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S02 [building - 101],"['Machine Learning', 'Algorithms', 'Mathematical Programming']",MA-55
"Cybersecurity breach probability functions describe how cybersecurity investments impact the actual vulnerability to cyberattacks through the probability of the attack's success. They essentially use mathematical models to make cyber-risk management choices. This talk provides an overview of the breach probability models that appear in the literature. For each of them, the form of the mathematical functions and their properties are described. The models exhibit a wide variety of functional relationships between breach probability and investments, including linear, concave, convex, and a mixture of the latter two. Each model describes a parametric family, with some models having a single parameter and others having two. A sensitivity analysis completes the overview to identify the impact of the model parameters - the estimation of the parameters which have a larger influence on the breach probability is more critical and deserves greater attention.",Security Breach Probability Models,[55364],438,"[33, 18, 126]",825,Risk Management in Private and Public Finance,4,13,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S14 [building - 101],"['Economic Modeling', 'Computer Science/Applications', 'Risk Analysis and Management']",WB-63
"We consider the lot-sizing problem with set-ups where the demands are uncertain. The demand in each time period is assumed to belong to an interval. We propose a novel approach to evaluate the inventory costs where between two consecutive production periods, the adversary chooses to set the demand either to its higher value or to its lower value in order to maximize the inventory [holding or backlog] costs. 
A mixed-integer model is devised and a column-and-row generation algorithm is proposed.  
Computational tests based on random generated instances are conducted to evaluate the model, the decomposition algorithm, and compare the structure of the solutions from the robust model with those from the deterministic model.
",A robust model for the lot-sizing problem with uncertain demands,[19905],807,"[61, 127]",826,Stochastic lot-sizing problems,32,7,49,"Lot Sizing, Lot Scheduling and Production Planning","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,M1 [building - 101],"['Inventory', 'Robust Optimization']",TA-49
"I will present the framework of slowly varying regression under sparsity, allowing sparse regression models to exhibit slow and sparse variations, through an application in energy consumption prediction. First, I will formulate the problem of parameter estimation as a mixed-integer optimization problem; then, I will demonstrate that it can be precisely reformulated as a binary convex optimization problem through a novel relaxation technique, convexifying the non-convex objective function while matching the original objective on all feasible binary points. I will develop a highly optimized implementation of a cutting plant-type algorithm, a fast regularization-based heuristic method that guarantees a feasible solution, and a practical hyperparamrter tuning procedure relying on binary search that, under certain assumptions, is guaranteed to recover the true model parameters.",Slowly varying regression under sparsity ,[76811],140,"[66, 14, 8]",827,Mathematical Optimization for Trustworthy Machine Learning,15,9,27,Mathematical Optimization for XAI,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,047 [building - 208],"['Machine Learning', 'Combinatorial Optimization', 'Artificial Intelligence']",TC-27
"This study presents a method using microscopic traffic flow simulation to assess urban expressway exit ramp layouts, addressing the problem of ramps too close to surface road intersections, leading to insufficient queuing space. Utilizing PTV VISSIM, a real-world case is analyzed over 65 minutes, including a 5-minute warm-up phase. The core simulation spans one hour, segmented into six 10-minute intervals. The research fine-tunes the Wiedemann 74 car-following model using comparative analysis of observed and simulated traffic volumes, adjusting parameters to match site conditions. Validation criteria require a percentage difference of under 10% and a GEH statistic below 5. Building on the validated base traffic model, which uses a 90-meter ramp length, the research extends the model to simulate scenarios with ramp lengths of 150, 200, 250, and 300 meters. Through linear regression analysis of the simulation data, relationships between traffic flow and density for these configurations are established separately. These findings are then mapped against the Highway Capacity Manual’s service level density ranges to determine the optimal lengths for varying traffic flow conditions. The findings indicate a significant flow increase when extending the ramp from 150 to 200 meters, peaking at 250 meters. However, it also shows that a 200-meter ramp connection length optimizes queuing lengths, suggesting a balance between increased flow rate and minimized vehicle congestion.",Optimizing Urban Expressway Exit Ramp Lengths through Microscopic Traffic Flow Simulation - A Case Study Using PTV VISSIM,"[40380, 76812]",505,"[131, 143]",828,Traffic flow modeling ,6,4,56,Transportation,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S04 [building - 101],"['Simulation', 'Transportation']",MC-56
"This paper investigates stochastic scheduling and routing problems in the online meal  delivery  [OMD]  service.  The  huge  increase  in  meal  delivery  demand  requires  the  service  providers to construct a highly efficient logistics network to deal with a large-volume of time-sensitive and fluctuating fulfillment, often using inhouse and crowdsourced drivers to secure the ambitious service quality. We aim to address the problem of  developping an effective scheduling and routing policy that can handle real-life situations. To this end, we first model the dynamic problem as a Markov Decision Process [MDP] and analyze the structural properties of the optimal policy. Then we propose four integrated approaches to solve the operational level scheduling and routing problem. In addition, we provide a continuous approximation formula to estimate the bounds of required fleet size for the inhouse drivers. 
Numerical experiments based on a real dataset show the effectiveness of the proposed solution approaches. We also obtain several managerial insights that can help decision makers in solving similar resource allocation problems in real-time.",Stochastic Scheduling and Routing Decisions in Online Meal Delivery Platforms with Mixed Force,"[23193, 4914, 67085]",198,"[14, 145, 32]",831,Combinatorial optimization approaches for freight deliveries and home services,64,2,52,Combinatorial Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,8003 [building - 202],"['Combinatorial Optimization', 'Vehicle Routing', 'E-Commerce']",MA-52
"In recent years, automatization has taken on a relevant role in the low-skilled operations of different productive sectors. One of the most impacted sectors is logistics, which significantly influences product handling, warehousing, and shipping processes. In this context, Automated Storage and Retrieval Systems, AS/RS, become an ideal solution for logistics warehousing systems. Such implementation allows inventory management based on storage and retrieval orders optimizing the use of available physical space and material handling processes and improving system performance in terms of storage and retrieval times, material flow, energy consumption, operating costs, and other associated variable measurements. Multiple physical and control characteristics make up the global design of an AS/RS, involving the type of system technology, the number of aisles, the rack dimensions, storage and sequencing policies, among others. 
In this work, we studied sequencing and storage assignment operations in a multi-aisle AS/RS with class-based storage, block sequencing strategy, and non-dedicated cranes. We proposed an Integer Linear Programming model to find the best operation decisions, considering the dynamic behavior of the AS/RS and evaluating some control decisions related to these operations. Our findings showed that the fill grade factor, the size of the classes, and the size of the sequencing block affect the performance of the system in terms of travel time metrics. 

",Integer Linear Programming Model for Sequencing and Storage Assignment Operations in a Multi-Aisle AS/RS With Non-Dedicated Cranes,"[68654, 55024, 69121]",761,"[65, 109, 146]",841,Warehouse Operations,5,7,58,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S07 [building - 101],"['Logistics', 'Programming, Integer', 'Warehouse Design, Planning, and Control']",TA-58
"Scenario and foresight practices enable organisations to explore new opportunities and innovate. However, the value contribution of these methods depends on whether managers are able to implement the findings. Behavioural issues such as risk avoidance, conflicting organisational demands, and inappropriate decision-making logics hinder this transfer. Organisations often tend to prioritise protecting their dominant value creation over exploring new opportunities. The research aims to understand how behavioral issues and subtle factors influence the initiation of exploration. 
The ongoing research examines exploratory foresight methods from an ambidexterity perspective. A bibliometric analysis was conducted to understand the variety of concepts and current discussion on ambidexterity.  Our findings show that antecedents and capabilities, as well as the outcomes of ambidexterity are considered, while the actual management practices remain largely unexplored. Due to the complexity and idiosyncratic management approaches in companies, causality and management mechanism are difficult to research. For companies that provide critical infrastructure, innovation management must explicitly consider the impact on existing value creation, making it possible to explore factors and causal relationships. Further, we aim to understand multiple case studies in critical infrastructure systems and how management practices are reconfiguring structures and resources to seize future opportunities.",Navigating the path to future - Insights in fostering exploration in critical infrastructure systems,"[76810, 76831]",569,"[10, 0]",842,Scenarios and foresight practices - Behavioural issues II,13,13,11,Behavioural OR,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,12 [building - 116],['Behavioural OR'],WB-11
"This study investigates the platform-based stable truck-matching problem with trailer-swapping mode [STMP-TSM]. The TSM is a novel collaborative transportation approach in which trucks participate in trailer swapping, significantly decreasing the rate of empty trucks and reducing transportation costs. In the STMP-TSM, a platform delivers a trailer-swapping scheme that satisfies all participating trucks. Correspondingly, an integer linear programming model is developed to maximize the total truck utility of the STMP-TSM. A specific preference list based on a chain-data structure is meticulously constructed to obtain a stable matching scheme. The preference list enables more generalized truck matching. In addition, a series of acceleration strategies is proposed to expedite the generation of a preference list while effectively reducing its length. An iterative preference-list-trim heuristic algorithm is designed, which strategically trims chains in the preference list to solve the STMP-TSM efficiently. As a benchmark, an integer linear programming model is developed based on the preference list to solve the truck-matching problem using the TSM. Finally, a series of numerical experiments are conducted to evaluate the performance of the proposed algorithm, assess the practicality of the TSM, and analyze the influences of the key parameters.",Platform-based stable truck matching problem with trailer-swapping mode,"[76833, 76834, 75634, 76835]",83,"[5, 84, 143]",847,Port-Hinterland Transportation & Corridors,52,4,62,OR in Port Operations,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S12 [building - 101],"['Algorithms', 'Optimization Modeling', 'Transportation']",MC-62
"In this study, we explore the integration of cargo delivery into public transportation as an innovative and eco-friendly approach for city-based last-mile deliveries. This freight-on-transit [FOT] study, is driven by a prominent courier company's [SF Express] vision for a future where freight is transported via metro systems in Beijing, aiming to utilize the most of idle metro carriage space. The complexity of this hybrid road-subway delivery system, which needs to adhere to specific time schedules and split delivery, makes it a challenging problem to solve. To tackle this, we formulate it as a mixed integer programming model and develop an exact algorithm within a branch-price-and-cut framework. This method is capable of achieving near-optimal solutions efficiently. Our real-world data testing demonstrates that our strategy significantly outperforms the company's existing delivery strategies. In addition, we conduct various policy tests to assess the impact of dynamic dispatching and routing, considering changes in the size of the delivery fleet and the frequency of dispatches.",Freight-on-Transit for urban last-mile deliveries with road transportation and metro line,"[67085, 76836, 76837, 76838]",861,"[65, 13, 119]",848,Combinatorial optimization approaches for freight deliveries,64,3,52,Combinatorial Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,8003 [building - 202],"['Logistics', 'Column Generation', 'Public Local Transportation Systems']",MB-52
"In this talk, we tackle general convex mathematical programs with a complementarity constraint. We introduce a novel decomposition method for this latter problem build on a sequence of convex parametrised sub-problems. Via Wolfe duality theory, we derive optimality conditions to improve the optimal value of the parametrised sub-problems and valid linear inequality cuts. Moreover, we present preliminary encouraging experimental results for quadratic convex problems with complementarity constraints and convex binary programs.",A dual decomposition approach for convex problems with linear complementarity constraints,"[72493, 35844]",132,"[113, 111, 21]",851,Topics in Mixed Integer Nonlinear Programming 1,86,8,04,MINLP,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,1001 [building - 202],"['Programming, Nonlinear', 'Programming, Mixed-Integer', 'Convex Optimization']",TB-04
"This paper examines the interactions between content creators and viewers in donation-based live-streaming
platforms. Specifically, creators produce their content, then viewers enjoy the live-streaming
and decide to donate money to creators. To capture the sequential decision process of the model, we introduce a multi-leader-follower game, in which creators act as the leaders of the game and viewers as the followers. Creators first optimize their performance level and the duration of the streams to maximize their profit. Then, viewers optimize the time spent watching a live stream to maximize their utility. Thus, the first stage of the game models the non-cooperative competition among creators, while the second stage represents the behaviour of viewers deciding on their content demands. We formulate these stages as Nash equilibrium problems, and then as variational inequalities. We analyze the existence and uniqueness of the Stackelberg equilibrium. Then, we derive a supervised learning algorithm to estimate the parameters of the associated artificial neural network model, focusing on the advertisement function, to test our achievements.
",A Multi-Leader-Follower Game for the Analysis of the Interactions in Donation-Based Live-Streaming Platforms,[9982],900,"[19, 50, 66]",852,Equilibrium detection in applications,63,12,40,"Interfaces Between Optimization, Hierarchical Problems and Equilibrium Detection with Applications","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,96 [building - 306],"['Continuous Optimization', 'Game Theory', 'Machine Learning']",WA-40
"The aim of the talk is to introduce a random elastic traffic equilibrium problem in a Hilbert space setting [see [1]]. The equilibrium condition is expressed by a random extension of the elastic Wardrop principle. Its characterization with a stochastic quasi-variational inequality is proved. Under suitable assumptions, the existence of a random equilibrium distribution is established. Furthermore, a numerical scheme to compute the random elastic traffic equilibrium distribution is presented. At last, a numerical example is provided. 

References
[1] A. Barbagallo, S. Guarino Lo Bianco, A random elastic traffic equilibrium problem via stochastic quasi-variational inequalities, Commun. Nonlinear Sci. Numer. Simulat. 131 [2024] 107798. 
",A random elastic traffic equilibrium problem - stochastic quasi-variational approach,[35269],262,"[84, 143, 136]",853,Recent advances on Variational Inequalities and Equilibrium Problems I,51,13,43,Variational Inequalities and Equilibrium Problems - From Theoretical Advances to Real World Applications,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,99 [building - 306],"['Optimization Modeling', 'Transportation', 'Stochastic Optimization']",WB-43
"We consider a reverse supply chain for waste from electrical and electronic equipment [WEEE]. In Europe, the manufacturers of EEE products are in charge of collecting WEEE and feeding it back into the production cycle. Manufacturers generally appoint collection companies to assume these duties. According to German legal regulations, the clearinghouse Stiftung Elektroaltgeräteregister EAR continuously allocates returned WEEE to the manufacturers based on their registered sales tonnage. In this talk, we focus on long-term contracting between a collector and a manufacturer. We propose a capacity reservation contract, which protects the collector's investment in capacity extension from the risk incurred by uncertain future order volumes. The contract parameters of the collector, who is in the position of a Stackelberg leader, include the capacity allocated to the manufacturer, the option price, the exercise price upon activation of the option, and the per-unit fee for extra short-term orders placed within the agreed capacity. The manufacturer optimizes the number of options bought subject to an upper bound on the value at risk and accepts the proposal if it provides the reservation utility from competing offers. We show that the contract is guaranteed to coordinate the dyadic supply chain under plausible assumptions. Numerical examples illustrate that the optimal exercise price can be negative, which due to EAR's role as an intermediary, does not impair incentive compatibility.",A capacity reservation contract for WEEE reverse supply chains,"[76842, 930]",918,"[138, 50]",856,Managing product returns,18,3,23,"Circular Economy, Remanufacturing and Recycling ","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,82 [building - 116],"['Supply Chain Management', 'Game Theory']",MB-23
"In this work, we consider a hierarchical facility framework consisting in three levels. The first-level is composed by a set of facilities which manufacture certain products, the second-level facilities act as warehouses, and finally,  the clients demanding some of these products compose the third-level. The products must be supplied to the clients by using the second-level facilities. It should be noted that, in order to satisfy the customers' demand, there must be a double coverage - the client must be covered by a second-level facility, and this, in turn, by a first-level facility. This kind of problem can be identified in several applications as healthcare systems, telecommunication networks or e-commerce.
The aim of the proposed model, denoted as SL-MCFLP, is to determine the location and product configuration of second-level facilities in such a way that the overall client satisfaction with respect their coverage is maximized. We propose a mixed integer linear programming formulation, and we introduce several valid inequalities to improve its performance time. In some cases, the number of valid inequalities is exponential and, consequently, separation algorithms are developed. In addition, several variants of a matheuristic approach are introduced.",Multi-product maximal covering second-level facility location problem,"[66315, 66322, 22045]",765,"[64, 109, 14]",858,Covering Location Problems,29,7,61,Locational Analysis,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S10 [building - 101],"['Location', 'Programming, Integer', 'Combinatorial Optimization']",TA-61
In this work we present a complete hybrid classical-quantum algorithm involving a quantum sampler based on neutral-atom platforms. This approach is inspired by classical column-generation frameworks developed in the field of operations research and shows how quantum procedures can assist classical solvers in addressing hard combinatorial problems. We benchmark our method on the minimum vertex coloring problem and show that the proposed hybrid quantum-classical column-generation algorithm can yield good solutions in relatively few iterations. We compare our results with state-of-the-art classical and quantum approaches.,Quantum pricing-based column-generation framework for hard combinatorial problems,[76189],374,"[5, 76, 114]",859,Hybrid Classical-Quantum Algorithms,83,2,42,Quantum Computing Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,98 [building - 306],"['Algorithms', 'Modeling Systems and Languages', 'Programming, Quadratic']",MA-42
"This study focuses on enhancing blister packaging operations at a Turkish pharmaceutical company to improve efficiency, reduce costs, ensure regulatory compliance, and contribute to reducing carbon emissions. Blister design significantly influences decisions regarding production, logistics, and investments. While existing research typically addresses challenges after selecting packaging materials and designs, we propose a novel mixed-integer mathematical model to address packaging design, machinery investment, and production planning simultaneously. However, solving this complex problem is computationally challenging. Therefore, we introduce a solution method using a constructive approach and Adaptive Large Neighborhood Search [ALNS]. Computational experiments on realistic datasets demonstrate that our proposed method effectively optimizes blister packaging and production planning for various product quantities and time periods. Additionally, our work supports the industry's sustainability goals by promoting eco-friendly packaging practices.",Optimization of Blister Package Design in Pharmaceutical Industry,"[57252, 15639]",836,"[111, 74, 69]",861,Lot-sizing with industrial applications II,32,5,49,"Lot Sizing, Lot Scheduling and Production Planning","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,M1 [building - 101],"['Programming, Mixed-Integer', 'Metaheuristics', 'Manufacturing']",MD-49
"In the 174th European Study Group with Industry, a footwear company put forward a challenge that consisted of optimizing the utilization of one of its shoe injection machines. This machine, capable of high cadence, has the versatility to produce various types of shoes in different colors and sizes. In order to increase its utilization, the company wanted to reduce setup times associated with mold and color changes, as it had a significant impact. To this end, it was to be determined the production sequence that would lead to the highest throughput in the given planning horizon.

To tackle this problem, new constructive and improvement methods were developed and incorporated into a metaheuristic, specifically the Greedy Randomized Adaptive Search Procedure [GRASP]. The GRASP was tested using both an academic and a real-world instance. For the academic instance, results were compared with an exact approach, while for the real-world instance a comparison was made with the actual planning of the company. This presentation will discuss the obtained results and will address some of the managerial insights gained from this study.
",Optimizing Shoe Injection Machine Utilization - A Heuristic Approach,"[9554, 24649, 43794, 76796, 76793]",805,"[151, 74, 129]",863,Lot-sizing with industrial applications I,32,4,49,"Lot Sizing, Lot Scheduling and Production Planning","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,M1 [building - 101],"['Practice of OR', 'Metaheuristics', 'Scheduling']",MC-49
"The task of state estimation in distribution systems faces a major challenge due to the integration of different measurements with multiple reporting rates and asynchronization. As a result, distribution power systems are essentially unobservable in real time, indicating the existence of multiple states that result in identical values for the available measurements. Certain existing approaches utilize historical data to infer the relationship between real-time available measurements and the state. Other learning-based methods aim to estimate the measurements acquired with a delay, generating pseudo-measurements. Our paper presents a methodology that utilizes the outcome of an unobservable state estimator to exploit information on the joint probability distribution between real-time available measurements and delayed ones. Through numerical simulations conducted on a realistic electricity network with asynchronized measurements, the proposed procedure showcases superior performance compared to existing state forecasting approaches and those relying on inferred pseudo-measurements.",Learning-based State Estimation in Distribution Systems with Asynchronized Measurements,"[76745, 18518, 39196, 62032]",147,"[66, 93, 47]",866,Learning-assisted Optimization in Energy Problems,23,2,19,OR in Energy,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,44 [building - 116],"['Machine Learning', 'OR in Energy', 'Forecasting']",MA-19
"Post unloading from vessels, import containers are temporarily stacked at seaport yards before being transported to inland customers. A significant challenge faced at container ports is the optimal storage of import containers, including proximity to out-terminals and minimizing reshuffling needs. However, uncertainties surrounding collection times and transportation mode pose challenges for port operators. Nevertheless, port operators possess knowledge of each import container’s contents upon unloading from vessels.  An interesting research question arises - can cargo content information be leveraged to enhance container storage management? Addressing this question entails categorising import containers into manageable classes based on cargo contents. This study tackles the challenge of categorising a high variety of cargo contents, often exceeding 100K unique items, using the Standard International Trade Classification as labels. Our novel unsupervised text classification approach employs pretrained Glove Word Embeddings and Cosine Similarity for label assignment. This integration offers an efficient approach to generate training datasets in data-scarce scenarios. Additionally, we address data imbalance through three strategie - upsampling/downsampling, class weights, and weighted loss function. Leveraging the Transformers based neural-network models on enriched and balanced datasets, we achieve promising results in accurately classifying container content. ",Classifying import containers based on cargo contents - an unsupervised text classification,"[1292, 35540]",310,"[66, 143, 7]",867,Industrial Optimization,14,2,03,Data Science Meets Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,1005 [building - 202],"['Machine Learning', 'Transportation', 'Analytics and Data Science']",MA-03
"The energy transition has implied changes in the behavioural dynamics of the electricity market participants, both from the energy supply and demand perspective. In this regard, it is important to analyse the possible market effects of these changes in the medium and long term. Thus, simulation is presented as a relevant tool for analysis as it allows us to explore the effect of pattern changes based on possible future scenarios. This paper uses the system dynamics simulation approach to evaluate the effect of changes in demand-side consumption dynamics in developing economies by assessing the load curve. The results show that some demand-side management alternatives have greater impacts than others on the load curve behaviours and that the combination of different participation alternatives could have challenging contributions to the technical performance of the intra-day market.",Analysing load curve variations in the energy transition – a simulation approach,"[74568, 36756, 50348]",844,"[36, 140, 131]",868,OR in Electricity Markets,23,3,19,OR in Energy,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,44 [building - 116],"['Electricity Markets', 'System Dynamics and Theory', 'Simulation']",MB-19
"We present an exact solution algorithm for two-stage stochastic programs under decision-dependent uncertainty. In such problems, first-stage decisions determine the probability distribution of second-stage uncertain parameters. Particularly, we focus on a broad class of problems where the number of potential probability distributions is finite but exponentially large. 
The proposed method extends the well-known L-Shaped method and is applicable also to two-stage stochastic program with integer variables at both stages. We show that the new version converges finitely. 
In addition, we present results from a computational study based on facility location problems under endogenous uncertainty. The results provide promising evidence of efficiency and scalability. 
",Solution of Two-Stage Stochastic Programs under Decision-Dependent Uncertainty,"[45480, 45712]",272,"[117, 5, 11]",870,Stochastic Optimization with Decision-Dependent Uncertainty,49,9,35,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,44 [building - 303A],"['Programming, Stochastic', 'Algorithms', 'Branch and Cut']",TC-35
"Maritime ports are strategic nodes in the global transport chain and play an important role in the efficiency of cargo transport as transfer nodes. One of the most important planning decisions at the seaside interface is the berth allocation problem [BAP], which consists of determining the berths' assignment to the incoming vessels in a port terminal such that spatial and temporal constraints are respected while minimizing vessel handling times. In this paper, we address the berth allocation problem considering the operations of a multipurpose terminal in Chile. Several terminals operate in the port and there exists a regulation that determines priorities for the incoming vessels and preemption of vessels is allowed. The port has recently suffered from weather surges or swells which resulted in port closures that affect the operations and seaside plans, in addition to more typical disruptions such as delays in the arrival times of vessels and black sailings. We propose a rolling horizon methodology and an extension to the generalized set partitioning-based formulation proposed by Christensen and Holst [2008]. We generated some instances to test the proposed approach based on historical data provided by the port and derive some recommendations.
References
Christensen, C. G., & Holst, C. T. [2008]. Berth allocation in container terminals. Master's thesis, Technical University of Denmark.
",On the berth allocation problem under disruptive events for a multipurpose terminal,"[50208, 71311, 36160]",670,"[70, 151, 143]",873,Seaside Planning II,52,5,62,OR in Port Operations,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S12 [building - 101],"['Maritime applications', 'Practice of OR', 'Transportation']",MD-62
"We study stochastic queueing-based optimization models to design networks in which the objective is to minimize the response time. The networks are modelled as collections of interdependent queueing systems in which the service times and the arrival of requests are random variables whose distribution parameters are determined endogenously. The optimization models take the form of nonconvex MINLP problems with fractional, exponential, and polynomial terms for which we propose a convex integer reformulation framework [MILP or convex MINLP].",Stochastic Queuing-Based Optimization Problems for Network Design,[74595],272,"[117, 113, 121]",876,Stochastic Optimization with Decision-Dependent Uncertainty,49,9,35,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,44 [building - 303A],"['Programming, Stochastic', 'Programming, Nonlinear', 'Queuing Systems']",TC-35
"To implement public policies to reduce continuous emissions of green house gases [GEE] and other air pollutants are paramount. One of the main sources of GEE emissions are the use of fossil fuel. In this way, research related to emissions of pollutants from vehicles is paramount. Thus, seeking public policies encouraging the use and the development of more sustainable are essential to preserve populations’ health. The World Health Organization estimates that 6.5 million premature deaths are related to air pollution. To better understand the health risks caused by emissions from mobile sources, is important to select the most important input variables. Therefore, this research aims to analyze and select the input variables that most affect the air pollution health risks. To do so, we applied three Artificial Neural Networks [Multilayer Perceptron, Extreme Learning Machines, and Echo State Neural Networks] to estimate the impacts of air pollution on outcomes for respiratory diseases [hospital admissions and mortality] using road vehicles fleet, distributed and sold fuels amount, and vehicle average mileage. The results showed that the best performance was achieved considering all the input variables. The ELM reached the best overall performance for hospital admissions, and ESN for mortality, both using deseasonalization methods.",Forecasting mobile sources risks using artificial neural networks,"[76803, 27997, 76801, 76859, 76860, 76800]",520,"[8, 47, 66]",878,"Advancements of OR-analytics in statistics, machine learning and data science 10",16,14,28,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,065 [building - 208],"['Artificial Intelligence', 'Forecasting', 'Machine Learning']",WC-28
"Qualifications for several world championships in sports are organised such that distinct sets of teams play in their own tournament for a predetermined number of slots. This paper provides a reasonable approach to allocate the slots based on historical matches between these sets of teams. We focus on the FIFA World Cup due to the existence of an official rating system and its recent expansion to 48 teams. Our proposal extends the methodology of the FIFA World Ranking to compare the strengths of five confederations. Various allocations are presented depending on the length of the sample, the set of teams considered, as well as the frequency of rating updates. The results show that more European and South American teams should play in the FIFA World Cup. The ranking of continents by the number of deserved slots is different from the ranking implied by FIFA policy. We recommend allocating at least some slots transparently, based on historical performances, similar to the access list of the UEFA Champions League.",The allocation of FIFA World Cup slots based on the ranking of confederations,[76862],948,"[99, 151]",879,Fairness in sports,37,12,16,OR in Sports,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,19 [building - 116],"['OR in Sports', 'Practice of OR']",WA-16
"Evolutionary algorithms have emerged as potent solutions for addressing multi-objective optimization challenges across various practical scenarios. This study introduces a multi-objective dairy feed resource allocation optimization model and conducts a performance analysis of four methods to approximate the Pareto front; two genetic algorithms [GA - NSGA-II, SPEA-2] and two differential evolution algorithms [GDE-3, a variant of Pareto-based DE].
Experiments utilizing real-world data were performed to evaluate the algorithms based on criteria such as execution times, Pareto front solutions, and performance indicator values, at the same time looking for the best parametric combinations for each algorithm.
The results showed significant differences among the algorithms, both in their proficiency to approximate the Pareto front and in their execution times. SPEA-2 demonstrated superior results in terms of convergence, diversity, and cardinality, but suffered prolonged execution times. Future research could address this issue, combining SPEA-2 with other algorithms for efficient Pareto front approximations.
Regarding the parametric combinations, significant differences were observed. For both GA, the importance of using a low value in the mutation probability is highlighted. For the GDE-3, a low value in the population size [N] and a high value in the crossover probability [CR] stand out. Finally, for the Pareto-based DE, higher values for N and CR gave better results.",Comparative Study of Genetic and Differential Evolution Algorithms for Pareto Front Approximation in Dairy Feed Optimization,"[56555, 57886, 57890, 2542]",591,"[89, 14, 5]",880,OR in Livestock farming,20,7,12,OR in Agriculture and Forestry ,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,13 [building - 116],"['OR in Agriculture', 'Combinatorial Optimization', 'Algorithms']",TA-12
"Achieving sustainable development is one of the biggest challenges today. It is also one of the reasons why the green economy has come to the fore in recent years. Therefore, nowadays, it is attractive to formulate new optimization goals based on established economic and ecological priorities, which are incorporated into economic and ecological models, as green initiatives provide an impetus to move away from the traditional understanding of economic processes. This implies the need to modify optimization processes to link economic and ecological approaches to evaluate the economic and ecological efficiency of the decision-making process. The first works since 1960 can be registered in problematics of environmental control via mathematical programming models. For example, individual companies can use management decision-making methods to address previously mentioned issues and formulate ecological-economic structural models, reverse logistics models, more ecologically friendly distribution models, air and water pollution models, eco-efficiency evaluation models [emission regulation], and resource allocation models. This work was supported by the Grant Agency of Slovak Republic – VEGA grant no. 1/0120/23 Environmental models as a tool for ecological and economic decisions making.",Environmental Models as a Tool for Ecological and Economic Decisions-Making,"[76792, 71953, 23450]",708,"[139, 25, 33]",881,"Sustainability and Equity in Ecosystems, Ecology and Food",80,14,53,Sustainable and Resilient Systems,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,8007 [building - 202],"['Sustainable Development', 'Decision Analysis', 'Economic Modeling']",WC-53
"Data has emerged as one of humanity's most critical resources. Specifically, the endeavor to forecast future events using data has garnered widespread attention. However, heightened volatility, infrequent occurrences, and rare events hinder data predictability, consequently elevating risk levels. Consequently, the inability to accurately predict future events exacerbates uncertainty and variability within a given scenario, signaling a subsequent rise in risk. In this paper, we examine data predictability by introducing a novel metric based on entropy and the wavelet transform. Notably, we demonstrate that data exhibit less predictability than anticipated due to the aforementioned fluctuations and low-frequency events. Moreover, we employ our methodology on real-world data, particularly focusing on commodity time series. Consequently, with this new metric, we ascertain a notable degree of unpredictability in the price time series under scrutiny, attributable to heightened volatility and the impact of low-frequency events.",Wavelet-Entropy Risk-Predictability Measure for financial time series,"[73004, 63970]",441,"[47, 126, 45]",883,Models for Financial Data and Risk Management,4,7,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S14 [building - 101],"['Forecasting', 'Risk Analysis and Management', 'Financial Modelling']",TA-63
"European societies are currently in a process of population ageing. Although this is the general trend, it would be desirable to know whether this process is being produced with the same characteristics and intensity in all European countries. In this work information from the last three waves of SHARE macro-survey is used for four countries [Germany, Poland, Denmark and Spain] as the basis for a longitudinal well-being and dependency indicator with the aim of studying whether the characteristics of ageing are similar in these regions. First, long-term population distributions are obtained according to the value of the aforementioned indicator. Next, classical and fuzzy Markov chains are used to obtain steady-state distributions regarding age group, gender, country and wave. Finally, a proper metric for probability distributions is used to cluster these profiles in several groups. Results lead us to conclude that the ageing process is not homogeneous among the studied populations.",Well-being horizons for silver and golden ages - An application of traditional and fuzzy Markov chains,"[76864, 76865, 76867, 76866]",189,"[49, 56]",884,Insurance Risk Management,4,3,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S14 [building - 101],"['Fuzzy Sets and Systems', 'Health Care']",MB-63
"We consider assortment optimization in retail and online marketplaces. Our research integrates the dynamics of product returns into the process of assortment planning, proposing a data-driven approach that refines existing optimization practices, aiming to improved revenue management and reduced waste.",Data-Driven Dynamic Assortment Optimization with Product Returns,[76868],481,"[5, 0]",885,Assortment Management,30,9,50,Retail Operations,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,M2 [building - 101],['Algorithms'],TC-50
"The Integrated Timetabling and Vehicle Scheduling [TTVS] problem has extensive applications in all sorts of transit networks. Recently, the emerging modular autonomous vehicles composed of modular units have made it possible to dynamically adjust on board capacity to further match space-time imbalanced passenger flows. In this paper, we introduce an integrated framework for the TTVS problem for a fixed line dynamically capacitated modularized bus network, taking the time-varying and uncertain passenger demand patterns into account. The modularized bus network comprises units that can be [de]coupled and rerouted to other lines through the network at different times and locations to respond to time-varying demand. We formulate a stochastic programming model to jointly determine the optimal robust timetable, dynamic formations of vehicles, and cross-line circulations of these units, aiming to minimize the weighted sum of operational and passengers' costs. To obtain high-quality solutions of realistic instances, we propose a tailored integer L-shaped method coupled with valid inequalities to solve the stochastic mixed-integer programming model dynamically through a rolling horizon approach. An extensive computational study based on the real-world operational data of the Beijing bus network shows the effectiveness of the proposed approaches. ","Integrated timetabling, vehicle scheduling, and dynamic capacity allocation under demand uncertainty","[76692, 36405]",574,"[142, 145, 136]",886,Network Design and Line Planning for Public Transportation 2,85,12,54,Public Transport Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S01 [building - 101],"['Timetabling', 'Vehicle Routing', 'Stochastic Optimization']",WA-54
"Water pollutants can be classified into three categories, each of which includes several classifications of substances. In this paper, we present a methodology based on bankruptcy models to determine the emission limits of polluting substances belonging to more than one category. We model the problem as a multi-issue allocation problem with crossed claims and introduce the constrained proportional awards rule to obtain the emission limits. This rule is based on the concept of proportionality and extends the proportional rule for bankruptcy problems. We also provide an axiomatic characterization of this rule. Moreover, this allocation rule is illustrated by means of a numerical example based on real-world data. Finally, managerial and policy implications of this approach for water pollution control are given.",Water quality management based on proportionality in multi-issue problems with crossed claims,"[71096, 11749, 1313]",384,"[50, 147]",887,"Game Theory, Solutions and Structures II",88,3,36,"Game Theory, Solutions and Structures","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,32 [building - 306],"['Game Theory', 'Water Management']",MB-36
"The intelligent upgrading of metropolitan rail transit systems has made it feasible to implement delicate demand management policies that synergize multiple strategies in practical operations. The interdependence of supply and demand motivates the following question - How to scientifically and rationally develop demand-side management policies and supply-side capacity allocations to enhance the whole urban rail transit ecosystem? To answer this question, we propose a mathematical and computational framework that optimizes train timetables, passenger flow control, and trip-shifting strategies, incorporating emerging trip reservations and service fairness. The problem is formulated as a nonlinear programming model and solved using a Benders decomposition-based solution algorithm within the branch-and-cut method. To further improve the computational efficiency, a novel decomposition method, and valid inequalities are developed, which decomposes the model into a train timetabling problem combined with partial passenger assignment and a passenger flow control problem. The computational results of real-world case studies based on the Beijing metro show that our solution method outperforms commercial solvers in terms of computational efficiency. Our proposed integrated optimization method consistently leads to a reduction of 16.3% in the system-wide waiting time of passengers while simultaneously reducing the number of operated trains. ","An exact method for integrated booking, directing and timetabling in urban rail transit systems","[76872, 63305]",818,"[142, 122, 119]",888,Timetabling 2,85,12,51,Public Transport Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,M5 [building - 101],"['Timetabling', 'Railway Applications', 'Public Local Transportation Systems']",WA-51
"In appointment scheduling, it is a common practice to reserve a number of slots for [semi-]urgent demand arrivals, that require service quickly. The other slots are then given to clients that request an appointment upfront. To determine the number of reserved slots, the [semi-]urgent demand arrivals are often modelled as a distribution with static or seasonal distribution parameters. However, in many appointment scheduling processes, more information becomes available about the urgent demand arrivals over time. An example of these processes is a radiology department, where the number of patients present in the hospital could forecast the required number of emergency scans.
In this study, we propose near-optimal scheduling policies that reserve slots for [semi-]urgent clients, using updated information on the arrival distribution of [semi-]urgent clients in the near future. We formulate the sequential decision making problem as a Markov decision process. We test this model on a Dutch real-life case study in the neurology department of Isala Clinics, Zwolle. This neurology department implemented a brain rehabilitation program in combination with an e-coach for stroke patients, where we can use the number of active patients in monitoring to forecast the number of semi-urgent requests for outpatient appointments. We discuss first results on this practical case study and theoretical instances, and present managerial implications of our near-optimal policies.",Optimizing dynamic reserved resource capacity in appointment scheduling with elective and semi-urgent patients,"[74869, 76879, 76878, 76877, 63476]",973,"[129, 56, 136]",894,Capacity and treatment planning in healthcare,3,14,10,OR in Health Services [ORAHS],"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,11 [building - 116],"['Scheduling', 'Health Care', 'Stochastic Optimization']",WC-10
"Mixed-integer semidefinite programming can be viewed as a generalization of mixed-integer programming where the vector of variables is replaced by mixed-integer positive semidefinite matrix variables. The combination of positive semidefiniteness and integrality allows to formulate various nonlinear optimization problems as  mixed-integer semidefinite programs [MISDPs].
In this talk we show that MISDPs induce bounds based on Lagrangian duality theory. By introducing MISDP-based projected bundle algorithm, we show that the resulting Lagrangian dual bounds are stronger than the standard SDP bounds for various optimization problems.",Lagrangian duality for Mixed-Integer Semidefinite Programming,"[71382, 76886]",393,"[21, 72, 14]",896,Convex and conic optimization,68,13,38,"Conic Optimization - Theory, Algorithms, and Applications","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,34 [building - 306],"['Convex Optimization', 'Mathematical Programming', 'Combinatorial Optimization']",WB-38
"Car sharing could support the transition toward net zero by reducing private car usage. Its success depends on its financial sustainability to service providers and attractiveness to end users. Dynamic pricing could incentivize users, balance supply and demand, and improve the cost-effectiveness and attractiveness of car sharing. 
We describe a fast method for optimizing the hourly rental price charged to a car sharing customer where the price may depend on the number of cars already on hire. The usage of the fleet can be described by a continuous time Markov chain model, which can be reduced to a multi-server queueing model under relatively unrestrictive assumptions. The analytical tractability of the queueing model enables fast optimization to maximize expected hourly revenue for either a single fare system or a system in which the fare depends on the number of cars on hire, while accounting for stochasticity in customer arrival times and the durations of hire. This allows for the development of dynamic pricing strategies for car sharing and supports answering more strategic questions such as the optimal fleet size.
We present the optimal prices for a given customer population and arrival rate and show how the expected revenue and car availability depend on the arrival rate into the system, the willingness-to-pay distribution, and the size of the customer population. We present the results of experiments showing the optimal fleet size for a given customer population.
",Price optimization for car sharing,"[51505, 10128, 35712]",108,"[124, 84, 143]",900,Revenue Management in Sharing/Platform Economy,11,3,59,Pricing and Revenue Management,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S08 [building - 101],"['Revenue Management and Pricing', 'Optimization Modeling', 'Transportation']",MB-59
"Finite-difference methods are a class of algorithms designed to solve black-box optimization problems by approximating a gradient of the target function on a set of directions. In black-box optimization, the non-smooth setting is particularly relevant since, in practice, differentiability and smoothness assumptions cannot be verified. To cope with nonsmoothness, several authors use a smooth approximation of the target function and show that finite difference methods approximate its gradient. Recently, it has been proved that imposing a structure in the directions allows improving performance. However, only the smooth setting was considered. To close this gap, we introduce and analyze O-ZD, the first structured finite-difference algorithm for non-smooth black-box optimization. Our method exploits a smooth approximation of the target function and we prove that it approximates its gradient on a subset of random orthogonal directions. We analyze the convergence of O-ZD under different assumptions. For non-smooth convex functions, we obtain the optimal complexity. In the non-smooth non-convex setting, we characterize the number of iterations needed to bound the expected norm of the smoothed gradient. For smooth functions, our analysis recovers existing results for structured zeroth-order methods for the convex case and extends them to the non-convex setting. We conclude with numerical simulations, observing that our algorithm has very good practical performance.",An Optimal Structured Zeroth-order Algorithm for Non-smooth Optimization,[76142],337,"[19, 5, 66]",901,Algorithms for machine learning and inverse problems - zeroth-order optimisation,84,9,32,Advances in large scale nonlinear optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,41 [building - 303A],"['Continuous Optimization', 'Algorithms', 'Machine Learning']",TC-32
"Demand Responsive Transport involves the provision of door-to-door services to customers which allows them to choose their pickup or drop-off times and locations, while sharing the vehicle with other riders travelling along the same route. This is similar to dial-a-ride services, however the aim of this work is to extend this service to the entire population rather than limiting it to customers with mobility issues. To operate such services, the service provider needs to check the feasibility of accepting or rejecting a customer’s request and also estimate the cost of offering the journey in real time. 

As such, we propose an insertion heuristic which uses iterative search methods and an exact method that uses constraint programming to instantly determine the feasibility of inserting the customer request at a given price whilst anticipating future demand. This will subsequently be combined with a background optimisation algorithm using matheuristics to enhance the effectiveness and efficiency of the proposed routes and schedules across the fleet. We would also discuss how this work fits into a larger project ADROIT, which involves the development of analytical methods for pricing and operating demand responsive transport using Leeds as a case study.

",A fast insertion heuristic for demand responsive transport,"[56643, 6291, 51505, 10128]",376,"[145, 129, 84]",902,Demand-responsive public transport 1,85,13,54,Public Transport Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S01 [building - 101],"['Vehicle Routing', 'Scheduling', 'Optimization Modeling']",WB-54
"Although traditional public transportation, known as a fixed-route transit [FRT] system, is a cost-efficient transit mode in areas with high demand, it is often perceived as inconvenient due to the lack of flexibility. On the other hand, demand-responsive transit [DRT] systems, known as a flex-route transit system [e.g. dial-a-ride services], have a high per-capita operating cost due to their personalized nature. To combine the flexibility of DRT with the cost-efficiency of FRT, the development of a hybrid transit system could be considered a solution. This research focuses on such a hybrid systems that integrate FRT and DRT systems, leveraging the advantages of both. In this research, first, a unifying framework that classifies different models of FRT and DRT integration is presented. Second, a bi-level optimization approach is proposed to model the design of a hybrid transit system in which users may travel through a combination of FRT and DRT. At the upper level, decisions are made on the  FRT lines to be included in the network and their frequencies. For a proposed FRT network structure, the possible sequences of travel modes for all users can be identified. At the lower level, for a subset of requests corresponding to users who cannot complete their trips solely by FRT, the routing and scheduling of DRT vehicles and the fleet size are optimized. Preliminary results of a heuristic solution strategy for this model will be presented.",Designing a hybrid urban mobility system - framework and bi-level model,"[74898, 23971, 76880]",587,"[143, 79, 145]",903,Demand-responsive public transport 2,85,14,54,Public Transport Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S01 [building - 101],"['Transportation', 'Network Design', 'Vehicle Routing']",WC-54
"Disruptions on the railway network lead to reduced availability of the railway infrastructure. In the face of such disruptions, rolling stock dispatchers are tasked with adjusting the rolling stock schedule. In this paper, we develop a rolling stock rescheduling method which ensures feasibility with respect to the availability of the railway infrastructure. In particular, we explore the option of performing shunting movements at stations where shunting is not allowed in current practice, due to the large number of trains that pass through or due to the complexity of the station layout. We introduce an iterative rolling stock rescheduling algorithm which alternates between two mathematical formulations, namely one that creates an interim rolling stock schedule and one that tries to fit the suggested shunting movements between the remaining railway traffic. We test our solution approach with instances that contain complete railway blockages on the Dutch railway network. We allow for shunting at some of the busiest stations in the country and model the infrastructure of these stations to evaluate the feasibility of the suggested shunting movements. Our algorithm succeeds in adjusting the rolling stock schedule within running times of around a few minutes. We successfully identify feasible shunting movements and therefore improve upon the rolling stock schedule that would otherwise be obtained if performing shunting movements at the considered stations is prohibited.",An Iterative Framework for Rolling Stock Rescheduling with Railway Infrastructure Availability Constraints,"[74851, 22950, 5932]",179,"[143, 129, 72]",904,Disruption management in passenger railways,85,2,54,Public Transport Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S01 [building - 101],"['Transportation', 'Scheduling', 'Mathematical Programming']",MA-54
"We move towards an era of smart cities and factories, where autonomous vehicles will provide on-demand transportation while making our streets safer, and mobile robots will automate processes in coexistence with humans. These applications require novel methods for real-time large-scale routing of thousands of vehicles and multi-objective task assignment. In this talk and building upon our seminal work on on-demand high-capacity ridesharing via dynamic trip-vehicle assignment, I will give an overview of our recent work in this field. I will first analyze some of the sources of uncertainty present in ride pooling systems, followed by a discussion of
methods for predictive routing that utilize a model of future demand, how these systems could be combined with public transit, and how an equilibrium can be achieved. Secondly, I will discuss an extension of these methods for flash deliveries and a multi-objective optimization that enables us to design the required vehicle fleet accounting for the trade-off between quality of service and operation cost. Finally, and looking into a multi-robot task pickup and delivery problem, I will discuss how we can compute statistically distinct plans for multi-objective task assignment, which could later be employed by a user to select the desired operational point of the system.","Dynamic vehicle routing and multi-objective task assignment for ride-pooling, last-mile logistics, and robotics",[69509],35,"[145, 77]",906,Javier Alonso-Mora,62,13,01,Keynotes,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,Sportshallen [building - 101],"['Vehicle Routing', 'Multi-Objective Decision Making']",WB-01
"We consider unconstrained smooth optimization problems where the evaluation of both the objective function and its gradient is subject to errors. Particularly, we assume that the function and gradient estimates are random and sufficient accuracy in the estimates can be guaranteed with sufficiently high probability. Following the popular Inexact Restoration framework, we reformulate the original problem as a constrained problem, where the constraint h[y]=0 represents the ideal case in which the function and the gradient are evaluated exactly, being y the noise level and h a non-negative function whose value is related to the accuracy that the estimates can achieve in probability. We show that our problem setting is viable for well-known optimization problems and then design a new trust-region algorithm that employs first-order random models. We analyze the properties of the algorithm and provide the expected number of iterations required to reach an approximate first-order optimality point. We also validate our proposed algorithm on a collection of least-squares problems, showing that it achieves comparable or lower noiseless values on average with respect to a state-of-the-art competitor.",Inexact Restoration trust-region algorithm with random models for unconstrained noisy optimization,"[67214, 21159]",338,"[72, 19, 5]",910,Algorithms for machine learning and inverse problems - optimisation for neural networks,84,10,32,Advances in large scale nonlinear optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,41 [building - 303A],"['Mathematical Programming', 'Continuous Optimization', 'Algorithms']",TD-32
"This paper deals with sustainable production in the mining industry. It aims at planning production activities in Open Pit Mines [OPM] of phosphate while considering the impact of extraction routings [ER], in ensuring the necessary diversity of minerals while preserving rare ones. Planning production activities in phosphate OPM for tactical/operational levels consists of selecting the extraction zones, and blocks, machine allocation, and ER choices. Several researchers have worked on machine allocation and block and zone choices, but less attention was given to the ER impact, especially in ensuring the balance between managing the Supply Chain [SC] efficiently and high-quality phosphate preservation. These ERs consist of extracting non-mineral [waste] layers with mineral ones or independently, controlling minerals' characteristics. We focus on a real case of a SC composed of several OPMs where a unique ER is adopted, making satisfying customer requirements challenging with excessive use of high-quality minerals. For this reason, we first conducted an exploratory study to understand the impact of ERs on OPMs. Second, we developed a MILP model to select the optimal ER to adopt. The model provides the corresponding production program of those mines, defined by the quality, volumes, and lead time of phosphate. The results shed light on the difference between the production programs resulting from the use of each ER and the balance ensured between efficiency and sustainability",Sustainable Planning Optimization in Phosphate Open Pit Mines - Exploring Extraction Routing Impact,"[76882, 76885, 74114]",513,"[84, 139, 97]",913,Raw Material Supply Chains,19,3,24,Sustainable Supply Chains,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,83 [building - 116],"['Optimization Modeling', 'Sustainable Development', 'OR in Mining']",MB-24
"This paper considers an extended setting of a Cournot oligopoly in which firms face a cost when deviating from a fixed quantity that is exogenously given. To analyze these situations, we assume that firms show a coopetitive behavior. The literature usually describes coopetition as the act of cooperation between competing firms by forming a strategic alliance designed to achieve mutual benefits and competitive advantages. Based on this concept, we consider that a firm has a coopetitive behavior when it cares both about its own utility [competition] and also, at least, about the utility of one of the others [cooperation]. Moreover, depending on the attitude that a coopetitive firm exhibits with respect to the utilities of the others, different types of firm can be distinguished. Within this framework, the aim of this paper is to analyze the effects of the degree of coopetition on the quantity offered by the firms at equilibrium. Interestingly, we show that in some situations, the firms with a higher degree of coopetition drive the less coopetitive firms out of the market. Furthermore, we obtain the path of equilibrium quantities for a set of values for the degree of coopetition.",A COOPETITIVE OLIGOPOLY WITH QUANTITY GOALS,"[57814, 57813, 7014, 7011]",620,"[50, 27, 10]",914,"Game Theory, Solutions and Structures V",88,7,36,"Game Theory, Solutions and Structures","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,32 [building - 306],"['Game Theory', 'Decision Theory', 'Behavioural OR']",TA-36
"The Analytic Hierarchy Process [AHP] method, developed by Thomas Saaty, is structured around pairwise comparisons to derive priority weights for decision criteria and alternatives. In this study, a new approach is presented by extending AHP method to be expressed in terms of belief degrees [BD]. In AHP method decision maker [DM] may prefer to use different types of preference formats for each criteria to provide information. They may have hesistancy in expressing their evaluation by a single expression and they can use several linguistic expressions to evaluate alternatives. Therefore, hessitant fuzzy linguistics terms can be used. In this study we develeop a new method to deal with problems where criteria weights are given as fuzzy preference relation and evaluations of DM is given as hesitant fuzzy sets.  To aggregate different preference formats we transformed criteria weights and evaluations of DM to BD without any loss of information. Weighted Cumulative Belief Degree [WCBD] operator is employed for aggregation of alternative evaluations and criteria weights expressed with BD. First, a satisfaction level s_r, q∈0,…,6 is determined for aggregations of evaluations by problem owner. By use of WCBD operator one can integrate two belief structures by considering t satisfaction level of problem owner. The developed method has been applied to various problems and has yielded satisfactory results.",Extending AHP with Belief Degrees for Hesitant Evaluations,"[76884, 1564]",892,"[6, 25, 49]",915,Pairwise comparisons and preference relations 2,44,10,44,Multiple Criteria Decision Analysis,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,20 [building - 324],"['Analytic Hierarchy Process', 'Decision Analysis', 'Fuzzy Sets and Systems']",TD-44
"In the awarding of public contracts, Italian contracting authorities are required not only to evaluate offers but also collective agreements applied to the workers of the participating undertakings. Until the innovations prompted by NGEU, the evaluation of contracts took into consideration the salary only. If it differed significantly from the values reported in tables published by the Ministry of Labour, the prospective contractor was excluded from the tender. A recent reform of the Italian Public Procurement Code introduces an assessment of the overall protection system guaranteed by the specific collective agreement applied to employees engaged in the execution of the contract. The contracting authority must therefore evaluate which of the contracts applied by competitors are equivalent by considering social aspects that are sometimes not directly quantifiable [maternity protection, sick leave, etc.]. We propose the use of AHP to support the decision-making process of contracting authorities in the field of social protection, to increase transparency of the process, to guarantee effective protection of workers, and to reduce litigation risk. AHP is chosen for its ease of use and for being already widely applied by contracting authorities in the selection of the most economically advantageous tender. AHP is tested on a case study in which stakeholders directly taking part in collective bargaining [trade unionists, associations, etc.] are involved in the assessment process.",The comparison of social protection - evaluating collective agreements in public procurement using AHP.,"[76186, 76756, 76146, 76888]",655,"[6, 15, 57]",917,Decision Support in the Public Sector and Policy Making,45,13,45,Decision Support Systems,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,30 [building - 324],"['Analytic Hierarchy Process', 'Complex Societal Problems', 'Human Resources Management']",WB-45
"UEFA country coefficients quantify the performance of UEFA associations’ football clubs in international club competitions. They are based on the results achieved over the previous five years, with points awarded for match results and bonus points allocated for reaching different tournament stages. The points earned by all clubs from each country are divided by the number of participating clubs to calculate the coefficient.
The ranking of UEFA associations by country coefficients is used to determine the number of clubs from the association that can participate in UEFA club coefficients, as well as the qualifying stage where they enter. Furthermore, two UEFA Champions League slots are provided for the two best associations in the previous year from the 2024/25 season.
Therefore, it is crucial to obtain a fair ranking of European football leagues. For this purpose, we propose new methods based on pairwise comparisons that—in contrast to the current UEFA country coefficients—account for the strengths of opponents, and calculate alternative rankings based on matches played in the UEFA Champions League, UEFA Europa League, and UEFA Europa Conference League over the least 10 years. They will be compared to the official UEFA ranking, and the number of upsets [matches where the favourite team loses] will also be determined.
",Alternative UEFA country rankings based on pairwise comparisons,"[76889, 76862]",440,"[99, 151, 6]",919,Ranking in sports,37,8,16,OR in Sports,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,19 [building - 116],"['OR in Sports', 'Practice of OR', 'Analytic Hierarchy Process']",TB-16
"In this work, we consider the ward capacity management problem with readmissions, where the decision-maker optimizes the elective schedule and early discharge policy, so as to minimize bed shortages. Existing research has shown that early discharge can lead to higher rates of readmission, and longer readmission length-of-stay. This sets up the need to balance the temporal trade-off between the immediate capacity freed up by early discharges and increased readmissions down the road. Such re-entry structure creates challenges when modelling via traditional methods. We appeal to the Pipeline Queues [Bandi and Loke 2018] framework, and propose an optimization model where the early discharge policy is expressed as a state-dependent decision rule. The model has a reformulation, which can be solved as a sequence of convex programs with asymptotically linear constraints. In our numerical study, we identify an intermediate region of the probability of readmissions where time-invariant policies can lead to as much as 77% more shortages. Ignoring the effects of early discharge on readmissions can lead to at least 75% and 150% more bed shortages in time-homogeneous and non-time-homogeneous settings respectively, even against un-optimized elective admissions. Using optimal early discharge strategies without jointly optimizing elective admissions will lead to 20% more shortages.",Optimizing Early Discharge - Trade-Offs between Capacity and Readmissions,[62300],610,"[56, 0]",921,Admission and discharge,3,15,10,OR in Health Services [ORAHS],"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,11 [building - 116],['Health Care'],WD-10
"When teaching a course in Inventory Management at the graduate level it is inevitable that the lecturer needs to use simulations for pinpointing some of the more complex matters that, cannot adequately be covered by any analytical formulas. The issue can then be whether the simulation program used in the course, should be a specialized discrete event simulation program like Arena, or it could be done in Excel, or maybe using both tools in the course. When teaching business students who are not that well acquainted with a program like Arena it can be of advantage to use Excel, entirely. However, Excel is at first glance mostly geared for working in discrete time, as it does not have any sort of event calendar file. The presentation will demonstrate how it is possible, by using some simple modeling effort, to conduct inventory analyses in Excel in continuous time.",Development of simulation models in Excel for conducting inventory analyses in continuous time,[1142],790,"[92, 0]",922,OR Education II,48,3,16,OR Education,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,19 [building - 116],['OR in Education'],MB-16
"Ensuring fairness is important when evaluating decision support systems aiming for impartial decision outcomes and trust within those being affected. During the COVID-19 pandemic, numerous nations implemented tiered restrictions as a localized strategy to balance economic activities against restrictions of movement independently in different geographical areas. However, such approaches sparked debate on the fairness of these restriction decisions. We examine fairness concerning the UK government's allocation system for tiered restrictions which can be modelled as an ordinal classification problem. For this, we collected and integrated data from multiple official sources to investigate potential inconsistencies, such as comparing the North and the South of England. We explore if there is inconsistency, by first training an ordinal classification model using only data from one geographical area, then testing the model with data pertaining to another geographical area. This helps us identify and measure classification errors in terms of underestimates and/or overestimates of predicted values compared to the actual values. Such analysis can be useful for exploring ordinal classification problems, both for post-hoc analysis of past decisions, and for providing transparency as part of future decision dissemination. Our approach could be applied to other ordinal classification problems, to explore fairness within other domains.",Investigating fairness in decisions involving ordinal classification - A COVID-19 case study,"[76331, 40226]",538,"[7, 26, 25]",925,"Advancements of OR-analytics in statistics, machine learning and data science 12",16,7,06,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,1013 [building - 202],"['Analytics and Data Science', 'Decision Support Systems', 'Decision Analysis']",TA-06
"In multilabel classification [MLC] problems, the standard confusion matrix is generally replaced by a set of confusion matrices, with one matrix per label. Although ordinal classification problems are slightly different from MLC, it shares the same set of  performance metrics used for MLC. We contend that ordinal classification has nuances which makes it different from MLC. As in binary classification, the notion of false positive and false negative are still valid in MLC problems in a sense that any wrongly predicted label can be termed as false positive [or false negative]. However, in ordinal classification, these notions are replaced by overestimates and underestimates. We propose the use of different mathematical operators to quantify the strength of overestimates and underestimates. The proposed metrics tailored for ordinal classifiers not only introduces a novel framework for evaluation but also signifies a crucial step forward in enhancing the performance evaluation of ordinal classification algorithms. We demonstrate its practical usefulness with the help of a number of case studies.",Refining the performance metrics and related mathematical operators for ordinal classification problems,"[40226, 76331]",517,"[7, 66, 26]",926,"Advancements of OR-analytics in statistics, machine learning and data science 8",16,12,28,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,065 [building - 208],"['Analytics and Data Science', 'Machine Learning', 'Decision Support Systems']",WA-28
"In the dynamic landscape of perishable product management, efficient inventory optimization is crucial for minimizing waste and maximizing profitability. This research focuses on enhancing inventory optimization for perishable products in a 2-stage divergent supply chain involving a distributor and multiple retailers. The study proposes a novel approach that integrates sustainable inventory routing and inventory ordering policies to address challenges in inventory management. The model approach is a two-stage stochastic optimization problem with a multi-period perspective. The framework considers price-dependent demand, perishable product characteristics, back-ordering at the distributor, lost-sales at the retailers, demand uncertainty, age-based markdown pricing, and distributor-led inventory rationing. The research introduces a comprehensive policy and routing framework combining innovative inventory ordering policies and routing strategies. The proposed hybrid policy will be benchmarked against established inventory models, including [s,Q], [s,S], Haijema [s,S,q,Q], and other continuous review policies, all integrated with inventory routing. The new inventory policy can facilitate informed ordering decisions, preventing overstocking and understocking, thereby addressing real-world uncertainties and practical constraints in perishable product supply chains. Additionally, a heuristic method can be developed to provide efficient solutions within a limited timeframe.",Sustainable Routing and Perishable Inventory Optimization under Demand Uncertainty for a 2- stage Divergent Supply Chain - A MILP-Based Hybrid Modelling Approach,"[76895, 76896]",488,"[111, 61, 65]",928,Sustainability in Vehicle Routing,19,7,24,Sustainable Supply Chains,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,83 [building - 116],"['Programming, Mixed-Integer', 'Inventory', 'Logistics']",TA-24
"It is usually beneficial for airlines that cabin crews travel in teams for entire pairings [sequence of flights, connections, and rest, forming one or several days of work]. In addition to improving morale and team efficiency, travelling in teams increases the robustness of the schedule. However, different flights may have different crew requirements, so it is not always possible to have a whole team work on two consecutive flights. In those cases, it may be beneficial to use a core team to cover most of the requirements and use complementary crews to cover the missing assignments. Finding the optimal way to form teams has been identified as an issue but has never been tackled. 


In this talk, we formulate the crew split problem, whose role is to determine the core team composition of each flight as well as the complementary crews that will join it. It is a multi-objective optimization problem that, among others, maximizes the size of each core team, minimizes the estimated connexion times, and minimizes the estimated deadhead costs. In addition, our model allows for downranking, which is the possibility of a higher-ranked crew to work a lower-ranked position. We show the benefits of using the crew split problem on a large real-world instance.

",The crew split problem - team creation for the cabin crew pairing problem with several qualification classes and varying requirements.,"[51646, 76898, 10966]",395,"[14, 13, 4]",929,Large Scale Optimization in Air Transportation,64,13,29,Combinatorial Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,157 [building - 208],"['Combinatorial Optimization', 'Column Generation', 'Airline Applications']",WB-29
"Primal-dual algorithms for the resolution of convex-concave saddle point problems usually come with one or several step size parameters. Within the range where convergence is guaranteed, choosing well the step size can make the difference between a slow or a fast algorithm. A usual way to adaptively set step sizes is to ensure that there is a fair balance between primal and dual variable's amount of change.
In this work, we show how to find even better step sizes for the primal-dual hybrid gradient. Getting inspiration from quadratic problems, we base our method on a spectral radius estimation procedure and try to minimize this spectral radius, which is directly related to the rate of convergence. Building on power iterations, we could produce spectral radius estimates that are always smaller than 1 and work also in the case of conjugate principal eigenvalues. 
For strongly convex quadratics, we show that our step size rule yields an algorithm as fast as inertial gradient descent. Moreover, since our spectral radius estimates only rely on residual norms, our method can be readily adapted to more general convex-concave saddle point problems. 
In a second part, we extend these results to a randomized version of PDHG called PURE-CD. We design a statistical test to compare observed convergence rates and decide whether a step size is better than another. 
Numerical experiments on least squares, sparse SVM, TV-L1 denoising and TV-L2 denoising problems support our findings.",Monitoring the Convergence Speed of PDHG to Find Better Primal and Dual Step Sizes,[46559],208,"[21, 81, 63]",930,Algorithms for machine learning and inverse problems - adaptive strategies,84,5,32,Advances in large scale nonlinear optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,41 [building - 303A],"['Convex Optimization', 'Non-smooth Optimization', 'Large Scale Optimization']",MD-32
"Deep reinforcement learning has made incredible strides in solving combinatorial optimization problems [COPs] and nearly outperforms the current state-of-the-art OR heuristics on several problems. However, a key drawback of deep neural network approaches is that they are not interpretable, that is, it is essentially impossible to understand how they actually solve optimization problems. To this end, I introduce a fully interpretable mechanism for generating interpretable models for solving COPs using a decision tree. The method harnesses a pairwise ranking mechanism to construct solutions, thus allowing it to learn to solve various instance sizes with a single model. To train the decision tree, I introduce an end-to-end learning technique to generate trees that are customized to specific datasets and show the effectiveness of this technique experimentally on several COPs.",Learning to solve combinatorial optimization problems with a decision tree,[27003],731,"[66, 143, 8]",931,	[Deep] Reinforcement Learning for Combinatorial Optimization 2,14,5,03,Data Science Meets Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,1005 [building - 202],"['Machine Learning', 'Transportation', 'Artificial Intelligence']",MD-03
"In the immediate advent of zero-emission zones fleet operators are transitioning to electric fleets. To maintain their current operations, a clear understanding of the charging infrastructure needed and its relationship to existing power grid limitations and fleet schedules is needed.
In this context, we optimize the charging infrastructure and the routing and charging schedules for a logistic distribution network in a joint fashion, and validate our results via agent-based simulations.
Specifically, we first cast the joint infrastructure design and operational scheduling problem as a mixed-integer linear program. Second, we devise an agent-based model that serves as a virtual experimental replica to test different design and operation solutions.
Finally, we run experiments where we compare rule-based with optimized design and operation strategies in key operational metrics and power requirements in a case-study of The Netherlands. 
The results indicate that having local vehicle charging rules helps decrease the impact of uncertainties in consumption, thus enhancing logistics reliability. However, central coordination leads to more cost-efficient solutions and less stringent power requirements.",E-Truck Fleet and Infrastructure Co-design via Joint Optimization and Agent-based Simulation,[76623],526,"[143, 79, 93]",935,Methods and models for sustainable transport solutions,6,7,56,Transportation,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S04 [building - 101],"['Transportation', 'Network Design', 'OR in Energy']",TA-56
"This study analyzes the secondary market dynamics of the European Financial Stability Facility [EFSF] and European Stability Mechanism [ESM] bonds  from 2014 to 2020. Findings reveal increased activity of private sector investors, notably fund managers, with significant market risk exposure. The study examines the secondary bond market response to the initial impact of the COVID-19 crisis, observing investor behavior aligning with previous models of capital repatriation and subsequent re-entry. Additionally, it highlights a trend towards heightened activity in long-term market segments, spurred by a low yield environment, leading to the normalization of long maturities as liquid instruments. The paper also delves into the digitalization and automation of fixed income markets through electronic trading platforms, noting a gradual increase in electronic trading adoption, particularly among smaller ticket sizes. Despite this trend, central banks and public entities exhibit lower electronic trading shares, suggesting less pressure or motivation for adoption. Finally, the study explores the transaction network of primary dealers, revealing regional and institutional patterns that support liquidity maintenance strategies, emphasizing the role of local banks in stabilizing turnover volume and enhancing investor diversity across different trading platforms and countries.",Investor activity in EFSF/ESM secondary bond markets,[66948],138,"[44, 53, 7]",936,Market dynamics and implications for portfolio decisions,74,9,57,Modern Decision Making in Finance and Insurance,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S06 [building - 101],"['Finance and Banking', 'Graphs and Networks', 'Analytics and Data Science']",TC-57
"The solution of the cubic equation has a century-long history; however, the usual presentation is geared towards applications in algebra and is somewhat inconvenient to use in optimization where frequently the main interest lies in real roots. In this note, we present the roots of the cubic in a form that makes them convenient to use and we also focus on information on the location of the real roots. Armed with this, we provide several applications in optimization where we compute Fenchel conjugates, proximal mappings, and projections.

This is a joint work with Heinz Bauschke and Shawn Wang.",Real roots of real cubics and optimization,[76901],771,"[21, 114, 19]",938,Variational techniques in conic optimization and mean field games,82,10,42,Variational Analysis and Continuous Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,98 [building - 306],"['Convex Optimization', 'Programming, Quadratic', 'Continuous Optimization']",TD-42
"The environmental hazards of improperly managed waste have gained universal recognition among scholars and stakeholders. These hazards are especially critical in the pharmaceutical sector as leftover medications contain active chemicals that threaten the environment and human health. Nonetheless, implementation of adequate measures to ensure proper collection and treatment of pharmaceutical leftovers remains insufficient, and tons of unwanted medications are discarded in landfills and wastewater annually. This is due to lack of coordination between the parties and poor incentive systems. To address this issue, we study coordination in pharmaceutical reverse supply chains and government incentive strategies. We employ the evolutionary game methodology to evaluate strategic behaviour of pharmacies, a recycler, and the government under four incentive plans. We compare the incentive plans based on return volume, participation rate, cost, and implementation time, to recommend the most effective plan. We conduct a numerical study to gain insights into the performance of the incentive plans in different conditions. The results reveal that a plan that provides proper incentives to pharmacies for targeting both, reward- and awareness-driven customers, coupled with contract-based coordination, outperforms other plans, and does not necessarily require a budget allocation. Our study is based on the UK’s National Health System but it is generalisable to other countries as well.",Circular economy application in pharmaceutical supply chains in the UK - a holistic evolutionary game approach,"[71036, 20832, 71037]",929,"[138, 50, 139]",940,Sustainable supply chains,18,15,24,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,83 [building - 116],"['Supply Chain Management', 'Game Theory', 'Sustainable Development']",WD-24
"Revenue functions are studied under the assumption that the technology is nonconvex. More specifically, we show theoretically that the convex revenue functions are larger or equal to the nonconvex revenue functions. However, with one input and constant returns to scale [CRS] technologies, both these revenue functions coincide. Including more inputs or varying returns to scale assumption [e.g., variable returns to scale, VRS] results in differences between the convex and nonconvex revenue functions. We use USA state-level agricultural data to showcase our theoretical result empirically. We first visualize the revenue functions under constant and variable returns to scale assumptions and demonstrate that the former is identical irrespective of whether technology is convex or nonconvex while the latter revenue functions are different. We further show graphically that the convex or nonconvex revenue functions are different both under CRS and VRS with 4 inputs. Additionally, we use statistical tests [equality of densities and stochastic dominance] to show that the convex or nonconvex revenue functions are statistically different. Finally, we perform scale efficiency analysis to show how using the convexity assumption may result in false policy implications.",Revenue Functions Are Nonconcave in the Inputs When the Technology is Nonconvex - The Unbearable Lightness of Convexification,"[49004, 62536, 61472]",946,"[24, 33]",941,DEA methodological developments II,89,15,48,Data Envelopment Analysis and its Application,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,60 [building - 324],"['Data Envelopment Analysis', 'Economic Modeling']",WD-48
"Perishable products are characterized by a predetermined expiration date, beyond which their economic value declines, rendering them unusable. This paper identifies two categories of perishable products - those consumable before expiration but hazardous post-expiry [e.g., medical products], and those with freshness degradation over time, becoming unusable after their shelf life [e.g., agricultural products]. Despite these distinctions, both types may be treated collectively due to their shared limited shelf life, impacting their marketability. The study introduces a model where a retailer manages the demand for a perishable product, considering its freshness level and selling price as multiplicative components. Further, the paper explores how decisions related to quantity optimization and profit maximization influence the order quantity. The model facilitates retailers in determining the optimal inventory level to maximize their profits.",Optimizing Inventory Strategies for Perishable Products - A Comprehensive Model Integrating Freshness Dynamics and Economic Value,"[76177, 70043, 76904, 76700]",910,"[138, 84]",942,Retail Inventory Management III,30,14,61,Retail Operations,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S10 [building - 101],"['Supply Chain Management', 'Optimization Modeling']",WC-61
" The aim of this talk is to introduce Loraine, a new code for the solution of large-and-sparse linear semidefinite programs [SDPs] with low-rank solutions or solutions with few outlying eigenvalues, and/or problems with low-rank data. We propose to use a preconditioned conjugate gradient method within an interior-point SDP algorithm and an efficient preconditioner fully utilizing the low-rank information. The efficiency will be demonstrated by numerical experiments using the truss topology optimization problems, Lasserre relaxations of the MAXCUT problems, and the sensor network localization problems. The code is available in Matlab and Julia, and it can be used not only for low-rank problems but also for any linear SDP.

", Loraine – An Interior-Point Solver for Low-Rank Semidefinite Programming,"[76903, 63249, 8809]",447,"[60, 21]",943,Preconditioning for  Large Scale Nonlinear Optimization,84,5,34,Advances in large scale nonlinear optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,43 [building - 303A],"['Interior Point Methods', 'Convex Optimization']",MD-34
"We collaborate with Glovo, a European technology company in the e-commerce grocery sector, to optimize hourly driver capacity allocations across their stores. We do this by introducing decisions at the strategic, tactical, and operational levels in a multi-tiered framework in which long-term decisions are employed to improve short-term decisions. Our suggested framework demonstrates a considerable improvement over Glovo's current operations when it comes to revenue and delivery costs.","Plan Ahead, Act in Real-Time - Optimization of Driver Capacity Allocation in the Omnichannel Grocery Sector","[49193, 76905, 76906, 76908]",482,"[138, 151, 32]",944,Omni-Channel Retailing ,30,10,50,Retail Operations,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,M2 [building - 101],"['Supply Chain Management', 'Practice of OR', 'E-Commerce']",TD-50
"We develop a stylized theoretical framework for the problem of tracking the population in a service system with noisy input and output observations. The motivation for the project is the problem of tracking the population of passengers in the TSA area at an airport in real time using noisy data from people counters.  Our goal is to devise and analyze policies that use past people counter data to estimate the population in the system over a finite and discrete time horizon. We evaluate the performance of policies in two distinct settings. In the busyness tracking problem, the objective is to track whether the policy correctly detects if the system census is larger or smaller than a threshold. In the population tracking problem, the objective is to minimize the expected magnitude of the estimation error in each period. We show that our problem is more challenging than dynamic learning problems studied in the bandit literature. In the busyness tracking problem, we derive a general lower bound on the cumulative expected loss that grows linearly with the time horizon. In the population tracking problem, we prove another general lower bound on cumulative expected loss that is on the order of the square of the time horizon. Given this complexity, we develop and analyze policies that achieve the best possible performance in terms of the growth rate of cumulative expected loss. Furthermore, we investigate the benefit of conducting periodic inspections of the true census in the system.",Data-Driven Population Tracking in Large Service Systems,[50594],510,"[130, 66, 121]",945,Advances in Stochastic Modelling and Applied Probability II,47,4,40,Advances in Stochastic Modelling and Learning Methods,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,96 [building - 306],"['Service Operations', 'Machine Learning', 'Queuing Systems']",MC-40
"We consider optimization of noisy black-box functions via gradient search, where the gradient is estimated using finite differences. Applying a recently developed convergence rate analysis leads to a finite-time error bound for a class of problems with convex differentiable structures. The results provide insight as to when using randomized gradient approaches such as simultaneous perturbation stochastic approximation might be advantageous, based on problem dimension and noise levels.",Gradient-Based Stochastic Optimization via Finite Differences - When To Randomize?,"[25318, 73699]",301,"[136, 0]",946,Optimization under uncertainty - theory and solution algorithms,49,7,35,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,44 [building - 303A],['Stochastic Optimization'],TA-35
"This paper tackles the integration of production and human resource management planning, which considers the operator as a production unit subject to degradation according to age, in the context of an unreliable production system in a considerable state of deterioration. The deterioration of the production unit process has a twofold effect on its reliability and continuity, and therefore, to moderate the effects of this deterioration, a replacement action can be conducted based on the state of the system. The objective of the study is to determine an appropriate production policy and replacement strategy in order to meet customer demand. A combination of stochastic dynamic programming and numerical methods are used to solve this optimal control problem. In addition, a numerical example is provided to illustrate the usefulness of the proposed approach, as well as to study the interaction between a given production strategy and human resource management. The effect of the new approach based on a logical implementation is presented following a sensitivity analysis of the numerical example. Regarding the results, a comparative study between recent research and the proposed policy is presented. Finally, an implementation chart is drawn up to guide decision-makers in setting production rates and managing human resources to meet customer demand.",Joint production and human replacement optimization policy for a deteriorating manufacturing system,[31956],339,"[105, 20, 140]",948,Optimal control in supply chain management,90,2,33,Optimal Control Theory and Applications,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,42 [building - 303A],"['Production and Inventory Systems', 'Control Theory', 'System Dynamics and Theory']",MA-33
"Given a set of small rectangular items and large non-identical rectangular bins, the variable-sized two-dimensional guillotine bin packing problem searches for a feasible packing of the items into the bins, with the items obtained via guillotine cuts. It minimises the total cost of the used bins. We propose a hybrid column generation based matheuristic that solves the problem approximately. Its reduced master problem, which is solved as an integer program, is the classical Dantzig-Wolfe formulation of the VS2BP. The master problem uses a collection of feasible packings that are constructed heuristically during the search process. The initial set of packings is obtained via a heuristic, which selects and packs bins sequentially. This warm-start heuristic employs a look-ahead mechanism that prohibits the search of infeasible directions and enforces directions of likely advance, halting only after the allocated time expires. After this, the incumbent is enhanced by solving a series of pricing problems for each bin type by finding a feasible packing with the largest reduced cost, providing extra columns for the master problem.  The search ends either when no solution to the pricing problem is found or when time expires. The packing process is done approximately via an algorithm that hybridizes constraint programming with a heuristic search. Extensive computational experiments provide evidence of good performance of the proposed approach, when compared to the state of the art. ",Primal and Dual Bounds to the Variable-Sized Two-Dimensional Guillotine Bin Packing Problem,"[76911, 58363]",89,"[5, 13, 111]",949,Cutting and Packing 1 - 2D rectangular,81,2,07,Cutting and Packing [ESICUP],"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,1019 [building - 202],"['Algorithms', 'Column Generation', 'Programming, Mixed-Integer']",MA-07
"Container Drayage Problem [CDP] refers to the optimization problem of routing and scheduling a set of container trucks around a container terminal. Conventionally, a driver is stuck to one container truck and allowed to perform multiple trips [multi-trip] to the terminal within their working time. The recent development of automation technologies enables semi-autonomous trucks to follow the leading human-driven truck as a platoon on the road; therefore, truck platooning can save human labor and reduce the fuel cost of following trucks through aero- dynamic drag reduction. In this paper, we study a multi-trip container drayage problem with truck platooning [MT-CDP-TP], where multi-trip, truck platooning, and fuel cost reduction are simultaneously considered in a CDP. Despite the operational benefits brought by the MT-CDP- TP, the problem is challenging to solve due to its NP-hardness when formulated as a multi-trip pickup and delivery problem with load-dependent cost. We propose a Branch-and-Price-and-Cut [BPC] algorithm, with a route-based set partitioning model and tight linear relaxations, to yield the exact solutions. Valid inequalities are generated based on a graph structure, where each node represents a feasible route, and each arc stands for the conflict between two routes. Moreover, we design a tailored pulse propagation algorithm with novel pruning procedures based on the dual information from the master problem and valid inequalities to solve the pricing problem eff",An exact algorithm for the multi-trip container drayage problem with truck platooning,[75694],83,"[72, 5, 65]",953,Port-Hinterland Transportation & Corridors,52,4,62,OR in Port Operations,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S12 [building - 101],"['Mathematical Programming', 'Algorithms', 'Logistics']",MC-62
"Hydrogen-based propulsion concepts for aircraft are considered a promising technology towards the decarbonization of aviation. While the development of respective aircraft models is in progress, questions regarding the supply network of green hydrogen are arising. We present a multi-period mixed-integer programming model for the hydrogen supply chain network design problem focusing on the aviation sector. The model minimizes the total network cost by making strategic decisions [e.g. suppliers, locations, capacities, transportation infrastructure] and tactical decisions [e.g. hydrogen flows, storage quantities] at different temporal resolutions. Our model formulation considers the spatially and temporally varying supply and demand of hydrogen, the techno-economic characteristics of hydrogen storage, liquefaction and transportation [e.g., economies of scale], as well as the specific requirements of hydrogen handling [e.g., losses]. Model application is illustrated for German airports with local production and hydrogen import options, considering the projected development of the European Hydrogen Backbone pipeline infrastructure. Optimal network designs and results are presented and analyzed for different hydrogen supply and demand scenarios.",Green Hydrogen Supply Chain Network Design for Aviation - Multi-Period Optimization Model with Seasonal Temporal Resolution,"[73990, 73983]",484,"[79, 138, 100]",955,Sustainable Supply Chain Design,19,2,24,Sustainable Supply Chains,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,83 [building - 116],"['Network Design', 'Supply Chain Management', 'OR in Sustainability']",MA-24
"5th gen district heating systems are bi-directional low temperature district heating and cooling networks with decentral generation units. They are well suited for providing flexibility to the power grid. A case study has been performed to model the flexibility such a system can offer. The energy system is represented as a linear optimization problem, that has been extended to model bi-directional flows within the district heating network in the form of energy balances. For the purpose of modeling flexibility provision in the form of balancing services, a two-stage stochastic optimization approach has been developed. The first stage represents planned operation without offering flexibilities, in the second participation on the control reserve market is modeled. Call probabilities for energy bids are implemented as randomized binary series. This concept is aiming at ensuring that the optimization model cannot apply perfect foresight of called balancing services, but a better representation of actual operation is achieved. Due to uncertainties of price development in balancing markets a sensitivity analysis w.r.t balancing service remuneration is conducted, yielding crucial tipping points at which capacity planning and operation in the considered energy system is altered to reduce system costs through providing flexibilities. Challenges resulting from the simplified representation in a linear optimization model are analyzed and suggestions for solution approaches formulated.",Modelling balancing service provision in 5th gen district heating systems,"[76915, 76916, 76917]",844,"[36, 135, 110]",956,OR in Electricity Markets,23,3,19,OR in Energy,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,44 [building - 116],"['Electricity Markets', 'Stochastic Models', 'Programming, Linear']",MB-19
"We often find ourselves struggling to determine the truth, hesitating due to our ignorance. In such circumstances, the exclusive disjunction [XOR, for short] is commonly used to express our uncertainty, in contexts ranging from daily life, like It was either Adam or John—but I couldn't determine which, to more professional scenarios, such as Global growth is projected to be 3% in 2024, or it may weaken to 2.7% depending on prevailing economic conditions. The primary objective of this paper is to propose a framework to address the uncertainty inherent in such expressions by raising a pivotal inquiry - how to quantify this uncertainty without resorting to oversimplification that leads to the loss of crucial details essential for understanding the problem under consideration? Attempting to address this question leads to the concept of 'xorness' and the 'XOR number.' In particularly, xorness is defined as a state of uncertainty stemming from hesitation, in which multiple objects [such as numbers in our context] are mutually exclusive, indeterminate, and none of them is dominating. This exploration proposes the 'XOR number' as an approach to quantify xorness, recognizing that the multiple options itself represents a choice. This investigation extends to arithmetic operations, metric spaces, and the ordering of XOR numbers. Multiple criteria decision analysis is employed to demonstrate the applicability of the proposed framework.",XOR Numbers - Theory and Application,"[76627, 74165]",135,"[25, 27, 93]",958,Preference Learning 1,44,2,44,Multiple Criteria Decision Analysis,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,20 [building - 324],"['Decision Analysis', 'Decision Theory', 'OR in Energy']",MA-44
"In data-parallel optimization of machine learning models, workers collaborate to improve their estimates of the model - more accurate gradients allow them to use larger learning rates and optimize faster. In the decentralized setting, in which workers communicate over a sparse graph, current theory fails to capture important aspects of real-world behavior. First, the `spectral gap' of the communication graph is not predictive of its empirical performance in [deep] learning. Second, current theory does not explain that collaboration enables larger learning rates than training alone. In fact, it prescribes smaller learning rates, which further decrease as graphs become larger, failing to explain convergence dynamics in infinite graphs. This paper aims to paint an accurate picture of sparsely-connected distributed optimization. We quantify how the graph topology influences convergence in a quadratic toy problem and provide theoretical results for general smooth and [strongly] convex objectives. Our theory matches empirical observations in deep learning, and accurately describes the relative merits of different graph topologies.",Beyond spectral gap - the role of the topology in decentralized learning,[76688],370,"[21, 136, 53]",959,Distributed and Federated Optimization,84,15,32,Advances in large scale nonlinear optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,41 [building - 303A],"['Convex Optimization', 'Stochastic Optimization', 'Graphs and Networks']",WD-32
"Disruptions in the operational flow of rail traffic can lead to conflicts between train movements, such that a scheduled timetable can no longer be realised. This is where dispatching is applied, existing conflicts are resolved and a dispatching timetable is provided. In the process, train paths are varied in their spatio-temporal course. This is called the train dispatching problem [TDP], which consists of selecting conflict-free train paths with minimum delay. Starting from a path-oriented formulation of the TDP, an IP is introduced. The IP selects the best of all potentially possible train paths via binary decision variables, which results in a big number of variables, so column generation is used to solve the problem. Instead of modelling pairwise conflicts, a stronger formulation is achieved by cliques formulated over the complete train path. The resulting pricing problem is a hard problem, because the shadow prices of the conflict cliques must be taken into account. When constructing a new train path, it must be determined whether this train path belongs to a conflict clique or not. This problem is tackled by a MIP. The methodology is tested on instances from a dispatching area in Germany. Numerical results show that the presented method achieves acceptable computation times with good solution quality while meeting the requirements for real-time train dispatching.",A Strong Formulation for the Train Dispatching Problem and its Solution,"[72621, 14909]",269,"[143, 13]",960,Disruption management and recovery,85,9,54,Public Transport Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S01 [building - 101],"['Transportation', 'Column Generation']",TC-54
"Multi-objective optimization [MOO] is a technique widely used in engineering to balance different, often conflicting decision criteria that are difficult to compare directly. However, common methods for solving MOO problems, including the weighted sum, normal boundary intersection [NBI], and sandwich methods, often struggle in addressing the complexity of nonconvex and discrete decision variables in engineering designs.

In our research, we introduce a robust approach, referred to as SDNBI, that blends the sandwich algorithm with a modified version of the NBI method [mNBI]. This new method is particularly effective in navigating the complex, non-linear parts of the decision space, and in quickly identifying areas where no further optimal solutions can be found. Our study explores the theoretical interplay between mNBI and the sandwich algorithm, focusing on three key aspects - the accuracy of its approximations, the decomposition of objective space based on Pareto front convexity, and its efficiency in circumventing redundant searches in disconnected Pareto segments. The effectiveness of this combined approach is benchmarked against existing MOO methods using standard literature problems and further validated in real-world scenarios such as solvent design for CO2 capture and the design of working fluids in Organic Rankine Cycle processes.",A development of a nonconvex and combinatorial bi-objective programming for molecular design problems,"[76918, 65931, 65932, 24484]",52,"[77, 14, 104]",962,Multiobjective Mixed-Integer Nonlinear Optimization,34,9,37,Multiobjective Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,33 [building - 306],"['Multi-Objective Decision Making', 'Combinatorial Optimization', 'Process Systems Engineering']",TC-37
"We consider structured minimization problems subject to smooth inequality constraints and present a flexible algorithm that combines interior point [IP] and proximal gradient schemes. While traditional IP methods cannot cope with nonsmooth objective functions and proximal algorithms cannot handle complicated constraints, their combined usage is shown to successfully compensate the respective shortcomings. We provide a theoretical characterization of the algorithm and its asymptotic properties, deriving convergence results for fully nonconvex problems, thus bridging the gap with previous works that successfully addressed the convex case. Our interior proximal gradient algorithm benefits from warm starting, generates strictly feasible iterates with decreasing objective value, and returns after finitely many iterations a primal-dual pair approximately satisfying suitable optimality conditions. As a byproduct of our analysis of proximal gradient iterations we demonstrate that a slight refinement of traditional backtracking techniques waives the need for upper bounding the stepsize sequence, as required in existing results for the nonconvex setting.",An interior proximal gradient method for nonconvex optimization,"[63215, 50060]",133,"[81, 63, 60]",963,"Nonsmooth optimization and applications, Part I",84,7,32,Advances in large scale nonlinear optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,41 [building - 303A],"['Non-smooth Optimization', 'Large Scale Optimization', 'Interior Point Methods']",TA-32
"The El Farol Bar Problem [EFBP] is introduced by W.B.Arthur[1994] to present Complexity Economics. Arthur claims that if all agents use deductive reasoning, either all attend the bar or all stay home. Thus, deductive reasoning fails. Therefore, all agents use inductive reasoning with different strategies in forming their expectations about weekly attendance, which then be used in this binary decision-making process. Arthur states that when inductive reasoning is used, the mean attendance always converges to the bar capacity. The best-known strategy in the literature in terms of minimizing the variance in the attendance values is random attendance with the unstated assumption that the bar capacity is known. We suggest a new agent type called Yasarcan-Çetiner agents, which use a hysteresis structure in decision-making. These agents persist on keeping their current decisions as it is for a number of trials before switching their decisions; each individual agent has specific persistence thresholds. Yasarcan-Çetiner agents perform better than all existing agent types in the EFBP literature in terms of improving bar utilization and in minimizing the variance of weekly attendance values. It is not necessary for Yasarcan-Çetiner agents to know the bar capacity a priori; learning the bar capacity is an emergent collective behavior of the swarm, which is another contribution. We anticipate that Yasarcan-Çetiner agents will be used in other type of economic decision-making problems.",A Promising Approach to the El Farol Bar Problem - Hysteresis in Decision Making,"[76924, 76927]",564,"[3, 27, 131]",965,Simulation in economics I,77,4,43,"Agent-based Models in Management, Economic and Organisation Sciences","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,99 [building - 306],"['Agent Systems', 'Decision Theory', 'Simulation']",MC-43
"In large events, improving visitor experiences involves a personalized planner for individual preferences. Crafting unique itineraries, it often overlooks suggestions given to others visitors. This is crucial in limited-capacity locations, as myopic decisions may lead to extended queues. We study a situation where a large number of people visit a popular venue [e.g., an art gallery, a mall, a theme park, or an exhibition] where points of interest are located [e.g., paintings, shops, attractions, or pavilions]. Visitors have a maximum time available for the overall experience. The points of interest have a binding capacity and, whenever the turnout of visitors exceeds such capacity, queues occur. Given the maximum time available along with the time spent queuing, a selection of the points of interest may become necessary. Visitors usually act as autonomous decision-makers and do not take into account their interaction with other visitors. This leads to remarkable inefficiencies that could be, to a certain extent, overcome  through a  coordination of the paths and schedules of the different visitors. The resulting optimization problem  is modeled on a time-space network as a Mixed-Integer Linear Program [MILP], where the goal is to minimize a weighted combination of the points of interest not selected and the  time spent queuing. Computational results show the benefits that can be achieved by using the model proposed as a tool to support decision-making. ",Optimizing paths and schedules in crowded events,"[50563, 12473, 1182]",873,"[14, 150, 111]",968,Optimization problems in scheduling,64,12,26,Combinatorial Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,012 [building - 208],"['Combinatorial Optimization', 'Network Flows', 'Programming, Mixed-Integer']",WA-26
"In pursuit of carbon peaking and carbon neutrality goals, this conference presentation introduces a comprehensive approach - the integration of the carbon capture, utilization, and storage [CCUS] strategy with the cap and trade principle. Addressing the challenge of securing commitments for both carbon capture and investment within the power sector, our proposed model incorporates linear CO2 storage, emissions allowance deposit, stochastic CO2 utilization demand, mean-reverting Lévy jump allowance price process, and a stochastic CO2 generation process during production. We present the derived optimal commitment policy, considering constraints such as physical carbon storage limitations and rational allowance deposit considerations. Through impact analysis, we demonstrate that our optimal policy allows companies to dynamically adjust investment levels to mitigate risks arising from uncertainties in CO2 generation and utilization demand. Specifically, under the cap and trade principle, companies can flexibly adapt to cover excess emissions or capitalize on remaining allowance deposits, thus enhancing profitability after achieving the optimal CO2 capture level under the CCUS strategy. This framework not only contributes to meeting environmental objectives but also provides a strategic and adaptive approach for power companies to navigate the complexities of carbon management.","Optimizing Carbon Capture, Utilization, and Storage Strategy - A Cap-and-Trade Integrated Approach for Power Companies",[22645],929,"[93, 108, 139]",973,Sustainable supply chains,18,15,24,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,83 [building - 116],"['OR in Energy', 'Programming, Dynamic', 'Sustainable Development']",WD-24
"We study the problem of variable selection in convex nonparametric least squares. Whereas the Lasso is a popular technique for simultaneous estimation and variable selection, its performance is unknown in convex regression problems. In this work, we investigate the performance of the Lasso regularized convex nonparametric least squares estimator in a high-dimensional setting and propose an alternative approach based on the unique structure of the subgradients. our proposed estimators perform favorably, while generally leading to sparser models, relative to the other predictive models via the standard Lasso. Further, our estimators can be expressed as solutions to convex optimization problems and are amenable to modern optimization algorithms.",Sparse convex nonparametric least squares via convex optimization,[72201],507,"[21, 66, 131]",975,Convex optimization algorithms,70,12,41,Nonsmooth Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,97 [building - 306],"['Convex Optimization', 'Machine Learning', 'Simulation']",WA-41
"To facilitate the transition towards climate-neutral energy systems, many decision-makers rely on Energy System Optimization Models [ESOMs]. However, the complexity associated with operational decisions in the numerous time steps can bring a tremendous computational burden. So far, most efforts have been dedicated to creating typical periods to represent the full horizon, with repetitions in a uniform resolution. On the other hand, owing to the various natures of operational decisions, a uniform resolution might not be necessary. In fact, it has been proposed to designate separate resolutions for certain sections of the system. In that method, temporal period lengths have to be multiples of each other. Taking this to a new level, we propose a concept and mathematical formulation of fully flexible temporal resolutions, where energy flows and balances can have independent and dynamically varying resolutions. Furthermore, we present a case study of the optimal operation of the Northwest EU power system to demonstrate its advantages, utilizing TulipaEnergyModel.jl, an open-source ESOM tool. Results show that compared to using traditional uniform resolution, a simplification enabled by the fully flexible resolutions gives a relatively more accurate solution while incurring less computational cost. The time spent formulating the optimization problem also drops proportionally. ",Fully Flexible Temporal Resolution for Energy System Optimization,"[76931, 61600, 56946, 69791]",397,"[37, 110, 149]",976,Decarbonized energy systems & markets,22,12,09,Energy Markets,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,10 [building - 116],"['Energy Policy and Planning', 'Programming, Linear', 'Problem Structuring']",WA-09
"This work explores graph optimization problems, focusing on bulk optimization setting where potential link failures in different scenarios introduce uncertainty. 
Bulk optimization, a recently introduced problem class in the literature, recognizes that in many practical cases, uncertainty affects only a small portion of the system. This contrasts with robust optimization, often considered overly conservative. Accordingly, the number of scenarios and the number of edges per scenario, although not necessarily low, are finite and limited.
Given a graph and a cost for each edge, our goal is to activate a subset of edges of minimum cost, ensuring that the resultant graph presents some desired structure [i.e., a perfect matching or a spanning tree] in each scenario. 
Our study develops a Benders' decomposition approach where the master problem encapsulates variables determining which links to activate, while the subproblems verify the feasibility of the master solution in each scenario. The transformation of the subproblems into an efficiently solvable flow problems enhances computational tractability.
Although the bulk optimization paradigm has been studied for what concerns its theoretical properties, and some approximation algorithms have been proposed, we are not aware of any computational approach.
The proposed approach demonstrates promising results and applicability, offering a robust and scalable solution algorithm for graph problems in the presence of uncertainty.",Uncertainty-affected graph problems solution through Bulk Optimization,"[59316, 14917, 7400, 2813]",206,"[127, 53, 109]",978,Graph and network optimization,64,9,25,Combinatorial Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,011 [building - 208],"['Robust Optimization', 'Graphs and Networks', 'Programming, Integer']",TC-25
"In this study, we present an on-demand system in order to improve the efficiency of a single bus line facing a clear demand imbalance during peak hours. The on-demand system aims to increase the efficiency of service by allowing vehicles to take shortcuts between the city center and the terminal given the set of passenger requests known at the time. Passengers indicate their origin, destination, and desired pick-up time through an app or an online platform. Since the passenger requests are explicitly taken into account, each passenger is assigned to a service that serves their origin and destination. This assignment is also communicated to the passengers some time before their desired departure time. Given the practical importance of efficiently optimizing the decisions of this system in a short time, a VNS algorithm is developed.  The algorithm decides which shortcuts to take by each bus and in each direction, when to depart from the city center and the terminal, and the corresponding passenger assignments in a matter of minutes. The performance of the system is analyzed under different circumstances.",Analysis of an on-demand public bus line with demand imbalance during peak hours,"[67226, 428, 46228]",376,"[119, 74, 143]",979,Demand-responsive public transport 1,85,13,54,Public Transport Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S01 [building - 101],"['Public Local Transportation Systems', 'Metaheuristics', 'Transportation']",WB-54
"Flowty is a network optimization solver that exploits the network structure in a column generation algorithm. This talk is about the resource constrained shortest path [RCSPP] algorithm at the core of this algorithm.

The RCSPP algorithm is a bidirectional labeling algorithm parallelized on a bucket level, that is pulling [as opposed to pushing] labels into buckets. The size of the buckets is small enough to avoid cycles within the bucket.

Pulling into buckets is done in parallel for independent buckets. The dependency graph between buckets is handled implicitly, and the bidirectional midpoint/stopping criteria is dynamic, meaning that buckets are extended until the opposite direction is encountered.

As is always the case for pull-based labeling algorithms; carefully choosing the order of which labels are created, it is possible to guarantee that only labels not dominated are ever stored. This not only limits the memory footprint, it also allows labels to be stored as immutable ‘object of arrays’ [as opposed to ‘array of objects’] thereby enabling the use of vectorized instructions for dominance checks and thus exploiting a second level of parallelization.

The Vehicle Routing Problem with Time Windows [VPRTW] is used as an illustrative case.",Flowty’s Resource Constrained Shortest Path Algorithm,"[76914, 63559]",749,"[13, 102, 145]",980,Vehicle Routing Problems With Time Windows,5,13,64,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S16 [building - 101],"['Column Generation', 'Parallel Algorithms and Implementation', 'Vehicle Routing']",WB-64
The generation of alternative policies is essential in complex decision tasks with multiple interests and stakeholders. A diverse set of policies is typically desirable to cover the range of options and objectives. Decision modelling literature has often assumed that clearly defined decision alternatives are readily available. This is not a realistic assumption in practice. We present a structured process model for the generation of policy alternatives in settings that include non-quantifiable elements and where portfolio optimisation approaches are not applicable. Behavioural issues and path dependence as well as heuristics and biases which can occur during the process are discussed. The behavioural experiment compares policy alternatives obtained by using two different portfolio generation techniques. The results of the experiment demonstrate that path dependence can occur in policy generation. We report thinking patterns of subjects which relate to biases and heuristics. ," Generating Policy Alternatives for Decision Making - A Process Model, Behavioural Issues, and an Experiment ","[2773, 40804, 31430]",102,"[10, 25, 40]",981,Behavioral Decision Analysis IV,13,7,11,Behavioural OR,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,12 [building - 116],"['Behavioural OR', 'Decision Analysis', 'Environmental Management']",TA-11
"M&A announcements can result in substantial positive or negative ab-
normal acquiring-firm stock returns and sizeable associated dollar value
gains or losses. Despite the extensive existing M&A literature, whether such
loses and gains are predictable remains unknown. Similarly explored are the
choices of model for such forecasting. This paper fills this gap. We employ
acquirer, target, deal and macroeconomic features commonly used in the
literature and test the accuracy of parametric and non-parametric models.
As expected, given the high noise-to-ratio inherent of financial forecasting,
predictability is low. However, non-parametric models are able to consis-
tently forecast abnormal returns associated with M&As, sometimes sur-
passing their parametric counterparts. Our analyses of feature importance
shows that a handful consistently outweighs the relevance of the remaining
subset. We further construct two portfolios of M&As, one containing ex-
pected value-generating M&As and another expected value-destroying MA,
and show that they are significantly different.
",Forecasting M&A shareholder wealth effects to prevent value-destroying deals - Can it be done?,"[76936, 77158, 76948, 71578]",661,"[1, 47, 66]",982,OR in Accounting - Wealth and Risk,7,15,59,OR in Financial and Management Accounting,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S08 [building - 101],"['Accounting', 'Forecasting', 'Machine Learning']",WD-59
"Storage and demand response could provide necessary flexibility to accommodate the influx of intermittent renewable energy and facilitate the low carbon energy transition. However, the operation of these flexible assets may inadvertently intensify carbon emissions, leading to a conflict between profit maximization and environmental impact. We develop a model to characterize the factors causing this effect. Essentially, both storage and demand response act as load-shifting agents [LSAs], and therefore, they indirectly influence emissions. The scope of this influence depends upon the carbon intensity profile of the supply function in the wholesale market and the round-trip efficiency of LSAs. Some of the insights are surprising, e.g., the more efficient LSAs do not always lead to lower carbon emissions. Next, we propose and analyze several policy instruments aimed at reducing the unintended emissions caused by LSAs. Finally, using data from the Dutch market, we simulate the impact of LSAs and the emission mitigation instruments. We find that in 2019, simply adding LSAs to the market would reduce the total carbon emission. Conversely, in 2022, some LSAs, such as pumped hydro storage and lithium-ion batteries, would increase the emission if no CO2 mitigation instruments were applied.    ",Analysis and Mitigation of Unintended Carbon Emissions from Storage and Demand Response.,"[53464, 76937, 72924, 10530]",344,"[36, 37, 40]",983,Energy transition and operations,21,5,22,Energy Management,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,81 [building - 116],"['Electricity Markets', 'Energy Policy and Planning', 'Environmental Management']",MD-22
"Gradient-based methods are known for their efficiency in solving unconstrained bilevel programs. However, when dealing with constrained bilevel programs, the application of this method becomes challenging due to the strict complementarity condition required for the continuous differentiability of the lower level solution mapping within the Karush-Kuhn-Tucker system. To tackle the lower level problem with inequality constraints without relying on strict complementarity, we propose  a family of smoothing functions to approximate the primal-dual solution mapping based on the smoothing barrier augmented Lagrangian function, which ensures the preservation of the continuously differentiability and the convexity of the lower level problem. By approximating the bilevel program as single-level constrained problems, we adopt a hybrid approach that combines the gradient-based method with the augmented Lagrangian method. This allows us to effectively solve the constrained bilevel programs. We prove that any accumulation point generated by the algorithm is a Clarke stationary point of the bilevel problem. Importantly, unlike conventional smoothing function methods, the accumulation point can also be a Bouligand stationary point under an additional verifiable condition.",A smoothing barrier augmented Lagrangian gradient-based method for constrained bilevel programs,[65320],955,"[5, 19, 81]",984,Nonsmooth optimization algorithms II,70,15,41,Nonsmooth Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,97 [building - 306],"['Algorithms', 'Continuous Optimization', 'Non-smooth Optimization']",WD-41
"This paper presents a game theoretical framework for data classification, based on the interplay of pairwise influences in multivariate choices. This consists of a voting game wherein individuals, connected through a weighted network, select features from a finite list. A voting rule captures the positive or negative influence of an individual's neighbours, categorized as attractive [friend-like relationships] or repulsive [enemy-like relationships]. Payoffs are assigned based on the total number of matching choices from an individual's neighbours. We show that our approach constitutes a natural generalization of the K-nearest neighbours’ method, establishing the proposed game as a theoretical framework for data classification. Computationally, we construct a mixed-integer linear programming formulation to approach the Nash equilibria of the game, facilitating their applicability to real-world data. Our results provide conditions for the existence of Nash equilibria and for the NP-completeness of its characterization. On the empirical side, we use the proposed approach to impute missing data and highlight its competitive advantage over the K-nearest neighbour’s approach.",A generalized voting game for categorical network choices,"[76938, 67067, 5426]",235,"[66, 50, 14]",986,Fairness and responsible AI,16,7,28,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,065 [building - 208],"['Machine Learning', 'Game Theory', 'Combinatorial Optimization']",TA-28
"Existing clinical practice guidelines tend to overlook the unique requirements of patients with multiple chronic conditions [MCC] by primarily focusing on the management of individual diseases. This inadequately personalized approach leads to adverse events and places a financial strain on the healthcare system, particularly in countries with large or growing populations. To address this issue, our study utilizes a stochastic modeling approach to optimize disease screening decisions while considering financial and resource limitations from a broader societal perspective. Our findings indicate that women with pre-diabetes should undergo less frequent screening compared to those without diabetes. Moreover, our numerical results demonstrate the robustness of the optimal breast cancer screening policy in the face of relative changes in mammography screening costs.",Insufficient Addressing of Patient Needs with Multiple Chronic Conditions in countries with large or growing populations - Implications for Clinical Practice Guidelines,"[76943, 4069]",979,"[56, 108, 73]",994,Medical decision making,3,5,17,OR in Health Services [ORAHS],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,40 [building - 116],"['Health Care', 'Programming, Dynamic', 'Medical Applications']",MD-17
"The p-median problem is a classical discrete location problem and is equivalent to the well-known k-medoids problem in the unsupervised clustering literature. The aim is to select p centers while minimizing the sum of distances from each customer to its nearest center. Recent advancements in solving the p-median and related problems have successfully leveraged Benders decomposition methods. In order to reduce the number of variables and possibly the number of Benders cuts in these models, it is possible to aggregate distance variables corresponding to customers. We propose to partially aggregate the distance variables based on an initial solution - aggregation occurs only when the corresponding customers are assigned to the same center in the initial solution. In addition, we propose a set of tailored valid inequalities for these aggregated variables. Initial experiments indicate that our model, post-initialization, provides a stronger lower bound, thereby accelerating the resolution of the root node. Furthermore, this approach seems to positively impact the branching procedure, leading to an overall faster Benders decomposition method.",Variable aggregation in a Benders decomposition for the p-median problem,"[68712, 76950, 23765]",43,"[64, 111, 14]",995,Advances in Location Analysis ,29,2,61,Locational Analysis,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S10 [building - 101],"['Location', 'Programming, Mixed-Integer', 'Combinatorial Optimization']",MA-61
"To limit greenhouse gas emissions in the public transportation sector, transit authorities are constantly increasing the proportion of electric buses [EBs] in their fleets, creating complex decision problems to charge the vehicles. In order to facilitate the scheduling of all charging operations, it might be efficient to charge EBs both overnight and at long-term stops during the day, to balance the charged quantities. In this study, we consider a multi-day electric bus assignment and recharge scheduling problem which can be defined as follows. Given a set of vehicle blocks [sequences of timetabled bus trips that should be operated by the same bus on a given day], a set of EBs and a set of chargers, the objective is to assign an EB to each block over multiple days and to schedule both overnight and en-route recharging operations. Chargers are installed at the depot, and sufficiently long stops. Each charger has its own charging function represented by a piecewise linear function. The decision are made ensuring that the EBs never run out of energy performing their assigned blocks, the charging capacity at any location should not be exceeded, and the sum of the total charging costs and long run damages on the battery are minimized. We first model this problem as a MILP that can be solved with a commercial solver. Then, to yield faster computational times, we propose an iterative matheuristic based on the MILP.",A matheuristic for a multi-day electric bus assignment and recharge scheduling problem,"[76947, 18350, 73397, 32492, 50078]",815,"[119, 100]",996,Electric Busses,85,8,51,Public Transport Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,M5 [building - 101],"['Public Local Transportation Systems', 'OR in Sustainability']",TB-51
"In the past lack of data was often cited as a major challenge in healthcare modelling, but in recent years large electronic datasets have become increasingly available for use in research.  This talk discusses the advantages and disadvantages of using such datasets, in particular routine health service data primarily collected for other purposes, compared with collecting prospective data or using the clinical literature.  These ‘pros and cons’ are illustrated by three case studies. The first dates back to the 1990s and concerns screening for diabetic retinopathy; the other two are connected, much more recent, and concern care for older people.  All three projects involved large multi-disciplinary teams of researchers.  The first project involved the development of a discrete-event simulation model, parameterised using data from the literature.  The two related projects involved the development of system dynamics simulation models, both using large routine health service datasets to estimate the model parameters.  While access to such data might seem to be a luxury, compared with the difficulties of obtaining data for the diabetic retinopathy model, deriving the required information from the data was not always straightforward. ",Using large routine health service datasets for modelling,"[9100, 70325]",971,"[56, 140, 131]",997,Simulation models in healthcare,3,4,17,OR in Health Services [ORAHS],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,40 [building - 116],"['Health Care', 'System Dynamics and Theory', 'Simulation']",MC-17
"It has long been established in the literature and observed in practice that sellers can benefit from allowing consumers to purchase in advance of the date of consumption. Because of this advance purchasing, consumers can find themselves either with a ticket that they no longer want or without a ticket that they wish to have. In the past, scalpers would facilitate transactions among these consumers for a fee. Sellers historically disliked those practices and actively worked to prevent them. We obtain a stark finding - an unfettered and efficient reselling market eliminates all of the benefits of advance selling, which justifies sellers’ historic hostility to reselling. But now ticket exchanges are common, growing, and even embraced by sellers. What changed? We present a theory that demonstrates reselling is actually beneficial for sellers under one crucial condition - the seller must be able to have some control over the reselling process, thereby allowing the seller to earn something from each transaction. The old-fashioned paper ticket did not give such control, but technology now enables electronic tickets, which do. In fact, a seller cannot earn more than what it receives from a properly designed and efficient reselling market [reselling is an optimal mechanism]. And such a market eliminates the opportunities for speculators and can also be beneficial to consumers. In sum, our results explain why the view towards reselling has shifted dramatically.",The Enigma of Ticket Exchanges [and Other Reselling Markets],[76953],427,"[50, 124]",999,Retail Operations and Marketing,30,8,50,Retail Operations,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,M2 [building - 101],"['Game Theory', 'Revenue Management and Pricing']",TB-50
"In retail settings conversion fractions [CFs] matter to managers because they measure the stores’ effectiveness to convert visitors into buyers. Despite CFs importance, available methods to evaluate effects of a given marketing action [MA] on CF are scarce and provide limited evaluations of these effects. We present an approach that [i] builds a model for conversion probabilities with data outside the MA period, [ii] uses this model to predict a baseline for conversion probabilities during the promotional period conditional on the counterfactual hypothesis that the MA had not occurred, and [iii] evaluates MA effects on CF by comparing this baseline to observed data during the MA period. We illustrate our approach with data from an actual MA consisting on a two-day promotion. The analysis combines arrival data from video images and conversion data from cash registers.",Dynamic effects of store promotions on conversion fraction - Expanding technology applications with innovative analytics,"[32924, 76959]",427,"[71, 47, 124]",1005,Retail Operations and Marketing,30,8,50,Retail Operations,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,M2 [building - 101],"['Marketing', 'Forecasting', 'Revenue Management and Pricing']",TB-50
"Decision making among options that consist of many attributes or that contain risk is commonplace in various everyday contexts, and understanding how these decisions are made is highly valuable for researchers and practitioners. Here I develop scroll tracking, a novel process tracing method that builds on a simple mobile web application where the decision maker scrolls between options and the app tracks their response dynamics in pixel-time coordinate space. I then use scroll data obtained from two studies, a consumer choice problem and an risky decision making problem, to study predictions made by different attentional evidence accumulation models. The information obtained from the app can be used to successfully predict different attention-choice relationships and to predict subjective value. I also compare several evidence accumulation models, equipped with the scroll data, in their predictive ability. All the models are able to benefit from the rich variety of data that scroll tracking provides. These findings demonstrate the potential of novel, naturalistic process tracing tools that can be used to covertly study latent decision processes.",Process tracing and attention in preferential choice,[43836],94,"[10, 25]",1006, Behaviour and decision processes ,13,12,07,Behavioural OR,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1019 [building - 202],"['Behavioural OR', 'Decision Analysis']",WA-07
"The optimal transport [OT] is a paradigmatic network problem and consists in finding the minimum cost transportation plan that moves quantities of a single item from a set of sources to a set of destinations, where, for each pair source/destination [i,j], a unit transportation cost holds. The interest in OT has been renewed lately in artificial intelligence applications where a set of instances, called DOTmark, is nowadays the benchmark for OT. Recently, a new exact algorithm for solving OT, called iterated Inside Out [IIO], has been proposed. The strength of this method relies on the fact that potentially many pivoting operations are performed for each computation of dual multipliers and reduced costs. Here, we propose a variant of IIO specifically tailored to the DOTmark instances. This variant solves these instances exploiting the peculiar structure of the transportation costs - these costs depend on the couple of indexes [i,j] of the related variables and present strong regularity in the way they increase or decrease according to a change of index i or j. Given a basic solution, the described structure of costs enables the algorithm to predict from scratch the positivity of a huge set of reduced costs, thus avoiding their computation. Also, it enables to detect the presence of non basic variables with negative reduced costs within a reduced subset of variables. Computational results indicate that this approach is largely superior to the current state of the art algorithms.",An improved variant of the Iterated Inside Out algorithm  for solving the optimal transport DOTmark Instances,"[2378, 76961, 47642]",872,"[143, 14]",1010,Optimization problems on graphs,64,5,26,Combinatorial Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,012 [building - 208],"['Transportation', 'Combinatorial Optimization']",MD-26
"With the exponential growth of e-commerce platforms, fueled by the increasing demand for meal delivery services, delivery efficiency emerges as a pivotal concern for businesses. This paper delves into the Meal Delivery Routing Problem [MDRP] within the realm of e-commerce, addressing the delivery applications, couriers, restaurants, and customers' objectives that often conflict among them. The study focuses on the complexities of last-mile logistics, emphasizing the imperative for a robust solution to achieve operational efficiency, enhance customer satisfaction, promote environmental sustainability, and minimize lost sales. This paper presents a GRASP metaheuristic solution that tackles the MDRP by optimizing courier assignment to orders while accounting for dynamic variables like courier availability, order demands, and geographical considerations. The methodology is validated using real-life scenarios based on data from a delivery app operating in South America. Comparative analyses with a simulation-optimization-based study underscore the efficacy of GRASP in enhancing order fulfillment and routing efficiency, showcasing its strengths across diverse locales such as Sao Paulo [Brazil], Bogota, and Medellin [Colombia]. As future research, we propose exploring optimization models geared toward maximizing the welfare of all stakeholders within the meal delivery services system. Such an endeavor aims to foster genuine democratization of delivery services.",A Metaheuristic Approach for the Meal Delivery Routing Problem,"[76962, 29757]",785,"[32, 65, 74]",1011,Heuristics for Vehicle Routing 2,5,15,64,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S16 [building - 101],"['E-Commerce', 'Logistics', 'Metaheuristics']",WD-64
"The contribution addresses a harvest planning problem in the context of a hierarchical agrifood supply chain, which integrates the definition of management zones for harvesting, the harvest scheduling problem and the coordination between the producer and the wholesaler. The problem is represented through bilevel programming models that allow the representation of the hierarchy between the producer [leader] and a wholesaler [follower]. The producer plans and schedules the harvest of the different homogeneous management zones into the resulting partition of the farm and the production levels to be offered to each wholesaler, while the wholesaler decides the quantity of product to be purchased to satisfy the demand requirements. First of all, a mixed–integer bilevel program is proposed and solved alternatively using an algorithmic strategy based on the Benders decomposition method. Then, a stochastic bilevel program is also proposed for representing the uncertainty in future crop yields, prices, and demands, using a finite set of scenarios. The bilevel optimization models considered are reformulated into mixed-integer linear programs using the Karush–Kuhn–Tucker conditions and replacing the nonlinear complementary constraints by the big-M method. The models were applied in a Case Study for selective harvesting of grapes with data collected from a farm. The results obtained when solving a set of instances highlight the importance of the problem and the proposed methodology.",Modeling a harvest planning and scheduling problem in a hierarchical agrifood supply chain by bilevel programming,[8451],4,"[89, 136, 138]",1013,Optimization in Agriculture,20,2,12,OR in Agriculture and Forestry ,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,13 [building - 116],"['OR in Agriculture', 'Stochastic Optimization', 'Supply Chain Management']",MA-12
"Clustering is an unsupervised learning task that aims to partition data into a set of clusters. In many applications, these clusters correspond to real-world constructs [e.g. electoral districts] whose benefit can only be attained by groups when they reach a minimum level of representation [e.g. 50% to elect their desired candidate]. This paper considers the problem of performing k-means clustering while ensuring groups [e.g. demographic groups] have that minimum level of representation in a specified number of clusters. We show that the popular k-means algorithm, Lloyd's algorithm, can result in unfair outcomes where certain groups lack sufficient representation past the minimum threshold in a proportional number of clusters. We formulate the problem through a mixed-integer optimization framework and present a variant of Lloyd's algorithm, called MiniReL, that directly incorporates the fairness constraints. We show that incorporating the fairness criteria leads to a NP-Hard sub-problem within Lloyd's algorithm, but we provide computational approaches that make the problem tractable for even large datasets. Numerical results show that the approach is able to create fairer clusters with practically no increase in the k-means clustering cost across standard benchmark datasets.",Fair Minimum Representation Clustering,[67130],140,"[8, 66, 109]",1014,Mathematical Optimization for Trustworthy Machine Learning,15,9,27,Mathematical Optimization for XAI,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,047 [building - 208],"['Artificial Intelligence', 'Machine Learning', 'Programming, Integer']",TC-27
"We propose trust-region and direct-search frameworks for large-scale stochastic derivative-free optimization by introducing the STARS and StoDARS algorithms. While STARS achieves scalability by minimizing random models that approximate the objective in low-dimensional affine subspaces thus significantly reducing per-iteration costs in terms of function evaluations, these goals are achieved by StoDARS through the exploration of the decision space by means of poll directions generated in random subspaces. These subspaces and their dimension are chosen via Johnson--Lindenstrauss transforms such as those obtained from Haar-distributed orthogonal random matrices. The quality of the subspaces and sets of poll directions, as well as the accuracies of estimates and models used by the algorithms are required to hold with sufficiently high, but fixed, probabilities. Convergence and complexity results are obtained for both methods using martingale theory. In particular, by leveraging the ability of StoDARS to generate a dense set of poll directions, its almost sure convergence to Clarke stationary points is established. Moreover, the analysis of second-order behavior of the well-known mesh adaptive direct-search algorithms using a second-order-like extension of the Rademacher's theorem-based definition of the Clarke subdifferential [so-called generalized Hessian] is extended to the StoDARS framework, making it the first in a stochastic direct-search setting to the best of our knowledge.",Stochastic derivative-free optimization algorithms using random subspace strategies,"[76957, 74972]",337,"[136, 63, 5]",1015,Algorithms for machine learning and inverse problems - zeroth-order optimisation,84,9,32,Advances in large scale nonlinear optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,41 [building - 303A],"['Stochastic Optimization', 'Large Scale Optimization', 'Algorithms']",TC-32
"Amidst rapid industrial growth, the UAE's public sector plays a crucial role in national infrastructure. Traditional procurement practices in this sector have primarily focused on commercial and technical criteria, with far less emphasis on environmental sustainability. This reliance on carbon-intensive goods and services has markedly increased the sector's carbon footprint. This research uses the Analytic Hierarchy Process [AHP] to improve sustainable procurement decisions, aiming to substantially reduce carbon emissions within the UAE's public sector. By integrating a comprehensive set of criteria—environmental, economic, social, and technical—the study develops a detailed AHP framework to systematically evaluate and select procurement options that support sustainability objectives.  The findings prioritize procurement practices that significantly reduce the sector's carbon footprint, thus aligning procurement strategies with the UAE's broader sustainability goals. 

",Enhancing Sustainable Procurement in the UAE's Public Sector - An AHP Framework for Minimizing Carbon Emissions,[71755],893,"[6, 138, 25]",1016,Pairwise comparisons and preference relations 3,44,12,44,Multiple Criteria Decision Analysis,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,20 [building - 324],"['Analytic Hierarchy Process', 'Supply Chain Management', 'Decision Analysis']",WA-44
"In the literature, different approaches have been proposed to approximate two-variable nonlinear functions. In particular, classic piecewise linear approximation, based on triangulation of the function domain is one of the most widely used in practice.
We start from a different approach, originally proposed in Rovatti et al. [2014], that allows one to obtain piecewise linear approximation by dividing into rectangles the nonlinear function domain. Rovatti et al. studied formulations for modeling such an approximation, using a number of binary variables that is equal to j + k, where j [respectively, k] is the number of intervals in which the first [resp. second] variable is divided. Note that the classic formulations of piecewise linear approximations based on triangulations need 2 x j x k binary variables instead.
In this work, we consider the generalization of the work by Rovatti et al. [2014] to the piecewise convex approximation case. In particular, we explore the strengthening of mathematical formulations to model such approximations via perspective reformulations. Finally, we show their interest thanks to some preliminary computational results.",On the Piecewise Convex Approximations of Two-Variables Functions,"[22410, 969, 1459]",132,"[111, 21, 14]",1017,Topics in Mixed Integer Nonlinear Programming 1,86,8,04,MINLP,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,1001 [building - 202],"['Programming, Mixed-Integer', 'Convex Optimization', 'Combinatorial Optimization']",TB-04
"This paper presents a mathematical modeling framework of a competitive multiperiod agricultural supply chain network under the triple bottom line sustainability, comprising financial, environmental, and social aspects. In our competitive supply chain modeling framework, agri-food firms strategically determine the flows of agricultural products to demand markets, the scale of farmland used for sustainable and conventional farming practices, and the allocation of labor for both sustainable and conventional farming practices. Profits determine the financial sustainability component, while environmental sustainability is assessed based on the total greenhouse gas emissions generated and the amount of water consumed. Social sustainability is measured through a social benefit function that considers the labor employed for both sustainable and conventional farming practices, as well as agri-food firms’ overall economic contribution through agricultural production in the regions. The competition between agri-food firms is studied through game theory, where the governing Generalized Nash equilibrium conditions correspond to a variational inequality problem. We use the Euler method as the algorithm to analyze a numerical study focusing on avocado supply chains. We draw attention to the potential impacts of climate change and deforestation in our numerical study. Given the trade-offs, our findings highlight the importance of achieving the triple bottom line.",Strategic Decisions and Sustainability in Competitive Agricultural Supply Chains - A Triple Bottom Line Perspective,"[76839, 76761, 53902]",537,"[100, 50, 138]",1018,Agri-Food Supply Chains,19,9,24,Sustainable Supply Chains,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,83 [building - 116],"['OR in Sustainability', 'Game Theory', 'Supply Chain Management']",TC-24
"This study proposes a novel approach to devise new drivers for bankruptcy prediction using complex network analysis. These drivers are company relational information-based drivers [CRIs] derived from the board of directors’ networks with different network configurations. The effectiveness of these new drivers is demonstrated on a dataset of UK companies listed on the London Stock Exchange. Numerical results suggest a significant improvement in predicting corporate bankruptcy.
Our research establishes the impact of incorporating network analysis of company relationships into bankruptcy prediction models. It sets the stage for more sophisticated financial analysis techniques that synergize traditional financial metrics with cutting-edge network analysis, and the advancement holds substantial promise for financial institutions and analysts, providing a more nuanced understanding of corporate bankruptcy risks.
",New drivers of bankruptcy based on Complex Networks,[76968],65,"[66, 53, 8]",1019,Network Analytics,17,5,31,Analytics,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,54 [building - 208],"['Machine Learning', 'Graphs and Networks', 'Artificial Intelligence']",MD-31
"The railway system frequently encounters abnormal events such as disturbances and disruptions, highlighting the importance of evaluating the resistance capability of train timetables. However, there is limited research focusing on their boundaries and dynamics. This study contributes to two main aspects - Firstly, to evaluate the functionality of robustness, we construct a delay propagation network to measure several indicators [heterogeneity, slack time, and delay]. Based on this methodology, we can clarify the generation and operating mechanism of robustness. Secondly, to quantify the resilience of timetables with different headways, we employ a heuristic approach to obtain saturated timetables, which serve as input for a rescheduling simulation engine. We establish several resilience metrics and utilize Data Envelopment Analysis [DEA] to evaluate the comparative performance of various timetables across different disruption scenarios. Finally, we apply this evaluation framework to the Beijing-Shanghai HSR line. The proposed method provides railway operators with a systematic approach to assess and enhance train timetables, thereby guiding decision-making processes and contributing to the development of more robust and resilient railway operations.",Robustness and Resilience Evaluation Method for High-Speed Railway Timetable,"[76942, 40486, 75779, 41723, 68135, 62360, 68076]",193,"[122, 24, 143]",1022,Resilience in Public Transport Planning,85,7,54,Public Transport Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S01 [building - 101],"['Railway Applications', 'Data Envelopment Analysis', 'Transportation']",TA-54
"In large office parking lots, capacity constraints and synchronization make it necessary to develop controllers to schedule the charging of electric vehicles [EVs]. We consider EV scheduling where each EV has an associated arrival time, departure time, energy requirement and maximum charging power. Furthermore, preemption is allowed, and EVs can charge simultaneously.
In practice, however, the availability and energy demand of individual vehicles comes with uncertainty. Therefore, existing controllers use fill-level approaches, where the scheduling occurs in an online fashion, following a target function of the aggregated charging power. It is known that when disregarding the maximum charging powers and given perfect non-probilistic information on the EVs upon arrival, an earliest deadline first approach will always result in a feasible schedule given a fill-level for which such a schedule exists.
In this work, we show that this is not the case for the problem with EV-specific maximum charging powers. In particular, even given a fill-level for which there exists a feasible schedule and given perfect information on the EVs upon arrival, there exists no deterministic online scheduling rule that reliably schedules EVs while following the given fill-level. This implies that even given the optimal fill-level for a chosen objective function, it is not possible to derive a rule that always finds an associated feasible schedule.",Online scheduling rules for Electric Vehicle Charging - why optimality is not an option,"[76921, 69713, 8513]",802,"[93, 129]",1024,Electric Vehicles within Electric Power Systems,23,5,19,OR in Energy,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,44 [building - 116],"['OR in Energy', 'Scheduling']",MD-19
"Battery-powered mobility-sharing companies face operational challenges in managing low-battery mobilities due to the dockless service, allowing pickups and drop-offs anywhere. To tackle this, such companies engage independent contractors to recharge these mobilities, offering them per-task compensation. Given the contractors’ liberty to dictate their work schedules and volumes, it is essential to devise a pricing strategy to motivate their participation. The core tradeoff involves balancing the wages paid to workers against the penalty costs from unsatisfied customers due to incomplete charging tasks. To this end, this study proposes a spatio-temporal pricing strategy that assigns tasks with differentiated regional prices by time interval, aiming to attract workers to areas with a scarcity of available workers. We use a reinforcement learning approach with proximal policy optimization to handle the high dimensionality of the problem. A simulation environment mimics dynamic worker participation and task reservation in response to price updates made by the RL agent. Computational experiments demonstrate the effectiveness of reducing overall platform costs under varying task and worker distributions. Moreover, the findings indicate that the RL-driven pricing policy mitigates the spatial and temporal discrepancies between task demand and worker availability. Our study is applicable to other spatial crowdsourcing platforms which necessitate resolving spatio-temporal imbalances.",Spatio-temporal pricing of battery-swapping tasks on mobility-sharing platforms using proximal policy optimization,"[76972, 6360]",600,"[8, 143, 18]",1025,Simulation in transportation and logistics,77,9,43,"Agent-based Models in Management, Economic and Organisation Sciences","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,99 [building - 306],"['Artificial Intelligence', 'Transportation', 'Computer Science/Applications']",TC-43
"We study the dynamics of hybrid hospitals offering on-site and remote hospitalization through telemedicine. These new healthcare models require efficient operational policies to balance costs, efficiency, and patient well-being. Our study addresses two primary operational questions - [i] how to direct patient admission and call-in policies based on individual characteristics and proximity and [ii] how to determine the optimal allocation of medical resources between these two hospitalization options.
We develop a model that uses Brownian Motions to capture the patient’s health evolution during remote/on-site hospitalization. By optimizing call-in policies, we find that remote hospitalization is cost-effective only for moderately distant patients, where the call-in threshold has a non-monotonic relationship with travel time. The impact of scarce resources is reflected through simultaneous increase of both remote and on-site costs by the same value, without altering the solution structure under abundant resources. 
Contrary to the widely held view that telemedicine can mitigate rural and
non-rural healthcare disparities, our research suggests that on-site care may actually be more cost-effective than remote hospitalization for patients in distant locations, due to increased risks for remote patients who
are called in to the hospital. This may be of particular concern in light of the growing number of “hospital deserts” amid recent rural hospital closures.",The Hybrid Hospital - Balancing On-Site and Remote Hospitalization,"[76973, 76974, 76975]",104,"[135, 56, 130]",1026,Just and ethical sustainability transitions,28,3,20,OR and Ethics,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,45 [building - 116],"['Stochastic Models', 'Health Care', 'Service Operations']",MB-20
"Sustainable development has become a central issue for policy decisions by governments and international organizations. The core framework of the UN’s 2030 Agenda for Sustainable Development defines 17 Sustainable Development Goals [SDGs], which set the basis for operationalizing specific metrics to evaluate and monitor the actions and policies that countries take to balance social, economic, and environmental sustainability. Towards achieving the goals of the UN Agenda, the role of local governments is crucial. In this context, the objective of this study is to describe the development and implementation of a multicriteria framework for assessing the performance of local governments from the perspective of the UN’s SDGs. The proposed framework is based on a comprehensive set of indicators that are aggregated through an outranking multicriteria decision aiding [MCDA] approach. The MCDA approach relies on a simulation process to consider the uncertainty and vagueness in the importance of the indicators across the various SDGs. Results are presented for a large sample of French municipalities, based on the most recent available data.",A Multicriteria Framework for Evaluating the Sustainability Performance of Local Governments - Application to French Municipalities,"[2127, 76978, 76977, 76979]",100,"[25, 101, 26]",1029,MCDA applications in Engineering and Management 1,44,2,47,Multiple Criteria Decision Analysis,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,50 [building - 324],"['Decision Analysis', 'OR/MS and the Public Sector', 'Decision Support Systems']",MA-47
"The energy transition requires complex decisions, which can be supported by energy system models. Modelling to generate alternatives [MGA] is a method to generate diverse, near-optimal solutions and address the structural and parametric uncertainties of such models. In energy planning, MGA generates almost cost-minimal but societally more feasible model outcomes. However, it neglects the economic feasibility resulting from [potentially lacking] profitability of investments based on market revenues, which in contrast is guaranteed for unconstrained cost-minimal solutions under certain conditions. Therefore, this paper aims at developing a market-economic interpretation of near-cost-optimal alternatives. Methodologically, we use results from multi-objective optimisation and the first-order Karush-Kuhn-Tucker conditions to show equivalence between MGA and cost minimisations with additional cost terms. On the way, we augment MGA to generate only Pareto-optimal solutions, i.e. to avoid costs that do not contribute to higher diversity between alternatives. Economically, the introduced cost terms can be interpreted as capacity-based subsidies or penalties that, as an intervention to an otherwise perfect market, lead to the same outcome as found with MGA. Such an intervention guarantees profitability and hence market-based feasibility. Practically, we demonstrate and validate our theoretical findings for the case study of a multi-national power system model.",Linking modelling to generate alternatives and market equilibria - On the economic interpretation of near-cost-optimal solutions in energy system models,"[69526, 24773, 33470]",407,"[36, 77, 37]",1030,Market-based analyses in long-term energy system models,22,9,09,Energy Markets,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,10 [building - 116],"['Electricity Markets', 'Multi-Objective Decision Making', 'Energy Policy and Planning']",TC-09
"Unmanned aerial vehicles [UAV] are becoming an increasingly prominent alternative to traditional methods in disaster response, providing rapid aid to regions where transportation becomes difficult after natural disasters. Optimizing the use of UAVs becomes even more important, especially considering the crucial role of rapid response and limited resources. This study focuses on developing efficient scheduling algorithms for UAVs that will be used in the distribution of emergency aid packages after natural disasters.

The goal of this study is to minimize the total weighted arrival time of aid packages to be distributed to the disaster area from multiple warehouses with UAVs. Given that UAVs can carry only a single package at a time, this problem becomes a scheduling problem. 
Within the scope of this study, the methods to be developed for the scheduling of UAVs to deliver aid packages to the disaster area will be tested on real-life scenarios designed for a potential earthquake in Istanbul, a region known for its high seismic risk. 

By developing a mixed integer linear programming model, it is aimed to reach the optimal solution for small data sets. A heuristic approach is developed in order to find near-optimal solutions in reasonable time for large-sized instances of these problems, which are known to be NP-hard. The solution methods are applied on the İstanbul example, and insights are provided for disaster management planning.",Scheduling of Unmanned Aerial Vehicles for Disaster Response,"[52147, 76987, 22492]",773,"[129, 30, 58]",1031,Post-Disaster Relief Distribution,38,12,21,OR in Humanitarian Operations [HOpe],"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,49 [building - 116],"['Scheduling', 'Disaster and Crisis Management', 'Humanitarian Applications']",WA-21
"The criteria modelling stage of the MCDA approaches constitute fundamental and critical process for the effective implementation and supporting the decision making. The level of consistency of the criteria family affects the accuracy and reliability of the application of the methodological approach used and the final acceptance of the assessed preference model. This research work is focused on the application of system dynamics methodological tools for the analysis of the decision problem statement and the identification of the relative features in a detailed manner. The tools of the system thinking approach and the techniques of the archetypes are employed in order to picture the points of view of the decision problem, their interrelations as well as their impacts. Exploiting the analytical features of the above-mentioned tools a comprehensive and easy to use approach for the analysis and presentation of the decision problem is available. The methodology is illustrated through a case study concerning the location selection problem for the establishment of Educational Units.",Applying System Thinking Approaches for Supporting the Criteria Modelling Process in MCDA ,"[23003, 76980]",100,"[77, 27]",1032,MCDA applications in Engineering and Management 1,44,2,47,Multiple Criteria Decision Analysis,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,50 [building - 324],"['Multi-Objective Decision Making', 'Decision Theory']",MA-47
"Renewable energy generators often rely on their battery deployments to meet their dispatch or purchase commitments in electricity markets. However, the literature optimizing the commitment decisions with storage considerations is scarce. In this paper, we study the joint energy commitment and storage problem for a wind farm paired with a battery. The power producer decides, in each hour of a finite planning horizon, how much energy to commit to dispatching or purchasing for the next hour as well as how much wind energy to generate and how much energy to charge or discharge. The power producer pays a penalty cost if she does not fully meet her commitment. We model this problem as a Markov decision process with random electricity price and wind speed. We prove the optimality of a state-dependent threshold policy under positive prices. This policy partitions the state space into several disjoint domains, each associated with a different action type, such that it is optimal to bring the storage and commitment levels to a different threshold pair in each domain. We employ our structural results to develop a heuristic solution procedure in a more general problem where the price can also be negative. Numerical results for data-calibrated instances indicate the high efficiency and scalability of our solution procedure - it yields optimal or near-optimal solutions with a speedup of two orders of magnitude over the standard dynamic programming algorithm.",Optimal Hour-Ahead Commitment and Storage Decisions of Wind Power Producers,"[76145, 71323, 77005, 43460, 56904]",344,"[93, 36, 108]",1033,Energy transition and operations,21,5,22,Energy Management,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,81 [building - 116],"['OR in Energy', 'Electricity Markets', 'Programming, Dynamic']",MD-22
" Efficient water resource management in agricultural contexts is a major challenge. The present study focuses on the allocation of irrigation water between competing crops at a watershed scale. The first step of the optimization process includes the division of the area into response units, which assists in maintaining complex modeling with high spatial resolution, while reducing the computation times by avoiding calculations over every pixel. This is done using the well-established SWAT+ model, which predicts, among others , the impact of different management decisions on fields within the watershed. A model-based, multi-objective optimization approach, which seeks to maximize the yield and to minimize the irrigation in each unit, is implemented on each of the response units. A curve is then fitted to the resulting Pareto front providing an estimate of the unit water productivity function [WPF]. The estimated WPFs are then used for determining the amount of water that should be allocated to each crop for the next irrigation season through a single-objective optimization process, where the goal is maximizing the economic profit. This optimization framework is applied to the San Joaquin watershed, which is predominantly occupied by agriculture fields and orchards, and relies heavily on irrigation.",Model-Based Optimization of Irrigation Water Allocation at the Watershed Scale,"[76952, 77026, 40394]",4,"[147, 89, 63]",1034,Optimization in Agriculture,20,2,12,OR in Agriculture and Forestry ,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,13 [building - 116],"['Water Management', 'OR in Agriculture', 'Large Scale Optimization']",MA-12
"Water banks constitute a market mechanism that enables the voluntary reallocation of flows to respond to both temporary and structural changes in the availability of this resource. Good management of water banks makes it possible to reduce social conflicts associated with water scarcity through temporary or permanent reallocation of the resource based on the supply and demand of users. Negotiation games are a methodology to formalise and analyse the consequences of the interaction between potential water sellers and buyers. The possibility of cooperation between the agents involved is analysed to obtain consensual solutions that improve the result that they can individually ensure. In particular, Nash and Kalai-Smorodinsky bargaining solutions are proposed to identify feasible reallocations of water that satisfy certain principles of rationality. In the absence of negative externalities, the water transfers that are determined with these solutions allow improving the economic efficiency associated with the use of water.",Water banks as bargaining games,"[70843, 7011, 76932, 10942]",342,"[50, 27, 147]",1035,"Game Theory, Solutions and Structures III",88,4,36,"Game Theory, Solutions and Structures","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,32 [building - 306],"['Game Theory', 'Decision Theory', 'Water Management']",MC-36
"Our study focused on the potential of a simple and common risk-averse strategy called greedy pair trading on the German intraday power market. This strategy aims to monetize the flexibility potential of a battery or similar assets. We analyzed real-world order book data from 15 selected days between 2020 and 2022 to estimate the theoretical potential of this strategy when optimal pairs are selected. We also investigated the impact of unbalanced auction wins, which can be a challenge for practical implementation. In our analysis, we compared the greedy approach to the ex post optimal strategy and examined the effect of different frequencies of order book accesses. We found that increasing the frequency of optimization points in the greedy approach did not significantly improve the profit. However, in the ex post optimal strategy, halving the time between trading points resulted in an average profit increase of 30%. Additionally, we observed that a loss of up to 10% of the bids had a negligible impact on the overall profit. Our findings contribute to a better understanding of arbitrage trading on the German intraday market and highlight the advantages of incorporating predictive information and frequent optimization updates.",On the potential of arbitrage trading on the German intraday power market,"[25021, 61771, 63558]",341,"[36, 93, 72]",1036,Modelling and Economics of Storage Technologies in Energy Markets,22,2,09,Energy Markets,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,10 [building - 116],"['Electricity Markets', 'OR in Energy', 'Mathematical Programming']",MA-09
"The energy sector is transitioning towards a climate-neutral future based on renewable energy sources. Besides increasing demands for decentralised, flexible, and environmentally friendly technologies, technology innovation will be imperative for this transition, offering unknown potentials. Energy system models are an important tool to address these challenges, using various input parameters for all technologies considered. However, low technology readiness level [TRL] technologies require estimates for those parameters which are usually not possible or poorly verified at an early stage of development. Thus, including low TRL technologies in energy models is associated with high uncertainty and lacking in current research. We develop a new, inverted approach for energy system modelling that considers both the common central-planning perspective of optimal system design and the technical design perspective of technology development. By turning model input parameters such as technology cost or efficiencies into decision variables, these can be used as additional optimisation objectives. A multi-objective optimisation then enables trade-off analyses between overall system cost and requirements for technology development, e.g. the least requirements that would still lead to the use of a technology in a cost-efficient system. We demonstrate the inverted method using the modelling framework Backbone and Carnot Batteries as location-independent energy storage with low TRL.",Multi-Objective Optimisation for the Inverse Analysis of Design Requirements for Low Technology Readiness Level technologies from an Energy System Perspective,"[76848, 69526, 33470]",341,"[84, 93, 112]",1037,Modelling and Economics of Storage Technologies in Energy Markets,22,2,09,Energy Markets,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,10 [building - 116],"['Optimization Modeling', 'OR in Energy', 'Programming, Multi-Objective']",MA-09
"The weighted geometric mean [WGT] is a commonly discussed procedure for aggregating judgements in multicriteria decision analysis [MDA] and social epistemology, but it has drawbacks. We combine MDA and social epistemology results to defend the ratio judgements aggregation with the WGT. We argue that aggregation with the WGT overcomes many drawbacks if one aggregates ratio judgements instead of absolute judgements. We use an axiomatic approach to argue that one should aggregate ratio judgements [we especially favour odds] instead of absolute judgements for the following reasons. First, the mathematical structure underlying ratio judgements [and their aggregation with the WGT] allows one to satisfy many desirable axioms that – for absolute judgements – lead to undesirable results. For example, we avoid dictatorial decisions, can weigh experts' judgements according to their expertise, and satisfy unanimity. Secondly, aggregation of ratio judgements is better suited to control and mitigate possible manipulations such as rank reversal than aggregation of absolute judgements, which can improve legal and ethical aspects of decisions. We present an intuitive sufficient condition that, if met, avoids any rank reversal by adding or deleting alternatives. We also present a sufficient condition for preventing rank reversal for marginalisation, roughly speaking, when initially separated alternatives are transformed into a single alternative using disjunction. ",Defending the Weighted Geometric Mean Aggregation,"[76981, 76984]",906,"[77, 27, 41]",1038,"How to support complex decisions. Negotiating the trade-off between Social, Environmental and Economic values 3",44,7,47,Multiple Criteria Decision Analysis,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,50 [building - 324],"['Multi-Objective Decision Making', 'Decision Theory', 'Ethics']",TA-47
"In this work, we introduce a generalization of the Vehicle Routing Problem for a specific application in the monitoring of a Water Distribution Network [WDN]. In this problem, multiple technicians must visit a sequence of nodes in the WDN and perform a series of tests to check the quality of water. Some special nodes [i.e., wells] require technicians to first collect a key from a key center. The key must then be returned to the same key center after the test has been performed, thus introducing precedence constraints and multiple visits in the routes. To solve the problem, a Mixed Integer Linear Programming model and an Iterated Local Search have been implemented. The efficiency of the proposed methods is demonstrated by means of extensive computational tests on randomly created and real-world instances.",Solution of a practical Vehicle Routing Problem for monitoring Water Distribution Networks,"[62157, 76986, 7965, 76985]",204,"[14, 151, 145]",1041,Vehicle routing I,64,4,29,Combinatorial Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,157 [building - 208],"['Combinatorial Optimization', 'Practice of OR', 'Vehicle Routing']",MC-29
We consider the energy planning problem of a heating plant that serves to heat a set of houses [flats]. Each flat served by the plant has certain temperature requirements throughout the day. These flats are heated by radiators that are fed by hot water distributed from and heated in the heating plant. The plant can remotely control hot water flow rates through these radiators. For each flat the plant can choose among a discrete set of flow rates. The plant can create energy by running three different boilers with different capacities and running costs. The problem is to create and energy plan with minimum cost to satisfy temperature requirements of flats. We analyze the problem within the context of economic lot sizing problem and present solution approaches to some special cases.,Energy Requirements Planning Problem  ,"[63001, 53000]",809,"[12, 37, 105]",1042,Lot-sizing with energy aspects,32,8,49,"Lot Sizing, Lot Scheduling and Production Planning","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,M1 [building - 101],"['Capacity Planning', 'Energy Policy and Planning', 'Production and Inventory Systems']",TB-49
"Due to the complexity of shop scheduling problems, research over the past three decades has mainly focused on the development of improvement heuristics, while construction heuristics are often neglected. Sometimes low-performing construction heuristics are deliberately used to demonstrate the robustness of the improvement heuristics. Yet, effective and efficient construction heuristics play a crucial role in solving large practical problems.
Most of the constructive heuristics in the literature are based on priority rules. Another type of heuristic is based on successive insertions of operations in partial schedules. Only a few, powerful insertion-based heuristics are known in the literature, especially for the flow shop and job shop scheduling with makespan minimization. However, in addition to being tailored to a specific problem, compared to priority-based heuristics, these methods suffer from a high computational cost if not carefully designed.
In this work, we show that it is possible to have a generic insertion-based construction heuristic that can solve any shop scheduling problem with regular objective function and for which an acyclic directed graph with nonnegative weights can represent a solution. The efficiency is achieved through novel procedures quickly evaluating the feasibility and quality of the insertion moves.  Results on the flexible job and flow shop instances and their extensions [e.g., multiple modes and nonlinear routes] will be presented. ",A generic and efficient insertion-based heuristic for shop scheduling problems,[76989],931,"[129, 69, 53]",1045,Job shop scheduling,35,13,60,Project Management and Scheduling,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S09 [building - 101],"['Scheduling', 'Manufacturing', 'Graphs and Networks']",WB-60
"In this talk, we show how to combine the stochastic Polyak step size with established practical techniques such as regularization and momentum.
In particular, we derive a proximal version of the Polyak step size, and a momentum version which we call MoMo.
MoMo can be seen as an adaptive learning rate for SGD with momentum; in fact we can derive a MoMo version of any momentum method, most importantly MoMo-Adam. 
These derivations are possible through the connection between the Polyak step size and model-based stochastic optimization, where the model is truncated at a known lower bound.
In machine learning, such lower bounds are typically known. By construction, our new adaptive learning rates reduce the amount of learning-rate tuning, which is demonstrated through deep learning experiments on the CIFAR, Imagenet, Criteo, and IWSLT14 dataset.",On the Stochastic Polyak Step Size for Machine Learning - Proximal and Momentum Versions,[76992],360,"[66, 136, 81]",1048,Adaptive and Polyak step-size methods,84,12,32,Advances in large scale nonlinear optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,41 [building - 303A],"['Machine Learning', 'Stochastic Optimization', 'Non-smooth Optimization']",WA-32
"This study addresses the limitations of existing intensive care unit [ICU] triage policies by developing an algorithmic model for admissions/discharges during times of intense demand in a pandemic. Embracing a multi-value ethical framework, our model aims to maximize aggregated benefits while considering the principles of fairness. The proposed model accounts for the illness pathways of individual patients, the overall mixture of patient profiles, and the uncertainty in the number of future patients. We introduce a cost for early discharges to account for psychological and ethical complexity and build a discrete time Markov Chain from actual patient data to capture disease progression of ICU patients. We conduct a survey among ICU physicians to estimate short-term benefits of intensive care. The problem is formulated as a discrete Markov Decision Process. To address the computational complexity, we relax the capacity constraint and formulate an alternative problem for each single patient. We characterise the optimal admission and discharge policy for individual patients, which allow us to develop a heuristic policy for the original problem. The performance of the proposed policy and alternative benchmarks are evaluated in a comprehensive simulation study. Our results reveal the scale of impact possible through appropriate clinical decision-making regarding patient admission and discharges, demonstrating potential benefits in both lives and life-years saved. ",Admission and early discharge policies for ICU patients during times of intense demand in a pandemic,"[66648, 16885, 74244, 62884, 45846]",595,"[56, 135, 108]",1051,COVID-19,3,13,15,OR in Health Services [ORAHS],"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,18 [building - 116],"['Health Care', 'Stochastic Models', 'Programming, Dynamic']",WB-15
"In collaborative human-robot order picking systems, human pickers and Autonomous Mobile Robots [AMRs] travel independently through a warehouse and meet at pick locations where pickers load items onto the AMRs. In this paper, we consider an optimization problem in such systems where we allocate pickers to AMRs in a stochastic environment. We propose a novel multi-objective deep reinforcement learning [DRL] approach to learn good allocation policies to maximize pick efficiency while also aiming to improve workload fairness amongst human pickers. In our approach, we model the warehouse states using a graph, and define a network architecture that captures regional information and extracts information from efficiency and workload features effectively. We develop a discrete-event simulation model, which we use to train and evaluate the proposed DRL approach. In the experiments, we demonstrate that our approach can find non-dominated policy sets that outline good trade-offs between fairness and efficiency. The trained policies outperform the benchmarks in terms of efficiency and fairness, and moreover, they show good transferability properties when being tested with different scenarios in different sizes of the warehouse.",Learning Efficient and Fair Policies for Collaborative Human-Robot Order Picking,"[76939, 79190, 51230]",312,"[8, 14, 77]",1057,[Deep] Reinforcement Learning for Combinatorial Optimization 1,14,4,03,Data Science Meets Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,1005 [building - 202],"['Artificial Intelligence', 'Combinatorial Optimization', 'Multi-Objective Decision Making']",MC-03
"Energy storage systems emerge as key elements in optimizing sustainable resource use, ensuring a steady flow of energy even when primary sources, such as sun or wind, are intermittent. In this scenario, the techno-economic integration of storage technologies becomes a key building block in the transition to a more resilient and efficient energy system. In this paper, we considered the possibility of using the electricity storage as a business, buying electric energy from the Italian electric day-ahead market when prices are low, and selling it back when prices are high. 
The theoretical modeling and the resulting simulations show that the storage can actually be a profitable business, and that its results are highly variable over time. In particular, the recent gas prices crisis changed the framework, showing very interesting revenue opportunities. 
",Is storage a business? A test on the Italian day-ahead electricity market,"[76958, 77055, 77068]",437,"[36, 45, 131]",1058,New Advances in Italian Energy Markets,4,12,02,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,Glassalen [building - 101],"['Electricity Markets', 'Financial Modelling', 'Simulation']",WA-02
"Traffic congestion imposes a huge economic loss. As such, there has been a huge effort to understand congestion using theoretical models. The dynamic model that gained most attention is the deterministic fluid queuing model.
A common drawback of most models is that users only aim for minimizing their arrival time. However, users are not always that single-minded. We extend the state-of-the-art game theoretic traffic models with a multi-criteria objective function. We assume that users try to minimize costs subject to arriving at the sink before a given deadline. Here, costs could be thought of as an intrinsic preference regarding different route choices and queuing dynamics only play a role for the arrival time.
We determine the existence and structure of Nash flows over time and fully characterize the price of anarchy for this model, which measures the ratio of the quality of the Nash flow and the optimal flow. We evaluate the quality both with respect to the throughput for a given deadline and the makespan for a given amount of flow. We prove the following results. [1] In series-parallel graphs, both prices of anarchy are unbounded. [2] In parallel path graphs the throughput-PoA is at most 2, or at most e/[e-1] if all transit times are 0. Both bounds are tight. [3] In parallel path graphs the makespan-PoA is at most e/[e-1], independent of transit times, and this is tight. All our upper bounds are also valid for dynamic equilibria in the deterministic fluid queuing model.",Bicriteria Nash Flows over Time,"[45060, 45274, 67699]",620,"[150, 50, 77]",1060,"Game Theory, Solutions and Structures V",88,7,36,"Game Theory, Solutions and Structures","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,32 [building - 306],"['Network Flows', 'Game Theory', 'Multi-Objective Decision Making']",TA-36
"Infectious diseases are a significant contributor to death rates. Furthermore, they have a significant effect on the economy due to increased public health costs and loss of income. Earlier identification of pathogens through testing can help achieve outbreak prevention. However, poor rural areas have limited access to health facilities, which causes a large delay between the emergence and the identification of a disease. Mobile laboratories can help identify outbreaks in such areas as they allow for fast preventive testing at different locations. However, the benefits of a mobile laboratory might not outweigh the benefits obtained from directly investing the money in the local health facilities. Therefore, we analyse the trade-off between sending mobile laboratories and investing in local health facilities such that outbreaks are prevented.",Analysing the Potential of Mobile Laboratories To Prevent Outbreaks of Infectious Diseases,"[67230, 76997, 776]",554,"[58, 0]",1062,Infectious diseases and pandemics,38,13,21,OR in Humanitarian Operations [HOpe],"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,49 [building - 116],['Humanitarian Applications'],WB-21
"The Hierarchical Directed Capacitated Arc Routing Problems [HDCARP] is a variant of the Capacitated Arc Routing Problems [CARPs], in which the arcs in a graph are partitioned into priority classes. However, unlike traditional CARPs that aim to minimise total time, the HDCARP focuses on minimizing the maximum completion time of each priority class in a hierarchical fashion. Practical applications of the HDCARP include snow plowing, salt spreading, street cleaning, and waste collection. In this study, we explore two variants of the HDCARP. The key difference between these variants lies in the consideration of precedence relations between classes within routes. We propose MILP formulations and matheuristics for both HDCARP variants. The MILP formulations enable us to find optimal solutions for small-scale instances and evaluate the quality of matheuristics. Our matheuristics are based on decomposing the problem into multiple sub-problems, resulting in faster running time for large-scale instances. We conduct extensive computational experiments to assess the performance of these approaches and present our findings.",On the Hierarchical Directed Capacitated Arc Routing Problem,"[67174, 67636, 76998, 32415]",217,"[145, 14, 111]",1063,Routing for hybrid fleets of vehicles,64,10,26,Combinatorial Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,012 [building - 208],"['Vehicle Routing', 'Combinatorial Optimization', 'Programming, Mixed-Integer']",TD-26
"While deep-sea vessels play an important role for the revenue streams of container terminals, barges and feeders play an important role in ensuring the functioning of the transshipment and hinterland transportation operations. Besides, the roles of feeders and barges are further leveraged as a cost and emission-efficient alternative to long-haul trucking. These different classes of vessels compete for the same berth space in container terminals. The terminal operators conventionally have the tendency to fully prioritize deep-sea vessels. However, with the increasing incentives and regulatory pressure for the increase of barge and feeder throughput, some terminal operators started to allocate dedicated berths for barges and feeders. In this paper we will study the berth allocation problem in order to find a pareto-optimality frontier that balances barge throughput and deep-sea vessel waiting times. Barges and feeders which are of smaller volume, occupy a smaller space and have a shorter expected handling time. By using analytical methods from Queuing Theory, we will first demonstrate the pareto-efficiency of different heuristics of berth allocation policies, i.e. dedicated terminal spaces, barge limits, motivated by the current industry practices. Based on the insights that we receive from analytically tractable systems, we will design a Markov Decision Process to propose a better pareto-optimality frontier with dynamic policies.
",Dynamic Berth Allocation Policies at Deep-sea Terminals,"[76999, 66697, 1024, 70286]",197,"[143, 135, 65]",1066,Combinatorial Optimization models and applications in Logistics and Transportation I,64,2,29,Combinatorial Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,157 [building - 208],"['Transportation', 'Stochastic Models', 'Logistics']",MA-29
"A symmetry of a mixed-integer nonlinear program [MINLP] is a bijective map that transforms feasible solutions into feasible solutions while preserving the objective value. Already for linear problems it is well known that symmetries can negatively impact the performance of branch-and-bound methods, since symmetric regions of the search space will be repeatedly explored without providing new information. Most of the existing approaches for detecting symmetries focus on permutation symmetries that exchange the order of entries in a solution vector. For many problems classes, however, permutation symmetries do not capture all symmetries. For example, when packing objects into rectangular containers, also reflection symmetries of the container can be taken into account.

In this talk, we present a novel mechanism for computing, next to permutation symmetries, also reflection symmetries of MINLPs. As existing approaches for permutation symmetries, we introduce a suitable graph whose automorphisms correspond to symmetries of a MINLP. Since reflection symmetries may nontrivially interact with nonlinear functions of a MINLP, however, our graph is much more involved than the previously known symmetry detection graphs. We also briefly discuss generalizations of state-of-the-art symmetry handling methods to reflection symmetries. The talk is concluded by numerical experiments showing the effect of handling reflection symmetries in the MINLP solver SCIP.",Detecting and Handling Reflection Symmetries in MINLP,[55298],702,"[72, 111, 113]",1069,Specialized Optimization Algorithms,76,13,30,Software for Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,53 [building - 208],"['Mathematical Programming', 'Programming, Mixed-Integer', 'Programming, Nonlinear']",WB-30
"We extend the Gibson-Schwartz [1990] and Schwartz-Smith [2000] models to include stochastic volatility and correlation based on the generalized Wishart variance-covariance matrix process. We study European option pricing via characteristic functions. Furthermore, we perform parameter estimation via maximum likelihood based on the state space formulation of the models.",Revisiting the Gibson-Schwartz and Schwartz-Smith Commodity Models,"[75681, 51622]",146,"[45, 126, 135]",1073,Modelling commodity markets dynamics,74,2,57,Modern Decision Making in Finance and Insurance,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S06 [building - 101],"['Financial Modelling', 'Risk Analysis and Management', 'Stochastic Models']",MA-57
"In 2018, the EU issued a Directive on Renewable Energy Communities [REC]. REC implementation will increase RES-based electricity supply and decrease GHG emissions.
National Governments are financing ecological transition with incentives, and a relevant amount of funds is reserved for REC. Anyway, the success of these investments depends on people's, firms' and municipalities' choice to join a REC. A direct economic advantage of joining a REC is that members can share an incentive tariff calculated on self-produced and self-consumed electricity.
In our work, we face the problem of maximizing self-consumption to promote RECs diffusion. For this purpose, we develop a bi-level problem with a policymaker and one REC. We consider a particular kind of REC composed of households [HS], that can install photovoltaic plants [PV], and a biogas plant [BG], because these are the most common technologies in agricultural and urban contexts. 
At the upper level, the central authority maximizes REC’s self-consumption by financing investment in RES plants. At the lower level, the BG and HS interact in the same REC, maximizing their profits subject to capacity and budget constraints. 
We developed two versions of this model to have results independent from the iteration order and to reach a Nash Equilibrium. Results are obtained through historical scenario simulation using Italy’s data from 2017 to 2021 and developed in Pyomo with Gurobi Optimizer.
",Efficient Incentive Policies of Renewable Energy Communities,"[77002, 2991, 55098]",437,"[50, 36]",1074,New Advances in Italian Energy Markets,4,12,02,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,Glassalen [building - 101],"['Game Theory', 'Electricity Markets']",WA-02
"We consider singular control in inventory management under Knightian uncertainty, where decision-makers have a smooth ambiguity preference over Gaussian-generated priors. We demonstrate that continuous-time smooth ambiguity is the infinitesimal limit of Kalman-Bucy filtering with recursive robust utility, heuristically proposed by Hansen and Sargent [Journal of Economic Theory, 146[3]:1195–1223, 2011]. Additionally, we prove that the cost function can be determined by solving forward-backward stochastic differential equations with quadratic growth. Using variational inequalities in a viscosity sense, we derive the value function and optimal control policy. By a coordinated transformation, we simplify the problem into two-dimensional singular control, offering insights into model learning and aligning with classical singular control free boundary problems. This study marks the first attempt, to our knowledge, to integrate singular control with smooth ambiguity.",Singular Control in Inventory Management with Smooth Ambiguity ,"[77003, 61552]",292,"[61, 25, 127]",1075,Dynamics of the Firm I,90,3,33,Optimal Control Theory and Applications,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,42 [building - 303A],"['Inventory', 'Decision Analysis', 'Robust Optimization']",MB-33
"Global freight transportation relies heavily on maritime shipping using containers. To avoid congestion and disruptions in supply chains, port terminals need to plan their operations carefully to make sure ships are loaded and unloaded on time.
At a terminal, it is common that some containers need to be transshipped between vessels. A missed connection requires finding alternative routes and can have a significant negative impact on delivery times and customer satisfaction. However, forcing a planned transshipment to happen under unfavorable circumstances can be very expensive if it affects other vessels calling the port, while allowing it to be missed and using alternative routes could be done at low cost. Therefore, terminal operators are frequently facing trade-offs that are challenging to arbitrate manually due to the scale of operations. In this context, the use of OR-based decision-support tools can be of great help.
We present a mathematical model of a problem where the objective is at the same time to 1] find a berthing and quay crane allocation plan for the vessels calling a terminal over a planning horizon and 2] assign an alternative route to each container that misses a transshipment, at a minimum total cost.  We call it the Berth Allocation and Quay Crane Assignment Problem with Missed Transshipment Recovery. Besides a MIP formulation, we present resolution methods for a set of instances derived from industrial cases and provide insights about the solutions",An integrated Berth Allocation and Quay Crane Assingment Problem with Missed Transshipments Recovery,"[71056, 31857, 31819, 53611]",82,"[70, 111, 65]",1076,Seaside Planning III,52,7,62,OR in Port Operations,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S12 [building - 101],"['Maritime applications', 'Programming, Mixed-Integer', 'Logistics']",TA-62
"Alongside the practice of offering new and remanufactured products through distinct marketing channels, manufacturers have implemented diverse internal channel structures upon establishing a dedicated remanufacturing division. Some firms, like Caterpillar and Volvo Group, opt for a decentralized structure by delegating remanufacturing-related decisions to the dedicated division. In contrast, companies like General Electric and John Deere choose to retain decision-making at the firm level, encompassing both manufacturing and remanufacturing, following an integrated structure. Motivated by the observed industrial practices, we analytically investigate the manufacturer’s internal channel structure decision within a manufacturer-retailer supply chain. Using a game theoretic model, our analysis demonstrates that while the decentralized structure encourages manufacturers to enter remanufacturing, the optimal approach is for manufacturers to choose the integrated structure when the production costs of new products are moderate. In contrast, when the production costs of new products are polarized [i.e., either low or high], it is advantageous for decision-making authority to be delegated to the remanufacturing division. A triple-win outcome for the manufacturer, retailer, and remanufacturing division is unattainable because the retailer can never benefit from the manufacturer's decisions made under the decentralized structure. ",Delegate or Not? When Divisional Conflict Exists in Remanufacturing Operations,"[77004, 77789]",925,"[138, 125, 139]",1077,Game theory for the circular economy,18,12,23,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,82 [building - 116],"['Supply Chain Management', 'Reverse Logistics / Remanufacturing', 'Sustainable Development']",WA-23
"We study the procurement decisions of a trade agent - The agent chooses a bid [unit price to pay] to procure the goods available from seller[s]. If the agent wins the bid, the supply is used to meet the buyer’s demand. The trade agent’s single-period, single-product problem is a new type of newsvendor problem. We analyze the agent’s optimal bid for a seller with yield uncertainty. We show that the bid outcome distribution needs to satisfy an easy-to-check condition but no conditions on the yield distribution are needed for a unique optimal bid to exist. We also show that the expected sales-to-supply ratio that measures scarcity affects the optimal bid. We investigate the monotonicity of the optimal bid with respect to economic parameters, demand, and distributions of bid outcome and yield. The agent’s problem with multiple sellers is also a novel newsvendor network problem. For the two-seller case, we show when diversification is optimal for the agent. We show that working with both sellers may not always be optimal despite the opportunity for risk pooling and bidding only at the unreliable seller may be optimal even when the other seller is reliable.",B2B Procurement - Bidding Decisions of a Newsvendor,"[38335, 77047]",795,"[61, 105, 135]",1079,Inventory Models,50,9,39,Stochastic Modelling,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,35 [building - 306],"['Inventory', 'Production and Inventory Systems', 'Stochastic Models']",TC-39
"In financial markets simple portfolio strategies often outperform more sophisticated optimized ones. E.g., in a one-period setting the equal weight or 1/N-strategy often provides more stable results than mean-variance-optimal strategies. This is due to the estimation error for the mean and can be rigorously explained by showing that for increasing uncertainty on the means the equal weight strategy becomes optimal, which is due to its robustness. In earlier work, we extended this result to continuous-time strategies in a multivariate Black-Scholes type market. To this end we derived optimal trading strategies for maximizing expected utility of terminal wealth under CRRA utility when we have Knightian uncertainty on the drift, meaning that the only information is that the drift parameter lies in an uncertainty set. The investor takes this into account by considering the worst possible drift within this set. We showed that indeed a uniform strategy is asymptotically optimal when uncertainty increases. In this talk we focus on a financial market with a stochastic drift process and possibly uncertainty. We combine the worst-case approach with filtering techniques by defining an ellipsoidal uncertainty set based on the filters. We demonstrate that investors need to choose a robust strategy to profit from additional information.",Robust Utility Maximization in Continuous Time - Filtering for Shaping the Uncertainty Sets,[77006],126,"[136, 45, 127]",1083,"Dynamic portfolio selection - stochastic optimization, filtering, and learning techniques",74,4,57,Modern Decision Making in Finance and Insurance,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S06 [building - 101],"['Stochastic Optimization', 'Financial Modelling', 'Robust Optimization']",MC-57
"My name is Thu Huong Dang, and I am currently a lecturer at Lancaster University.

My doctoral research, which received partial sponsorship from Routeware, Inc., was concerned with addressing Combinatorial Optimization Problems [COPs] arising in urban waste collection and recycling.

To date, I have authored several papers on Vehicle Routing Problems and Flowshop Scheduling Problems. I am honored to have received the YoungWomen4OR award from the WISDOM forum within EURO and the Kingsman prize from the Management Science department for being recognized as the best doctoral researcher in the period 2022-23.

Now, my primary focus lies in tackling various COPs, where I explore exact methods to achieve proven optimality and heuristic methods for addressing real-world challenges. Over the next few years, I am contemplating expanding my research to include interdisciplinary topics, such as the use of AI/machine learning in addressing COPs.",A biography,[67174],460,"[14, 0]",1086,YW4OR_4,39,15,12,WISDOM - Women in OR,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,13 [building - 116],['Combinatorial Optimization'],WD-12
"Given a weighted complete undirected graph with weights on the edges and a positive integer p, the symmetric Hamiltonian p-Median Problem [HpMP] is to find a minimum weight set of p elementary cycles partitioning the vertices of the graph. We focus on a variant of the HpMP [the HpMP>=] where solutions with strictly more than p cycles are also feasible.

We present new extensions of known node-depot assignment [NDA] compact formulations. NDA models are characterized by the inclusion of NDA variables, which are 1 if and only if node d acts as the depot of the cycle node i is in [or alternatively, if node i is assigned to node d], in addition to edge variables, which are 1 if and only if the corresponding edge is in some cycle. We extend these formulations with edge-depot assignment [EDA] variables, which are 1 if and only if node d acts as the depot of the cycle edge {i,j} is in. In this talk, we:

1 - Propose new formulations which use the EDA variables;
2 - Relate these models with formulations including only the edge and NDA variables;
3 - Present new inequalities in the space of the edge and NDA variables that are derived from the new models using EDA variables;
4 - Present computational results obtained with the new models.

Preliminary computational results show that the new models produce very strong LP relaxation bounds and, when used in combination with a modern ILP solver, allow for the resolution of instances with over one-hundred nodes.",Extensions of Node-Depot Assignment Formulations for the Hamiltonian p-Median Problem,"[72478, 5319]",195,"[14, 111, 145]",1088,Combinatorial optimization topics in transportation,64,2,26,Combinatorial Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,012 [building - 208],"['Combinatorial Optimization', 'Programming, Mixed-Integer', 'Vehicle Routing']",MA-26
"Rail defects pose a significant threat to railway safety and efficiency. Probabilistic modeling of defect propagation has the potential of improving decision-making for circumvention of dangerous rail degradation.

We propose a continuous-time Markov chain with transition rates regressed on location-dependent covariates to model discretely observed degradation trajectories discovered at the Norwegian rail network. We propose two estimation approaches. The first approach obtains the full data log-likelihood by Monte Carlo simulation of full data defect trajectories, which informs the Expectation-Maximization algorithm. The second approach maximizes the discrete data log-likelihood informed by analytical gradient information.

Both methodologies give rise to fast convergence of model estimates with similar numerical values. Moreover, the estimated coefficients for the covariates are found to be statistically significant and in directional accordance with their physical interpretation. Model checking suggests a robust framework that describes enough variance to satisfactorily account for the structural variability between the rail lines.

We apply the proposed model setting to produce probabilistic forecasts of a defect's growth based on its location in a railway network. The probabilistic setting allows for straightforward scenario generation constituting useful input to decision-making models under uncertainty for maintenance planning.",Markovian Degradation Modeling of Rails,"[47580, 77010]",502,"[135, 131, 122]",1089,Stochastic Modelling,47,5,40,Advances in Stochastic Modelling and Learning Methods,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,96 [building - 306],"['Stochastic Models', 'Simulation', 'Railway Applications']",MD-40
"IoT devices are often present around business processes [BPs], collecting data about parameters that have an impact on the process. E.g., the pressure in a tank can be a relevant aspect of the execution of a BP. 

Trace clustering [TC] is used to group similar process instances, usually based on their sequential activity patterns. TC is often used to produce models describing different process variants discovered from separate clusters. However, TC can also be applied for exploratory data analysis [EDA].

This is particularly interesting in the context of multi-perspective BPs, which are surrounded with IoT devices. Multi-perspective TC [MPTC] can be used to group together cases with similar activity sequences and sensor data. However, explaining these clusters to process users then becomes more challenging.

To shed some light in the results of MPTC, we propose to use various complementary explainability approaches:
1] Partial dependency plots - they give a visual representation of the contribution of each perspective to the TC, and help users explore their clusters.
2] Permutation feature importance - give a more quantitative measure of the impact of each perspective, on the global TC.
3] Prototypes and criticisms - Show concrete instances and actively try to challenge TC results, showing the robustness of the clusters.

Each of these approaches focuses on a particular aspect of the MPTC, and supports a better understanding of the sources of variation of the BP.",Approaches for multi-perspective trace clustering explainability,"[77011, 68588, 71151]",935,"[8, 0]",1093,Advances in Process Mining,15,8,27,Mathematical Optimization for XAI,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,047 [building - 208],['Artificial Intelligence'],TB-27
"This study comprehensively compares stochastic programming and robust optimization approaches for handling long-term uncertainty in the optimal sizing and placement of distributed energy resources [DERs] in LV distribution networks. The study covers various optimization models, ranging from a deterministic model to a two-stage stochastic programming approach, single-stage robust optimization, adaptive robust optimization [ARO], and a hybrid variant of stochastic robust optimization, considering the electricity load and power generated by PV systems as random variables. A decomposition method approach has been used to address the three levels raised by robust optimization using primal and dual cuts. The models have been tested using a modified version of the IEEE 33 bus system, comparing mainly the ARO with the hybrid models because the hybrid model allows the handling of PV generation through a stochastic approach and the electricity demand by a robust method, which could be more suitable to represent their uncertainty in long-term scenarios. Thus, the results show that the hybrid approach i] reduces the convergences times, ii] reduces around 80\% on average the number of iterations regarding the ARO for different Budget of Uncertainty [BoU] levels, and iii] delivers a notably less conservative solution than the ARO.",Robust Optimization for Long-term Uncertainty in DER Sizing and Placement in Distribution Networks,"[71351, 70098, 71381, 71146]",857,"[93, 136, 127]",1096,OR in Energy III,23,15,19,OR in Energy,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,44 [building - 116],"['OR in Energy', 'Stochastic Optimization', 'Robust Optimization']",WD-19
"Mexican energy pool has a big dependence on natural gas, most of which is imported from the United States of America [USA]. In this context a reliable, updated and adaptable tool to forecast natural gas demand in the medium term is required to support the Mexican government making decision process. There are now few studies related to natural gas demand forecast in Mexico, but they are either not updated in terms of historical data and variables used as inputs or they do not use the latest known methods such as Neural Networks [NN] losing the opportunity to leverage most recent forecasting techniques.
This study explores natural gas demand forecasting in Mexico using machine learning algorithms to support the energy transition. We compare time series models ARMA & ARIMA and Long Short Term Memory NN based on historical demand data from January 2005 to Nov 2023. Mean Absolute Percentage Error [MAPE], Root Mean Square Error [RMSE], and Akaike Criteria [AIK] reveal each method's strengths and weaknesses depending on data characteristics, like the presence of outliers. Our contribution lies in applying advanced machine learning to Mexican gas demand forecasting, expanding on existing research predominantly focused on countries like China, India, Turkey, and the USA. This work provides valuable insights for stakeholders navigating the energy transition in Mexico, helping optimize infrastructure planning, resource allocation, and energy security.",Natural gas demand forecasting in Mexico with machine learning algorithms to support making decision process during energy transition,"[77013, 37306]",518,"[37, 5, 53]",1100,"Advancements of OR-analytics in statistics, machine learning and data science 9",16,13,28,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,065 [building - 208],"['Energy Policy and Planning', 'Algorithms', 'Graphs and Networks']",WB-28
"The high short-term volatility of renewable energies raises supply security issues that can be addressed through energy storage technologies. The idea of introducing a long term energy storage market is the solution for two problems in the investment of storage technologies - high costs and high volatility of energy prices. The main goal of this paper is the modeling and solution of a profit maximization problem for a risk-averse energy producer, who owns an hydroelectric pumping storage facility, and aims to seize the opportunity to finance the investment through an energy storage market. We modeled the facility owner's decisions through an optimization problem. The owner profits from both selling storage capacity to a counterpart and participating in speculation within the energy spot markets and ancillary services. Our contribution to literature lies in the joint optimization of generation/pumping and the decision sell part of the storage capacity. One of the main profit opportunities for the plant owner is to fulfill customer orders through accounting rather than physically. This introduces a significant complication - it is necessary to consider water movements from both an accounting and a physical perspective. The decision maker risk’s aversion has been formulated including the Conditional Value-Risk as a risk measurement in the objective function.",Optimizing hydro-pumping plants management in the new storage market,"[77000, 2991, 27108]",437,"[83, 93, 94]",1102,New Advances in Italian Energy Markets,4,12,02,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,Glassalen [building - 101],"['Optimization in Financial Mathematics', 'OR in Energy', 'OR in Environment and Climate change']",WA-02
"Instant delivery services gained popularity for delivering meals to city dwellers, and the range of products available for delivery has significantly expanded. These platforms utilize crowdsourced couriers who operate their own vehicles, leading to a diverse courier fleet. In the face of great competition to attract and retain both customers and couriers, it is crucial for companies to develop advanced decision support tools. Such tools must be capable of generating real-time assignments that fulfill the expectations of service providers, customers, and couriers. To address the dynamic and complex nature of matching orders with couriers, considering the varied fleet and specific order-vehicle requirements, a time-driven simulation-optimization decision support tool was created. This tool is designed to manage incoming orders, couriers' availability, real-time courier location updates, and factors like dynamic traffic congestion and different regional speed limits for different vehicle types. The efficacy of the developed tool was validated through tests using instances from existing literature, where it demonstrated its capacity to effectively tackle the assignment problem. Furthermore, when applied to a real-world case study, the proposed tool achieved a reduction of roughly 4.5% in total delivery times—from the moment an order is placed to its arrival at the customer—compared to the original assignments.",Optimizing the dynamic heterogeneous order-courier assignment problem for instant deliveries,"[75658, 77015, 59022]",588,"[65, 109, 131]",1104,Crowdsourcing Logistics,6,8,56,Transportation,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S04 [building - 101],"['Logistics', 'Programming, Integer', 'Simulation']",TB-56
"Recent techno-economic studies have investigated procurement costs of hydrogen and related derivatives on various international trade routes. However, strategic behavior of exporters has rarely been considered in this context, despite similar behavior frequently observed in the fossil fuel world and market characteristics indicating some potential. This work introduces a novel techno-economic model of oligopolistic trade tailored around the value chain of potential future global hydrogen markets. It is formulated as a mixed complementarity problem with a convex reformulation, allowing to solve both competitive and oligopolistic equilibria much beyond the capabilities of traditional complementarity models. Illustrative results show, that assuming perfectly competitive price formation in early phases of future global hydrogen markets may lead to overly optimistic price projections, especially within Europe, Japan and South Korea. In extreme cases, hydrogen prices may increase to ∼200 €/MWh by 2030, compared against a competitive baseline of ∼150 €/MWh. In contrast, prices may decrease by ∼25 €/MWh in exporting regions, such as Northern Africa or South-Eastern America.
",A Multi-Commodity Partial Equilibrium Model of Imperfect Competition in Future Global Hydrogen Markets,[77016],631,"[37, 93]",1105,Game Theoretic Market Equilibrium Modelling,22,8,09,Energy Markets,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,10 [building - 116],"['Energy Policy and Planning', 'OR in Energy']",TB-09
"Approximately 10% of the global population lacks access to electricity, with a significant portion residing in rural areas, underscoring the importance of rural electrification efforts. In this study, we focus on determining the optimal technology choice and network design for rural electrification at macro level. To this end, we propose a series of MILP models that minimize costs by strategically allocating settlements to off-grid, grid, or mini-grid options. Furthermore, we introduce multi-stage versions of these models to guide optimal investment decisions over time, addressing the practical challenge of roll-out electrification plans.  Through real-life and synthetic instances, we demonstrate the potential cost advantages of our proposed enhancements.",BRIDGING THE GAP - MULTI-STAGE OPTIMIZATION FOR ELECTRICITY ACCESS,"[76682, 2646, 43460]",712,"[37, 79, 84]",1108,Empowering Energy Access,21,15,22,Energy Management,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,81 [building - 116],"['Energy Policy and Planning', 'Network Design', 'Optimization Modeling']",WD-22
"Dropping out of university degrees is a problem with a considerable negative impact not only academically but also economically. The objective of this work is to design a multicriteria model that allows classifying students based on the risk of dropping out of the degree they are pursuing after the first year. Ordered categories of higher to lower risk are generated using the Global Local Net Flow Sorting [GLNF Sorting] algorithm, and the quality of student assignments to a risk group is evaluated using the Silhouette for Sorting [SILS] quality indicator. To quantify the risk of dropping out, criteria both before access to higher education and related to the context and results of students in the first semester have been used. The model is validated with real data from students in various degrees at Complutense University of Madrid. The obtained classification is compared with those obtained using traditional classification techniques and machine learning. ",Classification of Students Based on the Risk of Dropping out of University Degrees ,"[31644, 77019, 73898]",100,"[77, 126, 66]",1109,MCDA applications in Engineering and Management 1,44,2,47,Multiple Criteria Decision Analysis,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,50 [building - 324],"['Multi-Objective Decision Making', 'Risk Analysis and Management', 'Machine Learning']",MA-47
"Understanding the geometric properties of gradient descent dynamics is a key ingredient in deciphering the recent success of very large machine learning models. A striking observation is that trained over-parameterized models retain some properties of the optimization initialization. This “implicit bias” is believed to be responsible for some favorable properties of the trained models and could explain their good generalization properties. In this talk, I will first rigorously expose the definition and basic properties of “conservation laws”, that define quantities conserved during gradient flows of a given model [e.g. of a ReLU network with a given architecture] with any training data and any loss. Then I will explain how to find the exact number of independent conservation laws by performing finite-dimensional algebraic manipulations. In the specific case of linear and ReLu networks, this procedure recovers the conservation laws known in the literature and shows that there are no other laws.",Conservation laws for gradient flows,[77018],338,"[66, 20, 31]",1111,Algorithms for machine learning and inverse problems - optimisation for neural networks,84,10,32,Advances in large scale nonlinear optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,41 [building - 303A],"['Machine Learning', 'Control Theory', 'Dynamical Systems']",TD-32
"Problem Definition - We study the finishing stage [weeks 23-26 in the pig’s life] planning problem of a pork producer who gets to see how market-ready hogs he has available at the beginning of each week for sale and the current market prices. Then, he must decide how many hogs to sell through to a meatpacker and on the open market. The producer has a contract to deliver a fixed quantity to the meatpacker each week according to a predetermined price formula that depends on commodity indices. He pays a penalty if he fails to deliver. The numbers of hogs that become market-ready every week, all costs, and all prices are stochastic. 
Methodology -  We use a dynamic programming approach [optimal policy, a one period look ahead approximation].
Results - We show there are two types of thresholds in optimally making decision switches [holding to contract fulfilling, holding to spot selling], each one specific to the under-weight and the regular-weight hogs. The thresholds are sensitive to the number of available hogs and prices and not straightforward to derive. An approximate dynamic program preserves the optimal policy structure and offers a practical solution approach. 
",Managing Hog Farm Operations under Uncertainty - Inventory and Selling Strategies,[77021],591,"[61, 89, 108]",1114,OR in Livestock farming,20,7,12,OR in Agriculture and Forestry ,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,13 [building - 116],"['Inventory', 'OR in Agriculture', 'Programming, Dynamic']",TA-12
"Hybrid energy projects are attracting global interest due to the essential search for sustainable development, the alternative to oil price volatility, and the economic and environmental advantages of renewable energy sources. To contribute to filling the gap in the existing literature, which lacks an analysis considering the randomness of variables in a multiobjective optimization model, this study proposes a stochastic model for assessing the economic feasibility of a hybrid energy system comprising wind energy, photovoltaic generation, and battery energy storage. The model incorporates risks and uncertainties inherent in the critical variables associated with such investments. The multi-objective optimization employs the Design of Experiments with Response Surface Methodology and utilizes the Desirability function to optimize the process. As a result, to maximize the average and minimize the standard deviation of the Net Present Value, an optimal configuration was obtained, represented by the combination of a system with 92% wind energy, 230 kWh/month of demand, use of a lithium-ion battery in a total scenario. Given the specific parameters and assumptions applied in this study, it is concluded that there is economic feasibility of wind photovoltaic projects with energy storage in batteries in regions with highly favorable wind regimes.",Stochastic multiobjective optimization for hybrid distributed generation with battery storage systems,"[35633, 77023, 45944, 71319, 71321]",839,"[37, 83, 93]",1115,The role of storage in energy problems,23,8,19,OR in Energy,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,44 [building - 116],"['Energy Policy and Planning', 'Optimization in Financial Mathematics', 'OR in Energy']",TB-19
"Millets can significantly contribute to achieving certain Sustainable Development Goals [SDGs] namely SDG 2 [Zero hunger], SDG 3 [Good health and well-being], SDG 12 [Ensure sustainable production and consumption pattern], and SDG 13 [climate action]. Traditionally a part of the Indian food basket, millets lost their share drastically since the Green Revolution due to the incentivization of other food grains. However, due to increasing malnutrition and environmental degradation, the Government of India is undertaking numerous initiatives to promote large-scale consumption of millets. To ensure a sustainable supply of millets, it is important to tackle the consumption barriers faced by the masses. The present study utilizes a stated preference survey to identify the consumption barriers and understand respondents’ likeliness of increasing millet consumption if these barriers are eliminated or reduced. An online survey was conducted in March 2023 for the same. Based on the survey responses, consumer segmentation is done using non-parametric tests viz. Kruskal-Wallis and Mann-Whitney test followed by Post hoc Dunn Bonferroni’s test. The results indicate demographic subgroups that have significantly higher likeliness of increasing their millet consumption when different strategies are implemented. The results of the study can serve as a baseline for millet-specific industry players and help them develop marketing tools incorporating features that help eliminate the barriers.",Millets for Sustainability - Consumer Segmentation using Stated Preference Analysis,"[76996, 43014]",689,"[139, 7, 28]",1119,Sustainability in Consumer Systems & Industry,80,9,53,Sustainable and Resilient Systems,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,8007 [building - 202],"['Sustainable Development', 'Analytics and Data Science', 'Developing Countries']",TC-53
"The focus on sustainable health-oriented food choices has become significant on a global scale. The trend towards consuming organic and sustainable products has seen notable growth, particularly catalysed by the impact of the COVID-19 pandemic. Consumers are now willing to pay a premium for organic and sustainable foods. The promotion of these products has the potential to uplift the income of farmers and processors relying on these products, especially in developing nations like India. 
This work involves a case study in India, with the motivation of assessing consumers' subscription inclinations to natural sweetener [jaggery]. Jaggery prepared from sugarcane juice is an unrefined medicinal sugar with the potential to substitute white sugar across various applications. This survey-based study utilizes structural equation modeling to analyse the impact of utilitarian values, customer attitude, customer awareness, hedonic motives and perceived barriers on subscription intentions. 
The promotion of jaggery consumption not only aligns with India’s commitment to endorse indigenous products with the potential to create rural employment but also contributes to the global efforts towards achieving SDGs, particularly SDG1, SDG8 and SDG13. This research aims to develop a comprehensive subscription model that considers consumer preferences, pricing strategies, economic viability and other adaptive mechanisms to effectively scale this product based on local and global conditions.",Exploring Consumer Preferences for Subscription - Promoting Sustainable Food Alternative for Synergy between Local and Global Communities,"[77027, 43014]",708,"[139, 28, 137]",1121,"Sustainability and Equity in Ecosystems, Ecology and Food",80,14,53,Sustainable and Resilient Systems,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,8007 [building - 202],"['Sustainable Development', 'Developing Countries', 'Strategic Planning and Management']",WC-53
"In order to address the large calculation scale and slow route-planning speed problems caused by the large global aviation network and complicated restrictions, this paper proposed an improved algorithm for route-planning within hybrid restricted searching area. Firstly, a machine learning algorithm was used to optimize the major axis parameter of the ellipse in the ellipse restricted searching area algorithm by analyzing the historical flight plan data. Then, based on the rectangle restricted searching area algorithm and the ellipse restricted searching area algorithm, the proposed algorithm considered restrictions on the search direction to further reduce the search scale and improve the search efficiency. Experiments show that compared with ellipse restricted searching area algorithm, the hybrid restricted searching area algorithm reduces the search size by about 50% and the total execution time by 34% on average, which verifies the effectiveness of the method.",Civil aviation route-planning method based on improved restricted searching area algorithm,"[77024, 57851]",516,"[4, 66, 5]",1122,"Advancements of OR-analytics in statistics, machine learning and data science 6",16,9,28,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,065 [building - 208],"['Airline Applications', 'Machine Learning', 'Algorithms']",TC-28
"Modular bus systems recently attracted attention due to their potential ability increase the flexibility and efficiency in transit networks scheduling. In this work, we focus on the potential advantages of modular buses with respect to fixed-capacity buses. 
We develop a static optimization model to minimize the total number of modules flowing in a given network of bus lines. The model assumes that for a set of origin-destination pairs located at the stops of the bus network, a rate of transport demand is given and must be satisfied. Buses are formed by one or more connected identical modules. Modules can be attached/detached at the endpoints of a line and at intersections of lines. Empty modules can be transferred among different endpoints and intersections if needed. The model establishes the rate of departure of modules from each endpoint and intersection along each line and the rate of transfer of empty modules among endpoints and intersections that allow to satisfy the demand of transport using a minimum number of modules. By analyzing two extreme cases, we bound the relative value of module sharing among lines and the relative value of empty module rebalancing. 
Computational experiments on instances derived from real-world transportation networks show the validity of the model and the saving in total transport capacity that can be obtained from a modular system with respect to a fixed-capacity system.
",The value of modular buses optimization,"[27108, 12473, 68156, 1182]",868,"[119, 150, 143]",1126,Combinatorial Optimization models and applications in Logistics and Transportation II,64,3,29,Combinatorial Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,157 [building - 208],"['Public Local Transportation Systems', 'Network Flows', 'Transportation']",MB-29
"The Cross-dock Door Design Problem [CDDP] consists of deciding on the number and capacity of inbound and outbound doors for receiving product pallets from origin nodes and exiting them to destination nodes. The uncertainty, realized in scenarios, lies in the occurrence of these nodes, the number and cost of the pallets, and the capacity’s disruption of the doors. The CDDP is represented using a stochastic two-stage binary quadratic model [BQM]. The first stage decisions are related to the cross-dock infrastructure design, and the second stage ones to the node-to-door’s assignments. This is the first time, as far as we know, that a stochastic two-stage BQM has been presented for minimizing the construction cost of the infrastructure and its exploitation expected cost in the scenarios. Given the difficulty of solving this combinatorial problem, a mathematically equivalent MILP formulation is introduced. However, searching an optimal solution is still impractical for commercial solvers. Thus, a scenario cluster decomposition-based matheuristic is introduced to obtain feasible solutions with  small optimality gap and reasonable computational effort. A broad study to validate the proposal gives solutions with a much smaller gap than the ones provided by a state-of-the-art general solver. In fact, the proposal provides solutions with a 1 to 5% optimality gap, while the solver does it with up to a 12% gap, if any, and requires a wall time two orders of magnitude higher.","Cross-dock Door Design Problem, CDDP, under uncertainty","[67934, 139, 23052]",456,"[14, 114, 136]",1131,Cross-dock Door Problems,49,5,35,"Stochastic, Robust and Distributionally Robust Optimization","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,44 [building - 303A],"['Combinatorial Optimization', 'Programming, Quadratic', 'Stochastic Optimization']",MD-35
"Pairwise comparisons constitute a fundamental part of many multiple-criteria decision-making methods designed to solve complex real-world problems. One of the pervasive features associated with any problems’ complexity is uncertainty. Experts are rarely able to consistently and accurately evaluate a set of alternatives under consideration due to time pressure, cognitive bias, the intricacy or intangible essence of the problem, a lack of requisite knowledge or experience, or other reasons. Interval pairwise comparisons [IPCs] allow for this uncertainty in a natural way; however, the problem of inconsistency [or infeasibility] may arise. That is, a set of interval comparisons may not allow experts to find a solution in the form of a priority vector. The aim of this paper is to provide a review of existing literature on this topic and to compare existing methods via numerical examples and simulations which indicate that the Interval Stretching Method is superior to other published methods. In addition, the problem of solution
uniqueness is investigated for selected methods. Last but not least, a new theoretical framework for the evaluation of the extent of the modification of an original IPC matrix is provided as well.
",Interval Pairwise Comparisons in the Presence of Infeasibilities - A Review and Numerical Experiments,"[42142, 9412, 77029]",892,"[6, 131, 25]",1133,Pairwise comparisons and preference relations 2,44,10,44,Multiple Criteria Decision Analysis,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,20 [building - 324],"['Analytic Hierarchy Process', 'Simulation', 'Decision Analysis']",TD-44
"The widespread deployment of wireless telecommunication networks and growing Internet traffic globally leads to a substantial increase in signal traffic demands in fiber optic networks. This demand surge places significant pressure on the survivability of optical networks, specifically when faced with fiber cable failures. To address this challenge, in this paper, we consider integrated planning of regenerator installation, optical connection routing, and wavelength assignment [RWA] for wavelength division multiplexing [WDM] optical networks under link failures. We minimize the number of backup regenerators required to fulfill all connection requests under all potential link failures while accounting for the wavelength and routing constraints. We formulate the problem as an integer program, while its size exponentially increases with the network size by nature. To solve large-scale instances for practical uses in the industry, we propose efficient algorithms to generate near-optimal solutions that are practically implementable. We perform extensive numerical experiments to demonstrate the effectiveness of our approach in solving real-world instances provided by our industry partners. The results show that our approach can efficiently obtain high-quality solutions while ensuring a minimal increase in backup regenerators as the network grows. Our solutions also offer valuable insights into planning a cost-effective and reliable fiber optic network for network operators.",Integrated Regenerator Planning and RWA in WDM Optical Networks under Link Failures,"[64212, 77459, 77460, 77461, 70665]",297,"[109, 151, 141]",1134,Advances in Optimization for Industrial Applications,64,12,29,Combinatorial Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,157 [building - 208],"['Programming, Integer', 'Practice of OR', 'Telecommunications']",WA-29
"We propose and analyze a stationary equilibrium model for a competitive industry which endogenously determines the carbon price necessary to achieve a given emission target. In the model, firms are identified by their level of technology and make production, entry, and abatement decisions. Polluting firms are subject to a carbon price and abatement is formulated as an irreversible investment, which entails a sunk cost and results into switching to a carbon neutral technology. In equilibrium, we identify a carbon price and a stationary distribution of incumbent, polluting firms, that guarantee the compliance with a certain emission target. Our general theoretical framework is complemented with a case study with Brownian technology shocks, in
which we discuss some implications of our model. We observe that a carbon pricing system alongside installation subsidies and tax benefits for green firms trigger earlier investment, while higher income taxes for polluting firms may be distorting. Moreover, we discuss the role of a welfare maximizing regulator, who, by optimally setting the emission target, may mitigate or revert some parameters' effects observed in the model with fixed limit. This is joint work with Felix Dammann.",A STATIONARY EQUILIBRIUM MODEL OF GREEN TECHNOLOGY ADOPTION WITH ENDOGENOUS CARBON PRICE,[67281],373,"[135, 78, 40]",1135,Optimal control in environmental economics,90,8,33,Optimal Control Theory and Applications,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,42 [building - 303A],"['Stochastic Models', 'Natural Resources', 'Environmental Management']",TB-33
"The transportation sector is a major contributor to CO2 emissions. We consider a pollution routing problem within the scope of the capacitated vehicle routing problem by explicitly considering CO2 emissions. We propose a novel MIP-based clustering approach that assigns customers to tours minimizing the estimated resulting CO2 emissions. Our approach extends the method of Fisher and Jaikumar by approximating the weight of the truck [including curb and payload weight] at each detour. That way it can be incorporated into any MIP-formulated routing problem [e.g., location routing problems, delivery and pickup problems or inventory routing problems] making it essentially different from other heuristics. After evaluating different seeding strategies, we perform tests on the Solomon benchmark and the XML dataset. Comparisons with known optimal solutions as well as a continuous approximation method are also drawn. ",A Novel Clustering Approach for VRPs Considering CO2 Emissions,"[76799, 22691, 61830, 36955]",788,"[145, 65]",1136,Logistics 2,5,14,58,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S07 [building - 101],"['Vehicle Routing', 'Logistics']",WC-58
"Estimating the difficulty of exam questions is crucial for effectively evaluating students’ knowledge and facilitating personalized exercise recommendations. Obtaining labels for the training dataset typically involves time-consuming and expensive pretesting and manual calibration. Despite recent advances in fine-tuned Transformer-based models outperforming traditional machine learning approaches, the labeling expenses remain considerable. Our study addresses this labeling challenge by leveraging active learning to drive the annotation process, directing human expert attention toward the most informative data points. Given the lack of uncertainty in standard regression neural networks, we employ Monte Carlo Dropout to capture model uncertainty in predictions on the unlabeled set. Model uncertainty tends to be high on data points in underrepresented areas of the input space, precisely the observations we aim to label. Fine-tuning a DistilBERT model with Monte Carlo Dropout on a dataset comprising science and math multiple-choice questions yields promising results.",Guiding labeling efforts in question difficulty estimation using active learning,"[71034, 35404]",415,"[7, 34, 42]",1137,Learning Analytics and other Text Analytics tasks,17,13,31,Analytics,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,54 [building - 208],"['Analytics and Data Science', 'Education and Distance Learning', 'Expert Systems and Neural Networks']",WB-31
"We consider robust tactical crew scheduling for a large passenger railway operator, whose timetable is subject to minor but uncertain updates on a weekly basis. Before the start of the planning period we must select a number of templates, specifying the time windows in which crew is supposed to work. These templates must be cost-efficient whilst providing sufficient capacity for the realised updated timetables. Moreover, the templates must be compatible with the rostering process, i.e., we wish to limit the number of different templates used and ensure that there are not too many templates on early or late moments of the day. Throughout the planning period, the templates are assumed to be fixed and the updated timetable is released on a weekly basis. All tasks in the timetable must be covered using the capacity provided by the templates, or, if necessary, costly reserve capacity. We model the template selection problem as a scenario-based robust optimisation model, for which we propose a Benders decomposition algorithm. We conduct computational experiments on real-life instances from Netherlands Railways and show that our algorithm is able to solve instances featuring up to three scenarios and up to 1,000 tasks per scenario to optimality. Including more scenarios, allowing for more flexibility in choosing templates, or enlarging the template length all lead to reductions in reserve crew costs without increasing template-based crew costs. ",Benders Decomposition for Robust Tactical Crew Scheduling,"[67345, 22950, 5932]",180,"[122, 127, 13]",1138,Crew Planning in Public Transport,85,3,54,Public Transport Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S01 [building - 101],"['Railway Applications', 'Robust Optimization', 'Column Generation']",MB-54
"This paper addresses an integrated scheduling problem in the pharmaceutical two-stage supply chain. The problem is composed of multiple pharmaceutical orders, multiple unrelated factories, and one distribution center. Each factory uses a distinct permutation flow shop line consisting of hybrid batch/continuous processes and considers sequence-dependent change-over time. The distribution center picks up the products by truck shipment. The objective function is to minimize the makespan. Lot sizing, assigning, and production and truck shipment sequencing processes are the main decisions for the addressed problem. A mixed integer linear program is developed, and two types of population-based search algorithms are proposed to effectively and efficiently find the near-optimal solutions for the large-sized instances. In the small-sized experiments, the performances of the algorithms are verified by the absolute comparison with the mixed integer linear program. In the large-sized experiments, the performances of the algorithms are evaluated by the relative comparison.",Population-based search algorithms for an integrated scheduling problem in the pharmaceutical two-stage supply chain,"[61992, 77030]",804,"[138, 129, 74]",1139,Integrated lot-sizing problems,32,3,49,"Lot Sizing, Lot Scheduling and Production Planning","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,M1 [building - 101],"['Supply Chain Management', 'Scheduling', 'Metaheuristics']",MB-49
"Online e-commerce giants are continuously investigating innovative ways to
improve their practices in last-mile deliveries. Inspired by the current practices at JD.com [the largest online retailer by revenue in China], we investigate a delivery problem that we call the traveling salesman problem with bike and robot [TSPBR], where a cargo bike is aided by a self-driving robot to deliver parcels to customers in urban areas. 

We present two mixed-integer linear programming models and describe a set of valid inequalities to strengthen their linear relaxation. We show that these models can yield optimal solutions of TSPBR instances with up to 60 nodes. To efficiently find heuristic solutions, we also present a genetic algorithm based on a dynamic programming recursion that efficiently explores large neighborhoods. 

We computationally assess this genetic algorithm on instances provided by JD.com and show that high-quality solutions can be found in a few minutes of computing time. Finally, we provide some managerial insights to assess the impact of deploying the bike-and-robot tandem to deliver parcels in the TSPBR setting.",Synchronized Deliveries With a Bike and a Self-Driving Robot,"[19719, 51294, 77031, 67085]",735,"[145, 109, 74]",1140,Routing Unmanned Aerial Vehicles 1,5,3,64,VeRoLog - Vehicle Routing and Logistics,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S16 [building - 101],"['Vehicle Routing', 'Programming, Integer', 'Metaheuristics']",MB-64
"With a complex network of warehouses and depots, optimizing ground transportation is a goal for sustainability and efficiency. Our problem aligns with CVRPTW, a Vehicle Routing Problem with Limited Capacity and Time Windows for pickup and delivery. However, since most warehouses must deliver to most depots, freight exchanges at predetermined nodes in the network play a leading role in an optimal solution to the problem.  

To tackle this challenge, our initial approach involved modeling the problem using direct mixed integer and linear programming techniques. We carefully tailored the model to meet our additional requirements, with emphasis on accurately representing the exchange of cargo at designated points. 

However, as expected, this problem with our logistics network and crossover points is not solvable in our time requirements due to its inherent computational complexity. To address this challenge and speed up the solving process, we explored some math-heuristic approaches, acknowledging potential compromises in optimality.",Optimization of Road Transport With Crossdocks - A Case Study at Inditex With Matheuristics,"[76929, 43831, 77035]",758,"[145, 151, 111]",1145,Real-Life Applications in Routing,5,5,58,VeRoLog - Vehicle Routing and Logistics,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S07 [building - 101],"['Vehicle Routing', 'Practice of OR', 'Programming, Mixed-Integer']",MD-58
"One of the biggest challenges in efficiently managing any organisation is drawing up a conscious strategic capacity plan. Therefore, it is essential to draft a mid-term tactical capacity plan with singular and measurable objectives. This work focuses on the business activity of urban water utilities, which is often faced with economic, operational, and institutional constraints, and it is fundamental to optimise their business activity. In this context, the water-energy nexus is important to these utilities, as it describes the relationship between the water used for energy production, including electricity and fuel sources, and the energy consumed to capture, treat, and supply water. The energy represents one of the highest financial costs for these organisations. We we propose a methodology to minimise energy expenditure in urban water supply service. It includes a mid-term tactical capacity planning for a Portuguese water utility over three years. A water supply network usually has five groups of infrastructure - catchments, pipelines, reservoirs, pumping stations, and water treatment plants, which can have redundancy installations. The energy consumption can be minimised by managing the operation of redundancy installations. In this way, it is possible to make the most use of the organisation's assets, considering its energy costs and the forecast of the average demand for water by the region's population.",Capacity planning to manage the operation of redundancy installations ,"[46329, 77040, 4934, 77041]",705,"[12, 110, 25]",1154,Resilient Networks,80,10,53,Sustainable and Resilient Systems,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,8007 [building - 202],"['Capacity Planning', 'Programming, Linear', 'Decision Analysis']",TD-53
"The sustainable development of cities requires a focus on what is essential for the global ecosystems and economies. In this sense, it may be useful to introduce the concept of weak and strong sustainability [WS and SS] in urban contexts. Indeed, it is well known that environmental problems have a local origin, even though they manifest themselves globally. The concept of WS and SS involves a different consideration of the relationship between natural and human capital, which can be interpreted in different ways of translating sustainability to decision-making processes. In evaluation, the choice towards one type of sustainability entails a different process of aggregating indicators - this element is identified as the one to be paid the most attention when conducting an evaluation towards SS. To stimulate the decision-makers reflection on which type of sustainability is supporting his/her urban transformation, this paper intends to contribute to articulate the reasoning and explication of the negative and positive elements of the project alternatives. Specifically, we employ the p-ary Choquet integral to represent various strategies across distinct regions of the decision space. To manage the extensive preference information that needs to be gathered, we adopt a parsimonious elicitation method introduced by Labreuche and Grabisch, ensuring the problem remains manageable. The advantages of this method are illustrated through the application to an urban case study.",Exploring the decision space to support the decision maker's reflection on strong or weak urban sustainability,"[61638, 32182, 15067]",716,"[139, 149]",1157,Supporting Planning and Sustainability,26,2,13,Soft OR and Problem Structuring Methods,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,15 [building - 116],"['Sustainable Development', 'Problem Structuring']",MA-13
"In this presentation, we tackle the downgrading maximal covering location problem within a network. In this problem, two decisions are made - determining the facility locations and increasing edge lengths to reduce coverage.  Two actors with conflicting objectives are involved:
a] The location planner aims to maximize the covered demand while anticipating that an attacker will attempt to reduce coverage by increasing the length of the edges.
b] The attacker seeks to maximize the demand initially covered by the facilities but left uncovered after the downgrade. To achieve this, they can increase the length of certain edges within a specified budget.

We introduce a bilevel mixed-integer program to formulate the problem, a preprocessing phase, and a matheuristic algorithm to address it. Additionally, computational results are presented to illustrate the potential and limitations of the proposed algorithm.",Downgrading edges in the maximal covering location problem,"[66322, 12140, 22045]",765,"[64, 53, 72]",1158,Covering Location Problems,29,7,61,Locational Analysis,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S10 [building - 101],"['Location', 'Graphs and Networks', 'Mathematical Programming']",TA-61
"Container terminals around the world are embracing automation to improve their efficiency and throughput performance for dynamic business requirements. However, to achieve the desired cost and operational efficiency of an automated container terminal, a thorough analysis of its performance under various design parameters is necessary at the design stage. In this paper, we propose a novel queuing model for the performance analysis of a container terminal that uses automated lifting vehicles for container transportation. A special type of two-phase server is used to capture complex two-phase operations of quay cranes, automated lifting vehicles, and stack cranes, which were ignored in previous studies. Since existing methods cannot solve our resulting queuing model, we develop a new network decomposition-based approach to solve it. First, we validate the proposed analytical framework by developing a simulation model and find that it is quite accurate with a less than 5% average absolute percentage error in the average throughput time. Next, we quantify the value of accurately modeling the two-phase operations of resources in a container terminal. In our numerical analysis, we find that ignoring two-phase operations can lead to an overestimation of the average throughput time from 25% to 65%. Thus, our new modeling approach assists terminal designers in accurately estimating the performance measures. ",Stochastic Modeling and Analysis of Two-phase Operations in Container Terminal with Automated Lifting Vehicles,"[77038, 78877, 78878]",680,"[65, 121, 143]",1159,Container Stacking and Yard Planning III,52,14,62,OR in Port Operations,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S12 [building - 101],"['Logistics', 'Queuing Systems', 'Transportation']",WC-62
"Timely response is crucial in humanitarian logistics to reduce suffering and loss. Network disruptions after a disaster, or due to unreliable networks, can cause prolonged travel times or isolate locations. These uncertainties necessitate a systematic approach to anticipate challenges in humanitarian logistics and ensure timely aid delivery. We propose a novel network design problem that strategically locates depots to enable efficient post-disaster deliveries. We incorporate the post-disaster uncertainties through stochastic travel times on the road network and formulate the resulting stochastic location-routing problem as a two-stage stochastic program. We explore the integration of drones with ground vehicles to overcome the network inaccessibility and long travel times on the road network to improve response times. We focus on independent truck-drone deliveries from depots to transport lightweight aid bundles, such as medical supply kits, weather protection items, or communication tools. We develop a tailored heuristic based on variable neighbourhood search and compare its performance with CPLEX in small instances. We implement the heuristic in a case study of the Van Earthquake 2011, Türkiye. Through this implementation, we evaluate the value of the stochastic modelling approach and the tandem delivery model and perform sensitivity analysis. Our results show that the use of drones can significantly enhance delivery times by more than 50% in our case study.      ",The Stochastic Location-Routing Problem with Truck-Drone Tandem for Humanitarian Aid Delivery,"[51223, 61009, 77046]",90,"[30, 58, 135]",1161,Humanitarian aid provision and disposal,38,7,21,OR in Humanitarian Operations [HOpe],"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,49 [building - 116],"['Disaster and Crisis Management', 'Humanitarian Applications', 'Stochastic Models']",TA-21
"We elaborate on inventory rebalancing in bike sharing. The goal is to decide how to reposition bikes between the stations of a bike sharing network so as to optimize service loss, based on the dynamic lot sizing model. Previous work computationally demonstrates the benefits of dynamic compared to static rebalancing. We propose new optimal algorithms for both problem variants. On the negative side, we show that static rebalancing results in O[1/epsilon] proportion of successful requests. On the positive side, we prove that dynamic rebalancing achieves an Omega[1-epsilon] performance guarantee. To our knowledge, this is the first theoretical characterization of the dynamic rebalancing benefits compared to static rebalancing. Next, we consider the more general problem of optimizing service loss with bounded rebalancing costs. For both the static and dynamic variants, we develop new scalable mixed-integer approaches to practically relevant instances. We computationally substantiate the proposed approaches using data from the London bike sharing network.",On Static and Dynamic Inventory Rebalancing in Bike Sharing,"[77033, 77044, 77879]",803,"[61, 5, 111]",1163,Lot-sizing with joint replenishment and routing decisions,32,2,49,"Lot Sizing, Lot Scheduling and Production Planning","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,M1 [building - 101],"['Inventory', 'Algorithms', 'Programming, Mixed-Integer']",MA-49
"In the contemporary business landscape, reverse logistics [RL] activities play a pivotal role for companies aiming to achieve cost savings, improved customer service, and sustainability. Owing to resource constraints and a shortage of technology and expertise, manufacturing firms frequently form partnerships with specialized RL service providers. Yet, assessing the optimal third-party reverse logistics providers [3PRLPs] poses a challenging and uncertain multi-criteria decision-making problem.
This paper introduces an innovative framework integrating interval rough numbers with the Dombi Bonferroni Weighted Assessment [DOBAS] model. The objective is to offer systematic decision support for organizations in selecting the most suitable partner. The proposed methodology was verified and implemented in evaluating 3PRLPs providers for an Indian food manufacturing company [FMC]. Fourteen criteria from sustainable aspects – environmental, economic, social, and risk factors- were used to evaluate and rank six 3PRLPs. 
The evaluation results provided valuable insights into the strengths and weaknesses of each provider, allowing the FMC to make an informed decision on which 3PRLP to partner with for their needs. The outcomes demonstrated that the proposed approach is a valuable tool for supporting decision-making, and it is not only applicable to the case discussed in this work but also to other situations of a similar nature. ",Assessment of Sustainable Third-Party Reverse Logistics Providers Using Interval Rough DOBAS Approach,"[76124, 77045]",760,"[65, 125, 55]",1166,Logistics 1,5,13,58,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S07 [building - 101],"['Logistics', 'Reverse Logistics / Remanufacturing', 'Group Decision Making and Negotiation']",WB-58
"In the 2D Strip Packing Problem [SPP], the objective is to pack items into a fixed-width strip, minimizing the height of the packing. A common approach to solving the SPP efficiently is through decomposition algorithms, which split the problem into primary and secondary subproblems. The primary problem involves solving a relaxed version of the SPP, like the one-dimensional contiguous bin packing problem or parallel processor scheduling problem with contiguity constraints. Using a feasible solution from the primary problem, the secondary problem constructs a valid SPP solution. If none is found, techniques such as adding a cut or banning the current solution are applied. Various formulations, including integer linear programming models or constraint programming models, exist for the primary problem. Empirical evidence suggests that different solution approaches perform better for different instances, with no single dominating formulation. Our research focuses on developing automated hyper-algorithms capable of selecting the most appropriate method or sequence of methods for solving a given instance, following the trend of integrating machine learning techniques into optimization algorithms. We utilize reinforcement learning to determine which mathematical model to employ in the primary problem, along with other features for incorporation into the decomposition scheme. The proposed framework's performance is evaluated using benchmark datasets.",Solving the two dimensional strip packing problem using reinforcement learning framework,"[77043, 62396]",518,"[66, 14, 23]",1168,"Advancements of OR-analytics in statistics, machine learning and data science 9",16,13,28,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,065 [building - 208],"['Machine Learning', 'Combinatorial Optimization', 'Cutting and Packing']",WB-28
"Fostering and strengthening the circular economy [CE] with all of its dimensions is one objective of the European Union to reach its goal of becoming climate-neutral by the year 2050. Therefore, the EU has started with revision, extension, and the new development of circular economy regulation and guidelines. 
We provide a comprehensive discussion that shows how operation research methods can underline the goals of the upcoming regulations and serve as decision support for actions of circular economy shareholders on the European market. Therefore, we develop case studies based on different European regulations. The EU legislations under consideration include the EU battery regulation, the directive on common rules promoting the repair of goods and the Eco-design requirements for sustainable products. We investigate how legal CE innovations affect different supply chain problems in case studies. Legal CE innovations include, for instance, digital product passports or the obligatory usage of recycled content in production. For each case study, we point out if the legal CE innovation is an emerging research field that needs to be investigated or if the OR community has picked up the problem.
",Do upcoming EU circular economy legislations require new OR applications?,"[64482, 2675]",921,"[137, 94, 125]",1169,Policy and legislation for a circular economy,18,7,23,"Circular Economy, Remanufacturing and Recycling ","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,82 [building - 116],"['Strategic Planning and Management', 'OR in Environment and Climate change', 'Reverse Logistics / Remanufacturing']",TA-23
"Playing a vital role for global food supply, agriculture contributes significantly to current challenges such as biodiversity loss and greenhouse gas emissions. The pursuit of sustainable agriculture aims to find a balance between food production, environmental conservation, and financial viability. Strip cropping presents a promising avenue, in this context, by organizing farmland into smaller plots of different crops to enhance their resistance to pests, increase biodiversity and improve overall soil health. However, at the same time, simultaneous crop management poses operational challenges that prevent the implementation of strip cropping at a large scale. The use of broader strips can decrease these implementation barriers for farmers yet may dilute the ecological benefits by reducing interactions between adjacent crops. These trade-offs complicate the process of devising efficient strip cropping strategies. The aim of this research is to support farmers in this process through the development of a decision support tool. For this purpose, we propose a crop planning model which integrates crop selection and land allocation decisions under consideration of both spatial and temporal diversity. By determining ideal strip widths and cropping schedules, the model steers farmers towards strip cropping practices that are both economically viable as well as ecologically favorable, thereby promoting a sustainable future for agriculture and global food supply.",Sustainable crop planning for mixed cropping systems – An optimisation approach,"[76294, 72182, 55094]",380,"[89, 139, 84]",1172,Sustainable Food Supply,78,14,13,Secure & Sustainable Food Supply,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,15 [building - 116],"['OR in Agriculture', 'Sustainable Development', 'Optimization Modeling']",WC-13
"Growing concerns about traffic congestion and environmental pollution necessitate innovative solutions to enhance urban mobility for both people and goods. A novel approach involves combining passenger and parcel delivery services using demand-responsive buses [DRBs] and drones. This integration aims to reduce the number of vehicles on the road by combining the movement of passengers and parcels. In this scheme, each DRB is paired with a drone to work together. While DRBs accommodate both passengers and parcels, drones are used solely for delivering parcels. We introduce the concept of the induced Route Planning Problem for DRBs and drones, termed the Share-A-Ride Problem with Drones [SARP-D]. We first developed an arc-based mixed integer programming model to formulate this problem, which can be solved by CPLEX for instances with up to 12 nodes. Then, we developed a path-based model and a column generation approach to solve medium instances with up to 50 nodes. The column generation approach achieves either optimal solutions or solutions very close to optimality within a 3-hour timeframe. Large instances with up to 200 nodes are solved by an adaptive large-neighborhood search metaheuristic, which can also efficiently solve vehicle routing problems with drones. Our experiments showed that SARP-D could reduce the number of used DRBs and DRB-traveled miles. We offer several key insights for management based on our research outcomes.",Passenger and parcel share-a-ride problem with drones - Models and solutions,"[76847, 77053, 74502, 64035, 55094]",195,"[145, 13, 74]",1174,Combinatorial optimization topics in transportation,64,2,26,Combinatorial Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,012 [building - 208],"['Vehicle Routing', 'Column Generation', 'Metaheuristics']",MA-26
"This contribution presents a novel approach to enhance the real-time resolution of occupancy conflicts at railway nodes by integrating Reinforcement Learning [RL] methodologies into a generic Conflict Detection and Conflict Resolution [CDCR] heuristic. Given the intricate nature of railway systems, the occurrence of stochastic events cannot be avoided. In this context, conflicting operations between train journeys must be resolved. These conflicts are harder to resolve within nodes than in lines due to the larger number of possible resolution combinations. The proposed approach foresees the integration of RL into an existing CDCR heuristic approach for resolving occupancy conflicts in nodes. The generic CDCR Heuristic provides a baseline for identifying and resolving conflicts, while RL enriches the system's intelligence by providing real-time feedback learned from historical data. The resulting hybrid approach facilitates adaptive decision-making, optimizing train movements, and minimizing the overall delay in the system. The integration of RL within the existing CDCR heuristic aims to enhance the system's efficiency and effectiveness, ultimately improving the overall efficiency of railway operations. The proposed approach is evaluated in a real-world scenario, demonstrating its potential to enhance real-time schedule adjustments by providing robust, optimized solutions to occupancy conflicts.",Integrating Reinforcement Learning methodologies into a generic CDCR Heuristic to effectively resolve occupancy conflicts at railway nodes in real-time.,"[69958, 69957, 75350, 77728, 75353, 75352]",816,"[26, 66, 42]",1176,Railway Traffic Management,85,13,51,Public Transport Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,M5 [building - 101],"['Decision Support Systems', 'Machine Learning', 'Expert Systems and Neural Networks']",WB-51
"Quality 4.0 represents a transformative paradigm in industrial quality management, integrating advanced technologies like the Industrial Internet of Things, artificial intelligence, and big data analytics. It enhances product quality and operational efficiency, extending beyond compliance and defect detection to focus on proactive measures, predictive analytics, and continuous improvement.

Despite ongoing debates about the appropriateness of the term Quality 4.0, embracing it implies a shift from reactive problem-solving to a proactive and preventive approach. Hence, research on Quality 4.0 is essential for advancing knowledge, preparing future professionals, and critically evaluating the implications of integrating advanced technologies into quality management practices. 

The talk comprises two parts. First, recent trends in Quality 4.0 will be presented, covering the features and benefits of Quality 4.0. Application cases in Korean industries like steelmaking and consumer electronics will also be discussed. Second, potential research directions in Quality 4.0 for the operations research community will be explored, such as developing dynamic quality management models, simulating quality processes in autonomous manufacturing environments, and measuring and optimizing supply chain quality. These directions leverage operations research expertise to address complexities and challenges in implementing Quality 4.0 in manufacturing and supply chain contexts.
",Quality 4.0 - Recent Trends and Future Research Directions,"[77050, 65543, 79447, 79448, 79443, 79442]",548,"[120, 59, 151]",1177,"Digitization in Knolwedge, Technology, and Innovation",54,14,08,"Knowledge, Technology, and Innovation","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,1020 [building - 202],"['Quality Management', 'Industrial Optimization', 'Practice of OR']",WC-08
"Ridepooling applications ask for fast computation of vehicle routes and passenger allocations. This so-called dial-a-ride problem [DARP] has to compromise between the efficiency of the ridepooling system [e.g., routing costs] and customer satisfaction [excess ride time and the number of served requests], subject to time window constraints, vehicle load constraints, and maximum ride time constraints. In this talk, we present a multi-objective variant of DARP and investigate the trade-off between the objectives of the service provider and those of the customers.

Moreover, we present a tight mixed-integer linear programming [MILP] formulation for the DARP by combining two state-of-the-art models into a novel location-augmented-event-based formulation. The formulation is tight in the sense that, if time windows shrink to a single point in time, the linear programming relaxation yields integer [and hence optimal] solutions. Strong valid inequalities and lower and upper bounding techniques are derived to further improve the formulation. We then demonstrate the theoretical and computational superiority of the new model - First, we prove that the new formulation has a tighter linear programming relaxation than the two-index formulation. Second, extensive numerical experiments on benchmark instances show that computational times are on average reduced by 49.7% compared to the state-of-the-art event-based approach.",The Multi-objective Dial-a-Ride Problem,[9695],181,"[143, 112, 111]",1178,Ridehailing & Ridepooling,85,4,54,Public Transport Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S01 [building - 101],"['Transportation', 'Programming, Multi-Objective', 'Programming, Mixed-Integer']",MC-54
"Traditional emergency medical services can be expanded by Community First  [CFR] systems. In such CFR systems, trained volunteers near the incident location can be dispatched, potentially reducing the emergency response time. 

LIVES is an example of such a CFR system; LIVES dispatches volunteers to medical emergencies in [mostly] rural areas of the UK, where the response time of EMS can be rather long. The volunteers of LIVES respond to a wide variety of medical emergencies. However, not all volunteers are qualified to handle all emergencies. Volunteers are classified into different categories based on their medical skills. LIVES provides training to its volunteers to increase their skill level, allowing volunteers to be dispatched to more emergencies as a result. LIVES has a fixed training budget and would like to know how to divide it over its volunteer base. We leverage historical data on incidents as well as volunteer availability, derived from individual sessions in which a volunteer was logged on within the system. Their stochastic availability, combined with a geographically diverse volunteer set, translates into a unique optimization problem - how to spend the training budget in such a way that response times are minimized. In this talk, we present our ongoing work on how to tackle this optimization problem. 
",Optimizing the training of community first responders ,"[77052, 39439, 64020, 65884]",954,"[56, 72, 73]",1182,EMS and crisis logistics,3,3,10,OR in Health Services [ORAHS],"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,11 [building - 116],"['Health Care', 'Mathematical Programming', 'Medical Applications']",MB-10
"Kidney exchange programs facilitate life-saving transplants by allowing patients with willing but incompatible donors to exchange their donors either through cycles, or through chains initiated by non-directed donors. In each matching run, one must identify a maximum weight packing of cardinality-constrained vertex-disjoint cycles and chains on a compatibility graph. This computational problem, known as the Kidney Exchange Problem [KEP], is both theoretically and empirically hard.
    
We conduct a survey of existing integer linear programming formulations and associated model-related enhancements for the KEP, extending upon prior work by including the most recent advances. Our key contribution lies in addressing a gap in the literature - whereas models for only cycles have been extensively studied, models for both cycles and chains have received less attention. We demonstrate that all cycle models can be adapted to model chains, thereby enabling a comprehensive approach to modelling cycles and chains by merging any cycle model with any chain model. Furthermore, we explore alternative strategies, such as modelling cycles and chains concurrently using a shared set of variables.
    
Additionally, we conduct extensive computational experiments on generated instances to evaluate and compare the performance of the different model combinations for varying instance parameters, providing valuable insights for practitioners and researchers alike in effectively addressing the KEP.",A survey on ILP formulations for the kidney exchange problem,"[76714, 77236, 62396, 37829, 62409]",206,"[109, 56, 53]",1184,Graph and network optimization,64,9,25,Combinatorial Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,011 [building - 208],"['Programming, Integer', 'Health Care', 'Graphs and Networks']",TC-25
"Traction unit drivers have become an increasingly critical resource for railway companies, making the railway crew scheduling problem an essential step in the planning cascade. While efficiency has traditionally been the primary goal, the vulnerability of schedules to disruptions requires robust planning. Compared to other planning stages in the railway sector, crew scheduling robustness has received less attention, although its counterparts in the bus and airline industries are thoroughly evaluated. We address this gap by investigating robustness in railway crew scheduling problems and its impact on efficiency. Our approach involves solving real-world problems using column generation. We assess existing robustness indicators and adapt a performance indicator from timetabling called t-robustness. This criterion is new for crew scheduling. On the one hand, t-robustness penalizes train changes, which are critical since delays are transferred to other circulations. On the other hand, buffers at important spots within a shift are rewarded. We evaluate the dependencies between efficiency and robustness through empirical analysis of real-world planning problems, highlighting the significance of our proposed t-robustness measure. By integrating robustness considerations into crew scheduling, railway operators can achieve shift plans that not only optimize efficiency but also better withstand unforeseen disruptions, ensuring resilient operations. ",Robustness in railway crew scheduling,"[72650, 32882, 47728]",180,"[122, 13, 127]",1187,Crew Planning in Public Transport,85,3,54,Public Transport Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S01 [building - 101],"['Railway Applications', 'Column Generation', 'Robust Optimization']",MB-54
"Path-based models represent a large class of DEA models in which the
efficiency score and projection point are derived using a path running
from the assessed unit to the boundary of the technology set.
The class includes the input and output-oriented radial models, the
directional distance measure and the hyperbolic distance measure
models as special cases. The particular models fit a general envelopment scheme that allows to derive its dual [multiplier] form, and to analyze the primal-dual properties for the whole class of models. Based on this primal-dual relationship, we describe a two-way connection
between the supporting hyper-planes of the technology set and the
optimal solutions of the multiplier model and utilize it in the
returns-to-scale measurement. We analyze and compare different approaches to the returns-to-scale measurement, depending on whether the supporting hyper-planes contain the [not necessarily strongly efficient] projection point or only strongly efficient benchmarks. The results are demonstrated in numerical examples.  ",Multiplier form of path-based DEA models and returns-to-scale measurement,[71963],946,"[24, 21]",1188,DEA methodological developments II,89,15,48,Data Envelopment Analysis and its Application,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,60 [building - 324],"['Data Envelopment Analysis', 'Convex Optimization']",WD-48
"We propose a method for optimizing daily red blood cell [RBC] matching policies to minimize long-term alloimmunization rates in regularly transfused patients. When antibodies are formed against an antigen, a risk that comes with RBC transfusions, the patient can never be mismatched on that antigen again. Clinical guidelines recommend extended preventive matching for patients at high risk of alloimmunization, such as sickle cell disease patients, who need a transfusion every six weeks. They must be matched not only for major antigens A, B, and D, but for an additional 5 to 12 minor antigens as well. We use linear optimization for daily RBC matching, including parameters for weighting elements of the objective function. We train machine learning models to predict the long-term outcomes of the matching strategy, then use these models to optimize parameter values. A wide range of potentially optimal parameter values is found, likely a result of the stochastic nature of real-world scenarios, combined with the interplay of factors influencing long-term outcomes. We implemented a selection of these for daily matching, and observed an increased performance for several long-term outcomes with respect to the baseline setting without tuned parameters. The methodology developed in this paper in the context of RBC matching, could also be used in applications where we optimize long-term performance via a parameterized short-term optimization model.",Optimizing red blood cell matching policies to reduce alloimmunization in regularly transfused patients,"[71048, 75260, 72596, 77060, 71751]",966,"[146, 56, 77]",1189,Machine learning and analytics in healthcare,3,3,15,OR in Health Services [ORAHS],"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,18 [building - 116],"['Warehouse Design, Planning, and Control', 'Health Care', 'Multi-Objective Decision Making']",MB-15
"We study the fundamental joint pricing and inventory control problem, where the customers are price sensitive. In the continuous review infinite horizon setting, the optimal policy is known to be an [s, S, p] policy, and the optimal price changes for every inventory state. We consider policies with only a small number of prices offered and compare these policies to the optimal policy. When customer valuations follow an MHR distribution, we provide policies with theoretical guarantees for revenue and costs. In the lost sales case, we show that there exists a single price policy that achieves at least as much revenue as the optimal dynamic policy while incurring costs at most sqrt[1+lnS] times more. The cost ratio is improved to 1.225 when the valuation distribution is uniform. When backlogging is allowed, we show that a three-price policy can achieve similar guarantees.",Simple Policies for Joint Pricing and Inventory Management,"[73391, 56626, 77061]",698,"[124, 61, 135]",1191,Pricing Strategies,11,9,59,Pricing and Revenue Management,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S08 [building - 101],"['Revenue Management and Pricing', 'Inventory', 'Stochastic Models']",TC-59
"As an open-source software framework available in C++, Python and MATLAB since 2010, CasADi has been empowering academic and industrial researchers to rapidly prototype and efficiently solve challenging [dynamic] optimization problems. 

Demonstrated through 3700+ citing papers, such optimization problems serve as an answer to many design, planning, estimation and control needs arising in engineering fields such as heat & energy, automotive, chemistry & pharmacy, or robotics. 

The focus of this talk is on how CasADi facilitates vertical integration. The key facilitator is CasADi's versatile expression graph, capable of efficiently representing engineering specifications, dynamic models and transcription methods. A plugin system maps these expression graphs to the needs of a range of open-source and commercial optimization solvers, often including sparsity detection, [repeated] algorithmic differentiation, and linearity detection. By manipulating, communicating and code-generating the expression graphs, one can introduce solid software abstractions that pair readability and maintainability with excellent runtime performance. In particular, the talk presents the case of robot motion skill tool and follows the journey of the involved expression graphs all the way down to the CasADi solver plugins.",CasADi as the bread and butter in optimization for problem-solving engineers,"[76778, 76775]",845,"[134, 84]",1192,Optimization Frameworks,76,14,30,Software for Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,53 [building - 208],"['Software', 'Optimization Modeling']",WC-30
"Cutting and packing problems have been studied for decades and the results of this research have a huge potential to improve many industrial production processes, both from an economic and environmental point of view. A widely studied problem is the two-dimensional irregular strip packing problem, aka nesting problem. This problem consists of placing a set of two-dimensional irregular polygonal pieces on a board with a fixed height and a length long enough to be considered infinite. The classical objective is to minimise the length of the board used to cut all the pieces. The two-dimensional irregular packing problem has been tackled by both heuristic and exact methods, allowing or not the rotation of the pieces. Despite the diversity of papers on this problem, there is a lack of research on the generation of cutting patterns that satisfy the constraints of guillotine cutting for irregular items, which is fundamental in the glass cutting industry, and only heuristic methods were proposed to solve two-dimensional irregular cutting and packing problems with guillotine cuts. We will present an innovative branch-and-cut method to represent and exactly solve all variants of irregular cutting and packing problems with guillotine cuts that deal with a single board [aka as bin, plate or strip], namely the irregular strip packing problem, the irregular placement problem, the irregular knapsack problem, and the irregular identical item problem. ",Solving Nesting Problems with Guillotine Cuts - A Novel Branch-and-Cut Algorithm,"[663, 45983, 12006, 58526]",862,"[23, 14]",1194,Applications of combinatorial optimisation in industry and services II,64,8,29,Combinatorial Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,157 [building - 208],"['Cutting and Packing', 'Combinatorial Optimization']",TB-29
"This talk will delve into the opportunities and challenges of AI for problem-structuring that takes place in immersive virtual environments [IVEs]. With the help of AI-powered technology, virtual reality [VR], is rapidly transforming how spaces for participatory problem structuring can be designed and experienced. For instance, AI is credited with making IVEs more realistic, interactive and adaptable, with 3D models that may aid in knowledge sharing and development. By allowing us to create immersive and interactive digital environments, participants are enabled to explore and refine their ideas in new and exciting ways. However, the use of virtual environments can alter the dynamics of interpersonal communication and collaboration. Moreover, while AI may enhance the usability of IVEs, questions remain about how interaction in IVEs generates specific impacts on knowledge production in such settings. In this talk, we will consider how problem structuring may evolve with the integration of AI-generated resources in and for interaction in IVEs while also considering some of the challenges involved relating to understanding, stakeholder engagement and decision support.","AI-enhanced spaces for problem structuring - better understanding, stakeholder engagement, and decision support?",[41621],128,"[133, 8, 10]",1195,Impact of AI on Soft OR - A,26,8,13,Soft OR and Problem Structuring Methods,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,15 [building - 116],"['Soft OR', 'Artificial Intelligence', 'Behavioural OR']",TB-13
"Shared automated vehicles are expected to be part of the supply of transportation systems in the future. Parallel to this evolution, there is the rapid penetration of battery electric vehicles [BEVs]. The limitations in battery capacity and charging speed of BEVs can influence the planning and operation of shared automated electric vehicle [SAEV] systems. The design of such systems needs to include these limitations so that their viability is properly estimated. In this paper, we develop a space–time-energy flow-based integer programming [IP] model in support of the strategic design of a regional SAEV system. The proposed approach optimizes the fleet [size and composition] and charging facilities [number and location], while explicitly accounting for vehicle operations in aggregated terms [including movements with users, relocations, and charging times]. The model is used to assess the impact of vehicle range and different types of chargers in the optimal design of an interurban SAEV transport system in the center of Portugal. Link to paper - https://doi.org/10.1016/j.trc.2022.103997",A space-time-energy flow-based Iinteger programming model to design and operate a regional SAEV system and corresponding charging network,"[77063, 67662, 77064]",547,"[150, 110, 109]",1197,Advancing mobility towards sustainable solutions II,6,10,56,Transportation,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S04 [building - 101],"['Network Flows', 'Programming, Linear', 'Programming, Integer']",TD-56
"In this paper, a new concept of generalized convexity is introduced for E-diﬀerentiable vector optimization problems. The concept of E-r-invexity is introduced as a generalization of the invexity notion. Further, a new vector exponential penalty E-function method for E-differentiable vector optimization problems is introduced. Furthermore, this work presents an investigation into two methods for solving multiobjective programming problems. First, it explores a sequence of vector penalized optimization problems utilizing a vector exponential penalty E-function. The convergence of this method is established. Next, it defines and analyzes the exactness property of a vector exact penalty E-function method within the context of the introduced vector exponential penalty E-function method. Conditions are provided to ensure the equivalence of sets of [weak] E-Pareto solutions between the original E-differentiable optimization problem and the associated vector E-penalized optimization problem using the vector exact exponential penalty E-function. This equivalence is established for E-differentiable vector optimization problems involving E-r-invex functions.                                                                                                  ",Vector exponential penalty E-function method for E-differentiable vector optimization problems,[77062],87,"[72, 112, 113]",1198,Advances in Continuous Multiobjective Optimization,34,8,37,Multiobjective Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,33 [building - 306],"['Mathematical Programming', 'Programming, Multi-Objective', 'Programming, Nonlinear']",TB-37
"We consider the problem of a ride-hailing platform [e.g., Uber, Lyft] that connects supply with demand over a network of locations. To this aim, the platform makes pricing, customer admission, and driver repositioning decisions. The customers are impatient and have distinct willingness to pay. The drivers can be repositioned by the platform, or can choose to relocate to other locations according to their own choice. We formulate this problem as a discrete-time Markov decision process, and find a near-optimal policy by combining mathematical optimization with deep reinforcement learning. Following our approach, we first find a heuristic policy by solving repeatedly an optimization problem based on the expected values. Then, we train two neural networks to replicate the heuristic policy, and learn the associated value function. We finally improve the policy further through the Proximal Policy Optimization algorithm. By applying our model to real data from New York City, we demonstrate the effectiveness of our approach in comparison with alternative methods. In addition, we explore the interplay between pricing, customer admission, and driver repositioning, and evaluate the effectiveness of these decisions in balancing supply and demand.  ","Pricing, Admission and Repositioning in Ride-Hailing Networks - A Deep Reinforcement Learning Approach","[71020, 24964, 9881]",334,"[143, 135, 8]",1199,Big data analysis and AI in transportation,6,14,55,Transportation,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S02 [building - 101],"['Transportation', 'Stochastic Models', 'Artificial Intelligence']",WC-55
"The majority of articles on participating life insurance contracts with guarantees assumes a deterministic and exogeneously given investment strategy for the insurer's assets, for example a geometric Brownian motion. However, the insurer may strategically adjust the risk of its asset investments depending on liability values. We look at the optimal design of such investment strategies with the aim to reduce insurer's risks. In the extreme case, the insurer may hedge its liabilities [self-heding].
This is joint work with Karim Barigou [Université Laval, Canada].  ",Insurer's management discretion - Self-hedging participating life insurance,[76030],690,"[45, 19, 82]",1200,Risk management and valuation of financial contracts,74,8,57,Modern Decision Making in Finance and Insurance,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S06 [building - 101],"['Financial Modelling', 'Continuous Optimization', 'Optimal Control']",TB-57
"In the realm of decision-making within highly complex environments, evolving numerous variables, circumstances, scenarios, and multiple decision-makers with different perspectives and knowledge concerning a problematic situation, the study proposes extensive modelling for the SAPEVO multi-criteria family of methods. The new modelling, SAPEVO-H², exposes a novel hybrid and hierarchical approach, promoting multi-criteria analysis facing the evaluation of a hierarchical structure of variables assessed by distinct decision-makers, segmented according to their specific areas of responsibility. The model embodies an axiomatic enhancement, enabling a more versatile analysis that accommodates quantitative and qualitative data. A notable feature of SAPEVO-H² is its capacity for group evaluation, organizing stakeholders into levels and assigning variables based on their expertise and prior knowledge without attributing disproportionate influence to any single decision-maker. To support practical application, we present an online computational platform. This technological framework fosters integration among diverse decision-making groups through an asynchronous evaluation environment, enhancing transparency in the decision-making process. It reveals individual and collective preferences alongside the traceability of utility construction, elucidating the intricate relationships among variables, established preferences, and the connections between stakeholder perceptions.",SAPEVO-H² a Hybrid and Hierarchical Decision Support System for Multi-Criteria Analysis in Complex Environments,"[68928, 10978, 50392, 201]",477,"[26, 55, 151]",1202,Multiple-Criteria Decision Support,45,7,45,Decision Support Systems,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,30 [building - 324],"['Decision Support Systems', 'Group Decision Making and Negotiation', 'Practice of OR']",TA-45
"First-order methods slow down when applied to high-dimensional non-convex functions due to the presence of saddle points. If, additionally, the saddles are surrounded by large plateaus, it is highly likely that the first-order methods will converge to a sub-optimal solution. A natural way to tackle the limitations of first-order methods is to employ second order information from the Hessian. However, methods that incorporate the Hessian do not scale to large models . To address these issues, we propose Simba, a scalable preconditioned gradient method. The method is very simple to implement. It maintains a single precondition matrix that it is constructed as the outer product of the moving average of the gradients. To significantly reduce the computational cost of forming and inverting the preconditioner, we draw links with  multilevel optimization methods and construct randomized preconditioners. Our numerical experiments and testing against other state-of-the-art methods verify the scalability of Simba as well as its efficacy near saddles and flat areas. We also analyze Simba and show its linear convergence rate for strongly convex functions.",Simba - A Scalable Bilevel Preconditioned Gradient Method for Fast Evasion of Flat Areas and Saddle Points,"[77066, 39009]",447,"[136, 66]",1204,Preconditioning for  Large Scale Nonlinear Optimization,84,5,34,Advances in large scale nonlinear optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,43 [building - 303A],"['Stochastic Optimization', 'Machine Learning']",MD-34
"The Frequency Setting Problem [FSP] represents a key element in transit design, specifically in public transportation operations. It consists of determining the number of departures to meet passengers' demands during a specific period, ensuring the profitability of the companies. However, the FSP also involves different interests that commonly conflict, making the planning of the transportation system complex. When this plurality is not correctly captured, we have an imbalance between demand and supply. So, we are interested in addressing these distinct objectives through a sustainable perspective, with an emphasis on buses.

This research introduces a mathematical model and optimization techniques to improve bus route frequency, minimizing social, operational, and environmental costs. The model comprises a mixed-integer non-linear formulation, and the optimization techniques involve linearization and a heuristic approach, which is based on sequentially solving relaxed mixed integer subproblems and fixing integer variables throughout the process. To validate our proposal, we consider the bus routes connecting the central and oceanic regions of Niterói, Brazil.

The findings suggest that the combined approach is effective in real-world scenarios, enhancing sustainable mobility through optimized fleet allocation according to current demand. The next research steps include creating a Multi-Criteria Decision Method [MCDA] to support operator decision-making.",New mathematical modelling for setting frequency of bus transportation,"[38159, 76852, 74758, 72209]",333,"[84, 100, 143]",1205,Public transportation ,6,13,55,Transportation,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S02 [building - 101],"['Optimization Modeling', 'OR in Sustainability', 'Transportation']",WB-55
"Every simple game is a monotone Boolean function. For the other direction we just have to exclude the two constant functions. Here we consider simple games with minimum, i.e., simple games with a unique minimal winning vector. We present enumeration results as well as formulas for their dimension. ",Simple games with minimum,"[23024, 67438]",619,"[50, 18]",1206,"Game Theory, Solutions and Structures IV",88,5,36,"Game Theory, Solutions and Structures","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,32 [building - 306],"['Game Theory', 'Computer Science/Applications']",MD-36
"We focus on a vehicle routing problem with stochastic time-dependent demand increments. 
In this problem, a homogeneous fleet of vehicles must visit a set of collection points with the aim of removing waste.    
We divide the planning horizon [day] in not-overlapping time slots.
Each collection point must be visited exactly once during the planning horizon and it has a known initial demand which may increase along the day. In fact, for each collection point and for each time slot, additional demand may arise. The [uncertain] demand increments are revealed upon arrival at the collection points. 
The goal is to minimize the expected total cost which also considers a penalty for the residual quantity of waste accumulated at the collection points after they have been visited. We present a two-stage stochastic program and discuss specific recourse policies for the problem under study.",The Waste Collection Vehicle Routing Problem with Stochastic Time Dependent Demand Increments,"[46040, 77067, 13546]",281,"[145, 117, 100]",1207,Robust and Stochastic Routing Problems,49,2,35,"Stochastic, Robust and Distributionally Robust Optimization","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,44 [building - 303A],"['Vehicle Routing', 'Programming, Stochastic', 'OR in Sustainability']",MA-35
"Given a set of train routes associated with route costs and a set of compatible route pairs associated with pairing costs, the train routing selection problem [TRSP] aims to assign a route to each train, minimizing the sum of the route and pairing costs while ensuring pairwise compatibility among the selected routes. This problem holds significant practical importance, particularly in rail traffic management, where it plays a central role.
We propose a binary quadratic programming formulation for the TRSP and employ a linearization technique from the literature, with a linear number of additional variables and constraints, to derive a novel and effective integer linear programming [ILP] formulation.
Exploiting the specific nature of the TRSP, we devise a technique to enhance the strength of the linear programming [LP] relaxation of the ILP model, resulting in a significant improvement in its computational performance.
We obtain in this manner the first effective exact approach for solving the TRSP with proven optimality. Our approach is capable of solving real-world instances of the French railway network in very short computational times. These instances have typically been tackled only by heuristic algorithms in the existing literature.
Furthermore, we theoretically and computationally compare the new linearization against the classical one with a quadratic number of additional variables and constraints and against the only ILP model from the literature.",A novel and effective integer linear programming formulation for the train routing selection problem,"[70498, 25257, 22042, 73747, 36073, 67927]",863,"[122, 53, 110]",1208,Mixed Integer Optimization II,64,8,52,Combinatorial Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,8003 [building - 202],"['Railway Applications', 'Graphs and Networks', 'Programming, Linear']",TB-52
"We study stable matching problems under contingent priorities, whereby the clearinghouse prioritizes some agents based on the allocation of others. Using school choice as a motivating example, we first introduce a stylized model of a many-to-one matching market where the clearinghouse aims to prioritize applicants with siblings assigned to the same school and match them together. We provide a series of guidelines to implement these contingent priorities and introduce two novel concepts of stability that account for them. We study some properties of the corresponding mechanisms, including the existence of a stable assignment under contingent priorities, its incentive properties, and the complexity of finding one if it exists. Moreover, we provide mathematical programming formulations to find such stable assignments whenever they exist. Finally, using data from the Chilean school choice system, we show that our framework can significantly increase the number of siblings assigned together while having no large effect on students without siblings. ",Stable Matching with Contingent Priorities,[63443],642,"[84, 151, 109]",1209,Market Design 2,87,12,43,Market Design,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,99 [building - 306],"['Optimization Modeling', 'Practice of OR', 'Programming, Integer']",WA-43
"This study addresses large-scale personnel scheduling problems in the service industry by combining mathematical programming with data mining techniques to enhance efficiency. The studied problem aims at scheduling skilled employees over a planning horizon, minimizing costs while meeting job demands. Shift scheduling in service industries depends on customer presence, leading to a complex optimization problem not easily solvable with commercial mixed-integer programming solvers. These problems are recurrent, with instances sharing common characteristics and solution structures differing only slightly over time. Consequently, we propose to use a data mining technique, namely, the k-nearest neighbors algorithm, to expedite the solution. We suggest using past solutions to reduce the problem size. Specifically, for an upcoming instance, we identify similar historical instances and streamline the enumeration of shifts to align with the comparable historical instances' schedules. This approach allows us to effectively solve the problem using a commercial solver within a reasonable timeframe while preserving solution quality. Significantly, our methodology allows decision-makers to flexibly scale down the problem to their desired extent. Our experiments, conducted on a total of 80 instances with up to 12 jobs and 190 employees, yield an average removal of 85.5% of decision variables. This resulted in a notable average speedup factor of 15.5, with an average cost increase of 1.2%.",Data Mining-Driven Shift Enumeration for Accelerating the Solution of Large-Scale Personnel Scheduling Problems,"[76316, 76950, 73478, 18350]",431,"[63, 66, 129]",1211,Interpretable Optimization Methods and Applications,14,13,03,Data Science Meets Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1005 [building - 202],"['Large Scale Optimization', 'Machine Learning', 'Scheduling']",WB-03
"This abstract focusses on ethical concerns for students’ usage of algorithms AI and Chat CPT in different nations, laws and decision making. Depending on care takers, universities and teachers’, the usage of those novel tools is quite different. For example, students might [a] be able or not able to [mis]use the tools, [b] find or lack access to true or false information, [c] loss of intellectual property rights, whether they and their parents or teachers are aware or not aware. 
This causes a discussion on ethical issues along the previously mentioned stakeholders and their complex societal or economic backgrounds. Using cases from Generative AI which describes algorithms such as Chat GPT, we will see what ethical issues mean for the students and their care takers and other stakeholders - Students who have access, usually like Chat CPT [Version 3, they cannot afford level 4] and they use it and their results sound good, but also lowers their own creativity as well as their analytical and critical thinking. This sample shows ethical concerns across biases, cultural/individual identities, autonomy, social justice, safety and accountability. 
In a nutshell - The existence of novel tools allows well-to- do users or universities to access new content and achieve support for various purposes like writing, speaking, marketing, etc., while issues like inclusion/social cohesion might be left, while commercial tool providers benefit from new customers and revenue. 
",An Ethical compass for Artificial Intelligence [AI] and Chat CPT for students’ and universities,[36972],107,"[8, 34, 41]",1212,Ethics of OR and artificial intelligence ,28,2,20,OR and Ethics,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,45 [building - 116],"['Artificial Intelligence', 'Education and Distance Learning', 'Ethics']",MA-20
"In container shipping, ports often face uncertain demand and supply of empty containers over time. Effectively and efficiently managing empty containers between ports in multiple time periods is challenging. In this paper, we consider two neighbouring ports that are facing independent and uncertain supply and demand of empty containers from shippers. We seek the optimal empty container transfer policy between two ports over a multi-period planning horizon to minimize the total expected cost, consisting of empty container transferring costs between two ports, inventory holding costs, and container leasing costs at both depots. The problem is formulated as a stochastic dynamic programming model. The local properties of the value function, such as the first and second derivatives on a region-wise basis, are analysed. The region-wise properties of the value function enable us to establish the structural characteristics of the optimal empty container transfer policy over multiple time periods. Specifically, the entire state space is divided into three control regions by two monotonic switching curves. The asymptotic behaviours of the switching curves are analysed analytically. The structural properties of the optimal policy and the asymptotic behaviours of the switching curves are then used to construct simple near-optimal and easy-to-operate policies. Numerical examples are provided to demonstrate the analytical results.",Optimal multi-period policy for empty container transferring between two ports under uncertainty,[35540],167,"[65, 61, 135]",1213,Sustainable freight transportation,52,10,62,OR in Port Operations,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S12 [building - 101],"['Logistics', 'Inventory', 'Stochastic Models']",TD-62
"We consider a repairable service system in which a server experiences randomly occurring service interruptions during which the server works slowly. Every service-state change preempts the task that is being processed. The server may then replace the task with a different one, or it may repeat the same task from the beginning, under the new service-state. We study the completion time of a task under the last two cases as a function of the task size distribution, the service interruption frequency/severity, and the repair frequency. We derive closed form expressions for the completion time distribution in Laplace domain under replace and repeat recovery disciplines and present their asymptotic behavior. In general, the heavy tailed behavior of completion times arrises  due the heavy tailness of the task time. However, in the preempt- repeat  service discipline, even in the case that the server still serves during interruptions albeit at a slower rate, completion times may demonstrate power tail behavior for exponential tail task time distributions. Furthermore, our results reveal that the stationary first order moments, i.e., expected completion time, expected number in the system in infinite server queues with Markov modulated service processes are insensitive to the way the service modulation affects the servers, system-wide modulation affecting every server simultaneously vs identical modulation affecting each server independently. ",Completion Times of Jobs on Two-State Service Processes and Their Asymptotic Behavior,[77071],799,"[121, 130, 135]",1214,Stochastic Models in Service Operations II,50,12,39,Stochastic Modelling,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,35 [building - 306],"['Queuing Systems', 'Service Operations', 'Stochastic Models']",WA-39
"There is a gap in music performance, education and psychology in terms of memorisation training for post-tonal piano music. Despite the repertoire spanning over 100 years, pedagogues and professionals still lack effective tools for developing this skill. Existing research mostly focused on observing practitioners’ behaviours during practice, to understand how these prepare for a memorised performance. However, a systematic method for effective memorisation is not provided. This paper discusses Conceptual Simplification - a new method for analysis, learning and memorisation of post-tonal piano music. This presents a novel implementation to musical memorisation building on mathematics and computer science to improve human memory and musical performance. However, Conceptual Simplification does not require previous scientific training to be successfully implemented and works for different learning styles and types of complexity. It could also be adapted to other instrumentalists, singers and conductors; and musical genres; and presents enough flexibility for other practitioners to incorporate additional strategies. The method also assists in preventing performance anxiety and reducing the potential for injuries that usually result from repeated practice. The method’s systematic approach engages conceptual memory and reasoning, leading to more confident memorised performances, while needing less repetition during practice.","Conceptual Simplification - A New Method for Analysis, Learning and Memorisation of Post-Tonal Piano Music",[77073],790,"[5, 18, 92]",1216,OR Education II,48,3,16,OR Education,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,19 [building - 116],"['Algorithms', 'Computer Science/Applications', 'OR in Education']",MB-16
"Serious disasters had occurred in Japan as Kobe earthquake, East-Japan Tsunami and Covid-19 and Noto earthquake of this year. The economic burden to recover from disasters is expanding government deficit further. Reinsurance and securitization such as catastrophe bond are analyzed for application to overcome future financial burden of disasters .",Catastrophe Risk Management,[5474],280,"[45, 30, 50]",1217,Decision making in Insurance and Pensions,74,3,57,Modern Decision Making in Finance and Insurance,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S06 [building - 101],"['Financial Modelling', 'Disaster and Crisis Management', 'Game Theory']",MB-57
"In the past years, augmented Lagrangian methods have been successfully applied to several classes of non-convex optimization problems, inspiring new developments in both theory and practice. In this paper we bring most of these recent developments from nonlinear programming to the context of optimization on Riemannian manifolds, including equality and inequality constraints. Many research have been conducted on optimization problems on manifolds, however only recently the treatment of the constrained case has been considered. In this paper we propose to bridge this gap with respect to the most recent developments in nonlinear programming. In particular, we formulate several well known constraint qualifications from the Euclidean context which are sufficient for guaranteeing global convergence of augmented Lagrangian methods, without requiring boundedness of the set of Lagrange multipliers. Convergence of the dual sequence can also be assured under a weak constraint qualification. The theory presented is based on so-called sequential optimality conditions, which is a powerful tool used in this context. The paper can also be read with the Euclidean context in mind, serving as a review of the most relevant constraint qualifications and global convergence theory of state-of-the-art augmented Lagrangian methods for nonlinear programming.",Constraint qualifications and strong global convergence properties of an augmented Lagrangian method on Riemannian manifolds,"[45761, 64956, 77077, 53502]",58,"[5, 113, 52]",1218,Optimization on Manifolds,69,8,41,Optimization on Geodesic Metric Spaces - Smooth and Nonsmooth,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,97 [building - 306],"['Algorithms', 'Programming, Nonlinear', 'Global Optimization']",TB-41
"A company's dedication to environmental, social, and governance principles [ESG] has led to an interest in developing and evaluating firm-level ESG scores. Despite the considerable interest in assessing firm-level scores, investment analytics lack standardized country-specific ESG indexes to comprehensively represent sustainability while facilitating asset predictability. This deficiency is noticeable in the emerging BRICS capital markets [i.e., Brazil, Russia, India, China, and South Africa [SA]]. Focusing on SA, the primary objective of this study is to formulate an innovative algorithmic framework to identify distinct dynamic ESG factors capable of assessing sustainability risk in securities traded on the Johannesburg Stock Exchange [JSE]. The proposed alternative data procedure utilizes asset returns in the exploratory factor analysis to generate maximum validity factor scores to formulate JSE-specific sustainability factors. The effectiveness of these factors is assessed through a machine-learning model operating within a complex information space that considers the interconnections of valuation signals. Employing an explainable AI technique [XAI], we determine that the newly created S-factor emerges as the primary feature for explaining next-period returns. Following closely behind is the E factor, the SA volatility index, and a precious metals factor. XAI reveals that these factors excel at discerning the varying emphasis South African firms place on their ESG signals.",Generative South African ESG indexes and multi-target neural complexity in return prediction,"[11548, 25951, 74483]",57,"[139, 45, 8]",1220,AI and ESG for the small economy SDG agenda [EWG-ORD Workshop 2],67,10,18,OR for Development and Developing Countries,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,42 [building - 116],"['Sustainable Development', 'Financial Modelling', 'Artificial Intelligence']",TD-18
"We can only function as a society with effective public administration. Without public administrators, a government would be unable to function. The responsibilities of public administrators are challenging. India, a nation of immense diversity and cultural richness, stands as a testament to the resilience of democracy and the challenges it faces in managing its vast and complex administrative system. With twenty-eight states and eight union territories catering to almost 1/6th of the world's population, India's public administration system is an intricate network of structures, processes, and institutions. Each Indian state is divided into smaller administrative entities called districts. Each district is managed by an Indian Administrative Service [IAS] officer known as a District Magistrate [DM] or District Collector [DC]. DM is one of the key authorities ensuring that the district's administration runs smoothly and efficiently. With different duties and tasks, the DMs are overburdened and overworked. As a result, the Decision Support System [DSS] is essential for facilitating daily operations and significantly enhancing the productivity of DMs. This study aims to comprehensively analyze the challenges faced by the district administration in India and to answer how a decision support system helps DMs make decisions effectively and improve quality and productivity.",DECISION SUPPORT SYSTEM FOR DISTRICT MAGISTRATE ,"[76102, 77079, 43014]",655,"[26, 7, 15]",1221,Decision Support in the Public Sector and Policy Making,45,13,45,Decision Support Systems,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,30 [building - 324],"['Decision Support Systems', 'Analytics and Data Science', 'Complex Societal Problems']",WB-45
"Aligned with the EU, Germany aims to reduce its GHG emissions by 65% compared to the 1990 levels, with the objective of reaching climate neutrality by 2045. While most sectors have achieved a notable decline in GHG emissions, measures to reduce emissions in the transport sector have proven largely insufficient. A key challenge for decarbonizing the transport sector remains the dominating role of fossil fuels in the current GHG emissions level. Thus, the adoption of renewable fuels is of high significance for meeting the climate targets. Renewable fuels, e.g., power-to-x, advanced biofuels, hydrogen are supported by the Renewable Energy Directive III, where the objective is to meet a minimum share of 5.5% in European energy consumption in the transport sector by 2030. Despite the advantages of renewable fuels, various economic, environmental, technical, regulatory, and social challenges exist to their adoption and diffusion. To investigate the importance of the barriers and their interactions from a system perspective, an analytical approach is developed based on the decision making trial and evaluation laboratory, K-means algorithm, the maximum mean de-entropy algorithm, interpretive structural modeling, and a ranking multi-criteria decision analysis method with Type-2 Neutrosophic Numbers. Insufficient renewable energy policies and lack of coordination in the supply chain are identified as the main system barriers. ",Renewable Fuels in Transport - An Analysis of Market Development Barriers,"[73715, 76902, 2650, 72344]",886,"[25, 37, 143]",1222,MCDA in energy,44,13,47,Multiple Criteria Decision Analysis,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,50 [building - 324],"['Decision Analysis', 'Energy Policy and Planning', 'Transportation']",WB-47
"The term “flow-sizing” in health systems refers to the strategic operations management of critical care resources according to the dynamic patients’ flow, adapting the resources’ availability to the fluctuating needs. The necessity of a comprehensive, quantitative approach to flow-sizing has become particularly relevant after the latest COVID-19 pandemic and planning for adequate capacity to confront the next critical care crisis is urgent. This research work attempts to develop a utility function that incorporates the major critical care capacity metrics. The objective is to match capacity and critical care demand to ensure that all patients receive appropriate and fair care, as well as capturing this fairness’ price. The vertical and horizontal integration capability of critical care delivery [mainly ICUs] for healthcare organizations, and its flow-sizing process in a fast and cost-effective manner during normal, strain and surge conditions, are taken into consideration. As a case study, the performance of the Hellenic healthcare system in terms of providing critical care during COVID-19 pandemic is examined. 

Acknowledgement - This work has been partly supported by the University of Piraeus Research Center.",What is the price for flow-sizing critical care resources with fairness? ,"[67344, 77080, 77081]",595,"[56, 12]",1224,COVID-19,3,13,15,OR in Health Services [ORAHS],"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,18 [building - 116],"['Health Care', 'Capacity Planning']",WB-15
"In a stochastic dynamic setting over a planning period of days, we consider a set of customers, each placing exactly one order on a known day. Each order is associated with an uncertain demand and can be either served or outsourced by paying a penalty. Customers have day windows that, if some flexibility is allowed, can be extended by anticipating or delaying the service by paying a penalty. At the end of each day, we decide which pending orders to serve on the following day by exploiting a heterogeneous fleet of vehicles. The goal is to minimize the total costs due to fleet sizing and customer inconvenience for anticipations, delays, and outsourcing. We formulate the problem as a Markov decision process and solve it with Approximate Dynamic Programming by integrating robust-optimization techniques in Direct Lookahead Approximation policies. Our study is motivated by an on-demand waste collection service. Nevertheless, the problem has applications in many other contexts, such as reverse logistics and technician routing [1, 2]. 

References
[1] S. Mishra and S. P. Singh, Designing dynamic reverse logistics network for post-sale., title, Annals of Operations Research, pages 1–30, 2022.
[2] M. Nowak and P. Szufel, Technician routing and scheduling for the sharing economy, European Journal of Operational Research, 2023.",Robust Policies for a Multi-Stage Assignment Problem Under Demand Uncertainty,"[77082, 33366, 79431, 19719]",741,"[136, 127, 12]",1226,Vehicle Routing Under Uncertainty 1,5,5,64,VeRoLog - Vehicle Routing and Logistics,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S16 [building - 101],"['Stochastic Optimization', 'Robust Optimization', 'Capacity Planning']",MD-64
"The European Union [EU] pursues a progressive policy course towards sustainable development, reflected in the implementation of a portfolio of targeted policies such as, e.g., the Green Deal, the Fit for 55 Package. While many of the EU's sectors have experienced declining emissions, the transport sector, aviation in particular, has shown the opposite trend. To address this challenge, the EU has launched a political effort to enforce a green transition from fossil fuels to sustainable aviation fuels [SAFs]. In 2023, the European Commission adopted the ReFuelEU Aviation Directive that mandates the uptake of SAFs, including power-to-liquid fuels and advanced biofuels, as part of a larger effort to achieve sustainable aviation. Analyzing and modeling the effects of such political frameworks is crucial for understanding their long-term impact on industry developments and consumer behavior. A sound system understanding to explore both direct but also indirect effects provides a crucial precondition for effective policy-design. For this purpose, an integrated approach based on joint System Dynamics and Agent-based Modeling is used to analyze the economic and environmental impact of the current regulatory framework on market development options of SAFs. Findings indicate that efficient regulatory interventions can support the transition and that achieving the green transition requires a shared responsibility among customers, industry players, and policymakers.",Toward Climate-neutral Aviation - The Market Adoption of Sustainable Aviation Fuels using Agent-based Modeling,"[72344, 77083, 73715]",687,"[3, 139, 4]",1227,Toward Climate Neutrality,80,8,53,Sustainable and Resilient Systems,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,8007 [building - 202],"['Agent Systems', 'Sustainable Development', 'Airline Applications']",TB-53
"Distributionally robust optimization [DRO] is motivated as a counterpart of the usually unknown underlying probability distribution [PD] followed by the uncertainty in dynamic  problems. An approach is presented for the highly combinatorial Cross-dock Door Design Problem [CDDP]  solving to  decide the number and nominal capacity of the strip and stack doors. The strategic uncertainty is represented in a finite set of stagewise-dependent scenarios and the operational one is done in a finite set of stage-dependent scenarios. It is assumed the availability of a Nominal Distribution [ND] for the realization of the strategic parameters in the immediate successors set of any node, and a ND of the  realization of the operational parameters in the stages through the multi-horizon scenario tree.  Those ambiguity sets are obtained from the projections of appropriate perturbations of the cumulative distribution functions of the ND realizations in a set of modeler-driven PDs, where the Wasserstein distance is satisfied for a given radius. A mixed binary quadratic DRO-MH modeling paradigm is presented to consider the strategic and operational ambiguity sets in CDDP. The aim is to minimize the overall highest expected DRO cost in the nodes, among the ambiguity set members, subject to the constraint system for each one. ","Cross-dock Door Design Problem, CDDP, Multi-horizon Distributionally  Robust Optimization","[139, 67934, 23052]",456,"[136, 14, 114]",1228,Cross-dock Door Problems,49,5,35,"Stochastic, Robust and Distributionally Robust Optimization","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,44 [building - 303A],"['Stochastic Optimization', 'Combinatorial Optimization', 'Programming, Quadratic']",MD-35
"We study the energy generation and storage problem for a very general configuration of a pumped hydro energy storage [PHES] facility having two connected reservoirs fed by natural inflow. The operator decides in real-time how much water to pump or release in the PHES facility, thereby determining the amount of electricity to buy from or sell to the market. We model this problem as a Markov decision process under uncertainty in streamflow rate and electricity price. We prove the optimality of a state-dependent threshold policy under positive electricity prices - The state space can be partitioned into several disjoint domains, each associated with a different action type, such that it is optimal to bring the total amount of water in the PHES facility and the amount of water in the upper reservoir to a different pair of state-dependent target levels in each domain. Leveraging our structural results, we develop a policy-approximation algorithm as a heuristic solution method when the electricity price can be negative. This algorithm yields near-optimal solutions in our data-calibrated instances one order of magnitude faster than the standard dynamic programming algorithm. Modifying this algorithm by imposing partial-state-dependent target levels leads to up to 29 times faster solution times and reduces total cash flows by only 1.32% on average from optimal.",Pumped hydro energy storage - structural results and solution algorithms,"[56904, 71323, 76145, 43460, 77084]",453,"[93, 108, 37]",1230,Optimization of energy storage systems,21,7,22,Energy Management,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,81 [building - 116],"['OR in Energy', 'Programming, Dynamic', 'Energy Policy and Planning']",TA-22
"The increasing global emphasis on sustainable waste management has stimulated efforts to refine transportation infrastructure and travel regulations to improve the efficiency of garbage collection as a pivotal element within sustainable urban municipal solid waste management strategies. Therefore, improving waste collection systems can lead to various enhancements in quality of life. The most common waste collection form is the Door-To-Door Collection System, where household waste is picked up using trucks or other vehicles. Some disadvantages of this system are using trucks on narrow streets, hygienic effects, and malodor related to waste storage. An alternative method is the use of automated waste collection systems.

This study focuses on developing a system that integrates a stationary pneumatic waste collection system with a door-to-door waste collection system. In this fashion, We provide a mathematical model for this combined waste management system with the goal of minimizing total transportation costs.  We present a holistic solution approach based on a set-partitioning formulation utilizing vehicle routing problems and price-collecting Steiner tree problems.",On combining conventional door-to-door and pneumatic waste collection systems,"[76995, 10538, 4161, 77086]",978,"[100, 145, 84]",1231,Vehicle routing II,6,3,60,Transportation,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S09 [building - 101],"['OR in Sustainability', 'Vehicle Routing', 'Optimization Modeling']",MB-60
"In order to achieve the goals of the European Green Deal set by the European Commission, a reduction of the usage of fossil fuels is necessary. This also applies to the sector of district heating, where gas is still widely used, specifically in Austria.
The district heating network in Neusiedl am See in the Austrian Burgenland region comprises a gas boiler, a biomass plant as well as four high-performance heat pumps, which are fueled by energy generated in the local wind park, as heat production components. Although the share of gas used for the heat production is already low with only a few per cent, depending on the current electricity prices, the gas boiler is still necessary for the operation of the district heating network. Additionally, an electrolyzer was integrated into the system in 2023 in order to use the surplus energy from the wind park for the production of hydrogen. However, when operating the district heating network most economically, the hydrogen production leads to a significant increase in the gas share, with the percentages ranging between 17 and 21 %.
The objective of this work is to reduce the usage of gas in the network while also operating the electrolyzer by implementing additional energy from a hypothetical PV plant nearby. The results show that the gas share is crucially reduced to a range of 4 to 10 %.
",Reduction of Fossil Fuel Usage for Hydrogen Production in District Heating Networks by Implementing PV Energy,"[75235, 70032, 70127]",840,"[93, 84, 111]",1232,OR in Heating Systems,23,9,19,OR in Energy,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,44 [building - 116],"['OR in Energy', 'Optimization Modeling', 'Programming, Mixed-Integer']",TC-19
"Livestreaming e-commerce has emerged as a highly effective channel for manufacturers and retailers recently. When two streamers from different rooms collaborate to host a co-live streaming event and individually promote products in their respective rooms, it attracts various types of consumers with assessments to products. In this background, we explore the optimal decisions and the best sales strategy for three common  ivestreaming sales strategies - pure component [PC], mixed bundling [MB], and pure bundling [PB]. The study shows that it is more advantageous for streamers of similar or identical types to collaborate in a co-live streaming event, and MB and PB strategies consistently outperform PC strategy; however, a co-live streaming event doesn’t offer significant advantages over a solo-live one when there is already sufficient traffic. The extended studies explore the robustness of the findings and uncover two additional insights. Firstly, a co-live streaming room with a higher proportion of co-subscribers yields more profits. Secondly, if two streamers sell similar or identical products, PB strategy is more effective if valuation discounts are low, whereas MB strategy shines. However, when co-live streaming sells significant different products, MB strategy is advantageous only when streamers with lower traffic recommend higher value products.",Product bundling strategy for co-live streaming E-commerce considering consumer subscription,[77088],658,"[32, 25, 84]",1233,Decision Support for Operations Management,45,12,45,Decision Support Systems,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,30 [building - 324],"['E-Commerce', 'Decision Analysis', 'Optimization Modeling']",WA-45
"We use the natural language descriptions of exotic and collectible vehicles from online auction markets to semi-nonparametrically estimate the primitives of demand [private valuations, number of potential bidders] for each individual vehicle in our dataset. The first stage of our estimation method fine-tunes a large language model [RoBERTa, Liu et al., 2019] to predict the inputs of our demand estimators using the descriptions provided for each vehicle. We then append a multi-layer perceptron to the tuned language model that projects into the parameter space of these estimators so that a second stage training can recover the primitives of interest. Our identification method relies on the structural features of specific bids submitted in these markets, which can proxy for valuations under our data generation assumptions. Finally, we demonstrate how this model can generate counterfactual analyses using new natural language descriptions within the defined product space. ",Giving Deep Attention to Consumer Preferences with Large Language Models,"[77090, 15633]",263,"[9, 66, 33]",1237,Pricing and applications 3,11,2,59,Pricing and Revenue Management,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S08 [building - 101],"['Auctions / Competitive Bidding', 'Machine Learning', 'Economic Modeling']",MA-59
"We study a basic scheduling problem with resource constraints - A number of jobs need to be scheduled on two parallel identical machines to minimize the makespan, subject to the constraint that jobs may require a unit of one of the given renewable resources during their execution. For this NP-hard problem, we develop a fully polynomial-time approximation scheme [FPTAS]. Our FPTAS makes novel use of existing algorithms for the subset-sum problem and the open shop scheduling problem.",An FPTAS for scheduling with resource constraints,"[2509, 13919]",349,"[129, 5, 14]",1238,Resource constrained scheduling,35,10,60,Project Management and Scheduling,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S09 [building - 101],"['Scheduling', 'Algorithms', 'Combinatorial Optimization']",TD-60
"In the transition to climate neutrality, hydrogen is expected to play a vital role as a clean and versatile energy carrier. However, large-scale deployment of hydrogen requires adequate production and transportation infrastructure. A promising option to produce renewable hydrogen involves the utilization of biomass gasification. To efficiently bridge local biomass potentials and hydrogen demand, decentralized, small-scale production networks with mobile production units present an opportunity. In this context, we recognize a new optimization problem that integrates the dynamic location-routing problem with the traveling purchaser problem, accounting for relocatable modular production facilities. We propose a novel integer programming formulation, considering resource management, production, transportation, and demand fulfillment. To address the computational challenges arising from practical applications, we develop a Variable Neighborhood Search [VNS] heuristic, which consists of sequentially optimizing location-allocation, resource selection, and routing aspects, using multiple neighborhood structures and random perturbation strategies. Extensive computational experiments illustrate the effectiveness of our VNS approach across a large set of problem instances. Furthermore, we present a case study focusing on local biomass-to-hydrogen production in Germany, leveraging straw as a feedstock and demonstrating the practical applicability of our proposed methodology.",Dynamic Location-Routing With Modular Production Facilities and Resource Collection - A Variable Neighborhood Search Approach,"[76982, 71977]",784,"[145, 64, 74]",1240,Dynamic Vehicle Routing 2,5,10,64,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S16 [building - 101],"['Vehicle Routing', 'Location', 'Metaheuristics']",TD-64
"COVID-19 is the first epidemic in the history of mankind such that daily data are available in all countries. It makes possible to get an understanding of the quantitative processes of pandemics.

A pandemic is a saturation process. This fact makes it possible to find good approximations of the quantitative processes of the pandemic including the total number of cases, and the number of vaccinations. The method is applied to 18 countries. The approximation has good quality even in the case of multiple waves.

The procurement of vaccines is made on the level of countries. Based on the obtained approximation, inventory policies are elaborated for countries. The first model minimizes the total inventory cost in the case of deterministic consumption and delivery.

The second model keeps the probability of shortage under a prescribed level in case of stochastic delivery. This model is a generalization of the so-called Hungarian inventory model.

Rich countries purchased huge quantities from vaccines. They could not use completely all these vaccines. On the other hand, poor countries could not get enough vaccines. Therefore, they repeatedly infected the population of rich countries. It was a reason of the multiple waves. It is assumed in the third model that one producer delivers to several countries. A fair distribution is the aim of the model with minimal total shortage.",COVID-19 quantitative processes and vaccine inventory policies ,[23034],774,"[58, 61, 73]",1241,Infectious diseases and pandemics 2,38,14,21,OR in Humanitarian Operations [HOpe],"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,49 [building - 116],"['Humanitarian Applications', 'Inventory', 'Medical Applications']",WC-21
"The e-waste management market is a major business opportunity with a projection to grow to USD 137.60 bn by 2029 being a key motivator to undertake research in this area. A formal, methodical, organised sector is essential to ensure the best possible collection of e-waste as well as its proper disposal or recycling. While the UN SDG Goal No. 12 aims for responsible production and consumption, businesses are still looking into ways to guarantee that e-waste collection, recycling, or disposal is not an expensive endeavour. This implies that a variety of activities throughout the supply chain would be involved, not only in the collection, disposal, and recycling of e-waste, but also in the establishment of a system for its management.
This paper builds on the novel Present-Unique-Repeatable-Environmentally-friendly [PURE] framework underlying the introduction of makerspaces and Centre for Refurbishment and Recycling of Electrical and Electronic Waste [CReW] & addresses quantitatively how the network flow should be to achieve sustainable development goals of minimising cost and emissions. Also generates employment opportunities while tackling health concerns for those residing within the proximity of landfills. Therefore it ensures a leaner process that can be adopted horizontally across industries while also coining electronic reverse logistics into e-waste's forward logistics as a formal supply chain design",Formalising e-waste Supply Chain for Sustainable Development,"[77072, 37748, 65605]",923,"[150, 84, 139]",1243,Optimization for the Circular Economy,18,9,23,"Circular Economy, Remanufacturing and Recycling ","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,82 [building - 116],"['Network Flows', 'Optimization Modeling', 'Sustainable Development']",TC-23
"The United States [US] opioid crisis has led to over 840,000 fatalities since the 1990s. It has strained hospitals, treatment facilities, and law enforcement agencies due to the enormous resources and procedures needed to respond to the crisis. As a result, many individuals who use opioids never receive or finish the treatment they need and instead have many interactions with hospitals or the criminal justice system.  This paper introduces a discrete event simulation model that evaluates three opioid use disorder treatment policies - arrest diversion, re-entry case management, and overdose diversion. Publicly available data from 2011 to 2019 in Dane County, Wisconsin [in the US], is used to forecast opioid-related outcomes through 2032. Through analyzing a variety of policy-mix implementations, this study offers a versatile framework for evaluating policies at various implementation levels. The results demonstrate that treatment policies that create new pathways and programming by utilizing treatment services and successfully divert at least 20% of eligible individuals can lead to more opioid-resilient communities and result in substantial savings.",Evaluating diversion and treatment policies for opioid use disorder,[61471],514,"[15, 101, 56]",1245,"Advancements of OR-analytics in statistics, machine learning and data science 5",16,8,28,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,065 [building - 208],"['Complex Societal Problems', 'OR/MS and the Public Sector', 'Health Care']",TB-28
"We study scheduling problems on parallel dedicated machines. Thus, each job can be processed on one specific machine only. The option of job-rejection is considered, and the total permitted rejection cost of all the jobs is bounded. Six scheduling problems are solved - [i] minimizing makespan, [ii] minimizing makespan with release-dates, [iii] minimizing total completion time, [iv] minimizing total weighted completion time, [v] minimizing total load, and [vi] minimizing maximum tardiness. Pseudo-polynomial dynamic programming algorithms are introduced for all these NP-hard problems. ",Scheduling on parallel dedicated machines with job rejection,"[29447, 29448]",233,"[129, 14, 108]",1246,Novel topics and recent advances in solution approaches in scheduling,64,3,26,Combinatorial Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,012 [building - 208],"['Scheduling', 'Combinatorial Optimization', 'Programming, Dynamic']",MB-26
"Operations Research [OR] is a discipline of applied mathematics usually taught at the university level. Nevertheless, many initiatives have been developed for younger students in the last few years. Starting from these, we designed Ricerca Operativa Applicazioni Reali [ROAR, i.e.., Real Applications of Operations Research], a learning path for higher secondary schools relying on active learning and constructionism. ROAR offers examples and problems closely connected with students’ everyday lives, balancing mathematical modeling and algorithms. ROAR aims to improve students' interest, motivation, and skills related to STEM by integrating mathematics and computer science through OR. ROAR is composed of three units. The first one, addressed to Grade 10, is dedicated to introducing mathematical modeling, linear programming, and using GeoGebra and Solver. The second unit, for Grade 11, focuses on graph theory and network applications, strengthening modeling skills and the development of heuristic algorithms. Finally, the third unit, for Grade 12, concerns the implementation of mathematical models with PuLP, an open-source Python library. Every teaching unit ends with presenting a final project to tackle in groups. In this talk, we provide an overview of the implementation of ROAR at the scientific high school IIS Antonietti in Iseo [Brescia, Italy] from March 2021 to January 2023, and we present derived initiatives and available resources.",The ROAR project - How to improve Grade 10-12 interest and motivation in Mathematics through a learning path based on Operations Research,"[77082, 68705, 77477, 78512, 78513]",455,"[92, 96, 84]",1247,Experienced Routes for the Teaching Students' Problem,48,4,16,OR Education,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,19 [building - 116],"['OR in Education', 'OR in Mathematics Education', 'Optimization Modeling']",MC-16
"The problem of partitioning objects into different groups according to predefined attributes has been extensively studied in OR research. A field in which grouping is frequently referenced is education, where students need to be divided into groups for collaborative learning. A major factor contributing to successful learning is student wellness, which is influenced by the group's social structure. Hence, the ability of the student to choose their group mates is crucial in the grouping process. Yet, students' wellness is not the only important factor in building groups. 
An additional important factor is the diversity of the group members in various features. Many studies in OR approached the grouping problem using the known setting of the maximally diverse grouping problem [MDGP], which does not concern student social preferences.

In this study, we consider a new version of the MDGP, in which each student declares a set of acceptable students, and the goal is to find a maximal diversity partition to groups in which every student is grouped with at least one of its acceptable mates. 
We formally defined the problem, studied its complexity, and found that a solution does not always exist and it is NP-complete. Driven by real-world challenges, we used students' acceptable choices to structure mid-school classes and project groups in academic courses using an integer programming solver. We analyzed the existence of a solution in each case and its quality.",Promoting Student Wellness through Maximally Diverse Grouping with Acceptance Relation,"[75308, 77097, 77098]",880,"[14, 34, 111]",1248,Exact methods in combinatorial optimization [Contributed],64,9,52,Combinatorial Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,8003 [building - 202],"['Combinatorial Optimization', 'Education and Distance Learning', 'Programming, Mixed-Integer']",TC-52
"As a major real-world problem, snow plowing has been studied extensively.
However, most studies focus on deterministic settings with little urgency yet enough time to plan. In contrast, we assume a severe snowstorm with little known data and little time to plan. We introduce a novel time-dependent multi-visit dynamic safe street snow plowing problem and formulate it on a rolling-horizon-basis. To solve
this problem, we develop an adaptive large neighborhood search as the underlying method and validate its efficacy on team orienteering arc routing problem benchmark instances. We create real-world-based instances for the city of Vienna and examine the effect of [i] different snowstorm movements, [ii] having perfect information, and [iii] different information-updating intervals and look-aheads for the rolling
horizon method. Our findings show that different snowstorm movements have no significant effect on the choice of rolling horizon settings. They also indicate that [i] larger updating intervals are beneficial, if prediction errors are low, and [ii] larger look-aheads are better suited for larger updating intervals and vice versa. However, we observe that less look-ahead is needed when prediction errors are low.",A Rolling Horizon Framework for the Time-Dependent Multi-Visit Dynamic Safe Street Snow Plowing Problem,"[2769, 57684, 74181]",745,"[145, 74]",1249,Dynamic Vehicle Routing 1,5,9,64,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S16 [building - 101],"['Vehicle Routing', 'Metaheuristics']",TC-64
"The goal of this research is to develop models for planning and scheduling production systems of rapidly decaying products. The F-18 radiopharmaceutical cyclotron [RC] production and its delivery is a specific and important example of such a system. F-18 isotopes are used for medical diagnosis and treatment. The RC production and delivery process consists of five stages - cyclotron, synthesis, vial filling, delivery to the hospital[s], and injection. The produced isotope decays exponentially throughout all these stages. The demand is ordered daily by each hospital, specifying each treatment’s dose and time of injection. The research focuses on the basic case of a single cyclotron, single material, and single hospital, which represents many existing radiopharmaceutical production and delivery systems. We develop and analyze a relaxed model for this case, together with a solution scheme for a detailed discrete injection plan. The proposed model is a novel relaxed optimization model with a convex cost function and a set of operations management constraints. The solution determines the number and sizes of the daily cyclotron batches to meet the hospitals’ demand while minimizing the costs of production and inventory holding, considering the radioactivity decay. The proposed solution process derives both symmetric solutions [of identical batches] and nonsymmetric solutions. A set of best feasible solutions are presented for the choice of the decision maker.",Integrated Production and Delivery of Rapidly Decaying Products,"[71826, 71839, 70019, 69590, 69280]",837,"[129, 113, 105]",1251,Production planning problems,32,10,49,"Lot Sizing, Lot Scheduling and Production Planning","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,M1 [building - 101],"['Scheduling', 'Programming, Nonlinear', 'Production and Inventory Systems']",TD-49
"Relief supplies are crucial in response to a disaster. However, current studies focus on the raise, delivery and allocation of relief supplies during the process of disaster relief, and it remains uncertain that how to deal with the recyclable relief supplies after responding to a disaster. Motivated by the field investigation and interview, in this paper, we aim to design a disposal mechanism to help the local authority decide the optimal value level, which determines whether the relief supplies should be stored for future use or disposed through an auction. Furthermore, we consider whether the local authority can benefit more from processing [e.g., classification and cleaning] the relief items for auction. A case study based on the practical background in China has been conducted to prove the feasibility of our mechanism. The result shows that through our disposal mechanism, the local authority can achieve a balance between reducing the high inventory cost and avoiding potential waste of useful relief items, which gains more benefit and fits for the Sustainable Development Goals.",Disposal mechanism design for relief supply recycling in post-disaster humanitarian logistics,"[77099, 77100, 77101]",90,"[30, 58, 139]",1252,Humanitarian aid provision and disposal,38,7,21,OR in Humanitarian Operations [HOpe],"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,49 [building - 116],"['Disaster and Crisis Management', 'Humanitarian Applications', 'Sustainable Development']",TA-21
"In the last years, the usage of Shapley value is being extended through machine learning algorithms as a method to determine the importance of a feature on its contribution to a certain target value. This is done by defining a cooperative game where features play the role of voters. In this talk we will focus in the need of using a predictor to define the game and its consecuences. We will point out other directions in order to obtain the importance of the features in a more robust way. Also we will comment real world use cases where the feature importances plays a key role.",Game theoretical approach to determine feature importance,"[67438, 77054, 67465]",619,"[50, 66]",1257,"Game Theory, Solutions and Structures IV",88,5,36,"Game Theory, Solutions and Structures","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,32 [building - 306],"['Game Theory', 'Machine Learning']",MD-36
"The aim of our study is to investigate the effects of migration on wages and incomes, and the impact of more liberal migration policies, which increase labour quotas by 5 per cent. The study focuses on the impact of migration in 21 regions, with particular attention to Slovakia and Germany. The study was based on data from the Global Trade Analysis Project database, using experimental designs by Walmsley, Ahmed and Parsons, and adjusted to capture current global migration flows. According to the findings of the study, the arrival of new, unqualified workers has a negative impact on the actual wages of unqualified residents, with the real wages falling by nearly 3 per cent, and the real wages of skilled workers rising by 0.73 per cent. Similarly, due to the arrival of new qualified immigrants, the real wages of qualified residents have declined by 2.62 per cent, while the real wages of non-qualified immigrants have increased by 0.53 per cent. These results can help policy makers to develop policies aimed at maximizing the benefits of migration. Finally, the study provides a new insight into the wage effects of migration in different areas, particularly in Germany and Slovakia.",Computable general equilibrium study of migration in Europe - the cases of Germany and Slovakia,[62017],74,"[33, 18]",1262,"Advancements of OR-analytics in statistics, machine learning and data science 3",16,4,28,"Advancements of OR-analytics in statistics, machine learning and data science","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,065 [building - 208],"['Economic Modeling', 'Computer Science/Applications']",MC-28
"This pedagogy-methodology paper addresses the challenges of preparing students for foresight fieldwork using AI. Foresight students engage with innovation forecasting, scenario planning, and technology roadmapping, applying them practically. Yet, universities hesitate to involve early career students with real-world stakeholders in fear of reputational risk. Teachers of these tools will often underestimate the role that their own experience has in their teaching, leaving students vulnerable to mistakes.

This experience bias influences preparedness. Effectively mitigating a risk, diminishes its perceived severity, and individuals who overprepare may be disappointed when nothing goes wrong because they overinvested in preparing, reducing the perceived value of preparation. Moreover, overconfident individuals underestimate the likelihood of failure, assuming normalcy as the norm, possibly dismissing threat warnings. 

Using foresight pedagogy as a case study, the paper proposes an approach to fieldwork preparation, that incorporates ChatGPT in a “roleplay” as stakeholder, for lowering risks, addressing cognitive biases, and emphasizing the importance of foresight in undergraduate education. We argue these tools can be used to develop futures literacy skills in controlled research settings, scalable to large classes and diverse case studies. There is potential for this approach to extend beyond future studies, aiding preparation for general community engagements.",Using ChatGPT in Foresight Fieldwork Pedagogy to Address the Preparedness Paradox by Mitigating the Over-optimism and Normalcy biases,"[77107, 77108, 77707, 77109]",677,"[10, 47, 8]",1263,Scenarios and foresight practices - Behavioural issues III,13,14,11,Behavioural OR,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,12 [building - 116],"['Behavioural OR', 'Forecasting', 'Artificial Intelligence']",WC-11
"In this talk, we provide a new algorithm to approximate equilibria of variational mean field 	game systems [MFG] with local couplings. 
Under suitable conditions on the coupling function, the dual of the 
variational formulation of the MFG reduces to the minimization of 
the sum of a proper convex lower semicontinuous function and a differentiable convex function whose gradient satisfies a locally Lipschitz-type 	condition. In this context, we provide a generalization of the 	proximal-gradient [or forward-backward] splitting algorithm for tackling the dual problem. We prove the convergence of our method and derive a linear convergence rate when the differentiable function is locally strongly convex. We recover classical results in the case when the gradient of the differentiable function is globally Lipschitz continuous and an already known linear convergence rate when the function is globally strongly convex. Compared with some benchmark algorithms to solve these problems, our numerical tests show similar performances in terms of the number of 	iterations but an important gain in the required computational time.",A dual proximal-gradient approach for variational mean field games,"[44381, 52161, 77552]",771,"[19, 21, 72]",1264,Variational techniques in conic optimization and mean field games,82,10,42,Variational Analysis and Continuous Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,98 [building - 306],"['Continuous Optimization', 'Convex Optimization', 'Mathematical Programming']",TD-42
"This study is motivated by optimizing the utilization of military resources within a joint force involved in targeting activities during operations.  The weapon-to-target problem [WTA] is type of assignment problem as each target requires a matching with an appropriate weapon and each weapon is a resource within a joint force’s targeting enterprise.  Engaging high priority targets necessitates consideration including its characteristics, vulnerabilities, and level of protection. Furthermore, one’s own weapons must be effective against the specific target to create a desired effect. High priority targets often require expensive weapons with high accuracy which makes these weapons scarce resources.  Emerging targets in the battlespace are associated with uncertainties, priorities, compatible weapon types and the corresponding number weapons to a desired effect. The matching of weapon-to-target is constrained by inherent capacities, the protection level of the targets, and subject to probability in the execution of each mission or target engagement.  All prioritized targets need to be engaged and neutralized, therefore potentially requiring a re-attack in a second stage. Therefore, the study employed a multi-stage and multi-objective model to replicate a dynamic targeting environment and linearized the probability aspect to enable integer linear programming [ILP] model. The study presents a new way of modelling WTA problems, augmenting a joint force targeting decisions.",Augmenting a Joint Force Dynamic Targeting Decisions,[77114],793,"[75, 77, 84]",1268,"Military, Defense, and International Security I",65,5,20,"Military, Defense, and International Security","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,45 [building - 116],"['Military Operations Research', 'Multi-Objective Decision Making', 'Optimization Modeling']",MD-20
"Before any binary classification model is taken into practice, it is important to validate its performance on a proper test set. Without a frame of reference given by a baseline method, it is impossible to determine if a score is “good” or “bad.” The goal of this paper is to examine all baseline methods that are independent of feature values and determine which model is the “best” and why. By identifying which baseline models are optimal, a crucial selection decision in the evaluation process is simplified. We prove that the recently proposed Dutch Draw baseline is the best input-independent classifier [independent of feature values] for all order-invariant measures [independent of sequence order] assuming that the samples are randomly shuffled. This means that the Dutch Draw baseline is the optimal baseline under these intuitive requirements and should therefore be used in practice.",The optimal input-independent baseline for binary classification,"[47048, 77115, 77116, 77118, 39439]",801,"[7, 66, 5]",1269,Recent Methodologies in Explainable AI [XAI] 2,71,3,04,Recent Advancements in AI ,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,1001 [building - 202],"['Analytics and Data Science', 'Machine Learning', 'Algorithms']",MB-04
"We consider the most standard SDP [semidefinite programming] program in a cutting-planes setting, i.e., the program is formulated using the infinitely-many linear cuts that define the SDP cone. We go beyond the most classical approach that relies on repeated separation; to separate a matrix X is enough to test if its minimum eigenvalue is above or below zero.  Instead, we upgrade the separation sub-problem to a projection sub-problem - given a feasible matrix X in the SDP cone and an arbitrary direction D, what is the maximum t such that X+tD stays in the SDP cone? This new sub-problem enabled the new algorithm to converge to the optimal solution through a sequence of both inner and outer solutions [with regards to the SDP cone]. The general Projective-Cutting-Planes logic was first proposed in a purely linear context [Daniel Porumbel, Projective Cutting-Planes, SIAM Journal on Optimization, 30[1] - 1007-1032, 2020].  We will show that the projection sub-problem can also be solved very efficiently in a new SDP context.  One way to solve it would be to compute the generalized eigenvalues of X and -D. But we seek maximum speed and we solve it using certain factorizations suitably tailored to our given context.  Results suggest that the overall method may be very competitive [compared to Mosek or ConnicBundle] for matrix sizes larger than 2000 × 2000. It is less competitive if the matrix size is low and if the initial program has many linear constraints.",Semidefinite programming by projective cutting planes,[76275],170,"[115, 21]",1270,Advances in Complex and Real Semidefinite Programming,68,8,38,"Conic Optimization - Theory, Algorithms, and Applications","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,34 [building - 306],"['Programming, Semidefinite', 'Convex Optimization']",TB-38
"Wildfires pose grave risks to human life, health, and infrastructure. To address these challenges, proactive fuel treatment strategies are crucial before each fire season. However, managing fuel treatment resources in south-east Australia becomes problematic due to species protection and forest maturity considerations. Existing models to tackle the multi-period problem consider inaccurate approximations based on estimated parameters in order to design treatment plan schedules. Our study extends previous work and introduces a robust modeling approach for devising multi-year treatment strategies. Initially, we introduce a new, more efficient formulation to solve the deterministic counterpart of the problem, which is then extended to account for a given fixed noise that surges the amount of fuel load due to uncertainties in vegetation growth and treatment effect. We then expand this formulation to accommodate a fixed noise factor, representing uncertainties in vegetation growth and treatment effects. Subsequently, we adapt the model to a robust setting, allowing nature'' to select values from a decision-dependent uncertainty set. Our polynomial size robust mixed-integer optimization model incorporates worst-case scenarios from the planner's objective perspective of fuel growth and treatment effects, offering adjustable uncertainty levels. Moreover, our modeling approach provides flexibility for planners, enabling them to minimize the total fuel load or optimize network connect",Robust Network-based Optimization for Multi-Period Fuel Treatment,"[77119, 74175, 13418, 50297]",4,"[127, 48, 94]",1271,Optimization in Agriculture,20,2,12,OR in Agriculture and Forestry ,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,13 [building - 116],"['Robust Optimization', 'Forestry Management', 'OR in Environment and Climate change']",MA-12
"This presentation focuses on a multiobjective elliptic control problem with state constraints. Specifically, we show how we can apply recent results on stability estimates for these kinds of problems [Khan, Sama. Optimization 72[4], 1009-1036, 2023] to obtain numerical error estimates for different discretization schemes. ",Stability analysis of state constrained multiobjective elliptic control problems,"[9069, 52999]",49,"[112, 21, 20]",1272,Vector and Set Optimization I,33,2,41,Vector and Set Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,97 [building - 306],"['Programming, Multi-Objective', 'Convex Optimization', 'Control Theory']",MA-41
"As a widely used tool in supply chain practice, audits can alleviate the supply chain inefficiencies under asymmetric information. In this study, we investigate the impact of the commitment to a cost audit on the optimal supply chain contract design, and the corresponding audit efficiency.",Non-committed cost auditing in supply chain contracts,"[57624, 77120, 77121]",339,"[138, 50, 82]",1275,Optimal control in supply chain management,90,2,33,Optimal Control Theory and Applications,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,42 [building - 303A],"['Supply Chain Management', 'Game Theory', 'Optimal Control']",MA-33
"The handicap is a matrix parameter associated with the linear complementarity problem [LCP]. While LCP is an NP-hard problem, there are special cases, such as when the handicap of its coefficient matrix is finite, where an interior point algorithm can efficiently solve the problem. However, the theoretical complexity of the algorithm is linearly dependent on the handicap. Consequently, a key and relevant inquiry is to give an upper bound on the size of the handicap. We show that, with a fixed matrix dimension, the handicap cannot be arbitrarily large, more precisely, we prove the conjecture that it cannot be double exponential in the bit size of the matrix.",The size of the handicap,"[38402, 9177, 66922]",139,"[19, 72, 110]",1276,Interior point methods,68,3,38,"Conic Optimization - Theory, Algorithms, and Applications","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,34 [building - 306],"['Continuous Optimization', 'Mathematical Programming', 'Programming, Linear']",MB-38
"In this talk, we introduce a framework that facilitates the analysis of discounted infinite horizon Markov Decision Processes [MDPs] by visualizing them as deterministic processes where the states are probability measures on the original state space and the actions are stochastic kernels on the original action space. More specifically, we provide a simple general algebraic approach to lifting any MDP to this space of measures; we call this to measurize the original stochastic MDP. We show that measurized MDPs are in fact a generalization of stochastic MDPs, thus the measurized framework can be deployed without loss of fidelity. Lifting an MDP can be convenient because the measurized framework enables constraints and value function approximations that are not easily available from the standard MDP setting. For instance, one could add restrictions or build approximations based on moments, quantiles, risk measures...etc. In addition, measurized MDPs are deterministic and are particularly beneficial when managing large populations. A reason is that high-dimensional weakly coupled MDPs can be reduced into a unidimensional MDP in the space of distributions when the state-components are independent and identically distributed. This implies that solving the measurized problem only requires aggregated information, eliminating the impractical assumption of constantly having detailed and updated information for each component in high-dimensional contexts.",Measurized Markov Decision Processes,[50113],898,"[20, 135, 108]",1279,Optimal control theory,90,10,33,Optimal Control Theory and Applications,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,42 [building - 303A],"['Control Theory', 'Stochastic Models', 'Programming, Dynamic']",TD-33
"Interest for organic products and concerns regarding sustainable practices raise an important question for farmers - should they run their farms conventionally or organically? We address this question by means of a mathematical programming model tailored to Norwegian cattle farms. The model aims to optimize gross margin, subject to government support stipulations outlined in the Norwegian agricultural agreement and a number of other features. A computational study allows us to assess the economic performance of different farm settings and the trade-offs between crop yield, livestock capacity, organic premiums, and government payments. We derive some insights into the conditions when organic outperforms conventional systems, and vice versa. These hold significant relevance not only to farmers, but also to farm advisors and policy makers.",Conventional or organic cattle farming? Trade-offs and optimal gross margin analysis,"[14317, 77126, 77129, 77130, 77127, 77128]",591,"[89, 0]",1283,OR in Livestock farming,20,7,12,OR in Agriculture and Forestry ,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,13 [building - 116],['OR in Agriculture'],TA-12
"Data is now unanimously considered a key firm asset for enabling better operational decisions. However, data-driven decisions can inadvertently expose private data, leaving firms vulnerable to unforeseen danger. How to manage data security risks by protecting data from being inferred from observable decisions thus becomes an important question. In this paper, we focus on data security in supply chains due to their data-intensive nature. Specifically, we examine a data-driven contextual newsvendor problem. To quantify and ensure data security, we adopt the notion of differential privacy, a mathematically rigorous measure of data security that limits an attacker's inference accuracy. Employing convolution smoothing and noise injection, we propose several differentially private algorithms that provably guarantee both data security and asymptotic optimality with [near] optimal rates. In the non-asymptotic regime, we further identify three drivers of the cost of data security; namely, dataset size, context, and number of products. This finding suggests that gathering more data, collecting detailed context, and pooling data from multiple products can lower data security cost. Lastly, we examine the impact of a newsvendor's private algorithms on supply chain partners. We discover additional distortion to the demand signaling process and lower profit share for an upstream supplier.",An Algorithmic Approach to Managing Supply Chain Data Security - The Differentially Private Newsvendor,"[77131, 40989]",726,"[5, 7, 138]",1286,Optimization in Online Environments,14,3,03,Data Science Meets Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,1005 [building - 202],"['Algorithms', 'Analytics and Data Science', 'Supply Chain Management']",MB-03
"The Traveling Purchaser Problem [TPP] has been a central focus in supply chain management, aiming to optimize the procurement process while minimizing total purchasing and transportation costs. We consider a new variant of TPP, one that incorporates shared vehicles among the purchasers and harnesses the power of bundle discount policy from suppliers. In our proposed structure, buyers can not only use the capacity of each other's vehicles but also integrate their demands, and by group purchasing receive their needed items through bundle discounts at the lowest price. In a typical bundle discount policy, a buyer might be reluctant to buy the whole bundle's items individually. Consequently, this study delves into the amalgamation of demands of different purchasers to harness the advantages of bundle discounts. Meanwhile, each bundle contains some environmentally friendly goods that determine its sustainability score. The innovation of this paper provides an opportunity for purchasers to obtain significant cost savings by engaging in group purchasing bundles with the highest sustainable scores. This allows them to procure eco-friendly products while simultaneously benefiting from discounted prices. We propose a tailored mathematical model for this new version of TPP, which we then solve using an approximated heuristic algorithm. The findings underscore the substantial role played by group purchasing strategies in cost mitigation and the enhancement of sustainable procurement.",Sustainable Group Traveling Purchaser Problem - Unlocking Value and Environmental Benefits with Bundle Discounts ,[77113],488,"[145, 72, 138]",1287,Sustainability in Vehicle Routing,19,7,24,Sustainable Supply Chains,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,83 [building - 116],"['Vehicle Routing', 'Mathematical Programming', 'Supply Chain Management']",TA-24
"Music in the current digital era is consumed primarily on-demand, for example by streaming from Spotify or YouTube, rather than by broadcasting, for example, by listening to a radio station. A strong correlation has been found between music and emotions. Hence, with our ability to select our own music and the availability of artificial intelligence [AI] tools, the importance of music emotion recognition [MER] is significant. This research proposes a novel methodology for predicting emotions from a musical piece based on a two-layer AI. The dataset and the process are exceptional in two ways - first, by their qualitative character, which minimized the inherent subjectivity of self-reported emotions, and also the high reliability of the collected data in comparison to other methods such as a questionnaire; and second, by their quantitative character, given n=9,090 songs from about 4,447 people. The model succeeded in predicting the primary emotion with accuracy=57%  and one of the two leading emotions [the primary and the secondary] with accuracy=75.1%, which is significantly higher than a random guess, and without accommodating demographic factors in the model. We believe this knowledge may be a useful tool for therapists, coaches, teachers, etc. and may also raise public awareness of a potential privacy violation. ",The Sound of Emotions - An Artificial Intelligence Approach to Predicting Emotions from Musical Selections,"[71534, 77332, 71593]",408,"[8, 85, 58]",1291,"Advancements of OR-analytics in statistics, machine learning and data science 4",16,5,28,"Advancements of OR-analytics in statistics, machine learning and data science","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,065 [building - 208],"['Artificial Intelligence', 'OR and the Arts\xa0', 'Humanitarian Applications']",MD-28
"A polynomial time O[n^2] algorithm for finding the optimum matching of panels to assemble TFT-LCD [Thin Film Transistor-Liquid Crystal Display] is presented in this talk. In the TFT-LCD cell assembly process, both TFT array and color filter [CF] substrate panels are pre-cut into several equal sized sub-panels. After the precut, two sub-panels are matched to produce the final product, i.e., the LCD. We explore the special structure of a novel linear programming formulation, where the sub-panels are sorted by the number of good displays via lexico-graphic order, based on the location of good/bad displays on each sub-panel. Enhanced by these new properties, efficient solution procedures to find the optimum matching are developed for real time operation and sequencing in the assembly process. Computational experiments with several common pre-cut types and different batch sizes of panels have indicated that the solution procedures are efficient and effective.",An Efficient  O[n^2]  Algorithm for Solving the Dynamic Matching Problem  in TFT-LCD Cell Assembly Process ,"[47160, 77133]",875,"[105, 84, 5]",1293,Optimization issues on graphs I [Contributed],64,14,29,Combinatorial Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,157 [building - 208],"['Production and Inventory Systems', 'Optimization Modeling', 'Algorithms']",WC-29
"In this talk, we are concerned with structured $\ell_0$-norms regularization problems, with a twice continuously differentiable loss function and a box constraint. This class of problems have a wide range of applications in statistics, machine learning and image processing. To the best of our knowledge, there is no effective algorithm in the literature for solving them. In this paper, we first obtain a polynomial-time algorithm to find a point in the proximal mapping of the fused $\ell_0$-norms with a box constraint based on dynamic programming principle. We then propose a hybrid algorithm of proximal gradient method and inexact projected regularized Newton method to solve structured $\ell_0$-norms regularization problems. The whole sequence generated by the algorithm is shown to be convergent by virtue of a non-degeneracy condition, a curvature condition and a Kurdyka-{\L}ojasiewicz property. A superlinear convergence rate of the iterates is established under a locally H\{o}lderian error bound condition on a second-order stationary point set, without requiring the local optimality of the limit point. Finally, numerical results highlight the features of our considered model, and the superiority of our proposed algorithm.",An Inexact Projected Regularized Newton Method for Fused Zero-norms Regularization  Problems,"[77132, 64377, 67090]",249,"[19, 5, 116]",1294,Lower-order composite optimization problems,70,10,41,Nonsmooth Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,97 [building - 306],"['Continuous Optimization', 'Algorithms', 'Programming, Sequential Quadratic']",TD-41
"In this talk, we consider the $\ell_0$ regularization problem for mix sparse optimization and investigate its mathematical theory and algorithm. In the theoretical part, we first introduce the notions of sparse eigenvalue conditions, one of the weakest regularity conditions in the literature, and then establish the oracle property without any regularity condition and provide a recovery bound for the mix sparse optimization problem under the weak assumption of sparse eigenvalue condition. Moreover, an asymptotic analysis is provided to advance the understanding of the convergence of the $\ell_p$ regularization to the $\ell_0$ regularization. In the algorithmic part, we propose an iterative mix thresholding algorithm with continuation technique [IMTC] to solve the mix sparse optimization problem and present its global convergence theorem and linear convergence rate to a local minimum. The significant advantage of the IMTC is that it has a closed-form expression and low storage requirement, and promotes the mix sparse structure of the solution. Numerical results on simulated data indicate that the IMTC has a strong promoting capability of the mix sparse structure and outperforms ",Mix Sparse Optimization - Theory and Algorithm,[8893],249,"[81, 66, 19]",1295,Lower-order composite optimization problems,70,10,41,Nonsmooth Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,97 [building - 306],"['Non-smooth Optimization', 'Machine Learning', 'Continuous Optimization']",TD-41
"In this study, we present the Covering Tour Problem with Arcs Upgrade [CTPAU]. This problem is an extension of the Covering Tour Problem [CTP] that considers the possibility of enhancing the network by reducing the length of some arcs, i.e., upgrading them. Hence, upgrading an arc means reducing its length, usually within certain limits, at a given cost that is proportional to the extent of the upgrade.

The CTPAU is formulated with three different sets of nodes, V, W, and, T that is a subset of V. Two decisions have to be made simultaneously - i] identify the tour of minimum length that passes through a subset of V, ensuring that all nodes of set T are included in the tour, and that each node in W is within a given coverage distance from a node on the tour, ii] decide with connections to upgrade.

Therefore, the CTPAU seeks to identify the minimum length tour while integrating arc upgrading and a budget constraint. In this context, we present some MILP formulation and compare them to illustrate the potential and limitations of each one.",Upgrading arcs in the covering tour problem,"[66322, 68927, 58471, 22045, 13833]",457,"[145, 53, 72]",1296,YW4OR_1,39,12,12,WISDOM - Women in OR,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,13 [building - 116],"['Vehicle Routing', 'Graphs and Networks', 'Mathematical Programming']",WA-12
"This study considers the optimal investment timing decision when the investment is not completely irreversible in a competitive market. 
As shown in Section 8.2 of Dixit and Pindyck [1994], the optimal investment decision is independent of the degree of competition when the investment is completely irreversible. This study finds that the optimal investment decision depends on the degree of competition when the investment is not completely irreversible. As an extreme case, if the competition is strongly intense, the firm never invests. The weaker the competition, the later the investment is exercised. 
","Investment, partial irreversibility, and competition","[2266, 8187, 36989]",92,"[45, 44]",1297,Real Option Analysis,8,15,57,Real Option Analysis,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S06 [building - 101],"['Financial Modelling', 'Finance and Banking']",WD-57
"In this talk, we focus on the feature selection problem in linear SVMs by using a hard cardinality constraint. The problem is first reformulated into mixed-integer form, for which a novel SDP relaxation is proposed. Exploiting the sparse pattern of the relaxation, we decompose it and obtain an equivalent SDP relaxation in a three-dimensional positive semi-definite cone. Based on the decomposed SDP relaxation, we propose heuristics using the information of its optimal solution. Moreover, an exact procedure is proposed by solving a sequence of mixed-integer decomposed SDPs. Numerical results on classical online datasets are reported. ",Feature selection in linear SVMs - a scalable SDP decomposition approach using a hard cardinality constraint,"[72518, 8503, 65962]",285,"[63, 111, 115]",1298,Large Scale Constrained Optimization - Algorithms and Applications,84,2,32,Advances in large scale nonlinear optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,41 [building - 303A],"['Large Scale Optimization', 'Programming, Mixed-Integer', 'Programming, Semidefinite']",MA-32
"With the competition between online meal delivery services growing, on-time delivery has become a crucial target for meal delivery platforms to pursue, thus calling for new technologies and operational models. Echoing such a need, we propose a drone resupply approach for meal delivery, where drones transport meals from restaurants to riders en route, and then riders deliver these meals to customers. This model involves the coordination of multiple trucks, drones, and depots, as well as the joint optimization of rider routing and drone scheduling. Our two-phase algorithm solves the problem by first identifying feasible rider routes along with the respective drone schedule for serving order bundles, and second, selecting rider routes using a set covering model. To address the issue of long service periods, we propose a rolling horizon strategy to solve it dynamically. Extensive computational studies are conducted to evaluate the efficiency of the solution method and the effectiveness of the drone-assisted delivery system. The results demonstrate that adopting drone resupply in meal delivery is more efficient than the traditional rider-only mode for maintaining the target on-time service level in large service areas.",On-Time Meal Delivery Assisted by Drone Resupply,"[77105, 53851, 61344]",735,"[65, 145, 14]",1299,Routing Unmanned Aerial Vehicles 1,5,3,64,VeRoLog - Vehicle Routing and Logistics,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S16 [building - 101],"['Logistics', 'Vehicle Routing', 'Combinatorial Optimization']",MB-64
"The automatic warehouse discussed in this article is based on a puzzle-based storage [PBS] system with block movement, which allows multiple items in a line to move simultaneously, and simultaneous movement, which allows multiple movements to occur at the same time. PBS is one of the most space-efficient types of storage systems, and the way of block and simultaneous movement makes it more time-efficient. However, retrieving multiple target items from this kind of automatic warehouse has not yet been rigorously studied. In this paper, we present a time-expanded based on mixed integral linear programming [MILP] formulation that aims to minimize the summation time of retrieving multiple target loads. Another objective of minimizing the makespan can also be minimized to serve the objective of summation retrieving time. In addition, constructive algorithms are developed for fast obtaining retrieving plan for multiple target items based on the shortest unimpeded retrieval for some special cases. Experiments show that the proposed formulation can solve small to medium size instances, especially when the storage density is not extremely high. We also conducted experiments to explore the efficiency of the proposed automatic warehouse and demonstrate the utility rate of empty locations.",Scheduling of Retrieval Operations in an Automatic Warehouse - Models and Algorithms [Preliminary Exploration],"[74130, 61344]",761,"[146, 129, 109]",1301,Warehouse Operations,5,7,58,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S07 [building - 101],"['Warehouse Design, Planning, and Control', 'Scheduling', 'Programming, Integer']",TA-58
"Large industrial corporations typically operate a network of several factories, leading to a distributed flow shop scheduling problem for flowline manufacturing processes. This requires assigning jobs to one of multiple distributed factories, each equipped with identical flow shops, where each job must be completed in their designated factory. Our research builds upon the traditional distributed flow shop scheduling problem by including the transport of intermediate goods between factories. This transport can occur after each machine operation, with the transportation times varying based on the distance between the origin and destination factories. The objective is to minimize the maximum makespan across all factories. We propose an iterated greedy search algorithm for the distributed flow shop scheduling problem with inter-factory transportation. Utilizing a graph representation, we have developed a speed-up technique to enhance the search efficiency of the algorithm. Through computational experiments, we demonstrate the efficacy and efficiency of our proposed algorithm. Finally, our research quantifies the value of introducing the possibility of inter-factory transportation. While transportation can potentially delay processing at subsequent machine stages, our findings suggest that this is outweighed by improved machine utilization across factories. Therefore, we observe an enhanced overall efficiency and reduced makespans.",An Iterated Greedy Search Procedure for the Distributed Flow Shop Scheduling Problem With Inter-Factory Transportation,"[71977, 32882, 47728]",933,"[129, 69, 74]",1302,Flow shop and single machine scheduling ,35,15,60,Project Management and Scheduling,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S09 [building - 101],"['Scheduling', 'Manufacturing', 'Metaheuristics']",WD-60
"We first construct a measure of corporate workplace equality based on textual analysis of equal employment opportunity [EEO] statements in online job postings. We validate this measure by demonstrating its predictive power for future employment discrimination lawsuits and identifying the costs that deter low-equality firms from mimicking high-equality peers. We then show a positive correlation between corporate workplace equality and consumer spending and propose two economic channels - preference and innovation. To establish the preference channel, we demonstrate that the correlation between corporate workplace equality and consumer spending increases when consumer preferences experience exogenous shocks in major social events, such as the MeToo Movement in 2017 and the Black Lives Matter Movement in 2020. This effect is particularly strong among gender and racial minority groups. To establish the innovation channel, we demonstrate a higher correlation between corporate workplace equality and consumer spending among more innovative industries and show that corporate workforce equality can predict innovation efficiency in terms of patent quantity, quality, originality, and generality. ",Rise of Conscious Consumers - Impacts of Corporate Workplace Equality on Household Spending,[77124],317,"[7, 57, 124]",1303,Data Science and Optimization,14,12,03,Data Science Meets Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1005 [building - 202],"['Analytics and Data Science', 'Human Resources Management', 'Revenue Management and Pricing']",WA-03
"Modeling human behavior is a highly complex task because humans don't always act rationally, and their actions can be influenced by a wide variety of stimuli that are difficult to measure. However, accurately modeling human behavior is one of the main challenges in developing useful agent-based simulations that can be applied to real-life situations.
We propose a new architecture called PRIAGE [Pursued, Requested, and Induced Actions for reaching Goals in the Environment], which offers an innovative approach to the influence of an agent's internal state on their actions. In the majority of existing human-behavior paradigms the agent’s internal state is used only to determine which action to choose from a set of possible actions.  PRIAGE focuses on describing the different reasons that can trigger an action - agents’ actions are either requested by the environment, induced by the agent's internal state, or actively executed by the agent to achieve goals. A second novelty is that the internal state is not only used to determine what action to do next, but may also influence how well the action is performed, creating performance measures that can modify the environment and the agent.
After describing the new proposed architecture, we apply it to a case study analyzing physicians in an emergency department. We focus on how the internal state of the agent influences two performance measures - the treatment time and treatment quality of patients.","How, when and why human agents act in a simulation?  The PRIAGE architecture.","[74932, 4941]",94,"[3, 56, 131]",1304, Behaviour and decision processes ,13,12,07,Behavioural OR,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1019 [building - 202],"['Agent Systems', 'Health Care', 'Simulation']",WA-07
"The enhancement of European Railway Traffic Management Systems [ERTMS] can lead to shorter headway requirements between trains and faster connections for passengers. However, the deployment of ERTMS or other digital infrastructure in a railway network is a lengthy and costly process subject to many constraints. Consequently, infrastructure decisions must be strategically planned over time to maximize passenger benefits.

This research introduces a novel model aimed at optimizing the deployment of infrastructure upgrades in a railway network over a given planning horizon. We propose a Mixed Integer Linear Programing formulation based on the Periodic Event Scheduling Problem formulation for cyclic timetabling. Furthermore, we develop heuristics to tackle this new problem and its operational requirements. We present results on a set of real-life instances from the Netherlands.
",A Cyclic Timetabling Approach to Determine Optimal European Railway Traffic Management System Rollout,"[71483, 27297, 5932]",394,"[143, 142, 79]",1306,Railway Capacity Management,85,15,54,Public Transport Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S01 [building - 101],"['Transportation', 'Timetabling', 'Network Design']",WD-54
"This study examines the influence of trust and risk on individuals' decision-making regarding the adoption of smart contracts. Two quantitative studies were conducted in the United States to conduct an in-depth analysis of trust, risk, and the decision to use smart contracts. The first study presents a trust model for smart contracts based on Hoff and Bashir's [2014] dimensions of trust and Lemieux et al.'s [2019] findings on trust towards distributed ledgers. The second study builds on the findings of the specific trust dimensions, using Mayer et al.'s [1995] trust model and Stuck et al.'s [2021] role of risk in automation. The results highlight the importance of situational social trust and trust in technology for individuals' risk behavior and the decision to adopt smart contracts. It also shows how perceived risk is moderated and how risk-taking propensity positively influence usage intentions. The study emphasizes the importance of trust dynamics and risk considerations in the adoption of smart contracts and makes a valuable contribution to the field with its original findings and comprehensive analysis. It contributes to theory by providing a novel, multidimensional model of trust and an integrative approach to examining the relationship between trust and risk behavior and its influence on decision making in an organizational context. In practice, this study provides insights for the design and implementation of smart contracts in various industries.","Trust, Risk, and the Decision to use Smart Contracts - A Dual-Study Analysis",[74142],127,"[10, 18, 27]",1308,Behavioural OR meets Information systems,13,9,07,Behavioural OR,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,1019 [building - 202],"['Behavioural OR', 'Computer Science/Applications', 'Decision Theory']",TC-07
"In recent years, the automotive industry and transportation systems have seen a remarkable surge in the development of autonomous vehicles [AVs], driven by advancements in automation and vehicle-to-everything [V2X] communication technologies. In the foreseeable future, when AVs come into our society, it is evident that semi-AVs will continue to play a predominant role in the market due to safety concerns and cost considerations. This study aims to provide models to derive and analyze the throughput and travel time of the transitional mixed traffic with semi- and fully-AVs. To accurately capture the dynamics of different platooning behaviors of fully- and semi-AVs and their impact on travel efficiency, we employ a Markov chain model to derive headway distributions under varying traffic densities, and integrate the Markov chain with a state-dependent queueing model to characterize the mixed traffic flow. The derived mean headways are then utilized to compute the service rate of a state-dependent queueing system, enabling the calculation of long-term throughput and mean travel time through a highway segment. This study reinforces the research on transitional stages of mixed traffic with semi-AVs, revealing the potential for semi-AVs to enhance traffic efficiency. Moreover, it provides valuable insights for the development of management and control strategies in the transitional mixed traffic stages, guiding future designs and implementations.",A Queueing-based Throughput Analysis of the Mixed Traffic with Autonomous Vehicles,"[77123, 77141, 78758]",151,"[143, 121, 8]",1309,Transportation Network Modelling and Optimization I,6,2,55,Transportation,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S02 [building - 101],"['Transportation', 'Queuing Systems', 'Artificial Intelligence']",MA-55
"Our research programme is working with Indonesian emergency ambulance services and the Indonesian Government to help them make critical decisions on the optimal types, capacities and geographical locations of response vehicles. Such factors directly impact on the probability of patient survival, ability to respond to major disasters, and the overall quality of care provided. There are however many challenges faced in the country, including vast geographical areas, traffic congestion, inadequate numbers of ambulances and a lack of a co-ordinated service. 

This talk will describe findings and impact of the research project to date, including an analysis of ED survey data that we undertook across Jakarta hospitals, and an approach to consider EMS allocations for both heterogeneous populations [multiple medical needs] using a heterogeneous fleet [consideration of multiple vehicle types each with differing travel speeds]. This has been achieved via the creation of an optimisation and simulation decision-support framework. 

",Modelling of emergency medical services in Indonesia,"[28046, 65156, 65157, 47682, 63586]",54,"[90, 56, 30]",1311,OR for Medical Services in Developing Countries,67,12,18,OR for Development and Developing Countries,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,42 [building - 116],"['OR in Development', 'Health Care', 'Disaster and Crisis Management']",WA-18
"In multi-period inventory management, decisions are made about the timing and quantity of purchases, as well as inventory control of the goods. To minimize costs, inventory policies are used for the procurement of goods. Most policies assume constant purchase prices and stochastic demand. However, products and services are often subject to price fluctuations. This raises three challenges - [1] the economic advantage of inventory policies when purchase prices fluctuate, [2] the integration of value-oriented decision mechanisms into inventory policies, and [3] their comparison. To address these issues, a simulation study on inventory policies was conducted taking value orientation and fluctuating purchase prices into account. The study focusses on developing, integrating, and evaluating value-oriented inventory policies for goods with fluctuating prices. It compares their performance with conventional inventory policies. The results suggest that integrating value-oriented decision mechanisms can yield benefits even with minor price fluctuations in the market. As price fluctuations increase and solution robustness becomes more important, the need to integrate value-oriented decision mechanisms into inventory policies remains. The results indicate that value-oriented inventory policies can leverage cost potential by up to 20%. Therefore, further research is needed to efficiently configure these policies for the economical procurement and storage of goods during price fluctuations.",Integrating and evaluating value-oriented decision making mechanisms into inventory policies,"[72516, 17130]",658,"[25, 105, 146]",1314,Decision Support for Operations Management,45,12,45,Decision Support Systems,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,30 [building - 324],"['Decision Analysis', 'Production and Inventory Systems', 'Warehouse Design, Planning, and Control']",WA-45
"In project “Network Capacity” we develop an approach allowing to compute the residual freight capacity for a given passenger timetable, considering freight traffic demand while maintaining punctuality and reliability in operations. Since timetable-based capacity assessment suffers from fluctuating freight traffic demand, we use pre-constructed slots on net segments which will be combined to train paths as the basis of our model. 
The approach consists of two parts, timetabling and delay prognosis, which are combined to an integrated planning framework. In the timetabling part, we solve a Multi Commodity Flow Problem to predict the required number of slots on network segments. Subsequently, we construct [possible overlapping] slots for freight traffic in the passenger timetable using mathematical optimization techniques such as Branch-and-Price. These slots are the basis for routing freight traffic demand based on a MIP-based slot allocation. In detail, slots are selected and connected to build up conflict-free trains paths. In the delay prognosis part, a stochastic [max,+]-approach is used to assess the reliability of the timetable on the network level. It allows to calculate the delay distribution for each train and every infrastructure element and to assess timetable stability by suitable network performance metrics. With this approach, network  capacity can be assessed by scaling the demand up to the maximum that can be reliably satisfied.
",Timetable-based network capacity assessment,"[76279, 77152, 77146, 14909, 55810]",394,"[12, 122, 26]",1317,Railway Capacity Management,85,15,54,Public Transport Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S01 [building - 101],"['Capacity Planning', 'Railway Applications', 'Decision Support Systems']",WD-54
"Given a vertex-colored graph G, any connected component of G is said to be colorful if all its vertices have different colors. We address three problems related to these particular components of a graph, motivated by comparative genomics - the Colorful Components [CC], the Maximum Edges in transitive Closure [MEC], and the Minimum Colorful Components [MCC] problems.
In social networks, where nodes represent individuals and edges represent connections, all three problems can be used to determine the most influential connections linking distinct [colorful] and cohesive [connected] communities. By solving the CC problem, the risk of echo chambers is mitigated. Addressing the MEC problem means promoting the propagation of information through the community. Instead, the MCC problem helps prevent the isolation of users within small disconnected groups. When instead considering cyberspace networks of devices with various types of connections [e.g., permissions, data flow], identifying a subset of edges to remove while ensuring that the network remains in connected colorful components helps optimize the network's resilience to cyber threats. 
We formulate the three problems as integer nonlinear programs, linearize them and propose families of valid inequalities. Furthermore, we provide bounds on the maximum number of colorful components at optimality and exploit them to reduce the size of the models. Benchmark instances are tested to evaluate the performances of the proposed models.",Exact approaches for the colorful components problems,"[66163, 23193, 72278]",242,"[14, 53]",1318,Exact Algorithms and Formulations for Network Optimization Problems,64,9,29,Combinatorial Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,157 [building - 208],"['Combinatorial Optimization', 'Graphs and Networks']",TC-29
"This talk presents a feature-driven model for hybrid power plants, enabling them to exploit available contextual information such as historical forecasts of day-ahead prices and wind power, and make optimal wind power and hydrogen trading decisions in the day-ahead stage. For that, we define and compare different linear and piece-wise linear decision rules. In addition, we propose a real-time adjustment strategy for hydrogen production. Our numerical results show that the final profit obtained from our proposed feature-driven trading mechanism in the day-ahead stage together with the real-time adjustment strategy is very close to that in an ideal benchmark with perfect information. ",Feature-driven strategies for trading wind power and hydrogen,[76234],234,"[36, 93]",1319,Production Optimization and Supply Chain Management of Green Hydrogen under Uncertainties,22,13,09,Energy Markets,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,10 [building - 116],"['Electricity Markets', 'OR in Energy']",WB-09
"In collaborative distribution networks, building partnerships hinges on intricate trust dynamics. The exchange of critical information is crucial, but when a host carrier holds sensitive data, like customer demands, challenges arise. The host's comprehensive data is pivotal, yet asymmetry may deter reciprocation, impacting collaboration. To surmount this, we use bi-objective two-stage robust optimization, employing an extended algorithm for effective resolution. Our framework navigates collaboration costs, offering insights into the intricate interplay with network design. Our study delves into trust's profound impact, enhancing understanding of collaboration dynamics in distribution networks.",Trust Dynamics and Collaboration Costs - Navigating Complexity in Collaborative Distribution Networks,"[77142, 76764]",533,"[138, 127, 77]",1320,Stochastic Optimization - Advanced Applications,49,13,34,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,43 [building - 303A],"['Supply Chain Management', 'Robust Optimization', 'Multi-Objective Decision Making']",WB-34
"A highly relevant problem of modern finance is the design of Value-at-Risk [VaR] optimal portfolios. Due to contemporary financial regulations, banks and other financial institutions are tied to use the risk measure to control their credit, market, and operational risks. For a portfolio with a discrete return distribution and finitely many scenarios, a Difference of Convex [DC] functions representation of the VaR can be derived. Wozabal [2012] showed that this yields a solution to a VaR constrained Markowitz style portfolio selection problem using the Difference of Convex Functions Algorithm [DCA]. A recent algorithmic extension is the so-called Boosted Difference of Convex Functions Algorithm [BDCA] which accelerates the convergence due to an additional line search step. It has been shown that the BDCA converges linearly for solving non-smooth quadratic problems with linear inequality constraints. In this paper, we prove that the linear rate of convergence is also guaranteed for a piecewise linear objective function with linear equality and inequality constraints using the Kurdyka-Łojasiewicz property. An extended case study under consideration of best practices for comparing optimization algorithms demonstrates the superiority of the BDCA over the DCA for real-world financial market data. We are able to show that the results of the BDCA are significantly closer to the efficient frontier compared to the DCA.",The Boosted Difference of Convex Functions Algorithm for Value-at-Risk Constrained Portfolio Optimization,"[77144, 66639, 46586]",291,"[19, 5, 83]",1322,Iterative Methods for Feasibility and Optimization Problems,82,13,42,Variational Analysis and Continuous Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,98 [building - 306],"['Continuous Optimization', 'Algorithms', 'Optimization in Financial Mathematics']",WB-42
"Real-time Train Rescheduling is applied to obtain a new feasible schedule when unexpected delays occur causing train conflicts and delay propagation. Train Rescheduling consists of train reordering, retiming, and rerouting. We consider the well-established approach for Train Rescheduling, called RECIFE-MILP - it consists of first solving a microscopic Mixed Integer Linear Programming [MILP] model within a given time limit by neglecting train rerouting and then solving the complete model with the computed solution as a warm-start. 

In this work, we extend RECIFE-MILP by including additional constraints on [mandatory or soft] train connections and restrictions on the access or speed to utilize the available infrastructure resources, and an objective function that penalizes, through a step-function, trains arriving late or early and differences from the desired travel time, the use of detours, and the violation of soft train connections. Computational results are reported on realistic instances to show the performance of the proposed approach.",Extension to RECIFE-MILP Train Rescheduling,"[22920, 58611]",179,"[143, 129, 111]",1324,Disruption management in passenger railways,85,2,54,Public Transport Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S01 [building - 101],"['Transportation', 'Scheduling', 'Programming, Mixed-Integer']",MA-54
"The 3D Bin-Packing-Problem [3D-BPP] is an NP-hard problem that is particularly important in Operations Research whenever a load must be placed on a carrier. This problem has been studied for a long time from a heuristic perspective in the area of container and pallet loading. Recently, Deep Reinforcement Learning [DRL] methods have been applied to solve 3D-BPPs. These methods show improved performance in decreasing the inference time for generating solutions while their generated solutions can outperform those generated by heuristics. However, the research has paid little attention to generating loading patterns subject to stability constraints, which are relevant for making the generated solutions applicable in real-world settings. This paper develops a DRL algorithm based on Proximal Policy Optimisation to explore whether DRL methods can be used to generate stable loading patterns for placing parcels on a Euro-pallet. Only the offline 3D-BPP is considered. The DRL algorithm is combined with a physical simulation of the generated loading pattern to ensure the load's stability. The algorithm is evaluated based on the recently introduced BED-Benchmark dataset of realistic orders. The evaluation shows that the developed DRL algorithm, on its own, struggles to find suitable solutions for stably placing parcels on a pallet. One reason for this might be the large action space for placing any parcel. This issue could be addressed by combining the algorithm with heuristic solvers.",Solving a 3D-Bin-Packing-Problem with Deep Reinforcement Learning,"[76679, 77720]",544,"[8, 23, 14]",1327,Cutting and Packing 3 - 3D loading,81,4,07,Cutting and Packing [ESICUP],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,1019 [building - 202],"['Artificial Intelligence', 'Cutting and Packing', 'Combinatorial Optimization']",MC-07
"The training of machine learning models, such as neural networks, relies on optimisation techniques that necessitate large volumes of data. The algorithms that have demonstrated satisfactory performance on this task frequently use linear searches on subsets of the data. This ensures that, despite the potential low quality of the search direction, the overall computational cost remains low, making this strategy globally efficient. In these methods, strategies employing a constant learning rate have proven to be particularly effective. This paper introduces a novel scheme designed to significantly expedite the convergence process of line search-based methods. Our approach incorporates additional high-quality linear searches derived from the convergence process of the methods, and by making use of an Armijo rule, it dynamically adjusts the step size through successive reductions or expansions based on the evaluated quality of the descent direction. This strategic adjustment enables more substantial progress in the search direction, potentially reducing the number of iterations needed to reach an optimal solution. We have applied our proposed solution to accelerate the performance of widely-used algorithms such as Gradient Descent [GD], Momentum GD, and Adaptive Moment Estimation [Adam]. To illustrate the practical implications and effectiveness of our approach, we present a comprehensive case study focusing on the training of Deep Neural Networks and Kernel Logistic Regression.",Enhancing the convergence speed of line search methods - Applications in Neural Network training,"[77153, 2501, 77162, 2503]",917,"[66, 63, 42]",1335,New Algorithms for Nonlinear Optimization,84,7,34,Advances in large scale nonlinear optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,43 [building - 303A],"['Machine Learning', 'Large Scale Optimization', 'Expert Systems and Neural Networks']",TA-34
"The liberalization of European railway markets necessitates a vertical separation governance framework, with Infrastructure Managers [IMs] providing resources and Freight Operating Companies [FOCs] utilizing them. Effective pricing of track access between these entities is vital for profitability and system efficiency. Additionally, achieving a modal shift of freight from road to rail is crucial for road safety and EU objectives alignment. This paper introduces a novel pricing methodology for train paths, optimizing infrastructure utilization by IMs while considering competitive dynamics and environmental impacts. It addresses challenges such as elastic demand, integrating network capacity into pricing strategies, and managing non-additive costs and path interdependencies. The proposed model incorporates dynamic demand patterns, temporally variable capacities, and total revenue considerations across the planning horizon. A discretization-based approach, focusing on irregular time intervals and unit freight tonnage, is developed for solving the continuous model. This involves linking freight tonnage units to a prototype train, with a discrete event simulation model employed. The objective function is also discretized, yielding a finite set of optimization variables to be optimized subject to simulation model. An application of this approach is demonstrated through a case study of the Mediterranean Corridor.",An Optimization Model for Determining Access Costs in Rail Freight Transportation,"[2501, 1598, 23407, 2503, 77153]",504,"[143, 122, 131]",1336,Freight railway transportation ,6,3,56,Transportation,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S04 [building - 101],"['Transportation', 'Railway Applications', 'Simulation']",MB-56
"Solving non-convex optimization problems is crucial for training machine learning models, including neural networks. However, non-convexity often leads to less reliable and less robust neural networks with unclear inner workings. While convex formulations have been used for verifying neural network robustness, their application to training neural networks remains relatively unexplored. In this work, we propose a semidefinite programming relaxation for training two-layer ReLU networks in a lifted space, which can be solved in polynomial time. Numerical experiments demonstrate that this SDP formulation provides reasonably tight lower bounds on the training objective across various prediction and classification tasks. ",SDP Relaxations for Training ReLU Activation Neural Networks,"[74924, 75185, 74101]",351,"[115, 66, 21]",1337,New Trends in Generative Adversarial Networks and Deep Neural Networks ,71,4,04,Recent Advancements in AI ,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,1001 [building - 202],"['Programming, Semidefinite', 'Machine Learning', 'Convex Optimization']",MC-04
"In case of a new epidemic, decision makers have huge responsibilities in terms of economic impact and public health security. When a vaccine becomes available, they need to strategically define who is going to be vaccinated and when. Moreover, the strategy could need an adaptation after a few days or weeks to be as effective as possible. We propose to use the Model Predictive Control method in order to have a dynamical choice of who is vaccinated and at which moment [with a maximum number of total daily vaccinations]. This method starts from the knowledge of the evolution of the disease without any vaccination and proposes a strategy over a fixed prediction horizon by iteratively adjusting the control inputs [vaccination] based on the ongoing system feedback to optimize the performance [minimize the number of dead individuals]. To simulate the disease spread, we consider a compartment model including the susceptible, infected, recovered and deaths compartments. The proposed method is illustrated thanks to data about the COVID-19 in Wallonia [Belgium] and the resulting strategy campaign is compared to the strategy really adopted by the government [decreasing order of ages]. We show that the two types of campaigns gave similar results in terms of number of deaths and infected, but the Model Predictive Control approach consumes only 63.95% of the vaccines used by the decreasing age strategy. It is thus economically very interesting.",Model Predictive Control to organize the first vaccination wave during an epidemics [Application to COVID-19 for Wallonia – Belgium],"[75214, 77156, 77157]",104,"[20, 26]",1338,Just and ethical sustainability transitions,28,3,20,OR and Ethics,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,45 [building - 116],"['Control Theory', 'Decision Support Systems']",MB-20
"Limited resources in healthcare give rise to efficient use of resources. As operating rooms [ORs] and associated resources are dominant factors in the hospital’s operational cost, our aim is to improve surgery scheduling. Considering Leeftink, G. & Hans, E.W. [2018] as our starting point – a large benchmark of surgery scheduling problems – our objective is to improve the scheduling by taking uncertainty about surgery durations into our approach. The motivation stems from the observation that in the literature, most studies tend to focus solely on specific cases of the surgery scheduling problem or operate under deterministic assumptions contrary to the problem's nature. 
Note that the classical problem is twofold - first, one needs to decide which procedures are done on which OR, and second, in which order do the procedures need to be scheduled. We propose a mathematical formula that merges applied probability and mixed-integer programming to minimize overtime and idle time. We demonstrate how our solution approach can be adapted to specific cases and integrated with additional constraints. We show superior results compared to the ones that simply assumed deterministic service times, and we also contrast them to scheduling rules. This underpins the importance of taking variation into account. Furthermore, our experiments show that even with a limited degree of information about the uncertainty, the performance of the surgery schedule can be considerably improved.",INTRA-DAY OPTIMIZATION OF THE SURGICAL SUITE - ALLOCATING AND SEQUENCING SURGICAL CASES,"[77154, 72783]",599,"[56, 129, 136]",1339,Surgery Scheduling and Operating Room Planning,3,8,10,OR in Health Services [ORAHS],"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,11 [building - 116],"['Health Care', 'Scheduling', 'Stochastic Optimization']",TB-10
"We introduce a novel approach using deep reinforcement learning to tackle the Dynamic Traveling Salesman Problem with Time-Dependent and Stochastic travel times [DTSP-TDS]. The main goal is to dynamically plan the route with the shortest tour duration and visit all customers while considering the stochastic nature of travel times. We employ a reinforcement learning approach to dynamically address the stochastic travel times to observe changing states and make decisions accordingly. Our reinforcement learning approach incorporates a Dynamic Graph Temporal Attention model that dynamically extracts information from the stochastic environment. We have tested the performance of our proposed approach on the simulation. Our approach can quickly provide high-quality solutions for all datasets using limited computing resources. The efficiency study on different models demonstrates that new components, such as the selection mechanism, dynamic attention component, and temporal pointer can improve the DGTA model. These findings highlight the importance of incorporating stochastic elements in the model to achieve better performance. Furthermore, the trained DGTA model exhibits generalization capability to different instances with varying customer locations and uncertainty levels of travel times without requiring additional training time. Our work contributes to advancing the field of DTSP-TDS, with potential applications in various industries, such as healthcare logistics.",The dynamic travelling salesman problem with time-dependent and stochastic travel times - a deep reinforcement learning approach,"[75776, 77159, 71948, 55094]",514,"[66, 65]",1340,"Advancements of OR-analytics in statistics, machine learning and data science 5",16,8,28,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,065 [building - 208],"['Machine Learning', 'Logistics']",TB-28
"Efficient planning of judicial capacity is crucial for ensuring a timely resolution of court cases. Accurate predictions of demand are essential for capacity planning. The demand of the court consists of cases that need to be resolved, either by one or multiple hearings or in writing. We analyze the arrival process of cases to the court and formulate a negative binomial regression model to predict the court demand on a weekly basis, taking seasonality into account. We test our prediction model on real-life data of the Dutch judiciary. Subsequently, we formulate an optimization model for planning capacity for one future year based on these predictions, considering capacity constraints, aiming to minimize the waiting time. We model the problem as a non-homogeneous queue with a time-dependent arrival rate and a service rate that is subject to control. By incorporating insights from a transient analysis of the queueing model, we solve the optimization model by stochastic programming, compare our capacity planning to the status quo, and run experiments to test several choices of capacity planning. ",Analysis of demand and planning of capacity for the judiciary,"[73111, 25223, 72624]",959,"[12, 101]",1341,"Projects, risk and law",35,8,60,Project Management and Scheduling,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S09 [building - 101],"['Capacity Planning', 'OR/MS and the Public Sector']",TB-60
"Energy systems are often very complex systems, where different components interact in a not-trivial manner. In this context, identifying faults in the system and especially identifying their reason can be a complex engineering problem. Take for example an offshore wind farm. As offshore wind turbines can be very expensive to reach, faults are very expensive to handle. Therefore it is very important for energy companies to reliably identify real alarms. For this reason, many companies are nowadays using ML to identify failures. We would like to look a step further - we want to understand how the status of the turbine should be changed not to have a failure. We will use Counterfactual Analysis, once a classiﬁer has been trained, to identify how records should be changed in their features to being classiﬁed in the “good” class. Finding counterfactual explanations amounts to solving a mathematical optimization model, where we want to minimize the number of changes [or the total cost associated to changes] to an instance to maximize its probability of being classified as a good instance, subject to different constraints. We will present some preliminary results on the topic",Counterfactual Analysis for Explicability and Fault prevention of Complex Energy Systems,"[77138, 4607, 39196]",638,"[8, 93, 66]",1342,Counterfactual Analysis Across Diverse Domains,15,7,27,Mathematical Optimization for XAI,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,047 [building - 208],"['Artificial Intelligence', 'OR in Energy', 'Machine Learning']",TA-27
"Considering a time horizon with different periods, the Period Travelling Salesman Problem [PTSP] is defined on a directed graph with a depot and n customers, in which each customer has a required number of periods to be visited in. The objective of the PTSP is to assign each client to as many periods as it is required while determining the set of routes, one for each period, with minimum cost. Each route starts and ends at the depot and visits the clients assigned to that specific period only once. 

The PTSP combines an assignment and a routing problem in a single problem, making it very difficult to solve to optimality, even for instances with few clients. Most of the works in the literature study heuristic methods for the PTSP, and the few exact methods in the literature only consider two periods in the time horizon, which is insufficient to model many real-life routing problems involving periodicity. We will present different flow-based compact formulations for the PTSP and we will use the concept of projection to derive new ILP formulations with fewer variables but exponentially-sized sets of constraints. Furthermore, we intend to propose new valid inequalities for the PTSP. The conclusions from our theoretical and computational study will be presented for instances with different characteristics.",Solving the period travelling salesman problem,"[77161, 2139]",859,"[14, 11, 72]",1343,"Discrete, continuous or stochastic optimization and control in networks, transportation and design III",64,4,25,Combinatorial Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,011 [building - 208],"['Combinatorial Optimization', 'Branch and Cut', 'Mathematical Programming']",MC-25
"Sparse optimization is a practical approach for tackling numerous big data challenges. Extensive empirical studies in the field have demonstrated that nonconvex regularization methods exhibit a significantly stronger sparsity capability to promote sparsity and a notably more robust stability in sparse recovery compared to convex penalty methods. In this paper, we explore various nonconvex regularization models to address sparse optimization problems. Theoretical analysis is conducted to establish recovery bounds for these regularization models. Additionally, we propose the widely recognized proximal gradient method so as to solve the regularization problems and establish convergence rates for the numerical algorithms. Finally, we apply our theoretical findings and numerical algorithms to tackle the problem of inferring the master gene regulator in cell fate conversion, presenting compelling numerical results.",Regularization Methods for Sparse Optimization with Applications,"[51064, 64658]",509,"[5, 84]",1348,Structured nonconvex optimization ,70,13,41,Nonsmooth Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,97 [building - 306],"['Algorithms', 'Optimization Modeling']",WB-41
"A key psychological principle underlying intuitive forecasting is the use of ratios. A finding is that heuristics, which operationalize such principles, can only predict well in comparison to more complex models when the training sample is small. Challenging this finding, we build on the bias–variance trade-off to develop three conditions under which a simple heuristic can match or even outperform more complex prediction models. To test these conditions empirically, we investigate revenue predictions using the “ratio heuristic” - multiply the quantity—revenue, in our case—observed in the first t time units by a constant to predict the future quantity. On 20 prediction tasks, the results are in line with the hypotheses showing that a smaller sample, a longer observation period, or unpredictable changes over time provide performance advantages for the heuristic such that it can outperform more complex, standard prediction models. Moreover, given sufficiently strong unpredictable changes over time can result in a setting where even a large sample and a short observation period do not provide performance advantages for the more complex forecasting methods. That is, managerial heuristics may hold value for business forecasting even in data-rich settings. ",Not Just Small Samples - Successfully Predicting Revenues With the Ratio Heuristic,[77163],113,"[47, 25, 10]",1350,Heuristics in BOR,13,15,11,Behavioural OR,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,12 [building - 116],"['Forecasting', 'Decision Analysis', 'Behavioural OR']",WD-11
"The safety and accessibility of infrastructure such as bridges and quay walls in cities is important for the everyday life of its inhabitants. However, due to wear and tear over the years, these infrastructure pieces need to be maintained or renovated. These large-scale renovation projects generate CO2 emissions by the material production chain, the logistics surrounding the supply of material and equipment and removal of waste, and the increased traffic due to traffic diversions. The magnitude of these emissions can be difficult to predict, especially when multiple projects are executed in parallel due to time pressure. Parallel execution of projects may yield economies of scale regarding logistics but can also cause excessive congestion. Optimizing the schedule of these projects by finding the right combinations of projects to perform simultaneously can reduce the total emissions and reduce project delays. To aid decision-making, we use multi-objective optimization to minimize total emissions while simultaneously reducing project delays. We present a Hybrid Genetic Algorithm to produce a partial Pareto frontier of schedules for large-scale urban infrastructure renovation projects. This research applies the described solution method to the problem of renovation of bridges and quay walls in Amsterdam. The results illustrate the trade-off between sustainability and tempo of renovations.",Scheduling Large-Scale Infrastructure Renovation Projects Minimizing Emissions Using a Hybrid Genetic Algorithm,"[76649, 37248, 73530, 46236]",920,"[138, 94, 77]",1351,Scheduling for sustainability,18,5,23,"Circular Economy, Remanufacturing and Recycling ","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,82 [building - 116],"['Supply Chain Management', 'OR in Environment and Climate change', 'Multi-Objective Decision Making']",MD-23
"We will consider airplane boarding as a paradigm in multi criteria decision making. In particular, we will consider ranking according to two criteria which could be SAT and GPA scores for students or risk and treatment efficacy for patients. We will relate the process of ranking students or patients to the process of airplane boarding and see that they are both related to the evolution of the universe from initial conditions.","Airplane boarding, multi criteria decision making and the evolution of the universe",[77164],256,"[77, 143, 56]",1355,Airplane Boarding,85,8,54,Public Transport Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S01 [building - 101],"['Multi-Objective Decision Making', 'Transportation', 'Health Care']",TB-54
"Transportation is one of the major contributors to global carbon emissions. This makes it necessary to shift from fossil fuels to renewable energy sources for a sustainable transportation system. Electric buses [EBs] play an important role in this transition because of their lower carbon emissions. But to integrate the EBs into the existing network, two main problems arise - timetabling and scheduling. These problems can be addressed either sequentially or in an integrated way. However, the sequential approach may produce a suboptimal bus schedule due to fixed timetable and operational constraints. Also, due to the fixed departure time, EBs miss their best sequence of trips, which could raise the further requirement for new buses. So, in order to develop an effective bus schedule, both timetabling and scheduling problems should be addressed simultaneously due to their interdependency. In response, this paper proposes an integrated method for solving the timetabling and scheduling problems. Initially, a mathematical model is developed to minimize overall costs, including bus procurement and charging expenses. The developed model uses service frequency as an input to generate a timetable and assign buses. Second, a heuristic algorithm is developed in order to produce the result for large instances. Later, we validated the methodology in Bangalore, India, using a realistic scenario to demonstrate its ability to assign EBs while also generating daily timetables.",An Integrated Approach to Electric Bus Timetabling and Scheduling for Sustainable Urban Mobility,"[77145, 77231, 77175, 43014]",823,"[129, 142, 143]",1356,Timetabling 1,85,14,51,Public Transport Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,M5 [building - 101],"['Scheduling', 'Timetabling', 'Transportation']",WC-51
"The Two-Dimensional Cutting Stock Problem with Usable Leftovers [2D-CSPUL] consists of cutting large plates to produce smaller ordered items, minimizing waste and allowing the generation and subsequent use of leftovers. This work explores a multi-stage stochastic approach to the 2D-CSPUL, incorporating demand uncertainty over multiple periods and enabling more adaptive and resilient production strategies. By adopting a scenario-tree approach, the study represents uncertainties throughout the planning horizon, offering a closer reflection of real-world scenarios. Additionally, anticipatory production of items ahead of immediate demand is allowed. Costs associated with the storage of leftovers and the inventory of items are also considered. The objective is to minimize material waste and storage costs of items and leftovers across all stages. The main contribution of this paper is a new mathematical model proposed to represent this problem. Due to its complexity, heuristic procedures based on the relax-and-fix strategy were proposed to achieve practical solutions efficiently. Computational results prove the effecitiveness of thre approach.
This research has been funded by the Fundação de Amparo a Pesquisa do Estado de São Paulo - FAPESP, Brazil [Process number 2022/05803-3 ] and Conselho Nacional de Desenvolvimento Científico e Tecnológico - CNPq, Brazil [Process number
317460/2021-8].
",Exact and non-exact approaches for the multi-stage stochastic two-dimensional cutting stock problem with usable leftovers,"[12006, 47107, 663, 35712]",226,"[23, 72, 13]",1357,Applications of combinatorial optimisation in industry and services I,64,7,29,Combinatorial Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,157 [building - 208],"['Cutting and Packing', 'Mathematical Programming', 'Column Generation']",TA-29
"Driven by due diligence requirements, social sustainability aspects receive more attention in global supply chains. In the automotive industry, e.g., companies search for sustainable supply sources, engage in supplier development, or invest in raw material mining projects to meet social standards. In current research on strategic network planning, however, social sustainability is rarely studied, and often approached in a simplified manner. To address this gap, we develop a novel network planning approach that combines social life cycle assessment, activity analysis, and mixed-integer linear programming. The model particularly decides how much raw material to source from multiple sites, which may operate at different levels of costs and social risks. To meet certain social standards and fulfill demand, different measures are available - commitment to long-term contracts with suppliers, investment in mining projects, and implementation of social development measures. The costs associated with these decisions are minimized in an economic objective function, while social risk is limited by additional constraints. The approach is applied to raw material extraction in the supply chain of lithium-ion batteries for electric vehicles. The analyses show the trade-offs between the sourcing decisions to meet social standards. In cobalt mining, e.g., investing in mining projects in Australia can reduce the social risks associated with sourcing cobalt from long-term contracts in the Congo.",Optimal design of socially sustainable raw material supply chains,"[75066, 17364, 47502, 48740]",513,"[100, 138, 79]",1358,Raw Material Supply Chains,19,3,24,Sustainable Supply Chains,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,83 [building - 116],"['OR in Sustainability', 'Supply Chain Management', 'Network Design']",MB-24
"Cloud computing and especially Infrastructure-as-a-Service [IaaS] are becoming more and more relevant in industrial applications. In IaaS, cloud providers offer customers resources such as compute, network and storage. From the provider’s point of view, the question arises of how many physical resources are required to cover the uncertain demand, to achieve utilization targets and deal with fluctuating workload demand. This decision must be made from a midterm perspective due to increasing supply chain cycles.
We compare two approaches to deal with this hardware capacity planning problem for cloud infrastructure under demand uncertainty [CPCI_DU] to determine the needed capacity of a datacenter. The decision problem is modeled as a two-stage stochastic optimization problem and solved optimally. The results are evaluated using metrics from stochastic optimization such as the value of stochastic solution and the expected value of perfect information. This method is compared to an optimization-simulation loop. Here, the deterministic optimization problem is solved on the first stage and the impact of uncertainty is evaluated in the subsequent simulation. This optimization-simulation loop is executed until a prespecified stopping criterion is reached. The results show the strengths of the respective approaches to enable robust hardware planning.
",A comparison of two approaches to hardware capacity planning for cloud infrastructure under demand uncertainty ,"[74553, 75061, 5078]",659,"[12, 26, 18]",1364,Planning Techniques for Decision Support,45,15,45,Decision Support Systems,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,30 [building - 324],"['Capacity Planning', 'Decision Support Systems', 'Computer Science/Applications']",WD-45
"The ENTSCHEIDUNGSNAVI is an open-source decision support system based on multi-attribute utility theory, which has implemented various methods for dealing with uncertainties. To model decisions with uncertainties, decision-makers can use two categories - Forecast and Parameter Uncertainty. Forecast uncertainty is modeled with [combined] influence factors using discrete, user-defined probability distributions or predefined ‘worst-median-best’ distributions. Parameter Uncertainty allows imprecision for utilities, objective weights, and probability distributions. To analyze these uncertainties, the ENTSCHEIDUNGSNAVI offers several methods and tools, like a robustness check, based on [Monte Carlo] simulations and a sensitivity analysis. The objective weight analysis provides insights into the effects of different objective weight combinations. Indicator impacts, tornado diagrams, and risk profiles visualize the impact of uncertainties in a decision under risk. The latter enables a check for stochastic and simulation dominance. This article presents a complete range of methods for dealing with uncertainties in the ENTSCHEIDUNGSNAVI using a hypothetical case study.",Integrating uncertainties in a multi-criteria decision analysis with the ENTSCHEIDUNGSNAVI,"[68859, 73178, 57663]",477,"[26, 27, 77]",1365,Multiple-Criteria Decision Support,45,7,45,Decision Support Systems,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,30 [building - 324],"['Decision Support Systems', 'Decision Theory', 'Multi-Objective Decision Making']",TA-45
"The selection of the best models in the ensemble has a pivotal role in the overall performance of ensemble learning algorithms due to the fact that redundant solutions in the ensemble library will decrease the overall prediction accuracy. Therefore, on account of eliminating such candidates, accuracy and diversity should be taken into consideration.

This study proposes a novel ensemble clustering method using a second-order conic optimization model that continuously and convexly solves integer programming for decision-making in various disciplines. The proposed model optimizes the balance between accuracy and diversity, resulting in the selection of the best candidates for prediction. The remarkable contribution of the study is the automatic sub-ensemble selection while optimizing for accuracy and diversity simultaneously. The model has been tested with real-world data and has achieved competitive prediction performance.",Auto-Pruned SOCP Clustering Problem,"[75629, 11028]",350,"[66, 84, 26]",1366,Hybrid Appraches in Deep Learning and Machine Learning,71,5,04,Recent Advancements in AI ,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,1001 [building - 202],"['Machine Learning', 'Optimization Modeling', 'Decision Support Systems']",MD-04
"The decarbonization of the transport sector is deeply intertwined with the evolution of the energy system. While modifications of the transport energy demand are often put forward in transport models, there is seldom any discussion on the long-term effects of the newly needed energy-related infrastructures. To investigate this issue, we perform a quantitative analysis of the reciprocal dependencies between modal and fuel shift in freight transportation and  energy systems, including energy infrastructure investments.

A soft-link is established between IFE-TIMES-Norway, an energy system model based on linear programming, and STraM, a freight transport model whose focus is on long-term infrastructure investments decisions in a multi-stage stochastic mixed-integer linear program framework. STraM generates an energy demand for the transport sector based on endogenously computed modal and fuel splits, which is fed into TIMES. Reciprocally, TIMES enhances STraM with its economic representation of technologies and evolution of fuel mix. We provide adaptable methods to reconcile disparities in resolution and aggregation between those models.

Then, comparative analyses allow to demonstrate the framework's improvement over the use of a single model and offer insights into the intricate dynamics of the transport-energy nexus. Finally, the gained knowledge is applied to a Norwegian case study to analyze coherent strategic investments decisions for the next few decades.",Linking freight transport and energy system models for coherent strategic investments decisions,"[77034, 57352, 38443]",547,"[37, 143, 93]",1367,Advancing mobility towards sustainable solutions II,6,10,56,Transportation,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S04 [building - 101],"['Energy Policy and Planning', 'Transportation', 'OR in Energy']",TD-56
"Social networks offer the capability to spread inspiring ideas and adapt innovations at unprecedented speed and convenience. Although such an information spreading capability is invaluable, social networks can also rapidly disseminate misinformation to a large number of people, with dire consequences for impacted individuals and societies. In this study, we focus on the spread of a specific type of misinformation - acute rumors [AR], which we define as misinformation with the potential to quickly spread in the network and mobilize individuals to take harmful actions. We present a new diffusion model for AR, which extends the classical linear threshold model by considering the base idea of the individuals regarding the topic of the rumor and the emotional impact its content creates on them. Based on this diffusion model, we propose a new centrality measure that can accurately detect individuals with a high potential to enhance the spread of AR. In a comprehensive numerical study, we evaluate the performance of our proposed centrality measure by comparing it to existing traditional centrality measures in the literature. Our results attest to the superior performance of our centrality measure not only in terms of detecting the individuals with the highest potential to increase the reach of AR but also in finding the correct ranking among the individuals in the network regarding their potential to contribute significantly to the dissemination of acute rumors.",A new centrality measure for misinformation diffusion,"[76941, 712, 66414]",515,"[132, 130, 86]",1373,"Advancements of OR-analytics in statistics, machine learning and data science 7",16,10,28,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,065 [building - 208],"['Social Networks', 'Service Operations', 'OR and the Internet']",TD-28
"In critical situations, the precision of emergency medical service [EMS] responses is of highest importance, as it can significantly impact patient outcomes. The effectiveness of EMS operations relies heavily on the strategic positioning and allocation of ambulances. This study addresses the ambulance location-allocation challenge in the Basque Country [Spain], where a fleet of 90 ambulances, including both Basic and Advanced Life Support resources, serves a population exceeding 2 million residents.

To tackle this issue, we leverage historical data and use a Box-Cox Cole and Green distribution to forecast response times. We propose a two-stage stochastic mixed 0-1 linear programming model aimed at optimizing the primary objective of maximizing expected coverage. Additionally, the model accounts for various secondary objectives, including minimizing average response time and incorporating risk aversion measures such as Conditional-Value-at-Risk, within a hierarchical compromise framework.

Due to the computational complexities inherent in this model, we introduce a novel matheuristic algorithm that combines primal decomposition and machine learning techniques. Computational  experiments on medium and large-scale instances exhibits promising performance in efficiently addressing the intricate EMS optimization problem. This research contributes to enhancing the effectiveness and responsiveness of EMSs, leading to improved patient care during critical emergencies.
",A Hierarchical Compromise Model and Matheuristic Algorithm for Stochastic Location-Allocation in Healthcare,"[1527, 73723, 30836, 77171]",952,"[117, 56, 64]",1374,Location planning in healthcare,3,7,15,OR in Health Services [ORAHS],"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,18 [building - 116],"['Programming, Stochastic', 'Health Care', 'Location']",TA-15
"The appropriate location of healthcare resources and a proper allocation of patients are essential to ensure access to healthcare for as many people as possible. Examples of healthcare resources include examination and treatment equipment, and healthcare facilities, such as emergency centers. We propose an integer programming formulation for the location-allocation of healthcare resources based on the p-median location problem with additional constraints. We aim to minimize the average distance between patients and resources. Besides that, we present an iterated greedy heuristic to solve large instances. We implement different construction and destruction procedures, restart and acceptance strategies, and also a local search method to improve solutions. Based on these components, we build an algorithm framework capable of generating different versions of iterated greedy heuristics. We evaluate both the exact and heuristic approaches to real-world instances of location and allocation of examination equipment [e.g. mammography units]. We use a parameter configurator to automatically search for the best iterated greedy heuristic to solve these instances, producing high-performing algorithms. The heuristic produces near-optimal solutions while solving larger instances that cannot be solved exactly in a reasonable time. Besides that, the proposed approaches can support decisions in healthcare, allowing better use of available resources and improving people's access to healthcare.",Improving the location and allocation of healthcare resources,"[77096, 77172, 77173]",952,"[56, 74, 101]",1375,Location planning in healthcare,3,7,15,OR in Health Services [ORAHS],"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,18 [building - 116],"['Health Care', 'Metaheuristics', 'OR/MS and the Public Sector']",TA-15
"With technological advancements and the growth of e-commerce industries, warehouses are taking advantage of robots' assistance to gain more productivity. This increasing usage, however, has directed them to face more challenges in terms of human-robot collaboration, specifically in the order-picking process. In this study, we address the problem of human-robot coordination in a picker-to-part picking system, where automated mobile robots [AMRs] that are equipped with bins move through the warehouse to collect the items on the batch lists. Human pickers, on the other hand, travel between item locations to load the AMRs. As there are fewer human pickers than AMRs, efficient routing of both human and robot pickers plays a pivotal role in determining the productivity of the system. Hence, the main decisions of the problem are the batching of orders for the AMRs and the routes of all pickers. To address this problem, we propose a mathematical formulation and a novel two-stage decomposition-based heuristic approach to minimise the makespan. The first stage of our heuristic solves the integrated batching-routing problem for the AMRs using a variable neighbourhood search approach, and then, in the second stage, by setting AMRs' arrival times at each location as due dates for the human pickers, the algorithm searches for their routes. We also test the performance of the heuristic and generate managerial insights using randomly generated instances on different warehouse structures.",Synchronizing Human-Robot Cooperation in Warehouses - Integrated Order Batching and Routing Problem in AMR-Assisted Picking Systems,"[77007, 61009, 62405]",733,"[146, 0]",1376,Routing in Warehouses,5,2,64,VeRoLog - Vehicle Routing and Logistics,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S16 [building - 101],"['Warehouse Design, Planning, and Control']",MA-64
"On trajectories with low demand, scheduled busses are often not efficient. If operated frequently, they may be able to capture a significant fraction of overall demand, but often run with few passengers on board or even empty. If frequencies are decreased, the service  becomes inconvenient for passengers, and demand will further decline.
On-demand ridepooling has been proposed as an alternative. However, while high service levels can be achieved if enough capacity is provided, such services are inefficient if demand is not properly pooled or if a significant amount of deadheading occurs.
For this reason, we propose an intermediate solution where services are line-based but not schedule-based. In the line-based dial-a-ride problem [liDARP ] we study the following optimization problem - Given a sequence of stops, a number of vehicles, and a set of stop-to-stop transportation requests with time windows for pickup and delivery, assign services to the vehicles and passengers to the services such that each service visits the stops in the indicated sequence, and passengers are transported from origin stop to destination stop by the service they are assigned to, whilem aximizing a weighted sum of transported requests, km driven, vehicles used, and car km saved. 
We compare three MILP  formulations of the liDARP, one that encodes assignment  of vehicles and passengers to services, one that relies on the 3-index formulation for the general DARP, and one event-based formulation.",Mixed-integer linear programming formulations for the line-based dial-a-ride problem,"[19182, 75317, 64798, 9695]",181,"[143, 145, 129]",1378,Ridehailing & Ridepooling,85,4,54,Public Transport Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S01 [building - 101],"['Transportation', 'Vehicle Routing', 'Scheduling']",MC-54
"SOS is an e-commerce format that is getting popular across many products like apparel, food, medicine, baby care, cosmetics, and more. Many brick-and-mortar firms are adopting SOS as their revenue model. SOS can efficiently provide sustainable revenue to industries facing economic sustainability issues, like the cottage industry, and help contribute to SDG 8. However, it is crucial to analyse customer's intention to subscribe to any SOS.
Thus, using the structural equation modelling method, this study analyses factors related to consumer attitude, motives, and barriers affecting the intention to subscribe [ITS] to handcrafted textiles that face sustainable revenue issues. This study is novel as the variables include product consumption and subscription viewpoint. Multiple theories from the literature support the model, which improves the model's generalizability. Results show that consumers' attitudes and motives towards consumption and subscription positively affect ITS. In contrast, perceived risks, market and consumption barriers negatively affect ITS. This work finds availability and cost issues as significant obstacles to cottage textile consumption. SOS's doorstep availability and reasonable price attributes can shift such consumers towards availing cottage textile subscriptions. SOS retailing thus indicates a positive market outlook. The results are checked for demographic and psychographic group differences that enhance the market implications of this study.",Exploring subscription-based online services [SOS] for sustainable businesses of indigenous cottage products - a behavioural study on cottage textile subscription via SEM,"[76150, 47829]",692,"[90, 10, 32]",1379,Developing Countries and Sustainable Humanitarianism ,67,15,18,OR for Development and Developing Countries,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,42 [building - 116],"['OR in Development', 'Behavioural OR', 'E-Commerce']",WD-18
"Balancing an assembly line involves assigning a required set of tasks to a given set of workstations, subject to task precedence constraints and achieving some production goals. The two studied goals are antagonistic. The first seeks to maximize line productivity, which means minimizing the load on the busiest workstation. The second aims to maximize line robustness, expressed by a specific measure, and lies in the potential presence of tasks with uncertain processing time. A reconfigurable environment enables an already provided assembly line balancing [or LB] to adjust or improve its level of productivity or robustness if necessary, by allocating extra resources to workstations. As their number is limited, it is important to use them as efficiently as possible, and therefore to keep only such allocations, called efficient reconfigurations [or ER], which are non-dominated in the Pareto sense for two objectives studied. Different LB options provide different varieties of ER. In order to compare them, we introduce the so-called measure of scalability, which is computed into two steps. At first, for each set of ER sharing the same number of used extra resources, the hyper-volume of the image of this set in the objective space is calculated. Then, the sum of all these hyper-volumes is viewed as the scalability measure for the studied LB. To find an LB with the greatest value of its scalability measure, a simulated annealing is developed and tested on instances of different size.",Bi-objective based scalable balancing of a reconfigurable assembly line,"[75621, 75846, 22050, 6645]",349,"[105, 127]",1380,Resource constrained scheduling,35,10,60,Project Management and Scheduling,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S09 [building - 101],"['Production and Inventory Systems', 'Robust Optimization']",TD-60
"Guarantees of origin [GOs] are a tool to trace and guarantee the renewable origin of electricity. Using GOs enable electricity suppliers to provide specific contracts to customers such as renewable energy contracts or even more specific contract such as German non-subsidized wind electricity. This also encourages investments in renewable energy technologies and provides an additional source of revenues for energy producers.
The European Power Exchange [EPEX SPOT] and European Energy Exchange [EEX] have launched in 2022 the first pan-European spot auction for GOs. GOs may differ in issuing country, production technology, subsidy schemes or production month. This auction is uncommon as it offers market participants with the option to bid for products which are either specific or generic, enabling purchasers to express precise requests, stimulate competition and create reliable price indices.
Artelys has worked closely with EPEX and EEX in the auction design and was in charge of the development of the market clearing engine. The algorithm has to ensure that various business rules are respected, and to strike the right balance between flexibility, complexity, fairness and intuitiveness. The algorithm objectives are to maximize the social welfare, the traded volume, and to compute all specific and generic market clearing prices. The problem consists in a multi-staged mathematical optimization algorithm which is solved using the state-of-the-art numerical solver Artelys Knitro.",Modeling and solving the first pan-European Guarantees of Origin market with Artelys Knitro,"[77042, 31710]",644,"[36, 93, 5]",1381,Electricity Market Design,22,3,09,Energy Markets,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,10 [building - 116],"['Electricity Markets', 'OR in Energy', 'Algorithms']",MB-09
"This paper describes a case study of the application of Lean Thinking in a Small-Medium Size[SME] manufacturing industry. Associated Utility Supplies [AUS] is in North of England and supplies [amongst other products] industrial steel work for Electrical, Rail, and Telecom sectors since 1998. It employs 54 staff with an annual turnover of £9 million. 

A programme of digitisation was implemented and one of the issues initially identified as a pilot was to address the “offcut issue”. During production excess raw material of varying lengths are left over and historically these are stored and are used in an ad hoc manner involving manual inspection and measurement of the offcuts before orders are placed with suppliers. This was an issue that management had been aware for some time but had been unable to successfully resolve.

Utilising Lean improvement techniques this process was mapped out and improvements suggested following 5S and 7 Wastes thinking, in addition as much of the process as possible was digitised and incorporated into the update of the organisation’s ERP system. 

This novel approach has resulted in a 40% increase in the productivity of the process and greater stock visibility. More importantly for the organisation, it has provided a successful case study of the application of a Rapid Improvement Cycle and upskilling of key operational staff in Lean improvement techniques, which has paved the way for further digitisation of the production planning.",Cutting off the offcuts,"[76227, 77199, 77265, 78649]",61,"[105, 69, 84]",1387,"Theory of Knowledge, Technology, and Innovation",54,12,08,"Knowledge, Technology, and Innovation","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1020 [building - 202],"['Production and Inventory Systems', 'Manufacturing', 'Optimization Modeling']",WA-08
"Network optimization problems are recognised by their underlying graph[s] and constraints. Examples are within planning & scheduling, vehicle routing, and multi-commodity flow problems. Flowty’s solver exploits the graph structure and solves resource constrained shortest path problems to generate variables using a column generation scheme.

Graphs are described as edge sets and a source and target vertices for a path. Paths can additionally be constrained with a set of resources. Constraint modeling is done directly on the vertices and edges of the graphs, the subproblem itself, as well as on traditional mixed integer variables. This allows for very compact models, e.g., set partitioning constraints on vertices and edge design variables in network design problems, and the graph and resource constraints avoids the need for linearisation of constraints [big-M].

The literature has shown that column generation based algorithms are superior for many network optimization problems. The solver combines the familiarity of mixed integer programming models and graph structures with algorithmic speed.
 
In this talk we will dive into modeling examples in Python, take a quick look under the hood, and present recent performance benchmarks.",Flowty - a network optimisation solver,"[63559, 76914]",701,"[63, 13, 53]",1389,Optimization Solvers,76,5,30,Software for Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,53 [building - 208],"['Large Scale Optimization', 'Column Generation', 'Graphs and Networks']",MD-30
"Hexaly Optimizer, formerly known as LocalSolver, is a model and run mathematical optimization solver based on various exact and heuristic methods. The presentation will introduce the different components of Hexaly Optimizer's local search through disjunctive scheduling problems.
We will first show how its modeling formalism can be used to express various academic and industrial scheduling problems using interval and list decision variables. These models are very compact, which enables the solver to handle even large-scale problems.
Detecting non-overlap constraints in the model provides the solver with valuable information, which can be exploited through various scheduling-specific movements implemented in Hexaly Optimizer's local search. However, due to the tightness of precedence and non-overlap constraints in good solutions to disjunctive scheduling problems [Job Shop Scheduling Problem, for example], such a small-neighborhood search alone struggles to obtain good performance.
Hexaly Optimizer overcomes this issue by reinforcing its local search component with a solution repair algorithm based on constraint propagation. When a move renders the solution infeasible, it is gradually repaired, one constraint at a time, by heuristically shifting the variables just enough to repair. To extend the local transformation rather than cancel it, and to ensure the procedure is fast, we impose never to backtrack on a previous decision to increase or decrease a variable's value.",Disjunctive scheduling using interval decision variables with Hexaly Optimizer,[72572],347,"[129, 107, 74]",1393,Advanced heuristics for machine scheduling,35,9,60,Project Management and Scheduling,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S09 [building - 101],"['Scheduling', 'Programming, Constraint', 'Metaheuristics']",TC-60
"We are interested in the kidney exchange problem with reserve arcs [KEPRA], an extension of the classical kidney exchange problem in which one is allowed to select arcs that do not appear in the compatibility graph [called reserve arcs, or RA]. We study two optimization problems - [i] what is the maximum number of transplants given an upper bound on the total number of RA that can be used, and [ii] what is the minimum number of RA that is necessary for every recipient to receive a kidney.

This problem is motivated by a recent breakthrough in the field of kidney transplant reported by the newspaper “The Guardian” stating that “researchers have successfully altered the blood type of three donor kidneys in a gamechanging discovery that could significantly improve the chances of patients waiting for a transplant finding a match”.

We show that, even though existing integer linear programming [ILP] formulations can easily be extended to the KEPRA, solving these models with a state-of-the-art ILP solver requires a long computation time due to the fact that the compatibility graph is complete. We then show that there always exists an optimal KEPRA solution in which every cycle contains at most one RA, and use that property to develop an effective variable reduction procedure for each model. We finally use that property to extend the well-known position indexed chain-edge formulation to the KEPRA and show that it clearly outperforms all the other ILP models.",Integer linear programming formulations for the kidney exchange problem with reserve arcs,"[62396, 77242, 37829]",592,"[14, 109, 56]",1394,Kidney Exchange I,3,9,10,OR in Health Services [ORAHS],"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,11 [building - 116],"['Combinatorial Optimization', 'Programming, Integer', 'Health Care']",TC-10
"Traditionally, Soft OR is operationalised in in-person workshops, where participants interact with a range of tools, which may include modelling software but are often just flipcharts and sticky notes. More recently, Soft OR interventions have been conducted in 2D online spaces [e.g. via Zoom and using Miro]. However, considering the growing potential of embodied interaction in immersive virtual reality [VR] spaces, what are the opportunities and challenges of using 3D VR spaces as part of Soft OR education? Drawing on empirical data from student participants interacting in immersive virtual environments [IVEs], we differentiate between challenges and opportunities pertaining to technology appropriation and in-IVE interaction for Soft OR. We conclude by reflecting on the broader implications for creating inclusive hybrid futures of OR education. ",Leveraging Virtual Reality for Soft OR Education ,[41621],790,"[92, 133, 151]",1396,OR Education II,48,3,16,OR Education,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,19 [building - 116],"['OR in Education', 'Soft OR', 'Practice of OR']",MB-16
"Amyotrophic Lateral Sclerosis [ALS] is a rare fatal motor neuron disease that causes degeneration of the upper and lower motor neurons. No diagnosis and prognosis biomarkers are identified to help patients’ management and research in this field is highly active. Different ALS centers are in charge of patients’ care in France with visits every 3 months for clinical and biological explorations. Many research projects are based on biological fluids samples exploration and involve different centers hospitals as ALS is considered as a rare disease. Consequently, the need to centralize samples in the specialized laboratories in charge of specific biological analyses is crucial. The question is how to organize the flow to optimize the use of these precious samples.
Several biological fluids [blood, cerebrospinal fluid, etc.] are collected from each patient in each cohort in several tubes of varying quantities. Each product must reach the other centers, which express needs for each product. Three constraints complicate this [sort of graph flow] problem - [1] dividing a sample into several portions is a tedious aliquoting operation, [2] a shipment requires a freeze/thaw operation and [3] a patient sample cannot arrive in two different tubes at the same site. We minimize the maximum number of aliquoting operations and limit the number of freezing operations. 
We model this problem, show that it is NP-complete, propose an MILP model, a lower bound and a heuristic algorithm.",Dispatching medical samples - a case study,"[22018, 77179, 77180]",202,"[14, 56, 53]",1397,Applications of combinatorial optimization I,64,7,25,Combinatorial Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,011 [building - 208],"['Combinatorial Optimization', 'Health Care', 'Graphs and Networks']",TA-25
"Closed-Loop Supply Chains [CSLCs] deal with the entire supply chain product cycle in both forward and reverse ways. This includes logistic operations such as handling, storing, and transporting recyclable products, waste, or packaging. This work focuses on a CLSC for Returnable Transport Items [RTIs] for packaging, inspired by a real-world case of a Chilean electrical conductor that uses wooden cable drums for product distribution. Currently, there is no plan for recovering or reusing this RTI. This study addresses this issue by considering both forward logistics and reverse logistics by reusing, recovering, or repairing the cable drums. A mixed-integer linear programming [MILP] model is designed to minimize the total costs of the CLSC while meeting demand and capacity constraints. The results show the convenience of adopting a CLSC approach, where it is possible to reduce costs by 30%-40% compared to using current forward logistics. ",Designing a closed-loop supply chain for wooden cable drums,"[74340, 53496, 77181, 77182, 77183]",922,"[125, 65, 100]",1398,Supply chain design in the circular economy,18,8,23,"Circular Economy, Remanufacturing and Recycling ","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,82 [building - 116],"['Reverse Logistics / Remanufacturing', 'Logistics', 'OR in Sustainability']",TB-23
"The Cross-dock Door Assignment problem [CDAP] consists of using a distribution-docking terminal for helping on supply chain distribution. Given a set of commodities to deliver from origin nodes to destination ones, the products are unloaded using a set of inbound doors, handled in the terminal, and reloaded using a set of outbound doors. The goal is to minimize the transportation cost of the commodities to be handled at the cross-dock. A class of strong lower bounds on the solution value of a Linearized Integer Programming reformulation is introduced for the binary quadratic optimization model to assign origin and destination nodes to strip and stack doors, respectively, in a cross-dock infrastructure. Strip- and stack-related decomposition submodels are developed by taking benefit of the Integer Linearization Property that appears in the new model. A linear search heuristic is also provided for obtaining feasible solutions by exploiting the special structure of the problem. We present an extensive computational study on a testbed of 55 instances to show that the proposed joint scheme for lower bounding and feasible solution providing is very efficient. ","Cross-dock Door Assignment Problem, CDAP, decomposition methodologies","[23052, 139, 67934]",456,"[14, 114]",1399,Cross-dock Door Problems,49,5,35,"Stochastic, Robust and Distributionally Robust Optimization","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,44 [building - 303A],"['Combinatorial Optimization', 'Programming, Quadratic']",MD-35
"The impact of ambiguity on financial markets continues to be a debated topic. Like most decisions in life, we face ambiguity, which is the uncertainty about true probabilities. However, this aspect of uncertainty is often overlooked, despite its widespread presence in financial markets and the significant impact it can have. A prime example of extreme ambiguity is a financial crisis, where substantial uncertainty arises and investor behavior is markedly influenced. Therefore, understanding the influence of ambiguity on financial markets is crucial.
This paper introduces two fundamental statistical tools for measuring ambiguity in financial data - the Wasserstein metric and the Kullback-Leibler divergence. These tools assess how close or distant probability distributions are from an expected probability distribution. These criteria are particularly useful for measuring ambiguity during financial crises, especially in multi-dimensional cases.
We conduct empirical analysis using major financial indices, such as the S&P 500, Nikkei 225, Eurostoxx 600, and DXY index, covering the period from January 2000 to December 2022. The findings have significant practical and operational implications for portfolio allocation. It is crucial to consider uncertainty regarding the true values of financial parameters when determining the optimal portfolio profile
",On the measure of ambiguity on financial markets,"[33712, 67695, 65411, 24436]",412,"[45, 126]",1401,Quantitative methods for systemic and climate risk,9,4,51,Risk management in finance,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,M5 [building - 101],"['Financial Modelling', 'Risk Analysis and Management']",MC-51
"In this work we propose a new primal-dual algorithm with adaptive step-sizes. The stochastic primal-dual hybrid gradient [SPDHG] algorithm with constant step-sizes [Chambolle et al. 2018, Ehrhardt et al. 2019] has become  widely applied in large-scale convex optimization across many scientific fields due to its scalability. While the product of the primal and dual step-sizes is subject to an upper-bound in order to ensure convergence, the selection of the ratio of the step-sizes is critical in applications. Up- to-now there is no systematic and successful way of selecting the primal and dual step-sizes for SPDHG. In this work, we propose a general class of adaptive SPDHG [A-SPDHG] algorithms, and prove their convergence under weak assumptions. We also propose concrete parameters- updating strategies which satisfy the assumptions of our theory and thereby lead to convergent algorithms. Numerical examples on computed tomography demonstrate the effectiveness of the proposed schemes.  ",Stochastic Primal Dual Hybrid Gradient Algorithm with Adaptive Step-Sizes,"[77185, 68846, 77188, 77190, 77191]",208,"[5, 21, 63]",1402,Algorithms for machine learning and inverse problems - adaptive strategies,84,5,32,Advances in large scale nonlinear optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,41 [building - 303A],"['Algorithms', 'Convex Optimization', 'Large Scale Optimization']",MD-32
"Renewable Energy Communities [RECs] can provide a useful contribution to the decarbonization of the energy sector if they bring about an actual change in physical electricity flows, i.e., if they lead to an optimized use of locally produced energy. Main tools for reaching this objective are load shifting and intelligent battery management. Based on forecasts of production and consumption profiles, the associated operational decisions can be optimized with the goal of improving the overall performance of the community. The strong interdependence between the community members’ individual resources prohibits the use of simple heuristic decision rules for this purpose. As actual conditions may well deviate from the forecasts used for optimization, not all optimized decisions will yield the intended outcome. Therefore, we developed a Model Predictive Control inspired planning framework that comprises an optimization model and a simulation model. Using the most recent available short-term forecasts as input, the optimization model is solved for a planning period of 96 time steps. The computed optimization output for the upcoming time step then serves as input for the simulation model, which models the actual real-world outcome of the planning decisions for the current time step. The introduced framework can be used to generate realistic performance measures and to reach a better understanding of the benefits of optimization-based decision-making in RECs. ",Assessing the performance of optimized energy communities under uncertainties,"[69670, 12695, 77192]",682,"[93, 37, 47]",1403,Sustainable Energy,80,4,53,Sustainable and Resilient Systems,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,8007 [building - 202],"['OR in Energy', 'Energy Policy and Planning', 'Forecasting']",MC-53
"Solving high-dimensional optimisation problems is a difficult task, and numerous methods have been proposed to compensate for the high cost in computation time. The approach explored in this work exploits the structure of these optimisation problems, in order to reduce the computational cost of their solution. 

Specifically, we focus on the multi-resolution structure at the heart of multi-level optimisation methods. These approaches take advantage of the definition of coarse approximations of the objective function to make its minimisation more efficient. In this talk, we present a proximal multilevel algorithm IML FISTA - Inexact Multilevel FISTA - suitable for the solution of optimisation problems where the non-smooth component of the objective function has no explicit formulation for the proximal operator. 

The proposed method is then adapted to solve imaging problem in radio-astronomy. To reconstruct such images, a high number of observations in the Fourier space are combined. Such high number creates a computational bottleneck. We demonstrate that IML FISTA can provide considerable acceleration by using coarse approximation constructed in the observation space.",Multilevel proximal methods for image restoration,"[77187, 58019, 67472]",133,"[21, 81, 18]",1404,"Nonsmooth optimization and applications, Part I",84,7,32,Advances in large scale nonlinear optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,41 [building - 303A],"['Convex Optimization', 'Non-smooth Optimization', 'Computer Science/Applications']",TA-32
"This research intends to investigate, within a decision-making process, the affordances enabled by the use of a specific artifact, a new software called Multi-Values Appraisal Methodology [MuVAM] created on a multi-methodological approach recently introduced in the literature [Lami and Todella, 2023]. MuVAM combines the Strategic Choice Approach with the Analytic Hierarchy Process and it is based on cloud-based technologies, which allow synchronized and connected use, digitally and also in asynchronous mode. The study investigates the contribution of the software to group interaction and the development of a “Plural Subject” through the analysis of eight workshops, held in four different European countries, in three languages [English, French and Italian] and on four different case studies, all related to sustainable urban development. In particular, the applications differed with respect to - i] the scale of intervention, ranging from the planning of an entire city area to that of a university campus; ii] the outcome, which in most cases concerned the definition of a master plan. Interestingly, the participants in the workshops were also quite diverse - Bachelor's, Master's, PhD and Lifelong Learning Master's degree students, with a varied experience in terms of educational level and background. Feedback from the workshops was collected through a report or a Google survey, through a series of questions covering both group dynamics and individual choices.",Triggering a “Plural Subject” through the software “MuVAM” combining Strategic Choice Approach and Analytic Hierarchy Process,"[56982, 15067]",129,"[149, 55, 134]",1405,Approaches for Integrating Quantitative Modelling,26,4,13,Soft OR and Problem Structuring Methods,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,15 [building - 116],"['Problem Structuring', 'Group Decision Making and Negotiation', 'Software']",MC-13
"We are interested in the bicycle network improvement problem. Let a set of paths obtained from GPS traces represent the cyclist's path on a graph. This graph consists of nodes, i.e. intersections, and arcs, i.e. roads with or without bicycle facilities. Thus, for each arc, we get the danger as a function of its length, the bicycle facilities, and the maximum car speed. A user's cost is given by the cost of his best path, while the cost of an arc is given by a linear combination of its distance and its danger, depending on his own preference. The user's preference between danger and distance is also given. 
We focus on where cycling facilities must be placed to minimize the sum of users' costs. Each time a cycling facility is decided, it costs money [depending on the length of the arc to be improved], and the total budget is limited. This problem is NP-hard. We present a greedy heuristic. 
At each step of the algorithm, we evaluate for each arc the impact on each user of upgrading that arc. We choose the arc that maximizes this impact on the sum of the users' costs. Using two Dijkstra algorithms [one forward, one backward] per user allows us to compute this impact efficiently. 
Moreover, this algorithm can be easily parallelized [the phase of computing the two Dijkstra algorithms per user] using multithreading. Some real instances [40 000 nodes and more that 20 km of plant proposal] can be treated in a few seconds on some classical desktop computer. 
",Greedy heuristic for solving real size instances of the bicycle network improvement problem,"[77189, 77193, 77194, 77254]",612,"[139, 53]",1408,Optimization of sustainable urban mobility,79,7,18,Sustainable Cities,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,42 [building - 116],"['Sustainable Development', 'Graphs and Networks']",TA-18
"After a disaster strikes, relief goods need to be delivered to the beneficiaries as efficient as possible. However, a disaster may destroy a part of the infrastructure. Knowledge about the state of infrastructure is essential to aid organizations in efficient routing. To map the state of the infrastructure, we estimate a distribution of the maximum speed one can drive on edges using GPS trajectories from relief vehicles, which are currently simulated.

This distribution is estimated in three ways:
• For frequently driven edges, we only use their own GPS data.
• For seldomly driven edges, we also use their own GPS data. Additionally, we use a measure on the likelihood of the edge being deliberately avoided after the disaster. We check how often it should have been part of optimized routes based on the pre-disaster infrastructure, while it is not in the driven trajectories. If the edge is likely avoided, we estimate the proportion of delay incurred on the edge, assuming that the pre-disaster route would be chosen if not damaged.
• Undriven edges in pre-disaster optimized routes are also predicted with avoidance estimation. Undriven edges which are not in pre-disaster routes are labeled unpredictable.

The approach can be used in a static setting, predicting the state of infrastructure only once. It can be extended to a dynamic version where the predictions from the prior time period are used as infrastructure during the next period’s trajectory generation.",Mapping post-disaster infrastructure using GPS data from relief vehicles,"[77032, 32308, 71440]",419,"[58, 30, 5]",1411,Humanitarian Aid,78,13,13,Secure & Sustainable Food Supply,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,15 [building - 116],"['Humanitarian Applications', 'Disaster and Crisis Management', 'Algorithms']",WB-13
"The relationship between environmental, social, and corporate governance [ESG] and corporate financial performance [CFP] has been widely explored and continues to be an active area of research. Two main directions could be revealed. Investor-centric studies seek to directly link ESG to CFP using benchmarks and adopting a portfolio accordingly, while corporate-focused research explores different elements of performance like innovation and operational efficiency to understand the impact of ESG initiatives on CFP. These studies commonly employ correlation analysis, mediating factors, or regression analysis to disclose those patterns.  Notably, some studies employ Data Envelopment Analysis [DEA] to assess corporate efficiency by minimizing their resources and maximizing ESG. We contribute to this field by using a benefit-of-the-doubt [BoD] model based on the DEA, which is used to calculate a composite indicator of ESG for manufacturing corporates. Then, in the second stage of this model the panel regression is used to identify elements of CFP and their significance for the ESG outcome. The results of corporate ESG efficiency is discussed in a view of corporate operating country, region and development level. The research has received funding from the Research Council of Lithuania [LMTLT], agreement No. S-PD-22-23.",Two-stage DEA model to evaluate corporate efficiency in sustainability ,"[46721, 77200, 76607]",930,"[139, 24, 100]",1412,Corporate finance risk management,9,7,51,Risk management in finance,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,M5 [building - 101],"['Sustainable Development', 'Data Envelopment Analysis', 'OR in Sustainability']",TA-51
"The family capacitated vehicle routing problem [F-CVRP] is an NP-hard problem that generalizes both the FTSP and the capacitated vehicle routing problem. The F-CVRP has practical applications in warehouse management in warehouses with scattered storage. We propose several compact and non-compact formulations to address the problem. These formulations are compared both theoretically and computationally. Additionally, sets of valid inequalities are also derived and tested computationally. A branch-and-cut algorithm is used as a solution method for the non-compact formulations. Computational results show that both the developed branch-and-cut algorithm and the best-performing compact model are suited to address small to medium-sized instances of the F-CVRP.",Comparison of Formulations for the Family Capacitated Vehicle Routing Problem,"[57605, 2139]",787,"[145, 111, 11]",1415,MILPs for Vehicle Routing 2,5,12,58,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S07 [building - 101],"['Vehicle Routing', 'Programming, Mixed-Integer', 'Branch and Cut']",WA-58
"The number of asylum applications in the European Union [EU] has significantly increased during the last years, highlighting the need for effective decision-making in the resettlement process. This research focuses on the refugee status, the most common outcome among asylum seekers, and the resettlement process that transfers refugees from an asylum country to a receiving country. Integration plays a crucial role in the success of resettlement, benefiting both refugees and receiving countries. Efforts from all parties, including refugees' adaptation and receiving communities' readiness, are necessary for successful integration. The National Integration Evaluation Mechanism project evaluates the integration outcomes of refugees in 15 EU member states, providing a comprehensive overview through a set of integration indicators. The priorities of receiving countries and the preferences of refugees also influence the assignment process. However, no comprehensive optimization framework exists that combines these dimensions for decision support. This study proposes a decision-making system that incorporates integration potential, receiving countries' priorities, and refugees' preferences, considering quotas and analyzing different scenarios. The system aims to aid organizations in making informed decisions and could be integrated into existing information systems. The research contributes to addressing the complex challenges of refugee resettlement and has potential societal impact.",Towards Effective Assignment Decision-Making in Refugee Resettlement,"[74238, 54717, 66434, 29273]",556,"[58, 15, 14]",1416,Complex societal problems,38,10,21,OR in Humanitarian Operations [HOpe],"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,49 [building - 116],"['Humanitarian Applications', 'Complex Societal Problems', 'Combinatorial Optimization']",TD-21
"This work focuses on the design of financial incentives to reduce medically unnecessary C-sections, resulting in enhanced birth quality with alleviated economic burden for healthcare payers. To this end, we develop an innovative semi-supervised fuzzy clustering algorithm to classify pregnant women into low- and high-risk groups by analyzing approximately 18 million birth records from US. Our experiments on real-life and synthetic data demonstrate the efficiency of our algorithm for large datasets. Then, we validate the optimal delivery methods for two risk groups through post-delivery outcomes for the mother and the newborn by using inferential statistical analysis. Furthermore, we develop a metric to quantify the maternity risk index to be used in stylized analytical models. Next, we develop an analytical framework based on a principle-agent model to analyze the effect of different payment schemes from the quality of care and cost perspectives. We propose three payment systems, hybrid payment, risk-sharing model, and penalty contracts to alleviate the shortcomings of fee-for-service and bundled payment schemes, thereby facilitating system optimal decisions. Finally, we present a simulation-based numerical study to empirically verify our analytical results.",On Reducing Medically Unnecessary Procedures Through Analytics - The Design of Financial Incentives for Maternity Care,"[77202, 77203, 77204]",608,"[56, 7]",1417,Healthcare Analytics,3,4,15,OR in Health Services [ORAHS],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,18 [building - 116],"['Health Care', 'Analytics and Data Science']",MC-15
"The surge in last-mile delivery demands, driven by the growth of e-commerce, poses intricate challenges in meeting tight schedules, navigating congested urban areas, and addressing environmental concerns. In this study we introduce an approach to address a part of last-mile delivery challenges by integrating a mobile two-echelon strategy. We extend traditional two-echelon systems to consider multiple depots, multiple commodities, and flexible meeting points [mobile satellites]. To solve this complex problem, we propose a mixed-integer linear programming [MILP] model and a math-heuristic algorithm, SALNS, combining large neighborhood search [LNS], LP scheduling, and a novel schedule approximation method. Comprehensive experiments demonstrate the algorithm's efficiency, achieving identical or superior results to exact benchmarks while running, on average, nine times faster. Notably, the algorithm robustly generates feasible solutions for medium and large instances in practical times. Furthermore, sensitivity analyses reveal the critical roles of fuel, wage, and time-window parameters in shaping delivery strategies, providing valuable insights for decision-makers.",A Two-Echelon Vehicle Routing Problem with Mobile Satellites and Multiple Commodities,"[71333, 36097, 50253, 55094]",205,"[145, 65, 129]",1418,Scheduling and Routing Problems ,64,5,52,Combinatorial Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,8003 [building - 202],"['Vehicle Routing', 'Logistics', 'Scheduling']",MD-52
"In this talk we present a new coderivative-based Newton algorithm for tackling unconstrained optimization problems whose objective function can be expressed as the difference of two generally nonconvex functions, namely, a continuously differentiable function with locally Lipschitzian derivatives and a locally Lipschitz and prox-regular function. Focusing our attention in the requirement of extended positive-definite properties of the generalized Hessian of the first function, we prove the well-posedness and various convergence results of the proposed algorithm. We conclude with some numerical experiments demonstrating the performance of the proposed algorithm in nonconvex settings.

This is a joint work with Boris S. Mordukhovich and Pedro Pérez-Aros.",Semi-Newton algorithms in nonsmooth difference programming,[39038],291,"[81, 5, 19]",1419,Iterative Methods for Feasibility and Optimization Problems,82,13,42,Variational Analysis and Continuous Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,98 [building - 306],"['Non-smooth Optimization', 'Algorithms', 'Continuous Optimization']",WB-42
"In recent years, efforts to address climate change and promote the creation of an environmentally sustainable economy have intensified. This commitment has spurred the emergence of financial instruments designed to support activities commonly referred to as green. Financial institutions, as well as non-financial corporations or local governments, engage in lending activities to fund challenging low-carbon transition objectives. This includes financing rooftop solar power, wind farms, environmentally friendly farming practices, water conservation, the leasing of hybrid and electric vehicles, and initiatives aimed at enhancing the energy efficiency of industrial, commercial and residential properties.
One prevalent approach to model portfolio credit risk involves employing the LHP [Large Homogeneous Portfolio] approximation within the one-factor Merton-type framework, commonly referred to as the ASRF [Asymptotic Single Risk Factor] model. In this work we consider the classical Vasicek Gaussian model under more general distributional assumptions for the systematic risk factors. We specifically examine how the quantiles of the loss portfolio are affected by mixing distributions characterized by skewed and heavy-tail properties. This analysis is conducted in the context of loans, some of which are designated as green, indicating their allocation towards financing environmentally sustainable assets.",LHP approximation for green-loan credit portfolios under skewed and heavy-tails returns,[72103],438,"[44, 45, 135]",1420,Risk Management in Private and Public Finance,4,13,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S14 [building - 101],"['Finance and Banking', 'Financial Modelling', 'Stochastic Models']",WB-63
"Problem structuring and alternatives generation are important and highly interconnected stages of decision-making and decision-aiding processes. An ill-formed problem formulation or a limited set of alternatives have a significant impact on the quality of the final decision. This is emphasised when complex multi-actor decisions or policy making need to be supported. Problem structuring methods are largely recognized for their valuable contribution to decision-and policy-making processes; on the other side, the alternatives generation phase is often overlooked in research and practice, leading to unsatisfying decisions. Yet, alternatives are generated before being analysed, explicitly or implicitly, formally or informally. 

Within this context, this talk presents the lessons learnt from a collaborative activity carried out with international Operational Research [OR] experts involved in an experimental workshop. The focus of this activity was the problem formulation and alternatives generation phases occurring between a client and six analysts using different OR methods. The objectives of the experimental workshop were to observe and discuss how different approaches deal with structuring decision problems and generating alternatives, and to share experience on procedures which might be transferable. In doing so, the talk catalyses",Structuring decision problems and generating alternatives - report of a workshop,"[46682, 77207, 7119, 3951, 10666, 46433, 13]",131,"[149, 133, 10]",1421,OR Innovations in Policy Making - A,26,3,13,Soft OR and Problem Structuring Methods,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,15 [building - 116],"['Problem Structuring', 'Soft OR', 'Behavioural OR']",MB-13
"Lottery based questionnaires that let subjects chose among several lotteries with identical expected values are a popular way to measure risk attitudes. However, respondents have been shown to select a “middle” alternative too frequently, thus leading to an overestimation of the share of risk neutral individuals in a population. In this paper, we present the result of a comprehensive set of experiments testing the robustness of and different possible explanations for this phenomenon. In particular, we test for robustness in terms of [small] differences in expected values among lotteries, different levels of stakes involved in the lotteries, or whether respondents are incentivized. We also test for alternative explanations such as careless and overly fast responding, misunderstandings of the implications of answers or a general tendency to select an answer that is presented in the middle of a list. Our results indicate that carelessness and higher stakes increase the bias, while higher cognitive abilities and extreme levels of subjectively self-assessed risk attitudes decrease it. We find no significant effects of small differences in expected values or incentives. ",Is lottery-based measurement of risk attitudes subject to a central tendency bias?,"[454, 77208, 72700]",570,"[10, 25]",1422,Behavioral Decision Analysis III,13,5,11,Behavioural OR,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,12 [building - 116],"['Behavioural OR', 'Decision Analysis']",MD-11
"Hospital-at-home [HaH] provides short-term acute care to patients at home for a condition that otherwise would require inpatient admission. Having acute illnesses, the patient mix fluctuates. Furthermore, more accurate information about resource availability becomes known with time. Thus, operational plans have to be established taking into account this updated information.
Given a baseline roster and patient requirements, simultaneous operational decisions on patient admission, nurse assignment, visit scheduling, and potential roster updates are taken for a weekly horizon, yielding a combined nurse routing and re-rostering problem. The integrated problem incorporates complex real-world characteristics related to patient admission, routing, rostering, and re-rostering constraints while ensuring continuity across consecutive planning horizons.
A lexicographic objective function first maximizes the number of patients admitted to HaH and second minimizes the total working duration of the nurses. This is achieved by employing destroy and repair strategies based on a large neighborhood search. A guided local search is embedded to modify nurse rosters to further improve the solution. Infeasible solutions are allowed to be explored by relaxing time windows, rostering, and re-rostering constraints.
The first results obtained by this algorithm will be presented, followed by a discussion about several challenges in balancing the different objectives of the studied problem.",A metaheuristic approach for an integrated nurse routing and re-rostering problem,"[70763, 11840, 43536]",967,"[56, 128, 145]",1424,Integrated planning in healthcare,3,13,10,OR in Health Services [ORAHS],"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,11 [building - 116],"['Health Care', 'Rostering', 'Vehicle Routing']",WB-10
"The aim of this talk is to introduce an exact point-based formula [involving only the problem data] for the Hoffman constant of the argmin mapping in linear optimization, which can be considered as the sharp Lipschitz constant restricted to its domain. 
The work is developed in the parametric context of right-hand side perturbations of constraint systems. To achieve our goal, we develop a new concept of its own interest, called well-connected
piecewise convex mappings. Indeed, optimal set mappings fall into this new classification of mappings, for which we provide a recursive construction to derive a crucial equality between the Hoffman constant and the supremum of the calm moduli.",Advances on the sharp Hoffman constant of the argmin mapping,"[71779, 3642, 9358, 3643]",294,"[19, 110]",1429,Variational Analysis and Subdifferential techniques,82,9,42,Variational Analysis and Continuous Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,98 [building - 306],"['Continuous Optimization', 'Programming, Linear']",TC-42
"In the UK our public sector services are described as being delivered efficiently. Each project will have been funded based on needs, but at a macroeconomic level there is great opportunity to make improvements. So, why don’t we make these changes? In part because it is difficult and the benefits won’t be seen by the people who deliver the change. There is also no enduring structure and mandate within government to perform systemic change, although there is a structure for audit – highlighting more focus on risk than on the opportunity. Transformational change is possible and innovation is achievable. There are many opportunities to design policy to realise significant benefits, but we lack political will and appropriate control systems. A simple assessment can be obtained by comparing publicly available reports from key UK agencies such as The Metropolitan Police. It can easily be shown that there is a wide gap between the mission for each organisation and what they deliver.  In considering systems level improvements, a project will be examined using methodologies such as Soft System Methodology [SSM] and the Viable System Model [VSM]. This project will demonstrate that there is a need to develop policy using systems approaches. It will highlight the broader opportunities that may be possible, and that transformational change can be delivered for the UK government. ",Reflections on the Opportunities to Innovate Policy and using Problem Structuring Methods to Transform UK Public Sector Organisations,"[77056, 78630, 77237]",718,"[16, 22, 101]",1431,OR Innovations in Policy Making - B,26,7,13,Soft OR and Problem Structuring Methods,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,15 [building - 116],"['Complexity and Approximation', 'Critical Decision Making', 'OR/MS and the Public Sector']",TA-13
"Assessing the impact that European Funds have on people's lives is an area that has received the attention of researchers and institutions. On the one hand, citizens demand better results and more information, on the other hand, institutions need methodologies that allow them to measure the impact of investments. Considering an investment horizon of 14 years [between 2007 and 2020] and a development assessment period of 7 years [between 2014 and 2020], this research uses a DEA methodology to evaluate how the 308 Portuguese municipalities use financial and human resources [inputs] to achieve sustainable development [output]. As a result, each municipality is characterized by a CCR efficiency measure, being positioned towards the efficient frontier. In this way, it is possible to identify municipalities with best practices and the effort required from the rest to achieve them. Graphic maps facilitate the communication of information by illustrating the distribution of efficiency throughout the territory.",An Analysis of the Efficiency of European Funds in Sustainable Development in Portugal,"[7141, 78788, 78957]",940,"[35, 139, 24]",1432,DEA applications in Environment and Sustainability I,89,8,48,Data Envelopment Analysis and its Application,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,60 [building - 324],"['Efficiency Analysis', 'Sustainable Development', 'Data Envelopment Analysis']",TB-48
"This article reassesses the assertions regarding algorithmic collusion presented in Calvano et al. [AER 2020] and subsequent literature. We first reveal an important mistake in the original papers, which refutes the idea that the phenomenon of supracompetitive prices arises from collusive mechanisms where reward and punishment schemes support high prices. Then, our theoretical and simulation-based analysis suggests that both the high prices and the apparent punishment schemes result directly from simultaneous experimentation and the learning inertia inherent in most reinforcement learning techniques. The co-occurrence of high prices and apparent price wars is not causal and both stem from imperfect learning. We demonstrate that such seeming collusion can arise in memoryless environments, even with myopic agents, and much faster than previously thought. Our findings alert on the risk of qualifying as collusion a phenomena that is not, and invite new, simple approaches to address the issue of algorithmic supra-competitive pricing.",Less than meets the eye - simultaneous experiments as a source of algorithmic seeming collusion,[77209],326,"[66, 33, 50]",1433,Machine Learning for Electricity Market Applications,22,4,14,Energy Markets,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,16 [building - 116],"['Machine Learning', 'Economic Modeling', 'Game Theory']",MC-14
"Batteries are the element of the microgrid with the shortest and limited lifetime, characterized by a finite number of charge/discharge cycles. By optimizing charge/discharge cycles, the system can extend battery life, contributing to overall sustainability. However, in microgrids, it is often in the interest of minimizing their economic operating cost, which is a contradictory objective to maximizing battery lifetime, as batteries can save energy from renewable energy sources for use during periods of high energy demand instead of using energy from the main grid. Therefore, the energy management system must address these objectives to ensure efficient operation of the microgrid.  This research presents a novel energy management system designed to simultaneously minimize energy costs and battery charge/discharge cycles in a microgrid. Our methodology employs a multi-objective optimization algorithm to identify solutions that satisfy both economic and operational objectives. The result is a set of equally valuable solutions, giving decision makers the flexibility to choose the option that best fits their current preferences. This adaptability allows users or operators to change solutions as conditions evolve, eliminating the need to re-solve the optimization problem. The effectiveness of the energy management system is demonstrated through its application in simulation to the microgrid located in a bioclimatic building at the University of Almería.",Multi-objective optimization for a microgrid energy management system - Controlling battery lifetime and economic cost.,"[51378, 77212, 77214, 77211, 59423]",472,"[36, 37, 77]",1434,Distributed energy systems,21,10,22,Energy Management,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,81 [building - 116],"['Electricity Markets', 'Energy Policy and Planning', 'Multi-Objective Decision Making']",TD-22
"The dynamical intertwining of many heterogeneous operational elements, agents and locations are oft-cited generic factors to make myriad socio-technical systems such as supply chains, [inter] national trade and human mobility, prone to large-scale disruptions. These disruptions are emergent phenomena, and their underlying causes are _endogenous, stemming from the systems' operational planning_. We demonstrate this by analysing spreading of delays on the Dutch train network - the emergence of large-scale disruptions rests on the dynamic interdependencies among multiple `layers' of operational elements [services, rolling stock and crew]. The interdependencies provide pathways for delay cascading, which gets activated when, constrained by local unavailability of on-time resources, already-delayed ones are used to operate new services. Cascading locally amplifies delays, which in turn get transported over the network to give rise to new constraints elsewhere.

Systemically building in [temporal] buffers can arrest delay propagation in socio-technical systems. However, the general tendency in Operations Research, reinforced by competitive pressures, is to myopically in reduced buffers. Ironically, such optimisations can push a system to the proverbial cliff edge, increasing its fragility to large-scale disruptions. We therefore draw attention to the proposal that any welfare function that a system operator seeks to optimise should contain a measure of resilience.",Delay cascading dominates large-scale disruptions in transport,[77213],193,"[143, 122, 65]",1435,Resilience in Public Transport Planning,85,7,54,Public Transport Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S01 [building - 101],"['Transportation', 'Railway Applications', 'Logistics']",TA-54
"Middle-mile logistics describes the problem of routing shipments through a network of hubs while respecting deadlines upon arrival. We consider that the hubs are linked by predefined lines, to which we have to assign vehicles. A very challenging aspect of the problem comes from the finite capacity of the vehicles - allocating a shipment to a given vehicle might block another one from using the same vehicle.

Typical exact solution methods, based on a multicommodity-flow formulation, scale poorly with the problem size and real-world instances become quickly intractable. Instead, we turn to reinforcement learning [RL] by rephrasing the middle-mile problem as a multi-objective Markov decision process, where the state is a graph - the lines [edges] between the hubs and the parcels [nodes]. At each round, we assign one shipment to a vehicle or decide that it stays in the same hub. The key ingredients of our proposed method are the extraction of small feature graphs from the state and the combination of graph neural networks [GNN] with model-free RL.

We use the PPO [proximal policy optimization] algorithm, which maintains both an actor and a critic, while being able to cope with a varying number of actions depending on the state. We compare linear functions and GraphNet [a particular kind of GNN] to approximate the policy and value functions. GNNs can deliver up to 40% more shipments than a linear function and both approaches scale well with the number of shipments per truck.","A Tale of Middle-Mile Logistics, Graph Neural Networks, and Reinforcement Learning","[76286, 77217, 77216]",312,"[66, 65, 14]",1436,[Deep] Reinforcement Learning for Combinatorial Optimization 1,14,4,03,Data Science Meets Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,1005 [building - 202],"['Machine Learning', 'Logistics', 'Combinatorial Optimization']",MC-03
"The Orienteering Problem [OP] is a well-known routing problem within Operational Research, from which various variants have emerged over time. This presentation introduces the Team Orienteering Problem with Variable Time Windows [TOPVTW] as a new variant based on the Team Orienteering Problem with Time Windows. The distinguishing feature of TOPVTW lies in its adaptive time windows, which evolve dynamically according to the solution. This problem arises within the context of spreading processes that need to be contained. An application to wildfire suppression will be presented. In this problem, the spread of fire determines the time windows for brigades to work in a parcel of the landscape or cross through it. Simultaneously, the brigades' work controlling parcels modifies the spread of fire, preventing it from spreading through the controlled area. The aim is to determine the location of the brigades' work and their routes through the landscape to minimize the value of the burned area while maintaining safety measures for the teams. Two alternative mathematical programming models will be shown for this problem, with computational results comparing the solutions.",The Team Orienteering Problem with Variable Time Windows - An application for wildfire suppression,"[14183, 73563]",5,"[48, 30, 145]",1437,OR in Forestry I,20,3,12,OR in Agriculture and Forestry ,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,13 [building - 116],"['Forestry Management', 'Disaster and Crisis Management', 'Vehicle Routing']",MB-12
"While many approaches were developed for obtaining worst-case complexity bounds for first-order optimization methods in the last years, there remain theoretical gaps in cases where no such bound can be found. In such cases, it is often unclear whether no such bound exists [e.g., because the algorithm might fail to systematically converge] or simply if the current techniques do not allow finding them. In this work, we propose an approach to automate the search for cyclic trajectories generated by first-order methods. This provides a constructive approach to show that no appropriate complexity bound exists, thereby complementing approaches providing sufficient conditions for convergence.",Counter-examples in first-order optimization - a constructive approach,"[77218, 66635, 75404]",366,"[21, 63]",1439,Computer-Assisted Proofs in Optimization,84,14,32,Advances in large scale nonlinear optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,41 [building - 303A],"['Convex Optimization', 'Large Scale Optimization']",WC-32
"Bank asset allocation is a major determinant of banks’ long-term performance and sustainability. Therefore, it is one of the most critical problems in bank management. Current bank asset allocation models fail simultaneously to use realistic scenarios and produce state-dependent asset allocations, so do not take advantage of the changes in business cycles. Using a multi-objective genetic algorithm, we develop a simulation-optimization method that determines dynamic asset allocation rules based on environment variables such as interest rates. Performance analysis on an independent testing set indicates that the algorithm outperforms other established methodologies, namely optimizing time-independent static allocations or choosing equal-weight policies. In addition to performance, we emphasize interpretability, developing an algorithm that allows results analysis and interpretation, and can thus be used in practice to support banks’ senior management dynamic decisions.",Simulation-optimization for dynamic bank asset allocation,[77012],475,"[44, 0]",1440,Portfolio optimization ,49,9,34,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,43 [building - 303A],['Finance and Banking'],TC-34
"The current macroeconomic context of high interest rates and economic slowdown, both ignited by the gas crisis following the pandemic outbreak, is profoundly impacting the energy sector as well as others. Over the last year, renewables, most likely favoured by a reduced overall demand of power across the continent, have taken over a large share of the market, fostering decarbonization. Gas storage at maximum capacity, almost complete fuel-switch out of coal and decreasing price of carbon allowances seem not to imply profitable margins, with Clean Spark Spread and Clean Dark Spread both looking gloomy. Hence, by contrast, this arrangement is shaping up in nothing but the next trap for utilities. 
Particularly, carbon allowances experienced a drastic bearish movement far from being forecast. Over-hedging must be deemed as a possible common scenario many operators are currently navigating. Revenues reductions and variation margins are a threatening combination in this context of lack of liquidity. 
Whereas volatility and skyrocketing prices were yesterday’s enemies, stagnation and industrial crisis appear to be the new challenges to tackle in power generation market.
",RISK MANAGEMENT ISSUES IN CURRENT ENERGY CONTEXT,[76851],438,"[36, 44, 126]",1442,Risk Management in Private and Public Finance,4,13,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S14 [building - 101],"['Electricity Markets', 'Finance and Banking', 'Risk Analysis and Management']",WB-63
"The scheduling of personnel is a common problem that healthcare institutions frequently face worldwide. It consists of assigning nurses to shifts to satisfy a series of constraints relative to the workforce's skills, employee preferences and legal regulations. The current healthcare landscape, characterized by rising demand and a decreasing workforce,  together with the effect that scheduling has over aspects like costs, turnovers, and job satisfaction, has become a great motivation for studying this problem. Cross-training, which consists of training nurses to cover additional units that require distinct skills, has been studied in different sectors from manufacturing to healthcare as a strategy to add flexibility to the healthcare systems and improve its responsiveness. In this paper, we study existing forms to include this type of flexibility and propose a mathematical formulation to incorporate it. Computational experiments over different scenarios show that the proposed extensions are functional and can produce solutions for the mid-term nurse scheduling problem. From the experiments, we derive some interesting insights about cross-training flexibility. First, our results show that it is more beneficial to invest in intensity, which is the number of cross-trained nurses, than investing in breadth, i.e., the number of additional units a nurse is trained to. Furthermore, there is a trade-off between flexibility and the amount of dedicated working time for each unit. ",A study of flexibility and the chaining principle in the mid-term nurse scheduling problem,"[27956, 77222, 77221]",950,"[109, 129, 72]",1443,Nurse rostering,3,10,15,OR in Health Services [ORAHS],"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,18 [building - 116],"['Programming, Integer', 'Scheduling', 'Mathematical Programming']",TD-15
"Renewable power producers participating in electricity markets build forecasting models independently, relying on their own data, model and feature preferences. In this paper, we argue that in renewable-dominated markets, such an uncoordinated approach to forecasting results in substantial opportunity costs for stochastic producers and additional operating costs for the power system. As a solution, we introduce Regression Equilibrium—a welfare-optimal state of electricity markets under uncertainty, where profit-seeking stochastic producers do not benefit by unilaterally deviating from their equilibrium forecast models. While the regression equilibrium maximizes the private welfare, i.e., the average profit of stochastic producers across the day-ahead and real-time markets, it also aligns with the socially optimal, least-cost dispatch solution for the system. We base the equilibrium analysis on the theory of variational inequalities, providing results on the existence and uniqueness of regression equilibrium in energy-only markets. We also devise two methods for computing the regression equilibrium - centralized optimization and a decentralized ADMM-based algorithm that preserves the privacy of regression datasets.",Forecast Equilibrium in Electricity Markets,[77223],326,"[36, 37, 93]",1445,Machine Learning for Electricity Market Applications,22,4,14,Energy Markets,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,16 [building - 116],"['Electricity Markets', 'Energy Policy and Planning', 'OR in Energy']",MC-14
"Different real-world decision-making problems involve Type 1 decision-dependent uncertainty, where the probability distribution of the stochastic process depends on the model decisions. However, the literature on two-stage stochastic programs with this kind of endogenous uncertainty is scarce, with a lack of general methodologies. In this work, we propose a general method for solving a class of these programs based on the transformation of random variables, widely employed in probability and statistics. Our method is tailored for large-scale problems with both discrete or continuous endogenous random variables. The random variable transformation allows the use of the Sample Average Approximation Method [SAA], which provides convergence guarantees under certain conditions. We show that, for some classical distributions, our method reduces to solving mixed integer linear or convex programs. Finally, we validate our methodology on a Network Design and Facility Protection Problem, considering distinct decision-dependent distributions for the random variables. While most distributions result in a non-linear non-convex deterministic equivalent program, our method solves mixed integer linear programs in all cases. Our methodology is effective in terms of the typical performance estimators of the SAA and the computational time. Besides, it outperforms the case in which the endogenous distribution defines a mixed-integer deterministic equivalent. ",The Sample Average Approximation Method for Solving Two-Stage Stochastic Programs with Endogenous Uncertainty,"[76290, 50735, 22655]",272,"[136, 117, 79]",1448,Stochastic Optimization with Decision-Dependent Uncertainty,49,9,35,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,44 [building - 303A],"['Stochastic Optimization', 'Programming, Stochastic', 'Network Design']",TC-35
"The prize collecting traveling salesman problem [PCTSP] is to find routes and subset of nodes that minimize a tour distance with the constraint to be less than given rewards by visiting nodes. This study estimates the PCTSP travel distance using a continuous approximation model. 
Continuous approximation model does not search for routes, but it estimates the travel distance using the area of the region and the number of nodes. In our proposed model, the size of the delivery zone and the number of nodes is given, and the spatial distribution of the penalty is represented as a demand function. It replicates the concentration of population and economic value in the center of cities.
The function makes it possible to determine the range of the penalty and the average trip distance within the zone.
In addition, we propose a method for estimating the PCTSP travel distance in multiple delivery zones. It takes into account the distance between regions and the relative balance of demand and distribution.
The method is expected to be applied to the evaluation of priority areas where delivery services such as home delivery, food delivery, and carpooling.
",Continuous approximation model for estimating average tour length in prize collecting traveling salesman problem,"[53795, 77228, 7955]",524,"[143, 65, 145]",1454,Freight transportation and logistic III,6,10,55,Transportation,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S02 [building - 101],"['Transportation', 'Logistics', 'Vehicle Routing']",TD-55
"The fusion reactor is widely regarded as the ideal future source of green energy. Nevertheless,the fusion reactor inevitably generates radioactive waste. The waste must be stored in waste disposal facilities for a significant length of time for legal and safety reasons. Subsequently, an optimised strategy of cutting and packing is critical to reduce overall costs which means a trade-off between cutting cost [the length of cutting trajectory] and the packing cost [the number of containers utilised].

My PhD research aims to tackle a 3D irregular cutting and packing problem - the primary shapes of the fusion reactor are treated independently, divided into distinct sub-problems. Each shape must be cut and subsequently packed into fixed containers. Regarding the containers, they are of a fixed, cylindrical or cubical design.  The values of their cross-sectional area and height are predefined as parameters within the model. For each container, we also consider other parameters including the maximum permissible weight, maximum volume and the upper limits of radioactivity. The objective is to minimise the overall cost of cutting and packing.

In this presentation, I will provide a comprehensive definition of the problem, describe the way I will apply the phi-function techniques alongside a construction heuristic, and provide some preliminary results to illustrate how the approach works.",3D Irregular Cutting and Packing for the Nuclear Waste from Tokamak Fusion Reactor,"[75523, 6291, 11875]",622,"[23, 113, 93]",1455,Cutting and Packing 4 - 3D irregular,81,5,07,Cutting and Packing [ESICUP],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,1019 [building - 202],"['Cutting and Packing', 'Programming, Nonlinear', 'OR in Energy']",MD-07
"Remaining useful life [RUL] prediction is a crucial task in prognostics and health management [PHM], gaining increasing importance in industrial applications. However, the accuracy of RUL predictions in real-world industrial can be compromised due to data imbalance and discrepancies in data characteristics caused by varying operational conditions. Inspired by the idea of transfer learning, this study proposes a novel RUL prediction framework that utilizes damage propagation and contrastive transfer learning. At first, the proposed method differentiates between health and degradation states within source domain data on a window basis, employing the augmented Dickey-Fuller [ADF] test and acceleration graphs. Then, it predicts the RUL by training on the history subset of degradation state data to maximize the similarity with the multi-scale feature representation of future subset. In the target domain, a comparable strategy is employed but with adjusted sensitivity towards the acceleration graph for more precise state differentiation, enabling the application of the model learned in the source domain to predict the RUL in the target domain. Finally, the proposed method is proved to be effective through two public run-to-failure datasets and one real-world dataset. Demonstrated through experimental results, the proposed method shows superior predictive performance and high transferability, indicating its potential to address the challenges in industrial environments.",Contrastive Transfer Learning Based on Damage Propagation for Remaining Useful Life Prediction,"[68255, 77233, 77239, 25193]",416,"[123, 47, 8]",1458,Recent Methodologies in Explainable AI [XAI] 1,71,2,04,Recent Advancements in AI ,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,1001 [building - 202],"['Reliability', 'Forecasting', 'Artificial Intelligence']",MA-04
"In this talk, we introduce a dynamic distributed conjugate gradient method designed to solve the strongly monotone variational inequality problem across the fixed-point sets intersection of firmly nonexpansive operators. Our method enables the independent computation of a firmly nonexpansive operator alongside a dynamically updated weight at each iteration. This approach aims to enhance the convergence rate of the algorithm by adjusting control factors for each iterative step. By imposing suitable control conditions on relevant parameters, we establish the strong convergence of the iterates towards the unique solution of the variational inequality problem under consideration. Additionally, we conduct numerical experiments and analyze key observations by applying this model to address image classification challenges using support vector machine learning techniques.",Dynamic distributed gradient method for strongly monotone variational inequality problem over the common fixed-point constraints,[43369],262,"[5, 19, 21]",1460,Recent advances on Variational Inequalities and Equilibrium Problems I,51,13,43,Variational Inequalities and Equilibrium Problems - From Theoretical Advances to Real World Applications,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,99 [building - 306],"['Algorithms', 'Continuous Optimization', 'Convex Optimization']",WB-43
"Renewable energy sources have gained prominence in reducing the dependency on fossil fuels and minimizing their negative environmental impacts. Considering renewables' uncertain and variable nature, it has become necessary to design and manage hybrid energy systems. We study the optimal design and management problem for hybrid energy systems involving a pumped hydro energy storage facility and a wind farm. We first develop two-stage stochastic programming models to find the optimal configuration and sizing decisions. We then utilize a Markov decision process model to find the optimal energy generation and storage decisions. Using real-life data and considering economic benefits, we investigate how hybrid systems can mitigate the negative effects of uncertainty in supply and demand.",Energy Storage System Design and Management for Renewable Supply and Demand,"[76689, 43460, 56904]",453,"[37, 93]",1463,Optimization of energy storage systems,21,7,22,Energy Management,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,81 [building - 116],"['Energy Policy and Planning', 'OR in Energy']",TA-22
"This paper studies a real-life waste collection problem inspired by a waste collection system in China. In this system, manually operated vehicles [MV] collect small, dense forms of waste on streetsides and unload at waste collection huts. Meanwhile, vehicles with compressors [CV] collect larger waste demands from the huts and stores along the streets. The routing problems of MVs and CVs are regarded as an open multi-trip capacitated arc routing problem and a capacitated general routing problem, respectively. The multi-level problem aims to implement integrated optimisation of these two problems, which identify a set of routes for MVs and CVs with minimum travel costs that collect all waste. 
We first build a mixed integer linear programming [MILP] model to solve this problem. Moreover, we propose a set covering model and a logic-based benders decomposition framework that embeds a branch and price [B&P] algorithm. Based on the feature of the two-level problem, this framework decomposes the set covering model into the master problem of MV routes and the sub-problem of CV routes. To solve the sub-problem, the B&P algorithm is then applied. We adopt three classic CARP branching schemes and extend the label-setting algorithm to generate CV routes with negative reduced costs. Preliminary experiments show that, compared with solving the MILP model with commercial optimisation software, the proposed framework can effectively improve the gap within the same time limitation.",An exact algorithm for the multi-level multi-trip capacitated arc routing problem,"[65512, 75810, 65825]",859,"[13, 65, 145]",1465,"Discrete, continuous or stochastic optimization and control in networks, transportation and design III",64,4,25,Combinatorial Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,011 [building - 208],"['Column Generation', 'Logistics', 'Vehicle Routing']",MC-25
"A patient who requires a kidney transplant, and who has a willing but incompatible donor, may be able to 'swap' their donor with that of another patient, who is in a similar situation, in a cyclic fashion.  Also, non-directed donors can trigger 'chains' of transplants involving multiple donor-recipient pairs.  In this talk we introduce the Half-Cycle Formulation [HCF], an integer programming [IP] model for finding optimal sets of exchanges involving cycles and chains among incompatible donor-recipient pairs.  In HCF, a cycle [i.e., set of kidney swaps] is represented by two compatible halves, and we also demonstrate how the model can be extended to incorporate chains.  After giving several algorithmic enhancements for HCF, we show through computational experiments with an IP solver that our model is competitive in relation to other formulations for certain cycle and chain size limits.",Half-Cycle - A new formulation for modelling kidney exchange problems,"[37829, 76714, 55240, 62409, 77244]",951,"[14, 109, 56]",1468,Kidney Exchange II,3,10,10,OR in Health Services [ORAHS],"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,11 [building - 116],"['Combinatorial Optimization', 'Programming, Integer', 'Health Care']",TD-10
"With the increasing number of parcels shipped worldwide, new ways to reduce the total shipping costs and CO2 emissions are actively searched. Past research primarily focused on finding bins that minimize the wasted volume for packing orders. Retailers, however, must decide about the bin sizes they will use for packing before any order will be packed. Having many bin sizes on stock will allow a retailer to more often select a bin with only some small wasted volume. Many bin sizes, on the other hand, increase costs for buying and storing bins as well as for selecting the best bin. To the best of our knowledge, the preliminary decision of what bin sizes a retailer should have on stock to minimize the total shipping cost is a yet unexplored optimization problem. 
We focus on the NP-hard problem of finding bin sizes [i.e., bin set] that minimize the costs for packing and transporting a given set of orders. More specifically, we i] define the problem of finding cost-minimal bin sets, ii] provide an adapted metaheuristic to identify good solutions to the defined problem and iii] discuss possible extensions of the provided solution.",Reducing shipping cost of packages by optimizing the bin set,"[76341, 76200, 77240]",674,"[74, 32]",1470,Cutting and Packing 5 - related topics,81,7,07,Cutting and Packing [ESICUP],"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,1019 [building - 202],"['Metaheuristics', 'E-Commerce']",TA-07
"Electronic coupons are a conventional tool used by brands to gain market share. However, with the rise of e-commerce live streaming, more and more brands are adopting AI live streaming as a new promotional method. In the context of sustainable development, AI live streaming will promote green products beyond geographical and language barriers. Therefore, integrating AI live streaming into product promotion has become a new direction to reveal the trade-offs between all-channel brands in AI live streaming and electronic coupon strategies. We develop a co-opetition model to compare the different promotion methods. In this model, we considered the potential impact of network externalities and consumer social responsibility. We found that although AI live-streaming promotion is widely used, its advantages are only evident when network externalities are significant. It is noteworthy that the intelligence level of AI anchors is closely related to the investment level of brand merchants, and differences in consumer social responsibility sensitivity may limit brand merchants' choices in AI live-streaming promotion strategies. In such cases, brand merchants may be more inclined to adopt electronic coupons, to increase sales revenue. Finally, we further examine the impact of logistics costs and the mixed promotion strategy on the sales revenue of supply chain members to show the robustness of the main findings.",The optimal green product promotion strategy for omnichannel brand merchants,"[77167, 77230]",925,"[138, 139, 32]",1471,Game theory for the circular economy,18,12,23,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,82 [building - 116],"['Supply Chain Management', 'Sustainable Development', 'E-Commerce']",WA-23
"Robustness is a crucial concept in the railway timetabling problem. It is often related to the ability of a timetable to absorb small delays and avoid the propagation of these delays. We propose a MILP model to improve the robustness of an existing microscopic timetable in a complex and heavily used part of the network. The objective is to maximize the buffer times between each pair of trains, leading to a good spreading in time of the trains. The timetable can be improved by adapting the timing and the routing of the trains. Since only a part of a larger network is considered, the allowed deviation from the initial timetable is limited such that the newly proposed timetable does not cause conflicts outside of the considered area. Depending on which properties the resulting timetable should have, certain constraints can easily be added by the user. Due to the flexibility of the proposed model, it can also be used to analyze and optimize alternative scenarios and provide more insight into the robustness of the timetable. The model is applied to a case study of a bottleneck area of the Belgian railway network, located just outside of Brussels. The traffic in this area is very heterogeneous, with InterCity, local, peak-hour and high-speed trains. The spreading objective is improved with 18% compared to the current situation, thus leading to a more robust timetable. Additionally, alternative scenarios, related to extending a line, adding trains and cancelling trains, are studied.",A MILP model to improve the robustness of a railway timetable in a bottleneck area - a case study in Belgium,"[71447, 46228]",818,"[122, 142]",1476,Timetabling 2,85,12,51,Public Transport Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,M5 [building - 101],"['Railway Applications', 'Timetabling']",WA-51
"The surge in customers' preference for online shopping has spurred the growth of on-demand delivery services, exemplified by companies like Getir, Gorillas, and Flink. These companies promise near-instantaneous deliveries, typically within a few minutes. To be able to fulfill this promise, multiple micro-warehouses and a courier fleet using e-bikes are employed. To address the assignment problem, the current practice of logistics companies is to statically define spatial areas as polygons for each micro-warehouse and assign all customers within this polygon to the respective warehouse. However, such a static assignment neglects real-time information that might be used to achieve a better workload balance of orders between warehouses. In this work, we suggest a dynamic and data-driven assignment of orders to warehouses and couriers based on the current workload and previously assigned orders to the warehouses. The problem is formalized as a sequential decision problem, as customers arrive dynamically over time. The goal is to minimize total tardiness. We develop methods to solve the considered problem and apply them to problem instances on a stylized grid as well as to instances derived from real-world data of Chicago. Our methods are benchmarked to current practices from the industry, showing that a dynamic assignment can substantially reduce tardiness.",The On-Demand Delivery Problem - Online Assignment of Orders to Warehouses and Couriers,"[70714, 72305, 45027]",745,"[65, 135]",1477,Dynamic Vehicle Routing 1,5,9,64,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S16 [building - 101],"['Logistics', 'Stochastic Models']",TC-64
"The exponential growth of e-commerce in the past years has led to an increase in packages. Amazon, the biggest player in e-commerce, has packed and shipped approximately 7.7 billion packages in 2021. To handle that many packages, it is of utmost importance for e-commerce companies to optimize the packing process. 
The packing process consists of two decisions typically made by humans - the selection of the best-fitting bin for a given order and the packing of all articles of that order into the bin. There has been little research into how people make these decisions. We conducted two experiments to study the driving forces on the two decisions.
In the first experiment, we varied the amount and heterogeneity of the available bins and the articles to be packed to examine the influence of these factors on the duration and quality of the bin selection process. We found, among other interesting insights, that the decision duration is, contrary to what is expected, not influenced by the heterogeneity of articles.
In the second experiment, we varied the number and heterogeneity of articles to be packed into a predefined bin to examine the influence on the packing duration. Key findings of this experiment are that humans exclusively start by packing the largest item and that packing an additional article increases the packing time by 4.3seconds on average.
Our findings contribute to the understanding and optimizing the packing process.
",Selecting and packing bins – a human perspective,"[76200, 76341, 77240]",570,"[10, 25, 23]",1480,Behavioral Decision Analysis III,13,5,11,Behavioural OR,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,12 [building - 116],"['Behavioural OR', 'Decision Analysis', 'Cutting and Packing']",MD-11
"In an era when the decision-making process is often based on the analysis of
complex and evolving data, it is crucial to have systems which allow us to incorporate human knowledge and provide valuable support to the decision maker. In this work, statistical modelling and mathematical optimization paradigms merge to address the problem of feature selection in additive models while deciding if the variables are linear or nonlinearly related with the outcome. We assume that the smooth functions involved in the additive model are defined through a reduced-rank basis [B−splines] and fitted via a penalized splines approach [P−splines]. A mixed-integer quadratic optimization model is proposed to address the variable selection model and a matheuristic approach, based on the Large Neighborhood Search paradigm, is developed to solve larger instances. The proposed methodology is tested in both simulated and real datasets and it is shown to be competitive against other approaches in the literature.
",Selecting variables in additive models as linear or non-linear terms,"[46717, 67414, 67545, 77249]",648,"[7, 111, 74]",1481,Feature attribution and selection for XAI,15,10,27,Mathematical Optimization for XAI,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,047 [building - 208],"['Analytics and Data Science', 'Programming, Mixed-Integer', 'Metaheuristics']",TD-27
"The car is currently the most popular transport mode, due to, among other things, its convenience and comfort. In many urban areas, the resulting high car usage leads to considerable road congestion. On top of this, it goes paired with several negative externalities like noise pollution and greenhouse gas emissions. Therefore, measures to shift towards more sustainable modes of transport, like public transport, may be beneficial. However, several trade-offs may occur, for example, shifting towards public transport might cause higher travel times. 

In the literature, it has been proven difficult to take into account all trade-offs in one single objective function. While some research has been conducted on the multi-objective optimisation of measures to improve the transit system, hardly any research efforts have incorporated the simultaneous optimisation of interventions to discourage car usage. In this presentation, we study this gap in the current literature. In doing so, we demonstrate the potential of combining the transit and car network design when trying to shift towards a more sustainable mobility system.",The Multi-objective Transit and Car Network Design Problem,"[77169, 68856, 68609, 40888, 77248]",625,"[143, 79, 139]",1482,Advancing mobility towards sustainable solutions IV,6,13,56,Transportation,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S04 [building - 101],"['Transportation', 'Network Design', 'Sustainable Development']",WB-56
"Wildfires are endangering forests, dwellings and the population. This
study was motivated by informative discussions with members of the wildfire directorate in a province of Turkey, Natural Resources Wales, the UK Forestry Commission, and Forestry England. We introduce, model, and solve a wildfire suppression problem that involves the coordination of various types of fire-suppression resources, as we realised the literature
models all types of fire fighting resources [i.e., fire engines, helicopters and razing teams] as generic resources, ignoring their different operational constraints and effects in extinguishing
or controlling the spread of a wildfire. Two integer programming formulations for the problem are presented, the performances of which are evaluated on a set of randomly generated problem instances. Realistic operational constraints about the different firefighting resources are embedded, and the fire spread is simulated depending on the wind conditions. Dwellings, infrastructures and areas of naturalistic beauty are prioritised for protection. The results indicate that the proposed formulations are able to solve instances of realistic size. The importance of the right mix of resources employed in tackling a wildfire as well as their coordination are highlighted. A case study inspired by the Yatagan district of the Mugla province of Turkiye has been developed, and
maps simulating the fire spread over time will be showcased.",The Wildfire Suppression Problem - coordination of the fire fighting resources,"[77178, 32292, 43410, 3340]",243,"[30, 111, 48]",1484,Models and algorithms for real-life combinatorial optimization problems,64,12,52,Combinatorial Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,8003 [building - 202],"['Disaster and Crisis Management', 'Programming, Mixed-Integer', 'Forestry Management']",WA-52
"We consider a virtual power plant [VPP] that expands its capacity by forming a mid-term coalition with distributed energy resources [DERs] such as conventional and renewable power plants, as well as with energy storage systems and flexible demands. The VPP competes with rival VPPs to aggregate the DERs to its own portfolio. This problem is formulated as a three-stage stochastic bi-level model, where the expected profit of the VPP is maximized in the upper-level problem, while the lower-level problems deal with the decisions of each DER regarding the selection of VPP. In the first stage, the VPP manager places bids to secure each DER auction. The second stage involves decisions to determine the DER auctions, forming the VPP coalition and the procurement of power from the day-ahead market. Uncertainties in this stage include bid prices of rival VPPs and the minimum selling price of DERs. Finally, in the third stage, the expanded VPP determines its optimal operation and manages uncertainties related to renewable energy production levels and market prices.  Variability of parameters such as market prices and renewable energy production levels [both solar and wind] is modeled using representative days generated by a clustering K-medoids method. The model incorporates Conditional Value-at-Risk [CVaR] as a risk metric. Through a case study using real data, it is demonstrated that weather conditions and electricity demand significantly influence the coalition formation.",Dynamic Expansion Planning of a Commercial Virtual Power Plant through Coalition with Distributed Energy Resources considering Rival Competitors,"[70807, 40488, 40311]",247,"[38, 36, 93]",1486,Planning problems in electrical energy systems,23,2,21,OR in Energy,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,49 [building - 116],"['Engineering Optimization', 'Electricity Markets', 'OR in Energy']",MA-21
"An increasing number of cities are incentivising the consolidation of deliveries to micro-hubs outside Low-Emission Zones [LEZs], and the use of lower footprint delivery vehicles such as cargo bikes and electric delivery vehicles inside the LEZs. We therefore present a single leader, multiple follower bilevel two-echelon multi-commodity routing problem to facilitate greener last-mile logistics. The leader represents a platform, whose goal is to minimize both emissions and the number of delivery vehicles operating in an LEZ. The leader is also responsible for allocating parcels to micro-hubs, as well as allocating parcels to last-mile Delivery Service Providers [DSPs]. On the other hand, the followers seek to minimize routing costs and maximize client satisfaction. There are two types of followers - first-mile followers who deliver parcels to micro-hubs, and last-mile followers who pick up the parcels from the micro-hubs and deliver to their final destinations. Each first-mile follower solves a Vehicle Routing Problem [VRP] with time windows, while each last-mile follower solves a pickup and delivery, time-dependent VRP with time windows. We also develop a cutting plane algorithm to solve our bilevel problem and evaluate the effect of our approach on several problem instances using real-world data.",A bilevel two-echelon multi-commodity routing problem to facilitate greener last-mile logistics,"[71808, 23165]",204,"[145, 65, 50]",1487,Vehicle routing I,64,4,29,Combinatorial Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,157 [building - 208],"['Vehicle Routing', 'Logistics', 'Game Theory']",MC-29
"Most optimization models of operations research problems are multimodal. Such problems are NP-hard. They include the optimization problems with continuous and Boolean variables. Existing methods and solvers do not allow us to effectively find optimal solutions to such problems. We propose to use the original method of exact quadratic regularization to solve such problems. Quadratic regularization allows us to transform the multimodal problems into the problem of the maximum of the norm of a vector on a convex set. For solving the transformed problem, it is enough to have a local solver. We solved many multimodal problems using this method. In particular, linear and quadratic knapsack problems, packing problems, scheduling problems, reliability of systems, and many others. We have performed significant comparative computational experiments. Our method allowed us to obtain significantly better solutions for various operations research problems.",Global optimization in operations research problems,[21175],452,"[52, 19, 14]",1489,Stochastic and Deterministic Global Optimization,93,4,41,Stochastic and Deterministic Global Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,97 [building - 306],"['Global Optimization', 'Continuous Optimization', 'Combinatorial Optimization']",MC-41
"The complexity of the innovation and entrepreneurship process calls for ever-increasing connectivity. As a result, scholars have begun to focus more on entrepreneurial ecosystems [EE], which emphasise the interconnection of every player in this process. In this vein, external collaborations have been encouraged by the development of digital technology in response to COVID-19. At the same time, the pandemic has posed challenges to the resilience of EE, and the digital dimension may foster resilience from this perspective. 
In this paper, we use complexity theory and Social Network Analysis to investigate how resilient a digital EE of European startups is to external perturbations both before and after the pandemic. By doing this, we help to clarify the relationship between digital platforms and EE as well as their role in enhancing resilience. 
Our results show a higher level of robustness to random node removal. Nevertheless, there have been shifts in the paths connecting members as startups diversified their interests across sectors and countries. The new configuration facilitates the shock propagation within the network, leaving room for designing targeted policies and managerial practices to increase the ecosystem resilience. The network's ability to propagate shocks is facilitated by the new arrangement, leaving room for the development of focused policies and managerial techniques that strengthen the resilience of the ecosystem.",Are digital entrepreneurial ecosystems resilient to external perturbations? A pre- and post-pandemic empirical analysis,[71467],694,"[132, 126]",1490,Complexity and Financial Patterns,4,15,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S14 [building - 101],"['Social Networks', 'Risk Analysis and Management']",WD-63
"Busy railway stations with multiple intersecting lines often form critical points in railway timetabling, where delays from one line can propagate through the network. Increases in railway traffic as well as plans for integrated clock-face timetables thus require careful planning of platform assignments and routes through the station to limit the effect of delays and disruptions on the overall service quality.

We present a new method for the long-term train routing problem in the station context. The goal is to find a microscopically feasible, robust train routing that is compatible with a predetermined network timetable. The method consists of a combined MILP model - A multicommodity flow formulation ensures the feasibility of the train routing by prohibiting conflicts between chosen routes. To ensure robustness to small initial delays, we directly include a scenario-based delay propagation model and minimize the delay propagation in the worst case, thus extending previous work on recoverable robustness. We discuss modelling choices and computational performance, present results on a case study and give an outlook on possible applications for long-term infrastructure planning.
",Enhancing recoverable robust train routing in railway stations,"[76645, 55810]",182,"[122, 127, 111]",1492,Europe's Rail MOTIONAL - Algorithms for railway planning,85,5,54,Public Transport Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S01 [building - 101],"['Railway Applications', 'Robust Optimization', 'Programming, Mixed-Integer']",MD-54
"
In signal analysis, noise often obscures crucial patterns in processes of interest, particularly in economic contexts where factors like liquidity trades and agent errors contribute to data distortion. Addressing this challenge involves managing noise through mathematical approaches. One common scenario arises when denoising signals characterized by step functions with numerous discontinuities, prevalent in economic and financial time series. The violation of stationarity assumptions complicates noise removal, often mischaracterizing noisy points as signal discontinuities, leading to erroneous forecasting and inference.

This paper proposes a flexible denoising approach leveraging a sparse representation framework, particularly focusing on approximating step functions using rectangular functions. The method adopts the Weak-Orthogonal Greedy Algorithm [WOGA] or matching pursuit [MP] to iteratively remove noise from step function-derived signals. By selecting elementary functions from a highly redundant set, the algorithm constructs an approximate orthogonal basis expansion of the signal, effectively capturing its discontinuities.

The paper establishes theoretical foundations for the proposed method, demonstrating its efficacy in capturing discontinuities while simplifying calculations. Specifically, theorems elucidate optimal conditions for waveform dictionary selection, facilitating efficient denoising iterations. ",Waveform Dictionary Matching Pursuit for Denoising Step Function Signals in Finance,[56892],441,"[84, 0]",1493,Models for Financial Data and Risk Management,4,7,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S14 [building - 101],['Optimization Modeling'],TA-63
"We use Grover’s algorithm to build quantum algorithms that can be used to solve discrete optimization problems on a universal quantum computer. We use our quantum algorithms to solve the binary knapsack problem, and highlight a number of challenges that are faced when effectively using Grover’s algorithm to solve discrete optimization problems. We also present a number of procedures that can match the performance of the best classical procedures for solving the binary knapsack problem and demonstrate that quantum computing may cause a revolution in the field of discrete optimization.",Discrete Optimization - A Quantum Revolution?,"[19453, 77257]",382,"[14, 18]",1494,Quantum Computing for Discrete and Combinatorial Optimization,83,4,42,Quantum Computing Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,98 [building - 306],"['Combinatorial Optimization', 'Computer Science/Applications']",MC-42
"The impact of Environmental Social and Governance [ESG] factors on the performance of financial stocks are still controversial in the literature. We aim to identify groups of companies according to their ESG temporal dynamics to evaluate whether their fluctuations impact the market risk
factors. We assess a potential relationship between financial and non-financial risks within the Fama and French framework [Fama and French, 2015] according to the ESG score and its components.
We discriminate companies that increase their ESG compliance using a hierarchical time-series clustering approach and computing the similarity between two-time series by Euclidean distance and Dynamic Time Warping [DTW]. The latter is useful when the time series have differen shifts and speeds. We compare four hierarchical methods with different linkage criteria [average, complete, Ward.D, Ward.D2]. The optimal number of clusters is selected based on 4 cluster validity indices
[Silhouette, Calinski-Harabasz, Clustering Order Preservation, and Dunn]. The data utilized in this paper refer to the S&P500 and come from the Renitiv database. The CVIs indicate two clusters as the best choice. However, while the Euclidian distance with the average method is the best combination for the ESG score and its components E and S, the DTW distance with the ward.D method is the best for the G score. Overall, we find different effects on the market risk, which depend on the ESG component ",Corporate Sustainability Committment and marker risk ,"[27575, 64952, 56890]",441,"[45, 40, 100]",1497,Models for Financial Data and Risk Management,4,7,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S14 [building - 101],"['Financial Modelling', 'Environmental Management', 'OR in Sustainability']",TA-63
Vector problems in a Reimannian geometric context are studied from the point of view of their Pareto optimality. Necessary conditions of efficiency are introduced. Adequate convexity structure is used to state sufficient efficiency conditions and also to prove duality theorems. ,On vector optimization problems,[77258],527,"[21, 51, 81]",1498,Nonsmooth optimization algorithms I,70,14,41,Nonsmooth Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,97 [building - 306],"['Convex Optimization', 'Generalized Convex Optimization', 'Non-smooth Optimization']",WC-41
"Hexaly Optimizer, formerly known as LocalSolver, is a model and run solver that integrates heuristics and exact methods. A set-based modeling formalism was introduced to simplify the modeling of certain combinatorial problems like routing or packing problems. For instance, in a routing problem, list variables can be used to model the sequence of visits made by each truck. These decision variables are well suited for a heuristic search but are much more difficult to integrate in a mathematical programming approach to compute lower bounds. A direct reformulation in a MILP model introduces a quadratic number of binary decisions with several big M constraints leading to poor scalability and bounds. Hexaly 13.0 automatically detects such structures in a user model and reformulates them in an extended MILP model to compute lower bounds. This model is solved efficiently using state of the art branch-cut-and-price technics from the literature. This talk will present the general approach and the algorithms used for the resolution and some benchmarks on the CVRP library.",Automatic model decomposition in Hexaly Optimizer,[18585],704,"[72, 134, 13]",1499,Optimization Tools,76,12,30,Software for Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,53 [building - 208],"['Mathematical Programming', 'Software', 'Column Generation']",WA-30
"Yard crane scheduling problems [YCSPs] arrange requests for loading containers by yard cranes on terminals. Of the scheduling nature and operational complexity is a real-world YCSP with the assignments of input/output [I/O] points that serve as dynamic buffers to receive and send containers. The YCSP schedules storage and retrieval containers coordinated with their time parameters at the I/O points, which are also limited and need to be assigned during the scheduling process. Instead of proposing hybrid or complicated approaches, we introduce a series of simple Iterated Greedy [IG] methods upon a classical vanilla IG. New destruction, reconstruction and local search operators are designed for task schedules and I/O assignments. It results in 3 IG algorithms which require little variation from the vanilla IG. Extensive computational campaigns have shown a statistically wide margin of the proposed IG methods over several state of the art approaches.

",Iterated Greedy for the Yard Crane Scheduling Problem,"[77255, 781, 5590, 32376]",679,"[129, 74, 5]",1500,Container Stacking and Yard Planning II,52,9,62,OR in Port Operations,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S12 [building - 101],"['Scheduling', 'Metaheuristics', 'Algorithms']",TC-62
"Delivery to locker boxes has become very popular in e-commerce as it is beneficial for companies and also convenient for customers who can flexibly pick-up their parcels. In our study we assume that parcels can be picked-up within three days after delivery. Afterwards, the parcel is removed by the company. However, the uncertainty on the pick-up time negatively affects order acceptance and allocation strategies of the company, yielding to potential inefficiencies of the overall system. To deal with this issue, we study the dynamic order allocation to locker boxes under uncertainty on parcel pick-up times, formulated as a multi-stage stochastic optimization problem, dealing with three types of decisions - [1] whether to accept or not an order, [2] when to delivery it, and [3] to which locker to assign it. To solve this problem, we provide an integer programming classification algorithm which, tuned on simulated optimal decisions made by an oracle model, derives decision rules to be dynamically applied. Results are compared against standard classification algorithms and parametric functions. Furthermore, valuable managerial insights regarding assignment decisions of logistics providers are presented. ",Dynamic Order Allocation to Locker Boxes Under Uncertain Pick-Up Times,"[77067, 74181]",788,"[65, 136]",1508,Logistics 2,5,14,58,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S07 [building - 101],"['Logistics', 'Stochastic Optimization']",WC-58
"Social media platforms have offered politicians and political parties new opportunities to communicate and interact with citizens. Especially in Belgium, political parties and politicians are spending a lot of money to social network sites to attract voters. Backed by Belgian party financing, Belgian parties have been among the biggest spenders in Europe, even outside of campaign periods. 
The main goal of this research is to gain a deeper understanding on the online engagement with political posts within the Belgian multi-party system. We focus on what politicians are talking about and how people engage with this on X, formerly known as Twitter, since it remains one of the most political social network sites to date. To achieve this, we propose a two-step approach to predict the level of engagement based on a unique set of features. This approach first classifies tweets into several like-minded categories, and then applies regression on each category. Supported by the Academic Twitter API we analyzed the posts of 72 Belgian politicians as well as posts from all the major political party Twitter profiles for a period of 2 years. 
The main contribution of this study is that it is the first to combine the added value of multiple distinct characteristics in the context of online engagement and disclose differences between posts originating from personal and party profiles within a multi-party political system.",Predictive Politics - Understanding Drivers of Engagement in Belgian Politics,"[73576, 56999, 77263]",415,"[7, 26, 66]",1511,Learning Analytics and other Text Analytics tasks,17,13,31,Analytics,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,54 [building - 208],"['Analytics and Data Science', 'Decision Support Systems', 'Machine Learning']",WB-31
"Option pricing is an active area of research in financial economics. The risk-neutral density is a critical element in pricing derivative assets, and it can be estimated using nonparametric kernel methods. Recent research considered large-scale optimization problems, which significantly improved the robustness of estimating risk-neutral densities through observed option prices. In nonparametric estimation methods, kernel bandwidth estimates are crucial elements. However, they also represent the most computational challenge in implementing such methods, namely when large-scale optimization problems are used in curve fitting. Some of the computational challenges can be solved by considering parallel computing algorithms performed using Graphical Processing Units. We propose a tailor-made Cross-Validation criterion function to define optimal bandwidths [two parameters] associated with the risk-neutral estimation problem, defined through an optimization problem with non-convex objective functions. By implementing a grid-search approach within a big data scenario and through a large-scale optimization problem, computational times can be prohibitive for applying these methods in real-time decision-making. Parallel computing methods within a grid-search optimization algorithm substantially reduce computational times, allowing for more effective decision-making processes related to risk-neutral density estimation and option pricing.",Parallel computing in optimization methods used in estimating risk-neutral densities through option prices,"[60231, 19797]",916,"[18, 54, 83]",1512,"Nonsmooth optimization and applications, Part II",84,8,32,Advances in large scale nonlinear optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,41 [building - 303A],"['Computer Science/Applications', 'Grid Computing', 'Optimization in Financial Mathematics']",TB-32
"A variant of the resource-constrained project scheduling problem [RCPSP] is the multi-skill RCPSP [MS-RCPSP] where each resource has one or several skills and each activity may require several resources with certain skills. The standard goal of the MS-RCPSP is to determine a start time and the allocation of resources for each activity to complete the project as soon as possible, i.e., to minimize the makespan. In this work, we consider a more realistic scenario where all the resources do not master all their skills with the same efficiency [this is often the case, for example, when human resources are involved]. To model this situation, we consider that a more efficient resource can contribute with more than one resource-unit to the execution of an activity. In addition, we consider the cost for resource usage and address this new version of the problem from a multi-objective perspective, minimizing both the makespan and the total cost. We present an optimization model for this new problem and develop exact methods to find the set of Pareto solutions. ",Bi-objective Multi-skill Resource-Constrained Project Scheduling with Resource Efficiency,"[59505, 6053, 9684]",960,"[118, 77, 14]",1513,RCPSP and extensions,35,5,60,Project Management and Scheduling,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S09 [building - 101],"['Project Management and Scheduling', 'Multi-Objective Decision Making', 'Combinatorial Optimization']",MD-60
"
The LPT heuristic for minimizing total load on a proportionate openshop


Enrique Gerstl and Gur Mosheiov

School of Business Administration,
The Hebrew University, Jerusalem 91905, Israel


Abstract

	We study the problem of minimizing total load on a proportionate openshop. The problem is proved to be NP-hard. A simple LPT [Longest Processing Time first]-based heuristic is proposed, and a bound on the worst-case relative error is introduced. The proposed bound is significantly smaller than the classical bound on the relative error of LPT when minimizing makespan on parallel identical machines. The algorithm is tested numerically and is shown to produce extremely close-to-optimal schedules.   

",The LPT heuristic for minimizing total load on a proportionate openshop,"[29448, 76086]",873,"[129, 14]",1514,Optimization problems in scheduling,64,12,26,Combinatorial Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,012 [building - 208],"['Scheduling', 'Combinatorial Optimization']",WA-26
"Most algorithms solving the transit network design problem [TNDP] or Line Planning Problem [LPP] focus only on one objective function and the provided input parameters, e.g. the distance matrix. By changing these parameters or the objective, algorithms can escape from local optima. In this work, a genetic algorithm is created that includes several alternative fitness functions across a population, focusing on different aspects of a solution. This can range from putting a focus on low-demand areas to putting a focus on minimising the transfers. Our implementation introduces the concept of sub-populations, which work in parallel and aim for optimal solutions for slightly different fitness functions. These sub-populations share their found improvements, allowing a synergy between them. When this idea is implemented in an algorithmic framework for network design, the results improve the current literature for all Mumford networks, a commonly used benchmark instance. For these benchmark instances, the objective function value, considering the average travel time of passengers, is reduced by [1, 0.5, 3, 2.5]%. Not only does this algorithm improve current benchmark results, but the output contains a set of high-quality, yet substantially different networks, which is interesting for transport operators and for the follow-up research we have in mind. Finally, this algorithm is successfully used in a case study in cooperation with the main public bus operator of Flanders, Belgium.",A Genetic Algorithm with Several Fitness Functions for Line Planning,"[76720, 46228]",283,"[74, 102]",1515,Network Design and Line Planning for Public Transportation 1,85,9,51,Public Transport Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,M5 [building - 101],"['Metaheuristics', 'Parallel Algorithms and Implementation']",TC-51
"IFRS 8 Operating Segments requires the grouping of operating segments into aggregated segments. A differentiated, aggregated presentation of assets and liabilities, income and expenses has to be reported separately for these reportable segments. A reportable segment should consist of one or more operating segments that are similar. Not all operating segments need to be aggregated into one reportable segment. The problem can therefore be characterised as a binary assignment similar to the Knapsack problem.

IFRS 8 defines sets of different requirements for the definition of reportable segments. The overall objective is up to the reporting company itself. To this end, we use a technical objective function that aims to minimise the dissimilarity inside reportable segments. By choosing different values for a weighting parameter in the formal objective function, we can generate solutions with a different number of reportable segments.

We formulate and solve the problem using AMPL and standard solvers. This also enables a discussion of IFRS 8 to improve future standard setting.",Defining reportable segments – a complex assignment problem in international accounting,[13364],255,"[1, 45, 14]",1516,"OR in Accounting - Planning, Taxation, and Reporting",7,13,59,OR in Financial and Management Accounting,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S08 [building - 101],"['Accounting', 'Financial Modelling', 'Combinatorial Optimization']",WB-59
"My research operates at the cutting edge of integrating operations research [OR] with artificial intelligence [AI], tackling real-world combinatorial optimization problems across both deterministic and stochastic settings. With a special focus on vehicle routing problems, I leverage state-of-the-art AI methodologies to improve decision-making frameworks. My ambition is to make a meaningful contribution toward enhancing the sustainability and resilience of businesses in a world that is ever VUCA, rapidly evolving, and increasingly interconnected.

Currently, I am engaged in several research that seek to harness the synergy between AI and OR. These projects are dedicated to the design and development of intelligent decision support systems that streamline operations and adeptly manage disruptions within cyber-physical systems. 

Looking ahead, my research trajectory is set to explore broader and more complex systems. I am particularly enthused about investigating how OR can intersect with ground-breaking technologies, such as Generative AI. This exploration is driven by the vision of advancing intelligent systems that are not only capable of self-optimization and innovation but are also sustainable and resilient against the backdrop of global challenges. 

",Artificial Intelligence to Improve Decision-Making Frameworks,[74086],458,"[14, 8, 26]",1517,YW4OR_3,39,14,12,WISDOM - Women in OR,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,13 [building - 116],"['Combinatorial Optimization', 'Artificial Intelligence', 'Decision Support Systems']",WC-12
"In many queueing systems customer satisfaction is determined more by the meeting of a deadline [missing a flight, service level agreements, etc.] than waiting time. 
We study the effect of such deadlines on strategic behaviour in priority queues.
We model a queueing game where customers have a choice to pay a toll for priority access, allowing them to skip part of the queue.
In this model, the meeting of the deadline is the most important factor in the utility functions of the customers.
When looking at symmetric Nash equilibria in this model, we find Evolutionary Stable Strategy equilibria which do not exist in the model without deadlines.
In addition, we model a heterogeneous situation where different groups of costumers have different deadlines and study how this influences social welfare and revenue.",Strategic Behaviour in Priority Queues with Deadlines.,"[71637, 70904]",645,"[50, 121]",1518,"Game Theory, Solutions and Structures VII",88,9,36,"Game Theory, Solutions and Structures","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,32 [building - 306],"['Game Theory', 'Queuing Systems']",TC-36
"We consider the problem of a retailer selling products that can be offered individually or in a bundle to a market that comprises heterogeneous customers who differ in terms of their valuations for the products and their patience levels. First, we develop a mathematical model that captures the retailer’s expected revenue maximization problem over an infinite horizon in a multi-segment market setting. Then, we present results from numerical experiments that assess the benefits of adopting an intertemporal bundling policy and investigate its behavior with respect to the market structure. Finally, we study, via behavioral experiments, whether human decision-makers are able to fully capture the benefits of offering the bundle at random intervals. Our computational results indicate that intertemporally randomized bundle offer attenuates the negative revenue impact of positively correlated valuations in the presence of patient consumers, thereby elevating the bottom line. Besides, the behavioral experiments show that decision-makers' responses significantly deviate from those of normative counterparts as the heterogeneity in the market structure and the complexity of the pricing mechanism increase.",Intertemporal Bundling with Patient Customers - A Comparison of Behavioral Response and Normative Equivalence,"[77206, 77275]",528,"[124, 10]",1522,E-Commerce,30,14,50,Retail Operations,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,M2 [building - 101],"['Revenue Management and Pricing', 'Behavioural OR']",WC-50
"One of the major challenges for humanitarian organizations when planning relief efforts is dealing with the inherent ambiguity and uncertainty in disaster situations. The available information that comes from different sources in post-disaster settings may involve inconsistencies, which can severely hamper effective humanitarian decision making. In this talk, we present a new methodological framework based on graph clustering and stochastic optimization to support humanitarian decision makers in analyzing the implications of divergent estimates from multiple data sources on final decisions and efficiently integrating these estimates into decision making. We illustrate the proposed approach on a case study that focuses on locating shelters to serve internally displaced people in a conflict setting, specifically, the Syrian civil war. We use the needs assessment information published by different reliable sources to estimate the shelter needs in Idleb, Syria. The analysis of data has revealed a high degree of ambiguity due to inconsistent estimates. We apply the proposed methodology to integrate the ambiguous and divergent estimates into the decision making for determining shelter locations. The results highlight that our methodology leads to higher satisfaction of demand for shelters than other approaches. Moreover, we show that our solution integrates information coming from both sources more efficiently thereby hedging against the ambiguity more effectively.",A Machine Learning Approach to deal with Ambiguity in the Humanitarian Decision Making,"[77270, 70899, 35594, 25620]",551,"[66, 58, 117]",1524,Demand Forecasting in Humanitarian Operations,38,8,21,OR in Humanitarian Operations [HOpe],"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,49 [building - 116],"['Machine Learning', 'Humanitarian Applications', 'Programming, Stochastic']",TB-21
"In times of crisis, healthcare systems and aid agencies face unprecedented challenges, necessitating robust resilience strategies to ensure uninterrupted service delivery. This talk emphasizes the importance of preparedness, adaptability, and the strategic use of Operations Research [OR] techniques within these critical sectors. For healthcare, building infrastructure resilience involves enhancing cyber security, securing supply chains, and integrating advanced decision support tools to maintain care continuity. OR techniques play a crucial role in optimizing resource allocation, forecasting demand, and planning emergency responses. Similarly, disaster management and aid agencies must focus on agile response strategies, leveraging OR for data analytics and informed decision-making, and fostering partnerships across sectors to mobilize resources swiftly. Central to these efforts is a holistic approach that combines physical infrastructure enhancements with systemic resilience through OR, ensuring services can withstand, adapt to, and recover from disruptions. By prioritizing resilience and applying OR methodologies, healthcare and aid organizations can safeguard vulnerable populations during crises, contributing to societal stability and well-being.",Building Resilience of Critical Infrastructures in Times of Crisis,[77270],457,"[136, 12, 26]",1526,YW4OR_1,39,12,12,WISDOM - Women in OR,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,13 [building - 116],"['Stochastic Optimization', 'Capacity Planning', 'Decision Support Systems']",WA-12
"We consider sensitivity analysis of the cost coefficients in multi-objective integer linear programming problems and define the sensitivity region as the set of simultaneous changes to the objective function coefficients for which the efficient set and its structure remain the same. In particular, we require that the component-wise relation between efficient solutions is preserved and that inefficient solutions remain inefficient. 
We present necessary and sufficient conditions on permissible changes that satisfy the defined properties. In this talk we concentrate on changes to a single objective function coefficient and show that the sensitivity region is a convex set, i.e., an interval. 
In order to avoid going through all pairs of feasible and efficient solutions, we introduce further properties that identify inefficient solutions that remain inefficient whatever the change is. Based on this, a subset of the inefficient solutions can be excluded from consideration. More specifically, we prove that it suffices to inspect the inefficient solutions of a q-objective problem that are efficient in one of two related [q + 1]-objective problems, in the case of only one change.
Computational experiments with multi-objective binary and integer knapsack problems demonstrate the general applicability of our technique.",Sensitivity analysis of the cost coefficients in multi-objective integer linear optimization,"[45220, 9253, 36601, 67745]",200,"[112, 109]",1529,Multi-objective Combinatorial Optimization,64,4,52,Combinatorial Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,8003 [building - 202],"['Programming, Multi-Objective', 'Programming, Integer']",MC-52
"Integrating urban parcel transport into a public passenger transport system is a promising concept to reduce traffic congestion and to use resources more efficiently. However, the last decade saw several promising pilot projects failing due to the lack of long-term support and profitability. Existing reviews show that financial viability is an important aspect for the long-term success of such projects. Integrated transport relies on a range of stakeholders, which differ in their incentives and want to profit from the system. For example, partially financing the public transit system with the revenue gained from parcel transportation may contribute to the success of a shared transport system from the point of view of public transit providers.
In this talk, we first describe the views of relevant stakeholders based on a review of the literature and case studies. Further, we propose performance indicators to evaluate a shared passenger and freight system from the perspective of the individual stakeholders. Secondly, we conceptualize an agent-based simulation model to evaluate business cases for a shared passenger and freight network. ",A Stakeholder-Based View of Business Cases for Integrated Urban Transport,"[67316, 19297]",600,"[65, 131, 119]",1530,Simulation in transportation and logistics,77,9,43,"Agent-based Models in Management, Economic and Organisation Sciences","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,99 [building - 306],"['Logistics', 'Simulation', 'Public Local Transportation Systems']",TC-43
"A food supply chain qualifies as short and local when it is characterized by a short physical distance between producers and consumers, with no more than a single intermediary between them. Such a configuration is referred to as the short local food supply chain [SLFSC]. Our focus lies in examining the logistical problems associated with the SLFSC.
Producers often set up organizations to simplify their logistical operations. For instance, they create producers's stores, design nodal points for third-party logistics operators to collect goods, or engage in reciprocal assistance. Our study focuses on the latter type of cooperation.
We consider that a producer is authorized to transport goods from one another to customers. This practice is now possible in France for short distances, specific products and limited quantities. In the case where a customer C orders products from producers A and B, two collaborative cases may arise - producer A delivers to customer C after retrieving goods coming from producer B, or producer A drops off his own goods to producer B, and the latter delivers customer C.
We model this new delivery sharing problem and present our first results. 
",Sharing deliveries in a short food logistic context,"[77276, 77268, 9743, 22018]",485,"[145, 143, 111]",1531,Sustainable Food and Health Care Logistics,19,10,24,Sustainable Supply Chains,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,83 [building - 116],"['Vehicle Routing', 'Transportation', 'Programming, Mixed-Integer']",TD-24
"The maritime industry must prepare for the energy transition from fossil fuels to sustainable alternatives, which makes the design of future-proof ships even more important. In the design phase of a ship, it is currently uncertain which fuels it will use in the future due to many external factors. In fact, a ship typically sails for decades, increasing the likelihood that it will have to use different fuels over its lifetime. When changing fuels, pipe route design, which ensures a connection between the fuel tanks and the engine rooms, is expensive, time-consuming, and mainly done by hand. Motivated by this, together with maritime experts, we propose a mathematical approach for modeling uncertainty in automatic pipe routing with deterministic, stochastic, and robust optimization. All three approaches are based on state-of-the-art integer linear optimization models for the Stochastic Steiner Forest Problem and adjusted to the maritime domain using specific constraints for pipe routing given by the maritime experts. We compare the approaches using both artificial and realistic data of a commercial ship design company and show that considering uncertainty using stochastic optimization and robust optimization leads to cost reductions. Additionally, we extend our approaches with decomposition methods to solve large-scale [industry] instances.",Sailing through uncertainty - mathematical optimization for ship pipe routing in the energy transition,"[71717, 65559, 56845, 65942, 39439]",830,"[136, 127, 72]",1533,Optimization under Uncertainty in Manufacturing and Supply Chain Management,49,15,35,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,44 [building - 303A],"['Stochastic Optimization', 'Robust Optimization', 'Mathematical Programming']",WD-35
"We consider the Tramp Ship Routing and Scheduling Problem [TSRSP] in which we
plan routes for a homogeneous fleet of tramp shipping vessels operating on a combined
contract and spot market. Earlier research has been fragmented due to variations in
the side constraints studied, hence we present the first unified model that can handle speed optimization, chartering costs, bunker planning, and hull cleaning. The model is solved by column generation, where the columns represent the possible routing of a vessel while the master problem keeps track of the binding constraints. The pricing problem is solved efficiently using a time-space graph, making it possible to solve relatively large instances of the TSRSP in a very short time. Detailed computational results are reported giving insight into both the algorithmic performance but also answering operational and tactical questions for the tramp shipping company and the tramp shipping sector.",A rich model for the tramp ship routing and scheduling problem — solved through column generation,"[77174, 64035, 71189]",197,"[13, 70, 145]",1534,Combinatorial Optimization models and applications in Logistics and Transportation I,64,2,29,Combinatorial Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,157 [building - 208],"['Column Generation', 'Maritime applications', 'Vehicle Routing']",MA-29
"Time can play a decisive role in practical applications of Multi-Criteria Decision Analysis [MCDA] aiming to support environmental and other public policy decisions. When we decide about options that affect environmental [or societal] systems, we often address long time ranges. An overview of applications of MCDA published in non-OR journals [Operational Research] indicates that time is heavily neglected. What about the OR literature? A systematic search of OR journals indicates that the temporal aspect of decision-making in MCDA is also neglected in our field. In this talk, I will present some data to support this claim. Additionally, I shortly present insights from own research projects, where we have started addressing temporal aspects of decision analysis. This includes a prominent question in economics that has not been thoroughly treated in our literature - do preferences change over time; and does it matter? Furthermore, using an example from wastewater infrastructure planning, I demonstrate how we can implement MCDA in complex real-world environmental projects and include time aspects. We used a very pragmatic approach to evaluate decision options over time with MCDA, which nevertheless seemed useful to support decision-making. Various mathematical approaches to deal with temporal decision-making have been proposed, but what about behavioral aspects? I conclude by proposing research questions that could be tackled by the BOR community.",Time and behavioral implications are neglected in applied environmental decisions,"[31727, 70973, 61199, 67390]",112,"[25, 10, 94]",1535,BOR in public policy and environmental decisions,13,10,11,Behavioural OR,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,12 [building - 116],"['Decision Analysis', 'Behavioural OR', 'OR in Environment and Climate change']",TD-11
"1 Introduction

Artelys Knitro is a mathematical programming solver for nonlinear and mixed-integer nonlinear problems. As input, it accepts linear structures, quadratic structures and black-box functions, with if possible, their first and second-order derivatives. Knitro relies on derivative-based algorithms to find locally optimal solutions. Knitro finds the global optimum for convex problems. For non-convex problems, Knitro converges to a first order stationary point [e.g. local optimum] for continuous models and is a heuristic for mixed-integer problems.

2 Recent improvements for MINLP applications

In this talk, we will present the algorithms implemented in Artelys Knitro for mixed-integer nonlinear problems in Knitro 14.0, and detail the recent developments for the nonlinear branch-and-bound algorithms. Since Artelys Knitro 13.0, the nonlinear branch-and-bound has been fully rewritten as parallel and deterministic. The algorithm has been greatly improved by adapting the ideas developed for mixed-integer linear programming. Those features include specific presolve operations and cuts for nonlinear applications, a heuristic portfolio to provide effort effective search improved better branching strategies and a restart procedure. It opens several perspectives for future developments and extensions for nonlinear models that we will present during this talk. We will show the improvements on the classical datasets for mixed-integer nonlinear problems.
",Latest developments for mixed-integer nonlinear programming in Artelys Knitro,"[77279, 77286, 77008]",239,"[113, 14, 5]",1536,Continuous Solvers,76,4,30,Software for Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,53 [building - 208],"['Programming, Nonlinear', 'Combinatorial Optimization', 'Algorithms']",MC-30
"Let $G$ be a bipartite graph with vertex color classes $V_1$ and $V_2$. A
biindependent pair in $G$ is a pair $[A,B]$, where $A \subseteq V_1$, $B
\subseteq V_2$ and the union $A\cup B$ is an independent set. We consider the following three parameters $\alpha[G]$, $g[G]$ and $h[G]$, that are defined, respectively, as the maximum sum $|A|+|B|$, the maximum product $|A|\cdot |B|$ and the maximum ratio $[|A|\cdot|B|]/[|A|+|B|]$, taken over all biindependent pairs $[A,B]$ in $G$. 

Similar parameters can be defined for bicliques in general graphs, which can however be reduced to the above parameters in bipartite graphs. While $\alpha[G]$ is easy to compute in bipartite graphs, Peeters [2003] showed that computing $g[G]$ is an NP-hard problem. The parameter $h[G]$ was introduced by Vallentin [2021]. The parameters $g[G]$ and $h[G]$ have many applications, in particular, to bounding product-free subsets in finite groups, and the nonnegative rank of nonnegative matrices. 

We investigate SDP upper bounds on $g[G]$ and $h[G]$, which can be seen as variations of the Lov\'asz $\vartheta$-number. We show links among them as well as with an earlier parameter by Haemers [2001], and we formulate closed-form eigenvalue bounds. We also show that computing $h[G]$ is NP-hard, by showing that it is NP-hard to determine if a bipartite graph has a biindependent pair [A,B] with |A|+|B|=\alpha[G] and |A|=|B|. We finally present a conjecture about the value of $h[G]$ on the hypercube graph.",Semidefinite approximations for bicliques and biindependent pairs,"[75004, 36254, 77289]",170,"[115, 19, 72]",1540,Advances in Complex and Real Semidefinite Programming,68,8,38,"Conic Optimization - Theory, Algorithms, and Applications","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,34 [building - 306],"['Programming, Semidefinite', 'Continuous Optimization', 'Mathematical Programming']",TB-38
"Ports are striving to improve operational efficiency in the face of ever-increasing trade volumes. In this context, the operation of port terminal storage yards is key, as complexity and poor coordination lead to containers being stacked without regard to retrieval schedules, resulting in time- and energy-consuming reshuffling operations. This problem, known as the block relocation [and retrieval] problem [BRP], has recently received considerable attention. While there are promising solutions to the BRP, the literature considers the problem in isolation, optimizing a single operational parameter for one of the many port stakeholders. This often leads to efficiency losses, since port processes involve different stakeholders and port parts. In this work, we explicitly focus on the scheduling of trucks for hinterland distribution - schedules are often shifted to minimize reshuffling, which causes losses to transport forwarders and reduces the competitiveness of the port. We discuss the trade-off between minimizing the number of container reshuffles and minimizing the deviation from the scheduled time windows for container retrieval by introducing a multi-objective optimization problem. Given the complexity of the problem, we present a greedy heuristic. Our results indicate that the number of schedule deviations can be reduced without significantly affecting the number of relocations.",Container relocation and retrieval tradeoffs minimizing schedule deviations and relocations,"[77277, 77292, 77293, 62486]",679,"[70, 109, 77]",1544,Container Stacking and Yard Planning II,52,9,62,OR in Port Operations,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S12 [building - 101],"['Maritime applications', 'Programming, Integer', 'Multi-Objective Decision Making']",TC-62
"Spurred by the growth of online shopping, parcel delivery has grown rapidly, driving a need for cost-effective last-mile delivery. Amidst this growth, sustainability concerns are gaining prominence. Crowdshipping leverages crowdsourcing for the personalized delivery of freight, turning ordinary citizens into couriers for the distribution of small items. In this collaborative delivery system, individuals already traveling from an origin to a destination take charge of all or part of the delivery, taking a package along with them and making a stop along the way to drop it off, potentially reducing freight trucks and enhancing sustainability. To fully leverage the potential of crowdshipping, real-time matching of crowdshippers to parcels is crucial, considering spatial and temporal uncertainties in crowdshippers’ and parcels’ availability. Due to its size, the dynamic assignment problem considered in our article cannot be solved with conventional dynamic programming methods. Using approximate dynamic programming with value function approximation, our algorithm learns value functions offline through a training horizon, enabling efficient decision-making. This adaptive learning algorithm provides nonmyopic behavior yet requires only solving sequences of assignment problems no larger than would be required with a myopic algorithm. Initial results demonstrate its superiority over myopic solutions in reducing delivery distances.",An Approximate Dynamic Programming Approach for a Crowdshipping Platform,"[76881, 24964]",588,"[65, 108]",1545,Crowdsourcing Logistics,6,8,56,Transportation,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S04 [building - 101],"['Logistics', 'Programming, Dynamic']",TB-56
"In today’s competitive environment with short product life cycles and high customer expectations, working in projects is a predominant form of work organization. As part of the planning process, individual activities of a project are scheduled according to an objective function [minimization of the project duration] and the available renewable resources [e.g., employees] are used efficiently to carry out the activities. The resource-constrained project scheduling problem [RCPSP], also known as the standard model for project scheduling problems, assumes a constant activity duration and a constant resource requirement. Uncertainties that lie in the future are ignored. This entails the risk that considerable disruptions may occur in the schedule and the generated solutions are too restrictive for many practical applications. In order to account for uncertainties, stochastic aspects and flexible resource profiles are integrated into our problem.

Based on a chance-constrained model formulation, a sample average approximation model for flexible and stochastic project scheduling problems is presented. Small and medium instances are solved in a performance analysis. Furthermore, a suitable serial schedule generation scheme is developed, which is also applicable for larger instances.",Solving the Stochastic Resource-Constrained Project Scheduling Problem with Flexible Resource Profiles using Sample Average Approximation,"[76757, 9524]",348,"[118, 129, 136]",1551,Project scheduling under uncertainty,35,4,60,Project Management and Scheduling,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S09 [building - 101],"['Project Management and Scheduling', 'Scheduling', 'Stochastic Optimization']",MC-60
"To evaluate the impact of increased shares of renewable energy sources in electricity [RES-E] on wholesale electricity prices and demand in Austria by 2030, we linked a bottom-up electricity system model [ATLANTIS] with a top-down macroeconomic model [DYNK]. This integrated approach enables a comprehensive analysis encompassing both energy and socio-economic dimensions, factoring in the continental European electricity grid constraints through ATLANTIS rather than relying on simplified representations. The study reveals the growing importance of gas-fired power plants in balancing the intermittent output from RES-E, indicating that these plants will continue to influence electricity market prices. It highlights the potential socio-economic benefits, including an 18% GDP growth by 2030 [compared to 2017] under Austria’s renewable expansion act [EAG] scenario, driven by population growth, export expansion, and increased total factor productivity, alongside a 20% rise in final electricity demand. A sensitivity analysis around CO2 pricing suggests higher electricity costs and reduced GDP growth, hinting at a slight dip in electricity demand versus the EAG scenario. The findings underscore the necessity for enhanced renewable integration and storage solutions to fully decarbonize Austria's electricity system, emphasizing the critical role of flexible gas-fired generation and the potential economic implications of transitioning to a more sustainable energy mix.",Assessing the Energy and Socio-Economic Impacts of Renewable Energy Expansion in Austria by 2030,"[77290, 40094]",407,"[131, 139, 12]",1552,Market-based analyses in long-term energy system models,22,9,09,Energy Markets,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,10 [building - 116],"['Simulation', 'Sustainable Development', 'Capacity Planning']",TC-09
"The European Union [EU] mandates environmentally friendly, economically viable, and socially sustainable fishing practices for long-term food security, as outlined in the Green Deal objectives. Despite these aims, fisheries still heavily rely on fossil fuels, contributing significantly to global greenhouse gas emissions. Efficient routing can help vessels optimize their fuel consumption by considering the shortest distance to the fishing fields, while effective scheduling can prevent over-fishing of specific areas by avoiding excessive fishing pressure during certain times. Moreover, effective scheduling significantly impacts the quality of fish unloaded at the harbor.
Motivated by a real-life case for a company operating in the Nordic fishing industry, we developed a mixed-integer programming model for vessel routing and scheduling considering fuel consumption. The resulting optimization problem is complex featuring various types of uncertainty, with the primary source of uncertainty stemming from the variability of fish population in fishing fields. To tackle this issue, we employ the Sample Average Approximation [SAA] approach. Additionally, a heuristic approach has been devised to efficiently solve large-scale problems. Finally, we examined our proposed approach using real data from the corresponding company. Our results highlight how efficient vessel routing and scheduling in upstream processes of the supply chain can improve overall supply chain efficiency.
",From catch to coast -  An optimization approach for vessel routing and scheduling in the Nordic fishing industry,"[76097, 39248]",233,"[95, 143, 129]",1554,Novel topics and recent advances in solution approaches in scheduling,64,3,26,Combinatorial Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,012 [building - 208],"['OR in Fisheries', 'Transportation', 'Scheduling']",MB-26
"We introduce a new extension of the Technician Routing and Scheduling Problem [TRSP], where tasks are associated with multiple potential resolution locations, each with a distinct probability of success. Our objective is to efficiently resolve all tasks while minimizing resource consumption, such as driving distance or time.
We investigate various routing and scheduling strategies and compare their performance against a known brute force approach employed in the telecommunication sector. Additionally, we model the problem as a Markov Decision Process [MDP] to find an optimal decision making policy for the technicians, that minimizes the expected resource consumption for solving all tasks. We will also analyze a range of 'what if' scenarios, where we relax common problem constraints such as technicians having to finish their current task before starting another.
Computational results are reported for simulated scenarios with a varying number of technicians, resolution locations, and tasks, along with different spatial configurations of the resolution locations for each task.",Technician Routing and Scheduling for Tasks with Uncertain Resolution Locations,"[71931, 64035]",873,"[145, 136]",1555,Optimization problems in scheduling,64,12,26,Combinatorial Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,012 [building - 208],"['Vehicle Routing', 'Stochastic Optimization']",WA-26
"Complex policy issues, decision aiding, and the intricate landscape of
conflict transformation and management demand meticulous attention. In
this context, our study delineates a structured three-step approach. We
first focus upon Conflict Transformation and Management, improving
Problem Structuring Methods, cognitive maps, and value trees to address
conflict situations and complex decision-making tasks. We prioritize the
transformation of subjective insights from Cognitive Maps into
structured Value Trees. We then move from decision theory to design
theory, where we aim to further enhance PSMs by introducing innovative
and out-of-the-box alternatives, with a particular focus on
Concept-Knowledge [C-K] theory to create a meaningful connection between
PSMs and C-K theory. Finally we discuss how these findings relate to
policy design, linking our research to Conflict Transformation and
Management within the sphere of public policy.
",From Cognitive Maps to Value Trees for Policy Design,"[70464, 53539, 46682, 13]",131,"[149, 27, 133]",1558,OR Innovations in Policy Making - A,26,3,13,Soft OR and Problem Structuring Methods,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,15 [building - 116],"['Problem Structuring', 'Decision Theory', 'Soft OR']",MB-13
"When delivering chilled/frozen, expensive, or odd-sized goods, customers must be present to receive the shipment. In current operations, this often results in failed delivery attempts or considerable inconvenience for customers who need to be present long, often the entire day. To alleviate this inconvenience, logistics service providers [LSPs] moved towards communicating time windows, either directly after the customer shipment is received or batched the day before. Longer waiting for a time window poses an inconvenience for customers. We suggest a scheme in which the LSP decides [i] whether to assign a time window to a newly arriving customer, and if the LSP assigns a time window, also if some previously postponed customers shall also receive a time window, [ii] the actual time window, and [iii] the routes satisfying the time windows. To solve the resulting semi-Markov decision process, we adapt three solution methodologies - Deep Controlled Learning, the Rollout Algorithm, and the Multi-Scenario Approach. We show that Deep Controlled Learning manages to balance solution quality and runtime. We further find that customers located very close or very far from the depot receive time windows quickly, while other customers have to wait longer. On the other hand, customer arrival time does not influence the waiting time substantially. ",Deep Controlled Learning for the Delayed Time Window Assignment and Vehicle Routing Problem ,"[70471, 71797, 54033, 55094]",741,"[145, 0]",1562,Vehicle Routing Under Uncertainty 1,5,5,64,VeRoLog - Vehicle Routing and Logistics,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S16 [building - 101],['Vehicle Routing'],MD-64
"The aim of this contribution is to present the existence of some best proximity points for different type of operators that respect implicit type inequalities on generalized metric spaces. Those operators are defined using various classes of mappings with adequate properties of monotony, continuity or other specific inequalities. 
The passing from classic metric spaces to generalized metric spaces is made by an additional condition that ensures the existence of a sequence that will converge to a best proximity point. Those points are also a part of the set of zeros for two lower semi continuous functions. Consequences related to some fixed point results are also provided. 
",On best proximity points,[77266],955,"[81, 0]",1564,Nonsmooth optimization algorithms II,70,15,41,Nonsmooth Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,97 [building - 306],['Non-smooth Optimization'],WD-41
"Efficient supply chains are vital for both the worldwide economy and environmental sustainability. Container shipping plays a key role in this, known for being an eco-friendly mode of transport. Liner shipping companies are actively working to improve operational efficiency through stowage planning. Due to many combinatorial aspects, some of which are NP-hard, stowage planning is a challenging problem in its representative form. Even though stowage planning can be decomposed into master and slot planning, the subproblems remain challenging. As a result, we are searching for scalable algorithms to solve the stowage planning problem.

In this work, we propose Proximal Policy optimization for master STOWage planning [PPSTOW], a deep reinforcement learning approach to address master planning with focus on global problem objectives and constraints. The experiments show the effectiveness of PPSTOW, as the framework efficiently finds near-optimal solutions for simulated problem instances with realistic vessel sizes and practical planning horizons. In the future, we aim to refine the representativeness of our approach by integrating revenue management, as well as local problem objectives and constraints.",PPSTOW - An End-to-End Deep Reinforcement Learning Model for Master Stowage Planning on Container Vessels,"[77297, 77305, 32277]",312,"[14, 66, 70]",1566,[Deep] Reinforcement Learning for Combinatorial Optimization 1,14,4,03,Data Science Meets Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,1005 [building - 202],"['Combinatorial Optimization', 'Machine Learning', 'Maritime applications']",MC-03
"In this talk, we highlight our work that secured first and third place in the EURO Meets NeurIPS 2022 Vehicle Routing Competition on the static and dynamic track, respectively. First, we introduce PyVRP, an open-source and state-of-the-art VRP solver in Python that builds on Thibaut Vidal’s hybrid genetic search algorithm. Through PyVRP, we hope to provide researchers and practitioners the means to build upon a state-of-the-art VRP solver easily and quickly without diving deep into the algorithmic details. Second, we present iterative conditional dispatch [ICD] - a simple yet effective algorithm for dynamic vehicle routing problems with stochastic requests. ICD iteratively solves sample scenarios to classify requests to be dispatched, postponed, or undecided. The set of undecided requests shrinks in each iteration until a final dispatching decision is made in the last iteration. A significant strength of ICD is that it is conceptually simple and easy to implement. This simplicity does not harm performance - we show that ICD can nearly match the winning machine learning-based strategy of the EURO Meets NeurIPS 2022 Vehicle Routing Competition. PyVRP and ICD are forthcoming in the INFORMS Journal on Computing and Transportation Science, respectively.",PyVRP and ICD - Results From the EURO Meets NeurIPS 2022 Vehicle Routing Competition,"[72281, 77301, 77300, 72501, 70683, 47048]",782,"[145, 134, 135]",1570,Vehicle Routing Under Uncertainty 2,5,7,64,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S16 [building - 101],"['Vehicle Routing', 'Software', 'Stochastic Models']",TA-64
"In this study, we explore the role of storage in multi-period risk-averse energy markets. We consider market agents, namely energy producers and consumers, optimising profits across both energy spot and derivative markets. Each agent acts as a price-taker in the environment where the prices are endogenously determined. The energy producers have storage devices and manage energy flows through charging/discharging. We use coherent risk measurement in its dynamic form to model risk aversion. To solve the agents’ stochastic optimisation problems, we adopt ADMM [alternating direction method of multipliers] algorithm through  decomposition and extend the algorithm to the multi-market framework.  A case study is conducted involving conventional power plants, pumped hydropower and consumption, with the focus on peak and based load scenarios. Specifically, we investigate how storage flexibility, encompassing charge/discharge and boundary conditions in storage and capacity constraints, influence agents’ behaviours and market outcomes. The relationship between optimal storage strategies and hedging activities in the derivative markets is also examined. Furthermore, we analyse how varying levels of risk aversion affect decision-making processes, along with their effects on prices and profitability. ",Impact of storage flexibility in energy markets under risk aversion,"[71155, 29367, 68618, 69248]",869,"[136, 93]",1571,Flexibility in future energy systems,22,3,14,Energy Markets,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,16 [building - 116],"['Stochastic Optimization', 'OR in Energy']",MB-14
"Container dwell time prediction plays a crucial role in optimizing logistics and supply chain operations at ports and container terminals. Accurate predictions of such an indicator can enhance resource allocation, reduce congestion, and improve overall efficiency at the point of the global trade network. Data preprocessing and feature selection are critical steps in developing robust and efficient predictive models. This study develops feature engineering not only for structured data but also for unstructured data where a textual feature is classified through GloVe word embeddings and Cosine similarity to leverage the strengths of both types of data into machine learning methods. In addition, various feature selection techniques such as Filter methods, Embedded methods, and Wrapper methods are applied to identify the most influential factors affecting container dwell time, ultimately selecting 12 out of the 19 features for prediction. Instead of developing a regression model to predict dwell time, we classify dwell time based on data distribution into two and three classes, highlighting the significance of dwell time ranges over singular values. Random Forest and XGBoost algorithms are employed for the classification, where the former outperforms in terms of accuracy. From an operational point of view, the results of classification can facilitate effective decision-making for operational management such as equipment scheduling, yard stacking planning, workload planning, etc.",Container dwell time prediction considering feature engineering and feature selection,"[77197, 1292]",392,"[47, 66, 70]",1573,Analytics for Decision Making,17,12,31,Analytics,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,54 [building - 208],"['Forecasting', 'Machine Learning', 'Maritime applications']",WA-31
"Typically, companies for home appliances offer after-sales services, especially repair services, to their customers. One of the challenging planning problems that has to be solved in this context is the allocation and routing of service technicians that satisfy customers' service requests. In this paper, we consider heterogeneous technicians with two different skill levels. Junior technicians have less experience and, therefore, may fail to repair the appliance successfully, and senior technicians will repair the appliance successfully. In case of a failed service repair by a junior technician, a second visit by a senior technician is necessary. We formulate the problem as a sequential decision problem, where new customer requests arrive in each period, and decisions on the allocation and routing of service technicians are made at the beginning of each period. While the customer requests are known for the current period, future requests and their locations are unknown. The objective is the minimization of the total expected technicians' travel and customers' waiting costs. Several heuristic methods are proposed to solve the multi-period problem. We perform an extensive numerical study comparing the performance of the heuristic methods in different problem settings and provide managerial insights.",Allocation and routing of service technicians with different skill levels,"[76797, 47722, 829]",860,"[143, 130, 135]",1574,"Discrete, continuous or stochastic optimization and control in networks, transportation and design IV",64,5,25,Combinatorial Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,011 [building - 208],"['Transportation', 'Service Operations', 'Stochastic Models']",MD-25
"Scheduling of medical staff represents a time-consuming task, even during periods of relatively stable patient demand and personnel availability. Since rosters are built based on long-term patient loads, significant discrepancies between expected and actual patient emergences may occur, especially in times of high volatility like the COVID-19 pandemic. To ensure adequate care in short-term, recourse decisions, such as extra shifts or transferring patients are necessary to cover high workload peaks in terms of demand. Nevertheless, exact forecasts about number of patients cannot be made for more than a few days in advance. This makes it nearly impossible for schedulers to anticipate unexpected demand when creating the upcoming schedule. In this work, we provide a Mixed-Integer Program [MIP] to [re-]allocate medical staff and therefore ensure appropriate care of all patients. Our MIP consists of a two-stage optimization model, which at first performs the initial schedule while on the second stage applying recourse decisions to cover realized patient demand. The expected workload is determined on forecasts about patient emergence and the acuity level of patients by partners of the PROGNOSIS consortium. Our temporal scope encompasses a time horizon of one week. As patient demand is subject to numerous uncertainties, we evaluate scenario-based rosters by applying Sample-Average Approximation and provide generalized insights about short-term adjustments in personnel scheduling.",Optimizing adaptive medical staff scheduling using recourse decisions amidst fluctuating patient demand,"[72640, 49216, 15060]",607,"[129, 56, 136]",1575,Staffing and workforce planning and scheduling,3,9,15,OR in Health Services [ORAHS],"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,18 [building - 116],"['Scheduling', 'Health Care', 'Stochastic Optimization']",TC-15
"Continuous annealing is a core process in steel cold-rolling facilities, where order-specific pieces of flat steel, wound up to coils, are processed through an annealing furnace to achieve defined material properties. To enable a continuous process, the coils are welded together before entering the line. Whenever two consecutive coils are incompatible, a special dummy coil called stringer is used to connect them, which reduces efficiency and adds costs and emissions. We consider the scheduling of coils with specific due dates and alternative order-specific processing modes on parallel heterogeneous lines. The problem is to simultaneously assign coils to lines and sequence them on these lines with a defined processing mode with adherence to tardiness constraints while minimizing the number of stringers needed and the associated energy consumption. To address this problem, we formulate a mixed-integer linear program and propose a branch-and-price algorithm based on a Dantzig-Wolfe decomposition by lines to solve it. The pricing subproblems are solved using a labeling algorithm. The approach is implemented using the SCIP framework. We present the underlying problem decomposition, implementation, and some first numerical results using a heuristic and a state-of-the-art commercial solver as a benchmark.",A branch-and-price algorithm for scheduling continuous steel annealing lines,[72566],835,"[13, 129, 59]",1576,Mathematical programming for machine scheduling,32,15,49,"Lot Sizing, Lot Scheduling and Production Planning","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,M1 [building - 101],"['Column Generation', 'Scheduling', 'Industrial Optimization']",WD-49
"The promised delivery speed enjoyed by Amazon's customers when glancing over the website's items [ASIN] is the result of a cascade of planning algorithms. At the core of this cascade is the modelling of the expected speed given a network design and inventory allocation. In this work, we present a scalable probabilistic approach to estimate the future glance-view speed, enabling us to query inventory aware speed variations in micro seconds. The close formulae derived in this work decouples inventory coverage from glance-view count forecasting, allowing us to answer valuable what-if scenarios regarding inventory placement, or new site launches. This in turn can guide Amazon's multi-million network design initiatives from short to long term horizons.",Stochastic Glance-View estimator,"[77302, 77307, 77347, 77348]",616,"[32, 66]",1577,Large-scale Speed-sensitive Network Optimization,92,12,57,Optimization at Amazon,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S06 [building - 101],"['E-Commerce', 'Machine Learning']",WA-57
"A customer choice model, based on the Markov Chain Choice Model with reservation prices, is proposed to solve the problem of finding optimal selling prices for substitutable products. The model includes both customers’ willingness to pay and substitution behavior explicitly, and can handle any type of correlation between products. A discrete version of the model is formulated and solved to optimality. This discrete model is instrumental in a simulation procedure to estimate optimal reservation prices in the original continuous model.

",A discrete choice model for multi-product pricing based on the Markov Chain Choice Model combined with reservation prices,"[77294, 77304, 46298]",15,"[135, 0]",1579,Advances in Stochastic Modelling and Applied Probability Ι,47,3,40,Advances in Stochastic Modelling and Learning Methods,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,96 [building - 306],['Stochastic Models'],MB-40
"Network design problems have been studied from the 1950s, as they have a wide range of real-world applications. In classical network design problems the objective is to minimize the cost of routing goods through a graph. 
We introduce a generalised version of such a problem, where the objective is to trade-off the speed at which goods can be sent to their destination nodes and the cost of routing them. Compared to standard time-aware network design problems, where speed is taken into account by adding deadline constraints - i.e., goods have to reach destinations before a target date - we introduce a coverage function mapping the set of goods served by origin-destination pairs for which we offer expedite delivery to the number of unique items for which fast delivery services can be offered. This coverage function models that sets of goods for different origin-destination pairs may not be disjointed, and it allows us to compute the share of unique items for which we can offer expedite delivery services, which we refer to as speed-coverage.
To minimize routing costs while jointly maximizing speed-coverage, we leverage parametric optimization to reformulate the coverage function as a convex program with an exponential number of constraints. Then, we propose a sampling strategy to reduce the dimensionality of such a reformulation. We show that when considering the overall costs given by routing costs minus speed revenues, our approach outperforms the baseline by 8.36% on average",Speed-aware network design - a parametric optimization approach,"[77306, 77307, 71738, 77310, 77309]",616,"[79, 0]",1581,Large-scale Speed-sensitive Network Optimization,92,12,57,Optimization at Amazon,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S06 [building - 101],['Network Design'],WA-57
"This work analyzes so-called connection scheduling problems [CSPs], a type of interactive Operations Research problem that combines elements from minimum cost spanning tree problems and sequencing problems. Given a graph, our goal is twofold - firstly, to establish an optimal connection order among players to minimize the overall cost of connecting them to a source, and secondly, to develop a cost allocation strategy for this optimal order among the involved players. We focus specifically on CSPs on trees, for which we propose a recursive solving method integrated with an allocation approach. 



",Optimization and allocation for connection scheduling problems,"[77054, 11882, 2466, 57199]",619,"[129, 53, 50]",1582,"Game Theory, Solutions and Structures IV",88,5,36,"Game Theory, Solutions and Structures","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,32 [building - 306],"['Scheduling', 'Graphs and Networks', 'Game Theory']",MD-36
"Different notions on regularity of sets and of collection of sets play an important role in the analysis of the convergence of projection algorithms in nonconvex scenarios. While some projection algorithms can be applied to feasibility problems defined by finitely many sets, some other require the use of a product space reformulation to construct equivalent problems with two sets. In this work we analyze how some regularity properties are preserved under a reformulation in a product space of reduced dimension. This allows us to establish local linear convergence of parallel projection methods which are constructed through this reformulation.",Local linear convergence of parallel projection algorithms with reduced lifting,[57915],291,"[19, 102]",1588,Iterative Methods for Feasibility and Optimization Problems,82,13,42,Variational Analysis and Continuous Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,98 [building - 306],"['Continuous Optimization', 'Parallel Algorithms and Implementation']",WB-42
"Stochastic Optimization [SO] typically requires knowledge about the
probability distribution of uncertain parameters. As the latter is
often unknown, Distributionally Robust Optimization [DRO] provides a
strong alternative that determines the best guaranteed solution over a
set of distributions [ambiguity set]. We present an approach for DRO
over time that uses online learning and scenario observations arriving
as a data stream to learn more about the uncertainty. Our robust
solutions adapt over time and reduce the cost of protection with
shrinking ambiguity. For various kinds of ambiguity sets, the robust
solutions converge to the SO solution. Our algorithm achieves the
optimization and learning goals without solving the DRO problem
exactly at any step. We also provide a regret bound for the quality of
the online strategy and also discuss how to perform a scenario
reduction with approximation guarantee. We illustrate the
effectiveness of our procedure by numerical experiments from popular benchmark libraries and give practical examples stemming from telecommunications and routing. Our algorithm is able to solve the DRO over time problem significantly faster than standard reformulations.

This talk is based on joint work with K. Aigner, A. Bärmann, K. Braun,
S. Pokutta, O. Schneider, K. Sharma, S. Tschuppik. Parts of the talk
are published in INFORMS J Optimization, 2023. ",Computationally Efficient Data-Driven Distributional Robustness Over Time,"[14713, 57234, 59498, 75122, 68483, 75186]",379,"[72, 127, 136]",1589,Trends and Open Problems in Robust Optimization,49,10,34,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,43 [building - 303A],"['Mathematical Programming', 'Robust Optimization', 'Stochastic Optimization']",TD-34
"In distributed optimization and machine learning, a large number of machines perform computations in parallel and communicate back and forth with a distant server. Communication is typically slow and costly, and forms the main bottleneck in this setting. This is particularly true in federated learning, where a large number of users collaborate to optimize a global model, based on their personal data that are kept private. In addition to communication-efficiency, a robust algorithm should allow for partial participation. To reduce the communication load, two strategies are popular - 1] communicate less frequently; 2] compress the communicated vectors. We introduce TAMUNA, the first algorithm that harnesses these two strategies jointly and allows for partial participation. TAMUNA converges linearly to an exact solution in the strongly convex setting and provably benefits from the two mechanisms of local training and compression - its communication complexity is doubly accelerated, with a better dependency on the condition number of the functions and on the model dimension.","TAMUNA - Doubly-Accelerated Distributed Optimization with Local Training, Compression, and Partial Participation","[67248, 25495]",370,"[21, 136]",1590,Distributed and Federated Optimization,84,15,32,Advances in large scale nonlinear optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,41 [building - 303A],"['Convex Optimization', 'Stochastic Optimization']",WD-32
"Truck platooning is a promising technology for reducing energy consumption, increasing vehicle safety, and improving traffic efficiency. In this research, we examine the cost-effectiveness of truck platooning from the perspective of a freight company performing full truckload pickup and delivery tasks over a transportation network. We propose two model formulations for this problem - a direct delivery model and an indirect delivery model, where the indirect delivery model allows trucks to visit intermediate locations during deliveries to facilitate the formation of platoons. We devise an improved dynamic discretization discovery [DDD] algorithm to solve the two models exactly. Through extensive computational experiments, we find that [1] the improved DDD algorithm can increase solution accuracy with much less computational effort compared with the original DDD algorithm; [2] the cost-saving effect of truck platooning is favorable, with an average of 80.55% trucks being followers in platoons; and [3] for freight companies operating on small transportation networks, using the direct delivery model may be more appropriate.",The Full Truckload Pickup and Delivery Problem with Truck Platooning,"[77112, 77321]",524,"[65, 143, 145]",1591,Freight transportation and logistic III,6,10,55,Transportation,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S02 [building - 101],"['Logistics', 'Transportation', 'Vehicle Routing']",TD-55
"A scrutiny analysis of the COVID-19 data is required to get insights into effective strategies for pandemic control. However, a particular gap between official data and methods used to assess the effectiveness of various containment measures [e.g., COVID-19 passports] hinders sound inference-making. Seeking to escape the burden of arising obstacles, employing the principles and methods of descriptive statistics is often tempting, but in-depth analysis demands more sensitive and reliable methods. In this regard, this paper advocates a maximum likelihood compartmental modeling approach, which provides the flexibility to raise various hypotheses about infectivity, recovery, and mortality dynamics and to find the most likely answers given the data. Our paper is based exclusively on COVID-19 deaths in light of official data limitations, as relatively fewer limitations characterize these data. Nevertheless, this paper does not solve the underlying problems but hints at potential improvements in official data reporting that could benefit COVID-19 modeling prospects.",On the gap between data and models in COVID-19 analysis,"[65628, 65736, 80042]",469,"[7, 31, 135]",1592,Modelling social-behavioural phenomena in creative societies,13,15,07,Behavioural OR,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,1019 [building - 202],"['Analytics and Data Science', 'Dynamical Systems', 'Stochastic Models']",WD-07
"Product allocation problem is a warehouse management that concerns allocate the products to the storage positions in a warehouse or to the warehouses in multi-warehouse problem. If the orders split to the several warehouses, the cost of picking them up is a non-added value facility. So, the aim of product allocation problem is to determine which product should be located to the which warehouse to minimize the cost of handling and transportation activities of the split orders.  
In multi-warehouse problem, trucks can visit more than one warehouse to pick up customer orders if an order split to others warehouse. In this study, we aim that is to minimize the number of visited warehouses for total orders under capacity of warehouse and demand of orders. The assumption is the one product can only storage to the one warehouse. The mathematical problem is constructed and solved with GAMS 38.3.0.  Also, product allocation problem is a clustering problem in which products can be clustered with respect to the number of warehouses. In this study, k-means algorithm is used for clustering the products to the four warehouses. It is shown that the similar results are obtained from mathematical problem and clustering algorithm. 
",MODELING PRODUCT ALLOCATION PROBLEM FOR SPLIT ORDERS AND SOLVING VIA CLUSTERING ALGORITHMS,[44205],506,"[65, 66, 38]",1594,Freight transportation and logistic I,6,8,55,Transportation,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S02 [building - 101],"['Logistics', 'Machine Learning', 'Engineering Optimization']",TB-55
"This paper innovatively addresses the effect of cooperation on sequencing situations with position-dependent effects. Specifically, we ensure convexity of the associated sequencing games under the fulfilment of certain conditions on the neighbour switching gains. Besides, we propose two families of allocation rules based on sharing the neighbour switching gains under two procedures, each one giving a path from the initial order to an optimal order. From a theoretical point of view, an axiomatization of both families of allocations is provided, and their stability is also ensured under those conditions related with convexity. Finally, we analyse the specific sequencing problems arising from the total flow time minimization under the consideration of exponential positional effects.",Sequencing situations with position-dependent effects under cooperation,"[67465, 70028, 12967]",342,"[50, 129, 121]",1595,"Game Theory, Solutions and Structures III",88,4,36,"Game Theory, Solutions and Structures","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,32 [building - 306],"['Game Theory', 'Scheduling', 'Queuing Systems']",MC-36
"This study delves into the interplay between gender, professional networking, career-path trajectory, and board director appointments in Canadian publicly traded companies. To obtain our results, we combine multiple publicly available sources into a unique dataset. This dataset [consisting of over 700 Canadian firms covering more than 19,000 senior managers and board members in the 2000-2022 period], charts detailed network information of both senior managers and board members across five key dimensions - education, current and prior employment, and current and prior social engagement. By matching senior managers of both genders based on their career trajectories and backgrounds, and applying Long Short-Term Memory [LSTM] deep learning alongside network analysis, our research uncovers the distinct network influences impacting the board appointment prospects of women versus men. The findings causally demonstrate a “glass ceiling”, suggesting that women necessitate more substantial networking credentials than men to secure equivalent corporate board positions, when controlled for career paths and backgrounds. This paper outlines our data compilation process, matching and analytical methodology, presents our empirical results and insights, and the broader implications these hold for policies aimed at encouraging both good corporate governance and gender equality at senior level.",Breaking Barriers - Unveiling Gender Disparities in Corporate Career Paths Using Deep Learning,"[18421, 77317, 77316, 47369, 77315, 13224]",65,"[7, 15, 132]",1598,Network Analytics,17,5,31,Analytics,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,54 [building - 208],"['Analytics and Data Science', 'Complex Societal Problems', 'Social Networks']",MD-31
"This study addresses the impact of search operators on the performance of meta-heuristics. The efficient selection and management of operators from a diverse set are essential for optimization. Two key aspects are explored - portfolio selection, determining which subset of operators to consider at each search stage, and operator selection, deciding how to choose operators within that subset for each iteration.

A novel framework is developed to dynamically handle the portfolio of operators, incorporating the tabu concept and Q-learning. Unlike traditional static methods, this study focuses on creating an adaptive online portfolio. Acknowledging that not all operators are useful, the mechanism adds effective operators and excludes ineffective ones at different stages, optimizing overall performance. Inefficient operators are temporarily removed based on a tabu list size and may be reintroduced later. Additionally, new operators may be selectively included for specific search stages. Following portfolio selection, Q-learning dynamically chooses the most efficient operator for each iteration.

To evaluate the proposed framework, the permutation flowshop scheduling problem is tackled. Results demonstrate improvements in optimality gaps and convergence rates compared to offline portfolio selection and existing algorithms. The study highlights the efficacy of the dynamic approach in enhancing meta-heuristic performance and adaptability in complex optimization scenarios.
",Dynamic operator management in meta-heuristics using machine learning,"[73435, 77318, 67397, 24902]",220,"[74, 66, 69]",1599,Advanced Topics in Combinatorial Optimization,64,8,26,Combinatorial Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,012 [building - 208],"['Metaheuristics', 'Machine Learning', 'Manufacturing']",TB-26
"Online platforms are leveraging its power to foster environmentally friendly supply chains. This study explores the blockchain application and business model decisions in a supply chain with a manufacturer selling through an online retail platform. The manufacturer decides whether to adopt blockchain technology to provide evidence of carbon reduction, aiming to secure an eco-friendly label from a platform, which in turn determines the business model, choosing between agency selling and reselling. We fully characterize the equilibrium decisions and show how they depend on the carbon reduction efficiency, blockchain efficiency, and commission rate. The presence of a carbon reduction manufacturer reduces the effectiveness of using the wholesale price as a lever by itself. Therefore, employing blockchain upon a reselling platform can hurt the manufacturer. We also show that blockchain technology and business mode are complementary, and therefore managers should not ignore the impact of one decision on the other decision even when the latter is not a primary motivation of the former. This study represents a significant stride towards the integration of blockchain in production and underscores collaboration with dominant platforms to offer guidelines for effectively reducing carbon emissions.","Carbon Reduction, Blockchain Technology, and Business Model in Online Retail Platforms","[77177, 77403, 77391]",927,"[50, 138, 32]",1601,New technology for sustainable supply chains,18,14,23,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,82 [building - 116],"['Game Theory', 'Supply Chain Management', 'E-Commerce']",WC-23
"Machine learning algorithms have been increasingly integrated into applications that significantly affect human lives. This has spurred interest in designing algorithms that train machine learning models to minimize the training error while imposing a certain level of fairness. In this paper, we consider the problem of fair clustering of datasets. In particular, given a set of items, each associated with a vector of non-sensitive attribute values and a categorical sensitive attribute, our goal is to find a clustering of the items that minimizes the loss [i.e., the clustering objective] function while imposing fairness measured by Rényi correlation. We propose an efficient and scalable in-processing algorithm, driven by findings from the field of combinatorial optimization, that heuristically solves the underlying optimization problem and allows for regulating the trade-off between clustering quality and fairness. The approach does not restrict the analysis to a specific loss function but instead considers a more general form that satisfies certain properties. This broadens the scope of the algorithm's applicability. We demonstrate the effectiveness of the algorithm for the specific case of k-means clustering, as it is one of the most extensively studied and widely adopted clustering schemes. Our numerical experiments reveal that the proposed algorithm outperforms existing methods by providing a more effective mechanism to regulate the trade-off between loss and fairness.",An Optimization-Based Order-and-Cut Approach for Fair Clustering of Data Sets,[73780],140,"[66, 14, 53]",1602,Mathematical Optimization for Trustworthy Machine Learning,15,9,27,Mathematical Optimization for XAI,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,047 [building - 208],"['Machine Learning', 'Combinatorial Optimization', 'Graphs and Networks']",TC-27
"Assemble-to-order [ATO] strategies are widely used in various industries. Despite their popularity, ATO systems remain challenging, both analytically and computationally. We study a general ATO problem modeled as an infinite horizon Markov decision process. In particular, we consider a system with mixed-Erlang distributed component production/leadtimes, and Poisson demand for products. Demand is lost if not immediately satisfied. As the optimal policy of such system is computationally intractable, we develop two heuristic policies based on decomposition methods - component-based and product-based. In order to evaluate the performance of the heuristics, we develop a tight lower bound using an Approximate Linear Programming approach that relies on a judicial choice of basis-functions, for approximating the optimal value function. Our results show that the heuristics perform within only few Average Percentage Deviation [ADP] from the lower bound and even a smaller ADP when compared to systems where the optimal policy could be obtained. Moreover, we show that our component-based decomposition heuristic only scales linearly with the number of components in the ATO system, and therefore is suitable for solving large-scale ATO systems.",Solving General Assemble-to-Order systems via Component-Based and Product-Based Decomposition Methods,"[10744, 70725, 59566]",159,"[105, 108, 135]",1603,Stochastic Models in Manufacturing,50,5,39,Stochastic Modelling,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,35 [building - 306],"['Production and Inventory Systems', 'Programming, Dynamic', 'Stochastic Models']",MD-39
"We address the solution of large-scale nonlinear least-squares problems by stochastic Gauss-Newton methods combined with a line-search strategy. The algorithms proposed have per-iteration computational complexity lower than classical deterministic methods, due to the employment of random models inspired by randomized linear algebra tools. Under suitable assumptions, the stochastic optimization procedures can achieve a desired level of accuracy in the first-order optimality condition. We discuss the construction of the random models and the iteration complexity results to drive the gradient below a prescribed accuracy, then we present results from our computational experience.",Solving large-scale nonlinear least-squares with random Gauss-Newton models,"[21159, 21177, 67179]",422,"[72, 63, 5]",1605,Optimization and learning for data science and imaging [Part II],84,3,34,Advances in large scale nonlinear optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,43 [building - 303A],"['Mathematical Programming', 'Large Scale Optimization', 'Algorithms']",MB-34
"Distributed recursive estimation under heavy-tailed sensing and communication noises is considered. Therein, the sensing and communication noises can be mutually correlated while independent identically distributed over iterations. A general setting is assumed where both the sensing and communication noises may have infinite variances. A consensus+innovations distributed estimator is presented, involving a general nonlinearity in both consensus and innovations update rules. We present several results on the estimator performance. It is shown that the mean squared error [MSE] of the estimation converges to zero, and moreover, an explicit MSE sublinear convergence rate is established. In addition, almost sure convergence and asymptotic normality results are derived. Analytical and numerical examples confirm that the presented method converges under the simultaneous heavy-tail communication and sensing noises, in contrast with existing estimators that break down under the same setting.",Mean square convergence analysis of nonlinear distributed recursive estimation under heavy-tailed noise,"[72139, 67207, 53288, 53587]",422,"[5, 0]",1606,Optimization and learning for data science and imaging [Part II],84,3,34,Advances in large scale nonlinear optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,43 [building - 303A],['Algorithms'],MB-34
The goal of Amazon's outbound network planning is to offer customers fast delivery at minimum costs. Key decisions are the legs and sorts to connect sites [connectivity] and Critical Pull Times [CPTs]. This problem is computationally hard to solve and was in the past decomposed into the connectivity and timing subproblems that were solved separately. This paper presents a local-search engine to jointly optimize connectivity and timing for Amazon's outbound network. It bundles a variety of scientific and engineering ideas to build optimized solutions efficiently.,Local Solver for Joint Connectivity and Timing Optimization for Amazon's Outbound Network,"[77320, 77349, 77302, 59684, 79405, 77310, 77309]",616,"[65, 5]",1607,Large-scale Speed-sensitive Network Optimization,92,12,57,Optimization at Amazon,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S06 [building - 101],"['Logistics', 'Algorithms']",WA-57
"In this article, our goal is to present a method that enables us to perform an analysis using the Graph Model for Conflict Resolution [GMCR] in conflicts scenarios where the preferences of decision makers show inconsistencies due to the Framing Effect. More specifically, we consider a GMCR with two decision makers who presented different preferences when, during the elicitation process, they were confronted with the same problem framed in two distinct ways. This behavior is inconsistent with Decision Theory and may be explained by the boundaries of human rationality. Our approach uses uncertain preferences to address the inconsistencies arising from the Framing Effect.",Uncertain Preferences to deal with the Framing Effect in the Graph Model for Conflict Resolution,"[77295, 76287]",619,"[50, 55, 10]",1608,"Game Theory, Solutions and Structures IV",88,5,36,"Game Theory, Solutions and Structures","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,32 [building - 306],"['Game Theory', 'Group Decision Making and Negotiation', 'Behavioural OR']",MD-36
"Social media sometimes empowers potential adversarial state actors, paid trolls, and extremists to incite hysteria and coordinate nefarious actions, e.g., deviant mobs. A mob is an event organized via social media or other forms of digital communication technologies in which a group of people gathers online and/or offline to conduct an act collectively and then disperses. To an outsider, such an event may seem arbitrary; however, a sophisticated amount of coordination is involved. These mob-like events like the January 6, 2021, U.S. Capitol attack, or GameStop's stock “flashmob investing” are becoming widespread due to the affordability of social media, ease of use, effectiveness of individuals or groups in conducting coordinated acts, anonymity of the internet, etc. The prevalence of these incidents underscores the inadequacy of current systems [security, financial, etc.] in addressing such coordinated behavior. This topic is understudied due to lack of data, theoretical underpinning, and computational resources required to analyze the complex and dynamic social processes among mobbers. In this study, we develop a model that can simulate mobs guided by constructs extracted from various social science theories. We then aim to use the model to study the behavior of the mobbers, the motivation of the organizers, and attempt to infer the mob’s outcome. Real-world data, albeit limited, will be used to evaluate the simulation-driven model in a real-world setting.
",Mobs Simulation Guided by Social Science-Based Multi-Theoretical Framework,"[67422, 77065]",469,"[15, 131, 3]",1609,Modelling social-behavioural phenomena in creative societies,13,15,07,Behavioural OR,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,1019 [building - 202],"['Complex Societal Problems', 'Simulation', 'Agent Systems']",WD-07
"This study addresses the complexities of two-echelon distribution systems in modern city logistics, highlighting the challenges faced by megacities like Jakarta, Indonesia. In collaboration with an industry partner, our research explores a logistics platform designed for efficient same- and next-day deliveries between local shops and consumers. In this context, we introduce the stochastic and dynamic order allocation and dispatching problem [SDOA-DP] aimed at minimizing costs by optimizing order allocations to linehauls between two-echelon systems and dispatching strategies within them. This paper makes the following contribution - advancing corridor-based logistics by explicitly integrating efficient linehaul planning with first and last-mile routing, employing a cost-function approximation within a two-stage stochastic programming formulation for dynamic order assignment, and leveraging a parameterized Adaptive Large Neighborhood Search algorithm for routing. Finally, our model was implemented using real-life data, demonstrating the approach's effectiveness in reducing logistics costs.",A Real-Life Order Allocation and Dispatching Problem in Megacities,"[77322, 54033, 55094]",503,"[145, 143, 136]",1613,Last mile delivery modeling,6,2,56,Transportation,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S04 [building - 101],"['Vehicle Routing', 'Transportation', 'Stochastic Optimization']",MA-56
"In this presentation, we introduce the bilinear branch and check algorithm, an exact method for solving nonlinear optimization problems. This approach extends the classical branch and check algorithm by integrating bilinear constraints into the search process. The enhancement of the bilinear constraint refines the problem formulation, expediting the convergence of the search. The algorithm decomposes problems into manageable subproblems, utilizing the branch and cuts to assess feasibility and generate feasibility cuts [no-good cuts], while also enhancing master problem solutions. A key innovation lies in the algorithm's methodology for improving master problem solutions and constructing high-quality no-good cuts among decision variables. This allows for further decomposition of the master problem, enhancing its effectiveness. Our numerical results demonstrate promising outcomes and exhibit greater efficiency compared to standard branch and cut methods like IBM CPLEX and the classical branch and check.",Bilinear Branch and Check Algorithm,"[77326, 74362]",721,"[5, 111, 113]",1621,Applications of Mixed-Integer and Nonconvex Optimization 1,86,14,04,MINLP,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,1001 [building - 202],"['Algorithms', 'Programming, Mixed-Integer', 'Programming, Nonlinear']",WC-04
"Large-scale natural disasters have caused significant casualties and enormous economic losses. Effectively responding to disasters has become primary challenge for logistics authorities. After a disaster, shortages or over-supply may occur in the resource supply chain. one of the challenges that must be addressed is how to make up for shortages. To rationalise the use of limited resources, decision-makers have to decide facilities’ operations and resources allocation based on changes in disaster phases. With this issue in mind, we propose a mathematical model addressing the facility location problem within the context of post-disaster phases across multiple periods, integrating equity, transshipment, and capacity limitations. To validate the mathematical formulation and assess the effect of individual and combined concepts on the foundational model. We conducted a series of numerical experiments using python and commercial solver, Gurobi. The results indicate that the equity primarily influences the maximum unsatisfaction rate on the demand side, while the transshipment predominantly impacts emergency logistics’ operations and resources allocation. The limitation of capacity mainly affects transfer ability between emergency logistics centers. Furthermore, we gleaned additional managerial insights through sensitivity analysis of critical parameters, and delineated practical application for integrating our model into disaster management system.",Multi-period facility location problem with resource allocation in humanitarian logistics,"[76970, 16227]",552,"[65, 64, 58]",1623,Site selection and aid allocation in humanitarian operations,38,5,21,OR in Humanitarian Operations [HOpe],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,49 [building - 116],"['Logistics', 'Location', 'Humanitarian Applications']",MD-21
"In Japan's local governments, the shortage of personnel has made it difficult to compile statistics. While regional Gross Domestic Product [GDP] is released promptly in countries like the United States and the United Kingdom, in Japan, there is a lag of about two years before annual figures are published. Several prefectures used to release preliminary figures for regional GDP on a quarterly basis, but their number has gradually decreased, and currently, only three prefectures do so. Many local governments are creating business cycle indices similar to the OECD's Composite Leading Indicator [CLI] to survey the current state of the economy. However, business cycle indices do not reveal the level of economic activity, nor do they easily allow for comparisons between different prefectures.
in this study, a method to create regional GDP from business cycle indices was explored. It is assumed that there is a long-term stable relationship [cointegration] between the economic business cycle, which represent the cycle of the economy, and regional GDP, which represents the overall movement of the region. By constructing an Autoregressive Distributed Lag [ARDL] model and initially applying it to the country as a whole, a long-term relationship was confirmed. By applying this relationship to the Gross Regional Product [GRP] of each prefecture, a method to estimate regional GDP was devised.
",Forecasting regional GDP using cointegration with business cycle indicators,[47701],209,"[47, 33, 7]",1624,Analytics and the link with stochastic dynamics I,17,7,31,Analytics,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,54 [building - 208],"['Forecasting', 'Economic Modeling', 'Analytics and Data Science']",TA-31
"Hedging against price increases is particularly important in times of significant market uncertainty and price volatility. For commodity procuring firms, futures contracts are a widespread means of financially hedging price risks. Recently, data-driven decision-support approaches have been developed, with deep learning-based methods achieving particularly good results in capturing non-linear relationships between external features and price trends. We employ a prescriptive deep-learning approach modeling hedging decisions as a multi-label time series classification problem. We backtest the performance in two evaluation periods for natural gas, crude oil, nickel, and copper. The approach performs well in the evaluation period of the testing framework [2018-2020] yet fails to capture the market disruptions [pandemic, geopolitical developments] in the second evaluation period [2021-2023], yielding significant hedging losses or degenerating into a simple hand-to-mouth procurement policy. We employ explainable artificial intelligence to analyze the performance for both periods. The results show that the models fail to perform well under market regime switches and disruptive events within the considered feature set.",Understand Your Decision Rather than Your Model - Towards Explainable Deep Learning Approaches for Sustainable Commodity Procurement,"[76853, 9112]",652,"[8, 25, 66]",1625,Artificial Intelligence and Machine Learning for Decision Support,45,8,45,Decision Support Systems,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,30 [building - 324],"['Artificial Intelligence', 'Decision Analysis', 'Machine Learning']",TB-45
"We propose an adaptive backtracking and restarting strategy to automate a variant of the Fast Iterative Soft-Thresholding Algorithm [FISTA] for structured optimization problems. Under a generalized [non-restrictive] strong convexity growth condition, we prove the linear convergence of function values generated by the resulting parameter-free algorithm. The algorithm's performance and versatility are demonstrated on exemplar imaging problems.",Parameter-free FISTA,"[75789, 80305, 50493, 76295, 80306]",422,"[81, 18, 21]",1626,Optimization and learning for data science and imaging [Part II],84,3,34,Advances in large scale nonlinear optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,43 [building - 303A],"['Non-smooth Optimization', 'Computer Science/Applications', 'Convex Optimization']",MB-34
"Topology optimization of frame structures that minimizes the weight while respecting free-vibration eigenvalue constraints presents a challenging nonconvex polynomial optimization problem. We address this challenge using a nonlinear semidefinite programming formulation, casting it as a bilevel program. This reformulation offers a unique structure - a quasiconvex, univariate lower-level problem that searches for scaling of fixed cross-section area ratios and an upper level that optimizes these ratios. We establish conditions for the lower-level problem solvability and demonstrate how to construct feasible solutions for the original polynomial semidefinite program. Given a feasible point, we prove that the convergence conditions of the Lasserre hierarchy are satisfied. By solving this hierarchy of convex relaxations, we obtain lower bounds. We further show that the relaxed solutions satisfy the conditions for the feasibility of the lower-level problem and thus enable us to construct feasible upper bounds in each relaxation. With this, we introduce a simple sufficient condition for global ε-optimality and prove that the optimality gap ε approaches zero in the limit when the set of global minimizers is convex. Numerical examples illustrate the convergence of the hierarchy in a finite number of steps.",Weight minimization of frame structures under free-vibration eigenvalue constraints with polynomial optimization,[66749],903,"[115, 19, 38]",1627,Advances in polynomial optimization and its applications,68,15,38,"Conic Optimization - Theory, Algorithms, and Applications","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,34 [building - 306],"['Programming, Semidefinite', 'Continuous Optimization', 'Engineering Optimization']",WD-38
"In South Africa, the Department of Health has introduced mobile clinics to improve access to healthcare for rural communities. In this study, we consider the Witzenberg region, where six mobile clinics have been deployed. The aim was to determine routes and schedules to improve the workload balance, fairness, and transportation cost, while ensuring patients get satisfactory care. The problem is modelled in three phases using both primary qualitative data and secondary quantitative data. In phase 1, a multi-vehicle routing problem is formulated to construct feasible daily routes for the mobiles. Phase 2 distributes the daily routes fairly between the mobile clinics to ensure fairness. Finally, in phase 3, a vehicle routing formulation is used to determine a 4-week schedule for each mobile clinic, by using the daily routes obtained during phase 2 as input. Four different service time estimations are used as input, resulting in four different schedules, each with their own advantages and disadvantages, including cost-effectiveness, robustness, fairness, and continuity of care. AHP was then performed with main decision makers to determine their preferred schedule. Final routes and schedules were determined based on model results, AHP results, and final practical input from the decision makers, resulting in an improvement in workload balance, a 23% reduction in total distance travelled and willingness by decision makers to implement the changes.",Mobile clinic deployment in the Witzenberg region of South Africa.,"[77259, 42965]",597,"[145, 6, 151]",1628,Mobility and transportation in healthcare,3,4,10,OR in Health Services [ORAHS],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,11 [building - 116],"['Vehicle Routing', 'Analytic Hierarchy Process', 'Practice of OR']",MC-10
"To reduce emissions, traffic, and noise, municipalities and governments are imposing fees and toll schemes to logistic companies for accessing particular roads. Because such measures increase delivery costs, transportation companies have to consider them when planning routes. In this talk, we address the edge set vehicle routing problem with time windows [ESVRPTW] initially introduced by Reinhardt et al. [2016]. The ESVRPTW is a generalization of the vehicle routing problem with time windows in which a fixed cost must be paid for accessing a set of edges. 

We propose an improved formulation for the ESVRPTW, new variable fixing techniques, and new valid inequalities. We introduce a new variant of the ESVRPTW to represent urban contexts in which waiting at customers for the opening of their time windows is not allowed. To solve these problems, we present a matheuristic, called MS-LBM, that consists of a multi-start local descent framework that uses a local branching scheme. 

In addition to its flexibility in solving all ESVRPTW variants addressed in this talk, MS-LBM provides high-quality solutions faster than a commercial optimization solver and finds new best-known solutions. With an extensive computational study, we derive insights into how allowing waiting at customers affects the edge-set activation and routing decisions depending on the type of infrastructure for which fixed costs must be paid.
",A Matheuristic for the Edge Set Vehicle Routing Problem With Time Windows and Variants,[67262],754,"[145, 111, 14]",1629,Heuristics for Vehicle Routing 1,5,14,64,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S16 [building - 101],"['Vehicle Routing', 'Programming, Mixed-Integer', 'Combinatorial Optimization']",WC-64
"This work introduces a recursive algorithm for the portfolio loss distribution similar, in spirit, to the one commonly used for CID [conditionally independent] models. We model default events under a contagion mechanism which is the result of two independent components - an infection attempt generated by defaulting entities and a failed defence from healthy ones. A recursive algorithm for the calculation of the portfolio loss distribution is presented that shows a good level of computational tractability even when an heterogeneous set of names is considered. We apply it to the pricing and hedging of CDO instruments and compare its performance to the standard one factor Gaussian model.",Pricing synthetic CDOs under infectious defaults with immunization,"[53607, 47415, 3141]",412,"[5, 45, 126]",1635,Quantitative methods for systemic and climate risk,9,4,51,Risk management in finance,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,M5 [building - 101],"['Algorithms', 'Financial Modelling', 'Risk Analysis and Management']",MC-51
"In this work we focus on the concept of optimization-by-continuation as a strategy for solving a set of optimization problems in about the time it would take to solve a single instance of them. Each cost function is a different linear combination of two convex terms, one differentiable and the other prox-simple. Such optimization problems occur frequently in the numerical solution of inverse problems [data misfit term plus penalty or constraint term], and the relative weight of both terms is often not known in advance. The algorithm's special feature lies in its ability to approximate, in a single iteration run, the minimizers of the cost function for many different values of the parameters determining the relative weight of the two terms. We also discuss the same problem in the presence of a third term in the cost function which is the combination of a prox-simple function and a linear map. As a special case, one recovers a generalization of the primal-dual algorithm of Chambolle and Pock.",Convergence analysis of optimization-by-continuation proximal gradient algorithm and some primal-dual extensions,[50492],421,"[21, 63, 5]",1638,Optimization and learning for data science and imaging [Part I],84,2,34,Advances in large scale nonlinear optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,43 [building - 303A],"['Convex Optimization', 'Large Scale Optimization', 'Algorithms']",MA-34
"We present a method for solving a large-scale stochastic capacity expansion problem which explicitly considers reliability constraints, in particular constraints on expected energy not served [EENS]. Our method tackles this problem by relaxing the EENS constraints. We solve the relaxed formulation in an iterative manner, using a subgradient-based method. Each iteration requires the solution of a stochastic capacity expansion problem, for which we implement subgradient and cutting plane decomposition schemes that have recently been introduced in the literature and have proven very effective in solving large instances of such models. We implement the proposed methodology on the Economic Viability Assessment [EVA] model that is used by ENTSO-e in the annual European Resource Adequacy Assessment [ERAA]. The EVA is extended to include reliability constraints. We are able to solve this “extended” EVA and obtain the least-cost capacity expansion plan for meeting specific reliability requirements per zone, fully accounting for uncertainty. Our approach also allows us to attain the shadow price of load shedding by zone [through the Lagrangian multipliers of the relaxed constraints] which can serve as an indication of the zone-specific VOLL or price cap that could in principle deliver the appropriate level of investments within an energy-only market.",Capacity Expansion Planning under Uncertainty subject to EENS Constraints,"[77330, 71097, 35985]",177,"[12, 136, 102]",1639,Long-term energy system planning,22,10,09,Energy Markets,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,10 [building - 116],"['Capacity Planning', 'Stochastic Optimization', 'Parallel Algorithms and Implementation']",TD-09
"This paper explores risk-sensitive control in discrete-time portfolio choice. The investor considers stochastic factors and price impacts, including permanent and temporary effects. Ambiguity aversion towards model estimation errors of asset returns and stochastic factors is incorporated. The objective is to maximize the investor's preference for local mean-variance on investment returns, while accounting for mark-to-market profits and losses, execution costs, and penalties related to model estimation errors. Our study reveals that the investor's trading strategy may differ from the aim portfolio. Ambiguity-averse investors tend to trade more conservatively compared to non-robust cases. Trading decisions are influenced by factors like higher permanent price impacts and lower market resilience rates, with trading speed linked to ambiguity aversion. Simulation studies validate the effectiveness of the robust trading strategy, outperforming the non-robust approach. Mark-to-market profits and losses from permanent price impacts enhance the net Sharpe ratio. Incorporating risk-sensitive control techniques in portfolio choice under uncertainty and ambiguity is crucial.",Risk-Sensitive Control in Portfolio Choice - Incorporating Ambiguity Aversion and Stochastic Factors,"[65052, 77336, 77338, 77340]",276,"[127, 20, 45]",1640,Optimal Portfolio Strategies,4,13,02,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,Glassalen [building - 101],"['Robust Optimization', 'Control Theory', 'Financial Modelling']",WB-02
"Robust optimization is an established technique for handling data uncertainty in optimization problems. However, most of the results refers to problems with a single decision maker, whereas less results are available for frameworks that include multiple decision makers. These settings are very common in several applied contexts, where different stakeholders may be involved in the decision making problem, with various roles and decision power. This is the case for example, for problems arising in healthcare, urban planning or shift scheduling problems. The present study aims at generalizing the results that are known for robust optimization problems with a single decision maker to non-cooperative games, where multiple decision makers [players] are present. In particular, we focus on the Generalized Nash Equilibrium Problem [GNEP], where both the objective function and the feasible region of each player are affected by the actions of the other players. The robust version of a GNEP with uncertain parameters is defined and its continuity, differentiability, convexity and monotonicity properties are investigated. Moreover, an existence result of robust equilibria is given. In the case of linear or quadratic dependence of the objective functions and constraints on the uncertain parameters, equivalent reformulations of the robust GNEP are provided.",Robust Generalized Nash Equilibria,"[9819, 35427]",549,"[50, 127]",1642,Recent advances on Variational Inequalities and Equilibrium Problems II,51,14,43,Variational Inequalities and Equilibrium Problems - From Theoretical Advances to Real World Applications,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,99 [building - 306],"['Game Theory', 'Robust Optimization']",WC-43
"We study the vaccine distribution problem over a three-echelon network during a sudden pandemic outbreak. We consider uncertainties in vaccine supply due to delays in production, disruption in global supply chain or lockdown policies in the early phases of the pandemic outbreak. We also consider limited resources including cold trucks for vaccine transportation and healthcare workers for vaccine administration. The problem is to dynamically allocate these limited resources over the network such that vaccine can be transported and administrated in a timely and cost-effective manner. We formulate a dynamic programming model for this problem and propose solution algorithms based on approximate dynamic programming. We conduct a case study on a vaccine distribution network in Iran and report computational results that demonstrate the effectiveness of our solution approach over two benchmark approaches. ",An Approximate Dynamic Programming Approach for Managing Vaccine Distribution with Uncertain Supply and Limited Resources,"[77327, 77321]",774,"[138, 58, 136]",1644,Infectious diseases and pandemics 2,38,14,21,OR in Humanitarian Operations [HOpe],"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,49 [building - 116],"['Supply Chain Management', 'Humanitarian Applications', 'Stochastic Optimization']",WC-21
"Optimization applications combine technology and expertise from many different areas, including model-building, algorithms, and data-handling.  Often, the gathering, pre/post-processing, and visualization of the data is done by a diverse organization-spanning group that shares a common bond - their skill in and appreciation for Python and the vast array of available packages it provides. For this reason, GAMS offers a new comfortable way to integrate with Python on the data-handling and modeling side. In this talk, we will explore the benefits of our Python library GAMSPy.",GAMSPy - The Best of Both Worlds - Integrating Python and GAMS,[52433],703,"[134, 151, 84]",1646,Python Modeling Tools,76,10,30,Software for Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,53 [building - 208],"['Software', 'Practice of OR', 'Optimization Modeling']",TD-30
"By using a variety of electrical appliances with smart control options, households might play a pivotal role in flexibility provision in the energy market. To exploit the flexibility potential in a most efficient manner from a system perspective, no distortionary impacts should affect operational and investment decisions. To date, most electricity tariffs do not reflect time-dependency in market prices. As a result, households’ investment and operating decisions are based on distorted cost considerations. Scenarios become even more complex when EV owner install a second meter for wallboxes. In this case, two different electricity tariffs may be concluded. We elaborate on the distortionary impacts of tariffs on households‘ investment and operational decisions under the current regulatory framework. Here we focus on investments in PV and battery systems under different tariff designs. Moreover, we exploit the case when a second meter with an additional tariff is in use for EV charging. We formulate the individual optimization problems corresponding to different types of households facing different electricity tariffs. Thereby households are differentiated according to their characteristics, i.e. [potential] Prosumer and EV ownership. A stylized wholesale market will allow to elaborate feedback effects from household decisions. The indvidual problems are combined into an overarching MCP problem.",Distortions in energy related decision-making at households – An MCP approach incorporating system perspective,[69728],343,"[36, 93, 37]",1647,Uncertainties in the Energy Transition,22,5,09,Energy Markets,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,10 [building - 116],"['Electricity Markets', 'OR in Energy', 'Energy Policy and Planning']",MD-09
"
The measurement of power for players in cooperative games traditionally focuses on the effort that single players exert on every coalition of other players. A more recent stream of works focuses on the evaluation of the power of groups [see Flores R., Molina E., Tejada J. [2019], Evaluating groups with the generalized Shapley value 4OR, 17 [2], 141-172] together with an estimation of their interactions, defined in Grabisch  M., Roubens M. [1999], An axiomatic approach to the concept of interaction among players in cooperative games. International Journal of Game Theory  28 - 547-565,  as the average of finite order differences for the players in the coalition.

We examine the use of such indexes in the context of monotone simple games and we verify that the  original interaction index captures the group’s relationship in terms of collaboration and competition for two players, but fails to do so for larger groups.

We therefore define a collaboration-competition index that is based on the notion of essential criticality and coincides with Grabisch and Rouben’s interaction index for two players. We analyze its properties and we put the index at work with some applications.
",A Coopetition Index for Coalitions in Simple Monotone Games.,"[67773, 5048]",342,"[50, 0]",1648,"Game Theory, Solutions and Structures III",88,4,36,"Game Theory, Solutions and Structures","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,32 [building - 306],['Game Theory'],MC-36
"This work presents a new hybrid classical-quantum approach to solve Mixed Integer Linear Programming [MILP] using neutral atom quantum computations. We apply Benders Decomposition [BD] to segment MILPs into a master problem [MP] and a subproblem, where the MP is addressed using a neutral-atom device, after being transformed into a Quadratic Unconstrained Binary Optimization [QUBO] model. To solve the QUBO, we develop a heuristic for atom register embedding and apply Quantum Approximate Optimization Algorithm for pulse shaping. In addition, we implement a Proof of Concept that outperforms existing solutions. We also conduct preliminary numerical results, outperforming classical BD approaches where the MP is solved using simulated annealing. To the best of our knowledge, this work is the first to utilize a neutral atom quantum processor in developing an automated, problem-agnostic framework for solving MILPs through BD.
Looking to the future, our objective is to scale and diversify our instances. This presents difficulties related to the limits on the number of qubits and the convergence of the algorithm. To overcome resource limitations, one can set a maximum number of qubits and implement an iterative process that guarantees the quality of the solution. When facing issues with convergence, we can consider multi-cuts, which involves generating multiple solutions for the MP and selecting a specific subset of Benders cuts to not surcharge the MP.
",Mixed Integer Linear Programming Solver Using Benders Decomposition Assisted by Neutral Atom Quantum Processor,"[77155, 76189, 77343]",374,"[111, 110, 131]",1650,Hybrid Classical-Quantum Algorithms,83,2,42,Quantum Computing Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,98 [building - 306],"['Programming, Mixed-Integer', 'Programming, Linear', 'Simulation']",MA-42
"Opinion surveys can contain closed questions to which respondents can give multiple answers. We propose to model these data as networks in which vertices are the eligible items and arcs are the respondents. This representation opens up the possibility of using complex networks methodologies to retrieve information and most prominently, the possibility
of using clustering/community detection techniques to reduce data complexity. We will take advantage of the implicit null hypothesis of the modularity function, namely, that items are chosen without any preferential pairing, to show how the hypothesis can be tested through the usual calculation of p-values. We illustrate the methodology with an application
to Eurobarometer data. There, a question about national concerns can receive up to two selections. We will show that community clustering groups together concerns that can be interpreted in a consistent way and in general terms, such as Economy, or Security or Welfare issues. Moreover, we will show how different society groups are worried by different class of items.",A network clustering model for multiple selection questions in opinion surveys,"[5405, 5876]",517,"[7, 0]",1651,"Advancements of OR-analytics in statistics, machine learning and data science 8",16,12,28,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,065 [building - 208],['Analytics and Data Science'],WA-28
"We will give an overview of the latest enhancements, the newest features, and the most recent performance improvements in the FICO Xpress Solver for mixed-integer linear and nonlinear optimization problems. These include new heuristics, cutting and branching techniques, an augmented API, and updates to our global MINLP solver.",What’s New in FICO Xpress Solver?,[16880],237,"[72, 111, 134]",1654,MIP Solvers,76,2,30,Software for Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,53 [building - 208],"['Mathematical Programming', 'Programming, Mixed-Integer', 'Software']",MA-30
"In the treatment allocation problem, a decision maker needs to decide who will receive the treatment. To define the individuals that benefit the most, a machine learning procedure is often used to predict the treatment effect. With these predictions, a model is built to make the allocation decisions. Unfortunately, the data used to build such a model may be discriminating against a group defined by a sensitive attribute such as gender or age. If not carefully trained, the model may provide unfair results, unequally allocating treatment to individuals in the sensitive and non-sensitive groups.
In this presentation, I introduce an unfairness measure that can be applied to treatment allocation problems. I propose to measure unfairness as the difference between the average treatment effects in the sensitive group and the non-sensitive group. I introduce a Mathematical Optimization model to have accurate treatment effect predictions and a good level of fairness, which will be the basis for the treatment allocation in forthcoming individuals. ",Fair treatment allocation via tree ensembles,[67331],121,"[66, 21, 8]",1655,On Mathematical Optimization for Explainable and Fair Machine Learning,15,3,27,Mathematical Optimization for XAI,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,047 [building - 208],"['Machine Learning', 'Convex Optimization', 'Artificial Intelligence']",MB-27
"We use a novel multivariate sensitivity analysis technique based on Optimal Transport to quantify the sources of uncertainty in future CO2 emissions to help support robust decision-making in climate policy. We apply these methods to an open-source climate-economy model [RICE50+], using the most up-to-date input distributions and long-term projections of key demographic and socio-economic drivers. We assess different scenarios to address different possible futures. We performed the same analysis on the global and regional levels, finding similar patterns but also regional specificities. We show the feasibility of applying new indices to a model with many correlated inputs and a time and region dependent output such as CO2 emissions pathways.",Multivariate global sensitivity analysis of a coupled climate-economy model,"[76628, 18483, 58273, 77346]",558,"[126, 26, 15]",1656,Behavioral Decision Analysis I,13,3,11,Behavioural OR,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,12 [building - 116],"['Risk Analysis and Management', 'Decision Support Systems', 'Complex Societal Problems']",MB-11
"Access to essential medicines and other health products remains a challenge in low- and middle-income countries, particularly in rural areas. Community health entrepreneurs [CHEs] play a key role in bridging this gap by selling health products in small rural stores. However, CHEs frequently experience stock-outs due to cash constraints, leading to lost sales. How to relieve cash constraints in rural areas is currently unclear. We address this question by conducting a field experiment that tests two interventions with 458 CHEs in Kenya in collaboration with social enterprise Healthy Entrepreneurs. The first intervention involved setting up stock-hubs. These are are small consignment stock locations where CHEs can replenish their inventory on-the-spot, thereby reducing the need for cash-on-hand. The second intervention is a cashflow game designed to help CHEs realize the value of retaining money in their business, thereby improving their cashflow resulting from increased sales. Although both interventions yielded positive effects on CHE sales, they were not statistically significant. The cost of accessing the hub and informational barriers limited CHEs’ ability to fully utilize the stock-hubs. The results of the cashflow game highlighted the need for additional measures beyond raising awareness to address cash constraints effectively. This study provides valuable new insights into addressing cash constraints in areas with high resupply costs and long resupply intervals. ",Health Product Availability in the Presence of Cash Constraints - A Study of Community Health Entrepreneurs in Rural Kenya ,"[77345, 67803, 27297]",556,"[56, 29, 138]",1657,Complex societal problems,38,10,21,OR in Humanitarian Operations [HOpe],"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,49 [building - 116],"['Health Care', 'Development', 'Supply Chain Management']",TD-21
"Despite the increasing recognition of local actors as pivotal agents for sustainability in food chains, numerous innovative products, particularly from startups, encounter difficulties in reaching consumers. Offering such products can facilitate consumer interest and position retailers as enablers of sustainable food systems. However, listing a new product can not only bring competitive advantages but also lead to risks, e.g., due to potential losses if the product is rejected. Retailers often have to rely on intuition and estimate a product's potential due to unknown rejection probabilities. This research delves into the early stages of product diffusion by examining the effectiveness of traditional stationary brick-and-mortar stores in contrast to the agility of mobile pop-up stores. Utilizing an agent-based simulation [ABS] model, we replicate the product diffusion process and employ a regression model to determine promising store locations. Through our investigation, we unveil how combining ABS and machine learning procedures can provide decision support for both retailers and MSMEs. For a limited number of stores, introducing a single mobile store can considerably expedite product diffusion. Generally, using mobile stores can enable a more pronounced market penetration, whereas stationary stores yield higher sales due to recurrent purchases. Furthermore, our findings show how the probability of rejection influences the shape of the product diffusion curve.",Decision support for product diffusion in retail - The impact of geospatial and temporal access.,"[70428, 41085]",427,"[3, 26, 131]",1658,Retail Operations and Marketing,30,8,50,Retail Operations,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,M2 [building - 101],"['Agent Systems', 'Decision Support Systems', 'Simulation']",TB-50
"We address the joint order batching and picker routing problem [JOBPRP]. 
Given a set of orders to collect, each order being composed of several items located in a warehouse, the JOBPRP consists in batching orders into capacitated trolleys such that the total travelled distance to collect all the items of the orders is minimized.

We are interested in algorithms based on column generation. 
A bottleneck of such approaches is to efficiently solve the pricing problem, that is a profitable order picking problem where all products of an order  must be collected together in the same tour and a price is gained for collecting an order. 
At the core of this pricing problem lies a profitable traveling salesman problem [TSP] in a
rectangular warehouse.
Dynamic programming [DP] approaches can solve efficiently this TSP for such layouts.

In this work, we extend the DP approach to the profitable variant. 
The directed acyclic graph underlying the DP can be used to provide a powerful Mixed Integer Programming [MIP]  formulation where the order requirements [all products of an order must be collected together] can be taken into account with linking constraints. 
Such MIP formulation has been studied for the special case of warehouses with a single bloc of aisles.
We generalize to the case with several blocks, and propose state space restrictions of the DP to heuristically solve the pricing problem.

The proposed approach is validated on instances from the literature.",Dynamic programming state space restrictions to solve the joint order batching and picker routing problem via column generation,"[54943, 37984, 31689]",198,"[13, 65]",1662,Combinatorial optimization approaches for freight deliveries and home services,64,2,52,Combinatorial Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,8003 [building - 202],"['Column Generation', 'Logistics']",MA-52
"Value-Based Healthcare [VBHC] is a framework that aims to shift the focus of healthcare systems from increasing volume to enhancing value, which is defined as the ratio of patient-relevant health outcomes to the costs of achieving them. This framework can help to address the rising healthcare expenditures by encouraging health facilities to organize around the patient and pursue the goal of maximizing patient value. However, measuring patient value is still challenging due to its multidimensional nature, impeding the implementation of VBHC. In this research, we therefore propose a four-step value measurement roadmap, which leverages operations research techniques to convert the value equation into a single value score for each patient, using Data Envelopment Analysis [DEA] as the main tool. This novel application of DEA enables a holistic benchmarking of patients by directly linking outcomes to costs, unveiling the relative value that is created. We illustrate how the roadmap can be customized and applied to different health contexts, using the case of psoriasis as an example. After collecting individual health outcomes and calculating treatment costs for 109 psoriatic patients, we construct a value frontier which reveals substantial differences in created value within the patient population. Subsequent detailed analysis of the value scores offers insights into the drivers of value creation, supporting health professionals to optimize healthcare delivery.",A Four-Step Roadmap for Measuring and Maximizing Patient Value in Healthcare Using Data Envelopment Analysis,"[73751, 66686, 74167, 2640, 69540, 74168]",970,"[56, 24]",1663,DEA in healthcare,3,3,17,OR in Health Services [ORAHS],"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,40 [building - 116],"['Health Care', 'Data Envelopment Analysis']",MB-17
"This paper explores the fusion of BPM and information entropy analysis. Its objective is threefold - to uncover potential synergies in enhancing organizational efficiency, present novel perspectives about decision-making processes, and introduce a novel scientific sub-field termed Managerial Econophysics. BPM, integrating IT and management, optimizes workflows, reduces costs, and boosts productivity. The review traces BPM's evolution from quality control to its modern manifestations. It addresses challenges like randomness and complexity, proposing entropy analysis to tackle uncertainty. Entropy metrics aid in performance optimization and resource allocation. The synthesis extends to econophysics, linking financial economics with entropy. Notable figures like Popovic and Shannon are discussed, highlighting entropy's role in financial market analysis. The broader scope of econophysics, including social systems, is explored. This paper argues for additional research into the application of econophysics in organizational settings, bridging the gap between economics, management, and physics. It advocates for integrating econophysics and information entropy analysis into managerial science, creating innovative approaches to process management. Just as natural vectors combine to form geometric spaces, diverse scientific disciplines can merge to forge new scientific frontiers. Managerial Econophysics emerges as an epistemological frontier in process management.",Managerial econophysics unveiled - a comprehensive literature review on the amalgamation of business process management [BPM] and information entropy analysis,"[77350, 69558]",60,"[19, 26, 67]",1666,Applications of knowledge and innovation in finance,53,4,08,AI & Innovation in Sustainable Finance,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,1020 [building - 202],"['Continuous Optimization', 'Decision Support Systems', 'Management Information Systems']",MC-08
"For manufacturers of high-tech products, such as aircraft, lithography machines, etc., it is of key importance that all required components are available at the time of assembly. This is a major challenge, as the number of components required for assembling such complex systems is enormous and most of them are sourced at external suppliers. For a supplier, the risk of over-investing in costly capacity is high, while the lost revenue of one missing component is much lower than the value of the manufacturer's end-product. Therefore, each supplier will trade off the cost of investing in additional capacity against the risk of being unable to deliver. Ultimately, the overall delay in delivery of the end-product is determined by the supplier with the largest backlog. To reduce the risk of component shortages, the manufacturer aims to motivate suppliers to invest in excess production capacity, for example by charging a fee for each time unit the supplier delivers past a previously agreed lead time. We model a manufacturing system consisting of a manufacturer and a large number of suppliers, each producing a single component that is required in the manufacturer's assembly process of the end-product. We consider the decentralized optimization problem where each supplier determines their own optimal service rate. We analyze how the contract parameters can be set such that the suppliers choose the service rates that are asymptotically optimal for the supply chain as a whole.",Achieving emergent symmetry in high tech supply chains,"[67939, 71101, 77353, 68520, 72282]",159,"[136, 63, 69]",1667,Stochastic Models in Manufacturing,50,5,39,Stochastic Modelling,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,35 [building - 306],"['Stochastic Optimization', 'Large Scale Optimization', 'Manufacturing']",MD-39
"Production planning systems are an integral component in human planners’ decisions and, ultimately, joint human-system performance. Although analytically superior systems are known to generate better suggestions, it remains unclear how human planners react to the suggestions and, thus, whether the resulting joint performance is enhanced when using these systems. We examine the impact of different system suggestions on joint performance in a rolling horizon setting with two products and three components under demand uncertainty. We test this in a laboratory environment. We compare human planners’ judgemental adjustments and joint performance using system suggestions with different analytical performance [control, Material Requirement Planning, Synchronized Base Stock]. We understand that human planners’ deluded self-confidence and a desire for control over the system result in adjusting system suggestions regardless of the analytical superiority of the system, which constantly degrades performance. This effect is stronger for systems with superior analytical performance. Additionally, we understand that human planners’ judgemental adjustment patterns are different across three systems. We replicate these findings in a field experiment with actual planners. ",A Behavioural Investigation of Feasibility and Demand Uncertainty in Production Planning Systems,"[76934, 72887, 36644, 21084]",558,"[10, 26, 105]",1669,Behavioral Decision Analysis I,13,3,11,Behavioural OR,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,12 [building - 116],"['Behavioural OR', 'Decision Support Systems', 'Production and Inventory Systems']",MB-11
"The number of initiatives involving volunteers contributing to initial response in emergency situations is increasing. Determining which volunteers to alert, and assigning tasks to them, is not a simple question to answer, especially when there are many different types of tasks. Therefore, we develop dynamic models and strategies for task assignment and alerting of volunteer responders. Additionally, we evaluate new alerting strategies using simulation [computer simulation and real-world simulation] to better understand their impact, enabling us to enhance the developed strategies.
Drones can also be used to transport equipment to an incident site; for example, drones are used to transport defibrillators to suspected cardiac arrest cases in the Västra Götaland region in Sweden. Since it is crucial for the operation where drones start their journey and that someone is available to use the defibrillator when it reaches the patient, we have developed models for locating drone stations, combining them with models for alerting volunteers. The results show that by joint planning of drones and volunteers, it may be possible to improve the survival probability from out-of-hospital cardiac arrest.
",Optimized planning of volunteer first response resources to emergencies ,[62485],954,"[64, 56, 30]",1670,EMS and crisis logistics,3,3,10,OR in Health Services [ORAHS],"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,11 [building - 116],"['Location', 'Health Care', 'Disaster and Crisis Management']",MB-10
"Proximal-gradient methods are iterative first-order techniques that prove useful in various applications, such as image deblurring and denoising. Alongside the potential issue of slow convergence, one crucial challenge is the assumption that the proximal operator is computable in closed form. To tackle this, adopting a variable metric approach and integrating an extrapolation step can enhance the efficiency of these methods. However, a significant concern arises from the inexact computation of the proximal operator, often addressed through a nested primal-dual solver.

In this work, we introduce a nested primal-dual method designed for efficiently solving regularized convex optimization problems. Our proposed method approximates a variable metric proximal-gradient step with extrapolation by executing a predetermined number of primal-dual iterates while adjusting the step length parameter through an appropriate backtracking procedure.

Furthermore, we investigate the numerical performance of our proposed method on an image deblurring problem, defining a scaling matrix inspired by the Iterated Tikhonov method. The numerical results demonstrate that the combination of such scaling matrices and Nesterov-like extrapolation parameters yields an effective acceleration towards the solution of the initial problem.",A Nested Primal–Dual Iterated Tikhonov Method for Regularized Convex Optimization,"[77352, 77380, 77378, 77379, 67214]",422,"[21, 63]",1671,Optimization and learning for data science and imaging [Part II],84,3,34,Advances in large scale nonlinear optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,43 [building - 303A],"['Convex Optimization', 'Large Scale Optimization']",MB-34
"Multiple vehicle types typically use the same roads simultaneously. These different types do not only differ in attainable speeds but also in feasible accelerations and decelerations. For instance, freight vehicles accelerate at a slower pace than normal vehicles leading to slower queue dissipation at for instance intersections and creating gaps in these queues. In this paper, we propose a novel discrete-time FCFS queueing model with two types of traffic [freight and regular vehicles for instance] where service times depend on whether freight vehicles are present or not at the start of these service times. We analyze the delay of individual vehicles of both types and characterize gap lengths created by different accelerations of different vehicle types.",Analysis of Waiting Times in a Novel Two-Class Queueing Model Motivated by Road Traffic,"[70751, 77358, 59550]",15,"[121, 143, 65]",1672,Advances in Stochastic Modelling and Applied Probability Ι,47,3,40,Advances in Stochastic Modelling and Learning Methods,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,96 [building - 306],"['Queuing Systems', 'Transportation', 'Logistics']",MB-40
"High-tech manufacturers produce and assemble state-of-the art products that consist of many complex components sourced from dozens of suppliers. Examples are the production of lithography machines by ASML or the production of aircraft by Boeing or Airbus. To assemble these high-tech end-products and deliver them to the customers, it is important that all required components are available as shortage of a single component may lead to costly delivery delays of the end-product and large inventories of other components. This requires sufficient capacity of the suppliers, who face their own trade-offs and may be hesitant to invest in too much capacity, or high inventory buffers, which is also costly and increases the risk of obsolescence. In this talk, we will discuss such challenges, resulting trade-offs and their effects on optimizing the decisions.",Complexities in high-tech manufacturing,[67939],915,"[135, 69]",1673,YW4OR_2,39,13,12,WISDOM - Women in OR,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,13 [building - 116],"['Stochastic Models', 'Manufacturing']",WB-12
"In this paper, we present how to apply the Deck of Cards Method [DCM] for ordinal regression. While the DCM is very well-known and applied in Multiple Criteria Decision Aiding, here, we apply it to assign a value to each alternative evaluated on a set of criteria hierarchically structured. The Decision Maker can provide precise or imprecise information at different levels of the hierarchy of criteria using the classical DCM framework. This information is therefore used to infer a value function compatible with it. The compatible value function can be a simple weighted sum, a piecewise linear value function, a general monotonic value function, or a Choquet integral. To provide robust recommendations to the Decision Maker, the Robust Ordinal Regression and the Stochastic Multicriteria Acceptability Analysis are applied. Even if in different ways, both of them take into account the whole set of models compatible with the preference information provided by the Decision Maker. The applicability of the new proposal is shown by an example in which Italian regions are evaluated on criteria belonging to Circular Economy, Innovation Driven and Specialization macro-criteria.","Deck of Cards method for Hierarchical, Robust and Stochastic Ordinal Regression","[76743, 36935, 5550]",110,"[26, 25, 77]",1675,"MCDA and Composite Indicators - Issues, Advances and Applications 1",44,14,44,Multiple Criteria Decision Analysis,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,20 [building - 324],"['Decision Support Systems', 'Decision Analysis', 'Multi-Objective Decision Making']",WC-44
"Air cargo typically moves through airports that serve as hubs. A container or pallet, usually referred to as a unit load device [ULD], arriving at such a hub often holds shipments bound for various destinations. Consequently, its contents must be unloaded and moved into storage [ULD break-down]. Once enough shipments for a single destination have accumulated, a new outbound ULD is loaded [ULD build-up]. Both the build-up and break-down procedures need to be executed on a limited number of workstations and within the strict timeframe dictated by the flight schedule. Resource conflicts, therefore, need to be prevented by a careful scheduling of build-up and break-down processes.

We explore the structure and challenges of these interdependent scheduling problems, both from a mathematical and industry viewpoint, and discuss appropriate solution strategies. We find that breakdown scheduling is easily solved by simple mixed-integer programming [MIP] models on realistic datasets. The situation differs for build-up scheduling, where a desire to build up ULDs in batches introduces an additional objective. Here, we discuss a tailored approach based on logic-based Benders decomposition that decomposes the time horizon based on cargo availability. Finally, we discuss how to integrate both break-down and build-up scheduling into a unified network design model.
",Scheduling ULD loading operations in air cargo hubs,"[69679, 14923, 29257, 70097]",349,"[4, 129, 65]",1677,Resource constrained scheduling,35,10,60,Project Management and Scheduling,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S09 [building - 101],"['Airline Applications', 'Scheduling', 'Logistics']",TD-60
"This study addresses the dynamics of an emergency medical service [EMS] system during the response to mass casualty incidents [MCIs], where timely and efficient decision-making is crucial to save more lives. The key decisions to efficiently respond to an MCI with limited EMS resources, such as ambulances and hospitals, include ambulance allocation to the MCI, balancing with response capacity for routine EMS demands, and hospital transportation for MCI victims, thereby avoiding congestion at the hospitals. A mathematical model is developed to represent the relationships between the decisions involved in the MCI response phase and the relevant EMS performance metrics. Based on the model, numerical analysis is conducted with the aim of offering insights into protocol and policy design for EMS systems during the MCI response.",Optimizing Emergency Medical Service Capacity to a Mass Casualty Incident - Balancing with Routine Emergency Medical Service Demands,"[77356, 76784]",954,"[151, 56, 137]",1678,EMS and crisis logistics,3,3,10,OR in Health Services [ORAHS],"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,11 [building - 116],"['Practice of OR', 'Health Care', 'Strategic Planning and Management']",MB-10
"In recent years, the adoption of line-search techniques within incremental gradient-based methods for finite-sums problems has gathered considerable interest among researchers.
These techniques enable the use of substantially larger step-sizes while maintaining robust convergence properties and empirically showing a reduction in the number of iterations required to achieve high-quality solutions. Most recent research focused on incorporating line-searches within Stochastic Gradient Descent [SGD], as the usage of a descent direction for the mini-batch objective is essential to ensure the line-search terminates in a finite number of steps.

In this talk we discuss the nontrivial integration of these line-search techniques within algorithmic frameworks employing Polyak's momentum terms. In particular, we are interested in structured ways to combine stochastic gradients and momentum to obtain a suitable search direction together with a proper initial step-size for the line search. 
We then present a computational comparison, carried out on both convex and nonconvex test problems, concerning different viable options to achieve this goal. We thus offer insights into the potential strengths and drawbacks of the considered approaches, with respect to other state-of-the-art methods.",Stochastic gradient descent with momentum and line-searches,"[77288, 66630]",305,"[63, 19, 113]",1686,Algorithmic Advances in Large Scale Nonconvex Optimization,84,3,32,Advances in large scale nonlinear optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,41 [building - 303A],"['Large Scale Optimization', 'Continuous Optimization', 'Programming, Nonlinear']",MB-32
"Recent calls for more sustainable practices in the food industry, eating healthier, and the desire to support regional farmers and rural communities, have seen an increase in the demand for local food. Particularly in the gastronomy sector, many restaurants have started offering more local food as a result of such trends. Despite various potential benefits of offering local food, real-world implementation settings often fail due to challenges in logistics operations. To enable efficient and sustainable logistics operations in the sale of local food in the gastronomy sector, this work investigates innovative and sustainable logistics concepts through the development of a simulation and optimization-based decision support system [DSS].  Using the DSS, various logistics concepts are implemented and tested on two pilot areas in Bavaria, Germany and one pilot area in Marche, Italy. Preliminary results highlight potential benefits translated into shorter distances, better vehicle utilization, improved food quality, and lower lead times.",Simulation Optimization-based Decision Support System to Enable More Local Food in the Gastronomy Sector,"[70397, 41085, 77480, 78584]",485,"[89, 26, 65]",1689,Sustainable Food and Health Care Logistics,19,10,24,Sustainable Supply Chains,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,83 [building - 116],"['OR in Agriculture', 'Decision Support Systems', 'Logistics']",TD-24
"The increased impact of extreme weather events and draughts has prompted
the rapid growth of the water market. This paper analyzes the optimal operation of a reservoir that generates electricity and manages the water by
trading water rights. We extend the framework introduced in a recent paper by Figuerola Ferretti, Schwartz, and Segarra [2023] to include the water
price process in a model that accounts for water inventory and the electricity
price as two independent price processes. The model is implemented under
the stochastic optimal control approach and calibrated using monthly data
for a reservoir in the estate of California. The water price dynamics includes the dependence on the California Drought Severity Index. Results show that the underlying water and electricity price dynamics exhibit a high degree of uncertainty and seasonality. They also demonstrate that, under the calibrated model parameters on average for the parameters used, around 25% of the revenue generated by the reservoir arises from the revenue obtained from selling water rights. The water contribution to revenue generation would increase under enhanced severity of climate change.
",Water as a commodity in hydropower generation,"[76093, 5628]",190,"[40, 37, 93]",1690,Natural Resource Management and Commodity Markets,4,4,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S14 [building - 101],"['Environmental Management', 'Energy Policy and Planning', 'OR in Energy']",MC-63
"Rising environmental awareness among consumers known as Consumer Environmental Awareness, led to the entry of green products into the car market. This led to the issue of competition between these green and non-green products which has been the subject of research lately. We focus on the pricing and the degree of greenness of a product in competition with an established non-green product while considering government intervention.
We build a model with two players, Government and Manufacturer, to focus on government intervention [taxes and subsidies] in the context of transition to Alternative Fuel Vehicles [AFV] under two scenarios. In one there is no Distributive Justice [DJ], and in the other DJ enters by modifying the social pillar in the government’s utility to maximize consumer’s access to vehicles. The government’s tax and subsidy override the manufacturer’s prices when determining the dynamics highlighting the decisiveness of government intervention in such a setting. The dynamics of the environmental and economic pillars show tradeoffs which are partially alleviated when we consider DJ. We show that when introducing DJ into the model, there is no Pareto front where all three pillars improve simultaneously and the government’s utility remains more or less the same. The manufacturer’s profits and consumer surplus exhibit a harmonious relationship whereby they increase together. Finally, the demand for AFV is always cannibalizing the demand for ICE.",Optimal Manufacturer Strategies and Government Intervention for AFV Transition under a Distributive Justice Perspective,[61607],560,"[50, 143, 100]",1691,Game Theory in Sustainable Supply Chains,19,14,24,Sustainable Supply Chains,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,83 [building - 116],"['Game Theory', 'Transportation', 'OR in Sustainability']",WC-24
"This study tackles challenges of irregular weights in Soft Cheese manufacturing, aiming to standardise weights and reduce giveaways. Soft Cheese [British Brie] is produced by placing wet curds into moulds to form specific shapes and weights without the opportunity for size adjustment post-formation. Initial hypotheses pointed to differences in acidification properties between outer and innermost cheeses in the mould tray arrangement as the main cause of weight variations. Theoretical experiments on Equivalent Classes generated analytical datasets. Investigative analytics and experiments identified causes of intra-tray and inter-tray weight variations. Using Python programming language; Descriptive Analytics visualisations, Hypothesis Testing, and Machine Learning were conducted, applying Causal Inference to quantify the effects of factors influencing cheese weight. Implementation of recommendations, including standardised curd tipping and improved mould trays, cut giveaways from 21% to 9% yielding substantial savings. Furthermore, variable height adjustments in the Automated Filling Equipment [AFE], based on identified weight distribution patterns, were observed to address inter-tray variations, more crucial with larger vats. This multi-faceted initiative combines experimentation, analytics, and industrial optimisation, showcasing an interdisciplinary approach between Operations Research and Data Science to address a key concern of overproduction in food manufacturing.",Addressing Irregular Weights in Soft Cheese Manufacturing - A Multifaceted Approach Integrating Analytics and Process Optimisation,"[76899, 77363, 58510, 78598]",336,"[7, 89, 59]",1692,Analytics for Combinatorial Problems from Health Care to the Food Industry,17,10,31,Analytics,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,54 [building - 208],"['Analytics and Data Science', 'OR in Agriculture', 'Industrial Optimization']",TD-31
"In this research we assess the performance trends of the OECD education systems in terms of equity. To do that, we apply a novel methodology that combines a robust conditional Benefit of the Doubt [BoD] model with Malmquist indexes. This approach allows us to compare the performance of education systems over time based on a comprehensive indicator, without losing its multidimensional nature and respecting the different priorities and peculiarities of each educational system, making the comparison between countries fairer. This assessment enables us to identify which educational systems have improved their performance in equity over time [best practices] and which countries have a greater room for improvement. In addition, we assess the impact of different institutional factors and educational policies [e.g., early tracking, school choice systems, ability grouping and retention grade practices, scholarship and voucher programs, among others] on equity performance, in order to identify effective interventions to promote more equitable education systems.",Performance Trends in Educational Equity in the OECD - An International Assessment using Malmquist Indices,"[60983, 67691, 67967, 47633]",939,"[24, 92, 110]",1694,DEA applications in Education and Health II,89,7,48,Data Envelopment Analysis and its Application,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,60 [building - 324],"['Data Envelopment Analysis', 'OR in Education', 'Programming, Linear']",TA-48
"Aggressive competition and wider production mixes are profoundly impacting manufacturing settings, which, in turn, have become more technological, flexible and dynamic. With growing uncertainty in the shop floor, dynamic scheduling emerged as a relevant topic to explore. In highly dynamic environments, dispatching rules [DRs] became a popular solution method for scheduling problems due to their reactive nature, ease of implementation and interpretability. However, DRs do not cope well with varying conditions in two main dimensions - shop load and job urgency. Therefore, researchers have proposed machine-learning-based systems that select DRs as conditions change over time. Two main approaches emerged in the literature - the periodic rule selection [PRS] and the real-time rule selection [RTRS]. The goals of this work are twofold. First, propose PRS and RTRS systems that can outperform state-of-the-art DRs by relying on effective state and action sets. Secondly, contrast PRS and RTRS in the same dynamic job shop instances for stationary and non-stationary conditions, filling a gap in the literature. Results show that the best rule selection system reduces tardiness, on average, 14.8% in stationary instances, and 3.5% in non-stationary instances. Even in the hardest instances, the system comes within 1% of the best DR. Moreover, PRS reveals to be the most effective approach for stationary instances. In non-stationary conditions, RTRS was better.
",Periodic and Real-Time Dispatching Rule Selection for the Dynamic Job Shop Scheduling Problem,"[70387, 30652, 23500]",931,"[129, 69, 66]",1695,Job shop scheduling,35,13,60,Project Management and Scheduling,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S09 [building - 101],"['Scheduling', 'Manufacturing', 'Machine Learning']",WB-60
"Given the infrastructure network and an origin-destination demand matrix, the set of lines of a bus, train or tram system is optimized with the Transit Network Design Problem [TNDP]. When addressing the TNDP with passenger travel time in mind, the passenger assignment problem [PAP] needs to be addressed. Due to the complexity of the TNDP, typically bi-level optimization models and/or metaheuristics are used, where the PAP is solved every time a line plan is evaluated. In doing so, the transit network needs to be expanded, typically with a so-called ‘Change-and-Go’ [CNG] network with dummy transfer nodes, in order to model transfer penalties as part of the passenger travel time. Then, the all-pairs shortest path problem needs to be solved for this expanded network representation, which is by far the most time consuming part of the algorithms. To overcome this, we present a much more efficient network representation, the ‘Direct Link Network’ [DLN], where additional edges are added instead of additional nodes. We compare the computation time required to solve the PAP by using CNG and DLN on the most commonly used benchmark networks and several real-life networks. The results show that with the DLN representation, the PAP can be solved on average 350 times faster than with CNG. Consequently, DLN can significantly speed up all TNDP algorithms that solve the PAP over and over when designing a public transport network.",An alternative network representation to speed up network design,"[46228, 67226]",283,"[79, 5, 143]",1697,Network Design and Line Planning for Public Transportation 1,85,9,51,Public Transport Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,M5 [building - 101],"['Network Design', 'Algorithms', 'Transportation']",TC-51
"Due to the uncertain environment in terminals, scheduling vehicles for doubling-cycling operations is challenging. If actual operation mismatches with the schedule, unwanted waiting will occur. Thus, to increase the possibility of forming double-cycling operations during operation and reduce empty travels, we propose a flow planning problem considering traffic congestion, a tactical level decision-making study. Acknowledging that congestion significantly hampers the efficiency of horizontal transportation, we referred to a probabilistic and physics-based model for interruptions to define the congestion generated by vehicles and quantify the impact of vehicle assignment and loop planning on congestion effect. A novel branch-price-and-cut approach is developed where valid inequalities are used to cut the infeasible solutions arising from non-linear increases in travel time due to congestion. Numerical experiments demonstrate 1] the problem's practicality and the algorithm's effectiveness in reducing the total travel distance by an average of 34.78% across different scales of instances. 2] the algorithm can solve up to 33 Quay Cranes, 330 vehicles, and 1200 jobs, equivalent to the real scale of Singapore's mega port. 3] solutions ignoring congestion are found to be infeasible within congestion effect model, significantly as the scale of the instances increases, highlighting the importance of incorporating real-world constraints like congestion into optimization models.",Improving Traffic Efficiency through Double-cycling Operations - A Traffic Congestion considered Flow Planning Problem in Mega Container Ports,"[77341, 61927, 65825]",680,"[143, 137, 11]",1704,Container Stacking and Yard Planning III,52,14,62,OR in Port Operations,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S12 [building - 101],"['Transportation', 'Strategic Planning and Management', 'Branch and Cut']",WC-62
"Emergency Medical Services [EMS] are essential for providing timely pre-hospital care. Efficient EMS dispatching, a critical yet complex decision-making process, involves not just quick responses to emergency calls but also optimal resource allocation amidst unpredictable demands. While traditional dispatch strategies typically prioritize sending the nearest vehicle, research has shown this approach does not yield the best outcomes. We introduce a comprehensive solution that integrates factors like scalability, uncertainties, dynamic environments, and adaptive strategies, thereby enhancing the EMS system's ability to respond more effectively and efficiently.

We propose a novel approach using Deep Reinforcement Learning [DRL] to refine EMS response efficiency. By integrating key metrics such as emergency severity and response time, we offer a strategic decision-making framework that significantly improves operational outcomes. Utilizing publicly available data from San Francisco, we highlight our DRL model's effectiveness in surpassing traditional dispatching methods. Our approach uniquely leverages a larger state-action space for DRL and incorporates crucial performance indicators, making EMS deployment more nuanced and efficient than in previous studies. This results in faster, more effective incident responses than traditional strategies, showcasing the potential of advanced machine learning in emergency dispatch optimization. ",Enhancing Emergency Response Efficiency via Deep Reinforcement Learning - A Novel Model for Dynamic Dispatching,"[75528, 77383, 77385]",329,"[66, 56, 26]",1705,ML & OR Applications in Transport Modelling,6,7,55,Transportation,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S02 [building - 101],"['Machine Learning', 'Health Care', 'Decision Support Systems']",TA-55
"The spherical code problem asks how to arrange N points on the unit sphere in dimension n such that the distance between the closest pair of points is maximized. We prove that for 24 points in dimension 4, the D_4 root system is the optimal configuration. We prove this by showing that it is the unique solution for the kissing number problem in dimension 4, up to isometry. For this we use a semidefinite programming relaxation of the the second step of the Lasserre hierarchy for spherical codes, for which we obtain an exact optimal solution by rounding the numerical solution using the techniques of [Cohn, de Laat, Leijenhorst, 2024+]. ",Optimality and uniqueness of the D_4 root system,"[76625, 75160, 75017]",155,"[21, 115]",1706,Applications of polynomial optimization,68,4,38,"Conic Optimization - Theory, Algorithms, and Applications","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,34 [building - 306],"['Convex Optimization', 'Programming, Semidefinite']",MC-38
"Witsenhausen's problem asks for the maximum fraction αn of the n-dimensional unit sphere that can be covered by a measurable set containing no pairs of orthogonal points. We extended well known optimization hierarchies based on the Lovász theta number, like the Lasserre hierarchy, to Witsenhausen's problem and similar problems. These hierarchies are shown to converge and are used to compute the best upper bounds known for αn in low dimensions.",SDP hierarchies for distance-avoiding sets on compact spaces,[77364],903,"[21, 113]",1707,Advances in polynomial optimization and its applications,68,15,38,"Conic Optimization - Theory, Algorithms, and Applications","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,34 [building - 306],"['Convex Optimization', 'Programming, Nonlinear']",WD-38
"Quality-speed trade-offs occur in many service systems in which employees have discretion over their service times. An increase in service time often results in an increase in the quality of a service but also increases congestion. We investigate this trade-off in a dynamic setting with time-dependent and stochastic demand. To the best of our knowledge, this work is the first analysis of the quality-speed trade-off with exogenous changes in the demand. It shows how service providers can anticipate future expected demand changes with optimized service rates. 

We investigate a piecewise-stationary approach whereby periods are not interrelated. Based in this, a closed-form stationary smoothing heuristic accounts for future demand changes. A performance evaluation based on a stationary backlog-carryover [SBC] approach considers the interrelations of performance measures between periods directly, and hence, allows to determine service rates simultaneously over all periods of the planning horizon. 
 
We show that forecasted demand changes are anticipated by service rates several periods in advance. Substantial benefits are achieved by incorporating forecasted demand changes in the decision making of time-dependent service systems affected by the quality-speed trade-off.
 ",Quality-Speed Trade-Offs in Dynamic Service Systems - How to Anticipate Future Demand Changes,"[10255, 49124, 77367]",161,"[135, 130, 121]",1709,Stochastic Models in Service Operations I,50,7,39,Stochastic Modelling,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,35 [building - 306],"['Stochastic Models', 'Service Operations', 'Queuing Systems']",TA-39
"In response to the complex challenges of disaster relief logistics, we present a stochastic network modeling approach to develop insights into strategic facility location, capacity planning, and resource allocation of relief supplies. The primary purpose of the proposed model and numerical experiments is to present a cost-effective logistics network designed to efficiently handle multiple relief items across a spectrum of different crisis scenarios. By incorporating stochastic elements into the analysis, we aim to capture the inherent unpredictability of demand fluctuations and supply chain disruptions. Moreover, our approach centers on optimizing facility sizes to harness economies of scale and refining allocation decisions. To demonstrate the practical applicability of our model, we conduct a computational case study utilizing instances from the German national food stockpiling system. Additionally, we present a sensitivity analysis highlighting the impact of network modifications, crisis intensity, and scale effects on facility location and assignment decisions. The results provide managerial insights and economic considerations for decision-makers, enhancing the resilience and adaptability of cost-effective disaster preparedness and network design strategies.",Strategic Decision Support in Disaster Relief Logistics - Stochastic Network Optimization for Efficient Resource Location and Allocation ,"[74883, 77374, 64482, 2675]",552,"[58, 79, 30]",1712,Site selection and aid allocation in humanitarian operations,38,5,21,OR in Humanitarian Operations [HOpe],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,49 [building - 116],"['Humanitarian Applications', 'Network Design', 'Disaster and Crisis Management']",MD-21
"In a flexible job shop problem, a typical job consists of a set of operations with precedence relationships among them. Each operation may be performed in various modes, where each mode requires certain renewable resources, each for some specific time-period within the duration of the operation. We call the resources’ timings for performing a specific operation in a specific mode a scheduling segments. We assume that scheduling experts develop and keep for reuse a library of alternative scheduling segments for each operation. The reuse of a library of scheduling segments is of much importance for the engineer to order process where a manufacturer meets the specifications of their customer by engineering and producing the product after an order has been received. Developing a scheduling segment is a complex and time-consuming task, for which scheduling experts can benefit by reusing existing scheduling segments. The objective is to minimize the makespan by combining scheduling segments into a feasible schedule. We propose a new efficient heuristic for addressing this challenge. Previous methods for optimally solving this problem suffered from severe scalability issues, while previous heuristics experienced high rates of infeasibility that hurt both efficiency and performance. The proposed approach integrates a genetic algorithm with a greedy procedure for overcoming these difficulties while enabling, for the first time, the solution of practically sized jobs.",Job Scheduling by Choosing and Tiling Existing Scheduling Segments,"[70019, 71839, 72068, 69590, 52824, 69280]",808,"[129, 5, 111]",1713,Machine scheduling problems,32,14,49,"Lot Sizing, Lot Scheduling and Production Planning","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,M1 [building - 101],"['Scheduling', 'Algorithms', 'Programming, Mixed-Integer']",WC-49
"Implementation of climate change mitigation and adaptation measures are urgently needed in cities as they experience severe impacts from climate change including heat and flood while they continue to produce greenhouse gases that contribute to climate change.  A city is composed of diverse districts.  While urban structure, land use and form matter at the city or regional scale, physical environment of districts should be improved to make them inclusive, safe, resilient and sustainable.  The district scale is said to be an appropriate scale for climate actions as it is small enough to innovate quickly and large enough to have a significant impact.  Participatory urban design methodology is needed at the district scale as there are various stakeholders that shape the physical environment composed of buildings, open spaces and public spaces.  Based on the history of participatory urban design, or Machizukuri in Japan, we are starting to incorporate digital tools such as thermal simulation and carbon mapping into stakeholder workshops to initiate place-based climate change actions.  Both tools are developed on a publicly available three-dimensional urban model.  We will present several cases of participatory urban design practices in Japan using these tools and discuss about the achievements and issues.",Participatory Urban Design Methodology for Climate Actions at the District Scale - Thermal Simulation and Carbon Mapping,"[77366, 77368, 77369, 77371, 77372]",76,"[139, 137, 131]",1714,"Assessment Methods for Shaping the Green, Inclusive, and Digital Cities  I",79,2,18,Sustainable Cities,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,42 [building - 116],"['Sustainable Development', 'Strategic Planning and Management', 'Simulation']",MA-18
"This study focuses on promoting green, sustainable, and healthy urban development via comprehensive earthwork management, integrating both ready-for-use and over-wet soil. We enhance the latter by blending it with specifically recycled incinerated waste granules and lime, converting unusable wet soil into a valuable resource. This not only saves disposal costs but also notably reduces carbon emissions. Our mixed-integer linear programming model, designed to optimize acquisition, dispatch, and utilization of earthwork from diverse sources, effectively tackles transportation and resource allocation challenges, ensuring efficient earthwork planning aligned with sustainable construction and conservation principles. Applied to an aerotropolis project in Taiwan, our model demonstrated significant cost efficiencies in earthwork acquisition and disposal, achieving considerable savings compared to traditional methods. Sensitivity analyses on different parameters are also provided. This approach exemplifies the integration of innovative waste management and optimization techniques in urban construction, offering a replicable model for future sustainable development projects.",Optimizing Earthwork Management for Sustainable Urban Development - A Mixed-Integer Linear Programming Approach,"[19965, 40380]",708,"[143, 94, 84]",1716,"Sustainability and Equity in Ecosystems, Ecology and Food",80,14,53,Sustainable and Resilient Systems,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,8007 [building - 202],"['Transportation', 'OR in Environment and Climate change', 'Optimization Modeling']",WC-53
"Among many effective strategies to speed up first-order solvers for smooth nonlinear optimization problems, Polyak's momentum [or Heavy-Ball] certainly stands out. The idea of Polyak consists of exploiting information about past iterates, carrying out a step along a direction which is a linear combination of a suitable gradient-related descent direction and the previous step. The idea of this update rule is that, partly following the direction of last iteration, oscillations can be controlled and acceleration can be obtained in low curvature regions.
However, except for the quadratic case, the selection of the two parameters defining the update is not trivial; to this aim, we develop an approach where these coefficients are simultaneously determined by a simple inexact bidimensional search. This strategy allows us to define a class of gradient methods with momentum enjoying, under reasonably weak assumptions, global convergence guarantees and an optimal worst-case complexity bound in the nonconvex setting. To the best of our knowledge, these results are novel to the literature. Moreover, extensive computational experiments show that the gradient methods with momentum here presented outperform conjugate gradient methods and are [at least] competitive with the state-of-art method for unconstrained optimization, i.e, L-BFGS method.


",A globally convergent gradient method with momentum,"[66630, 68101, 10662]",305,"[19, 113, 63]",1718,Algorithmic Advances in Large Scale Nonconvex Optimization,84,3,32,Advances in large scale nonlinear optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,41 [building - 303A],"['Continuous Optimization', 'Programming, Nonlinear', 'Large Scale Optimization']",MB-32
"The widespread adoption of digital distribution channels both enables and forces more and more logistical service providers to manage booking processes actively to maintain competitiveness. As a result, their operational planning is no longer limited to solving vehicle routing problems. Instead, demand management and subsequent vehicle routing problems are integrated to steer the booking process with the aim of optimizing the downstream fulfillment operations. The resulting integrated demand management and vehicle routing problems [i-DMVRPs] can be modeled as Markov decision process and, theoretically, solved via the well-known Bellman equation. Unfortunately, the Bellman equation is intractable for industry-sized instances. Thus, in the literature, i-DMVRPs are often addressed via opportunity cost approximation approaches. However, the overall performance of the respective approaches largely varies between different instance structures. Furthermore, to the best of our knowledge, there is neither a structured procedure to analyze the corresponding root causes nor general guidelines on when to apply which class of approximation approach. In this work, we address this gap by proposing a structured method to analyze, explain and compare the performance impact of different opportunity cost approximation-based solution approaches for i-DMVRPs. Further, we identify common patterns in approximation errors and derive general guidelines for an informed algorithm development process.",From approximation errors to optimality gap - exploiting structural knowledge of opportunity cost in integrated demand management and vehicle routing problems,"[61926, 67944, 14031, 16305]",699,"[124, 32, 145]",1719,Pricing and applications,11,10,59,Pricing and Revenue Management,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S08 [building - 101],"['Revenue Management and Pricing', 'E-Commerce', 'Vehicle Routing']",TD-59
"Autonomous Mobile Robots [AMRs] collaborate with humans in order-picking and can increase order-picking efficiency by reducing walking. AMRs can operate under two policies - Swarm or System-Directed. This study models the Swarm policy using a closed queuing model to determine system throughput capacity. The queuing model incorporates AMR-picker matching via a synchronization station. We derive closed-form expressions for network steady states and use these to calculate order throughput. The impact of various factors on overall system performance is analyzed such as order profiles, matching policies [closest or random], AMR and picker speeds, warehouse size, item assignment methods, and AMR-to-picker ratios. Furthermore, we compare the order throughput of the swarm with the system-directed policy to ascertain superior performance under different circumstances. Results indicate that the swarm policy generally outperforms, particularly at higher AMR-to-picker speed ratios. However, its effectiveness diminishes with larger order sizes and lower AMR-to-picker ratios. Overall, the study provides insights into optimizing e-commerce warehouse operations through effective decision-making in swarm collaboration, aiming to enhance productivity and efficiency in order fulfillment.
",Swarm or System-directed? Analysis of Human-Robot Matching Policies in Collaborative Order Pick Systems,"[71451, 70286, 2069]",635,"[65, 121, 146]",1722,Warehouse Management,30,5,50,Retail Operations,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,M2 [building - 101],"['Logistics', 'Queuing Systems', 'Warehouse Design, Planning, and Control']",MD-50
"The e-commerce sector is rapidly automating warehouse operations, necessitating efficient control systems to manage this growing complexity. This study investigates Robotic Mobile Fulfillment Systems [RMFS], employing autonomous mobile robots [AMRs] for inventory rack management. These systems eliminate the need for human operators within the storage area, with AMRs responsible for both storing and retrieving movable inventory racks. Human operators stationed at workstations alongside the storage area fulfill customer orders by picking goods from these racks. RMFS dynamically adjusts rack positions based on usage frequency. Furthermore, effective restocking timing is crucial to prevent operation delays. Real-time scheduling, especially in inventory rack storage and replenishment, is essential for optimal performance. 

To minimize cycle time, a deep reinforcement learning [DRL] approach is proposed. The storage area is divided into zones to facilitate efficient decision-making. The learning agent interacts with the environment, observes changes in its state, and learns through trial-and-error from these interactions. Thereby, it is able to construct a policy by mapping environment states to actions. Experimental results demonstrate significant cycle time improvements compared to traditional decision rules. The study underscores the importance of real-time decision-making for the storage assignment and replenishment problem.
",Learning a Policy for the Real-Time Inventory Rack Storage Assignment and Replenishment Problem,"[69659, 54627, 53667, 46228]",801,"[32, 146, 66]",1723,Recent Methodologies in Explainable AI [XAI] 2,71,3,04,Recent Advancements in AI ,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,1001 [building - 202],"['E-Commerce', 'Warehouse Design, Planning, and Control', 'Machine Learning']",MB-04
"The competitiveness of manufacturing companies, especially those with multiple plants, is increasingly determined by their operational efficiency and agility. A challenging issue when considering multiple factories is the optimal allocation and scheduling of orders, which is further complicated by the lack of tools and personnel that limit the ability to process an order in each factory. Thus, this paper contributes by solving a bi-objective distributed permutation flow shop scheduling problem with eligibility constraints focusing on minimizing total cost and total weighted tardiness. 
For the first time, factory-dependent due dates and qualification options through tool transfer to an initially ineligible factory are investigated. In addition, emission costs are considered alongside transfer, delivery, and production costs to promote a holistic problem view. Extensive computational experiments are used to analyze the effects of varying the strictness of factory eligibility constraints, changes in product and tool weight factor levels, and an increase in the emission price. The results illustrate the critical role of transportation in distributed scheduling and highlight the benefits of factory qualification in terms of on-time order delivery and cost minimization. Finally, the integration of factory eligibility constraints and factory-dependent due dates strengthens the robustness of the management implications by better reflecting industry constraints.
",A bi-objective distributed permutation flowshop scheduling problem with factory eligibility and qualification options,[77285],834,"[129, 59, 77]",1724,Flow shop scheduling problems,32,13,49,"Lot Sizing, Lot Scheduling and Production Planning","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,M1 [building - 101],"['Scheduling', 'Industrial Optimization', 'Multi-Objective Decision Making']",WB-49
"The premarshalling problem aims to rearrange the containers in a port yard bay, comprising a set of adjacent stacks, to prevent containers from being placed above others that must be retrieved earlier. This arrangement streamlines retrieval from the bay for transfers to inland or maritime transport, as additional relocations are avoided at the time of retrieval. Premarshalling is only performed during crane idle times, and the original formulation of the problem assumes these times are unlimited. However, constraints in practice may limit the time available, hindering full bay arrangement. In such cases, partial premarshalling can still enhance retrieval efficiency within time constraints, but existing formulations for complete premarshalling lack adaptation for partial solutions, yielding suboptimal outcomes. Furthermore, while the goal of premarshalling is clear, to achieve a fully arranged bay, various objectives for partial premarshalling can be pursued. This study explores different criteria for partial premarshalling and investigates several constraint programming-based solution approaches.",Partial container premarshalling approaches using constraint programming,"[68812, 31857]",174,"[65, 14, 107]",1731,Container Stacking and Yard Planning I,52,3,62,OR in Port Operations,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S12 [building - 101],"['Logistics', 'Combinatorial Optimization', 'Programming, Constraint']",MB-62
"We study the joint routing and sorting problem in Amazon’s middle mile network, which optimizes the routing of packages from fulfilment centers to delivery stations by using intermediate hubs to sort and consolidate flow. Packages are sorted into pallets, and packages in the same pallet are routed over the same path unless the pallet is dismantled in some intermediate hub. While the problem resembles a multi-commodity flow optimization problem, the operational constraints related to connectivity and sorting capacities and decisions create additional complications for routing that are not present in the literature.
 
We present a scalable, edge based model to solve this problem using a combination of search space reduction via edge pruning, a warm-start heuristic based on combinatorial optimization and a tight MIP formulation which uses a destination based aggregation of commodities. We observe significant improvements over path-based formulations in both run time and overall costs.",A scalable solution to joint routing and sorting problems,"[77381, 77382, 76049, 77310, 77309]",289,"[143, 14, 79]",1733,Large-scale network optimization and inventory management,92,10,57,Optimization at Amazon,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S06 [building - 101],"['Transportation', 'Combinatorial Optimization', 'Network Design']",TD-57
"In football analytics, various metrics for teams' offensive efficiency have been presented. As the objective of the game is to score goals, the number of goals scored and the number of shot attempts are the most obvious efficiency metrics. The commonly used expected goals [xG] metric estimates the quality of shot attempts by assigning them a probability of scoring based on shot location and other factors. However, all these metrics share a common weakness as they focus solely on possessions where shots are attempted, neglecting the broader context of play. This presentation proposes an approach where this limitation is avoided using event data consisting of the locations and times of various ball touches such as passes, dribbles, and shots. The data are then used to divide the match into possessions, i.e., continuous sequences of touches by one team, and to estimate the probability that a given touch on the ball is a shot attempt. These probabilities are used to calculate the shot probability for each possession, i.e., the possession's expected shots value [xS]. This metric allows one to measure the quality of possessions and teams' ability to create scoring chances without restricting the analysis to shot attempts that actually took place. The proposed approach is compatible with various extensions such as the skill level and tendencies of individual players as well as a more detailed description of the game state as provided by player tracking technologies.",Expected Shots Concept for Football,"[76751, 77418]",665,"[99, 0]",1735,Football analytics,37,9,16,OR in Sports,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,19 [building - 116],['OR in Sports'],TC-16
"This study introduces a novel model addressing the Vaccine Allocation Problem [VAP], focusing on distributing vaccines across different population locations over multiple pandemic periods. It incorporates disease progression and vaccination effects to minimize total expected mortality and location-based mortality inequalities while considering constraints like vaccine supply and healthcare capacities. Utilizing an extended Susceptible-Infected-Recovered [SIR] epidemiological model, the VAP is formulated as a nonlinear mixed-integer programming problem and solved using the Gurobi solver. Through a series of scenarios spanning 12 weeks in the UK, the study highlights the significant impact of vaccine availability and disease spread parameters on optimizing vaccination strategies.",Fair and Effective Vaccine Allocation During a Pandemic,"[15639, 3340, 22044, 77384]",963,"[113, 30, 56]",1736,COVID-19 [2],3,14,15,OR in Health Services [ORAHS],"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,18 [building - 116],"['Programming, Nonlinear', 'Disaster and Crisis Management', 'Health Care']",WC-15
"We show that the efficiency of the security process at airports can be significantly improved by adoption of a system where passengers are able to book a time slot. This time slot allows passengers to get priority access at the given time for security. We show that a simple load balancing heuristic already guarantees that participants have minimal waiting time while at the same time reducing the average waiting time of all passengers.
Our proposed method is able to reduce the waiting time even further. Furthermore, our approach specifically takes into consideration the uncertainty in the arrival pattern and security capacity at the moment the time slots need to be allocated. Therefor, it can guarantee minimal waiting time in all scenarios for participating passengers. Furthermore,  this approach leads to significantly lower wait time for non-participants than the baseline heuristic or having no virtual queuing at all in almost all scenarios.
Finally our numerical experiments show that our approach is able to reduce the sensitivity to the uncertainty in the arrival behavior of passengers and the security capacity.",Improving Congestion in an Efficient & Robust manner using time slotting at Airport Security ,"[70904, 39439]",161,"[135, 121, 130]",1738,Stochastic Models in Service Operations I,50,7,39,Stochastic Modelling,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,35 [building - 306],"['Stochastic Models', 'Queuing Systems', 'Service Operations']",TA-39
"In non-cooperative game theory, it may happen that players’ payoffs are represented by vector parametric functions, with no information available on the probability distribution of the parameters. A fruitful approach to this problem is to consider all the realizations of the parameters in a set-valued payoff. The worst case scenarios can be considered by the introduction of an appropriate quasi-order relation between sets. We address the question whether scalarization and robustification can be commuted in a non componentwise framework.",Robust games with vector payoffs,"[57366, 61281, 11677]",50,"[127, 50]",1739,Vector and Set Optimization II,33,3,41,Vector and Set Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,97 [building - 306],"['Robust Optimization', 'Game Theory']",MB-41
"The analytic hierarchy process [AHP] is a useful method for multiple criteria decision making. It provides a priority weight estimation method from a pairwise comparison matrix [PCM]. A PCM evaluated by a decision maker [DM] is frequently inconsistent. The inconsistency is assumed to come from the evaluation errors. The priority weights have been estimated by minimizing the errors. However, human perception is vague. Assuming that the inconsistency comes from the vagueness, estimating interval propriety weights instead of crisp priority weights from a PCM was proposed. Many interval priority weight estimation methods have been proposed because the original interval priority weights estimation method tends to estimate insufficiently wide interval priority weights. Numerical experiments showed that ranking alternatives by interval priority weights estimated by several methods performs better than the crisp priority weights estimated by the conventional methods when we assume the DM’s vague evaluation. It is shown that the solution to the interval priority weight estimation problem is frequently non-unique although representative solutions have been used in the numerical experiments. In this presentation, we show first that the set of all rankings of alternatives under the estimated interval priority weights can be obtained easily. Using the set of all rankings in the numerical experiments, we confirm the advantages of several interval priority weight estimation methods.",Comparing Interval Priority Estimation Methods by Estimation Accuracy of Solution Set in Terms of Ranking Alternatives,"[2028, 68979, 68891]",135,"[6, 77, 25]",1740,Preference Learning 1,44,2,44,Multiple Criteria Decision Analysis,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,20 [building - 324],"['Analytic Hierarchy Process', 'Multi-Objective Decision Making', 'Decision Analysis']",MA-44
"Lot scheduling problems are a significant focus in scheduling theory due to their broad applications and effect on operational efficiency. Traditionally, research in this field assumes a knowledgeable scheduler and truthful agents. However, this study delves into a more realistic scenario where these assumptions are challenged, proposing a game theory approach to handle the complexities arising from incomplete information and strategic behavior. We examine lot scheduling scenarios with uniform capacities for lots comprising orders of varying sizes. Departing from the conventional paradigm, we introduce the concept of a scheduler with limited information and agents prone to providing misleading information for personal gain. We investigate five fundamental objective functions in lot scheduling - [i] minimizing the completion time of the last job exiting the system, [ii] minimizing the total completion time, [iii] minimizing the total weighted completion time, [iv] minimizing the number of tardy orders, and [v] minimizing the total weighted number of tardy orders. Notably, we show that problems [i] to [iv] can be efficiently solved in polynomial time, while problem [v] is solvable in pseudo-polynomial time. Our research advances the understanding of decentralized scheduling, where agents' behavior and information gaps affect decision-making. It underscores the need to address real-world complexities in scheduling theory and provides insights for designing adaptable algorithms.",A Game Theoretic Approach for Solving Lot Scheduling Problems,"[71691, 29447]",646,"[129, 50]",1741,"Game Theory, Solutions and Structures VIII",88,10,36,"Game Theory, Solutions and Structures","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,32 [building - 306],"['Scheduling', 'Game Theory']",TD-36
"Maintaining a balance between electricity generation and consumption is critical for grid stability. While not extensively studied in scheduling problems, explicit [incentive-based] demand response [DR] is an effective strategy to tackle this concern. Here, the ancillary service agrees to customers' proposals to reduce energy consumption during specific time periods. This paper aims to investigate the explicit demand response approach at an Italian company consisting of multiple press machines on the shop floor, specialized in producing diving equipment. A heuristic procedure is developed to propose a baseline production plan and multiple alternative plans, where a subset of machines are deactivated, to provide flexibility services to the market. The several electricity consumptions profiles resulting from the different production plans will be submitted to the ancillary market. During the production, the ancillary service may request the manufacturer for a transition from the baseline to an alternative production plan, in exchange for an agreed monetary incentive. Hence, the proposed heuristic is applied to assess the effectiveness of the incentive-based demand response strategy in the Italian plant. The results indicate that the proposed heuristic amplifies profitability for both the service provider and the manufacturing system.",Addressing explicit demand response in a day-ahead electricity market in Italy - Case study on the diving equipment manufacturing sector,"[75751, 77393, 77395, 71292, 71845, 77397]",869,"[129, 36, 84]",1744,Flexibility in future energy systems,22,3,14,Energy Markets,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,16 [building - 116],"['Scheduling', 'Electricity Markets', 'Optimization Modeling']",MB-14
"Decisions around medium and long-term allocation of healthcare resources are fraught with challenges arising from the inherent uncertainties in population growth as well as developments in the economy, society and technology. Against this background, health and care planners are constantly facing challenges on how to allocate healthcare resources with limited budget. The aim of this paper is to describe the development and early use of an innovative mathematical model and accompanying tool that captures the likely impact of long-term changes in a regional population and associated healthcare resource requirements. The modelling tool works in two stages - first, it derives 20-year population projections based on a Markov chain; second, it calculates healthcare activity and related costs for same time horizon. Population projections are based on segmentation of the population with respect to the Cambridge Multimorbidity Score derived from a state-of-the-art system wide dataset that provides patient-level linkable data for the entire population of a region in England of about 1 million people. Our innovative model and tool, which is already informing decisions in the collaborating health care system, offers planning support by generating long-term population projections as well as provides capacity for running scenarios that flex the 'do nothing' [baseline] scenario by considering various mitigations.",Supporting long-term decision making at regional level through modelling long-term changes in population health state and associated healthcare resource usage,"[45846, 72782, 77387, 62884]",968,"[56, 0]",1746,Decision support in healthcare,3,2,17,OR in Health Services [ORAHS],"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,40 [building - 116],['Health Care'],MA-17
"The inter-day [which day?] and intra-day [when?] appointment scheduling problems are often studied in isolation, whereas in reality, they are intertwined. If the waiting list becomes too large [small], one would want to extend [reduce] the appointment book, but such decisions directly impact the optimality of the schedule. Using a continuous intra-day scheduling paradigm, these problems are connected in a dynamic environment. Extending convexity properties of the appointment scheduling problem renders the corresponding dynamic program amenable to building upon recent advances in literature.

Since the optimal solution is not tractable, two intra-day scheduling paradigms are introduced - a theoretical approach [based on re-optimization] and a heuristic, which is based on schedules that exhibit so-called sequential refinability. Together, they bound the optimal policy for the dynamic inter/intra-scheduling problem. At the same time, their small difference underscores the potential of using this heuristic in healthcare practice.
",Convexity in Transient Queues with Appointment-driven Arrivals and Implications on Optimal Appointment Scheduling,"[72783, 71572]",596,"[56, 12, 135]",1747,Appointment planning,3,8,15,OR in Health Services [ORAHS],"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,18 [building - 116],"['Health Care', 'Capacity Planning', 'Stochastic Models']",TB-15
"Nature-based Solutions [NbS] are increasingly acknowledged as vital strategies against climate change. However, citizens and ecosystems benefit differently from NbS. To account for NbS trade-offs, theory and practice have embraced Inclusive Climate Actions [ICAs] that simultaneously tackle climate change and urban inequalities. Currently there is a notable gap in the assessment of environmental, economic, and social implications of ICAs for NbS.
This research develops an evaluation and monitoring [E&M] framework to identify Key Performance Indicators [KPIs] for evaluating the effectiveness of ICAs for NbS. The methodology is structured in three phases - identification, refinement, and prioritization. This approach integrates quantitative and qualitative techniques and involves stakeholders in the process. 
A set of 13 KPIs is proposed. These results can be further enhanced by engaging local stakeholders to select supporting indicators tailored to address contextual challenges. 
The E&M framework assesses the effectiveness of completed or ongoing projects, or evaluates future scenarios. This step serves for empirical assessment in five European cities, and for the design and planning of future scenarios in five Urban Living Labs. Results will help cities to implement ICAs that incorporate fairness and distribute NbS impacts equitably.
This study is part of the DUT project GREEN-INC, growing effective and equitable nature-based solutions through inclusive climate actions.",A Framework to Identify and Prioritize Key Performance Indicators - Assessment of the Effectiveness of Inclusive Climate Actions for Nature-based Solutions,"[75641, 53128]",76,"[26, 29, 100]",1748,"Assessment Methods for Shaping the Green, Inclusive, and Digital Cities  I",79,2,18,Sustainable Cities,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,42 [building - 116],"['Decision Support Systems', 'Development', 'OR in Sustainability']",MA-18
"When we build the model of probability distributions, we can work with one distribution or use a mixture of distributions. In general practice, one density distribution is usually used. We will show another approach, using a mixture of distributions. The whole process will be demonstrated on wage data from the Czech Republic. For the construction of mixtures, it is possible to use densities of classical distributions for modelling wage data such as normal, logarithmic normal, Johnson´s and other distributions. When creating the final mixture, we solve two problems. First, we need to determine the number of components in the mixture. Secondly, we need to estimate the parameters of individual densities in the mixture. Since we have a large amount of data broken down by different criteria [age, gender, region, education, etc.], we can naturally divide the data into a large number of categories. Each category is then represented by one component in the resulting mixture. Detailed data breakdown allows us to calculate weights of individual components in the resulting mixture. In addition to the classic procedure known from theory, the authors in the article also offer a non-standard approach, which allows not only to model the course of past data, but also the possibility of estimating the future development of the wage distribution. In the conclusion, attention is also paid to the stability of the models.",Probability models based on a mixture of densities,[65405],210,"[33, 47, 84]",1749,Analytics and the link with stochastic dynamics II,17,8,31,Analytics,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,54 [building - 208],"['Economic Modeling', 'Forecasting', 'Optimization Modeling']",TB-31
"Traditionally, appointment schedules have been determined by minimizing a specific cost function consisting of clients’ waiting times and server idling. Under stochastic service times, this intra-day problem is predominantly studied in a static sense, assuming a fixed schedule. However, technological advancements have opened up the possibility of communicating with clients during the day. Using such channels allows for updating appointment schedules on the fly, for example, by postponing appointments in case of overcrowding. Yet, while its promise is clear, sending updates at the wrong moments, or too many, may be ineffective or unrealistic, potentially causing client confusion and frustration. Therefore, the static appointment scheduling problem is extended to a dynamic setting that allows the rescheduling of future clients. 

Three rescheduling paradigms - via arrivals, the timeline, or the waiting room - are introduced that can be solved to optimality via dynamic programming. By rescheduling, the scheduler takes control over the running session, and the experiments indicate that total costs are significantly decreasing, even with relatively few updates. The benefits concentrate on increasing utilization [less idling]. The magnitude of the decrease is moderated by the length of the grace period, which accounts for not sending updates to clients possibly underway. Also, we argue that the third paradigm is most effective for rescheduling.",Appointment Scheduling with Updates - An Exact and Optimal Approach,"[77392, 72783]",799,"[108, 135, 56]",1754,Stochastic Models in Service Operations II,50,12,39,Stochastic Modelling,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,35 [building - 306],"['Programming, Dynamic', 'Stochastic Models', 'Health Care']",WA-39
"Public transit agencies make ad-hoc supply-side changes with an aim to improve service quality. However, the benefits of these supply-side improvements heavily depend on the complex behaviour of users, who typically prioritize multiple criteria, such as travel times and the number of transfers while choosing routes. Agencies, on the other hand, decide trip-to-vehicle assignments [Multi-Depot Vehicle Scheduling, MDVS]. However, these decisions should [1] account for temporal and spatial variation in demand and [2] consider the changes in passenger choices they may trigger. The study proposes a local search framework that assigns a heterogeneous fleet to trips [HMDVS] so that the resulting capacity on the route is commensurate with the demand for travellers along routes. We assess user responsiveness via a simulation-agent-based public transit assignment [PTA] that predicts travellers’ route choices based on transit schedule and capacity constraints. Our local search operators are designed to effectively capture the user sensitivity to supply perturbations, leveraging the interaction between PTA and HMDVS. Analysis on real-world transit networks such as Santa Barbara, Columbus, and Surat show that our PTA framework can simulate millions of passengers in a few hours. Additionally, we observe savings of up to 10% in operational costs on switching from a homogenous to a heterogenous fleet.",Integrating Heterogeneous Multi-Depot Vehicle Scheduling and Public Transit Assignment for Enhanced Transit Operations - A Simulation-based Framework,"[71187, 77401, 66067]",824,"[128, 129, 119]",1756,Transit,85,15,51,Public Transport Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,M5 [building - 101],"['Rostering', 'Scheduling', 'Public Local Transportation Systems']",WD-51
"Multi-sided assembly line balancing problems typically arise in plants producing large-sized, high-volume products such as automobiles. The difference between these assembly lines and traditional ones lies in the ability to install multiple single stations at each position along the line, enabling operators [whether workers or robots] to simultaneously perform various tasks on the same product. This paper addresses the robotic multi-sided assembly line balancing problem. Here, it is assumed that robots can be selected from a set of candidate models differing in speed and energy efficiency. Consequently, they can execute assembly tasks at varying speeds while consuming different levels of energy. The objective is to allocate robots to workstations and tasks to minimize the total cost of installing a new assembly line for a given cycle time, encompassing fixed robot costs, workstation installation costs, and energy costs. To tackle this optimization problem, a mixed-integer programming formulation is proposed. Given the problem's NP-hard nature, a metaheuristic algorithm based on the variable neighbourhood search approach is developed to address real-size instances. Computational experiments are conducted to evaluate the performance of both the proposed model and the heuristic. ",Energy-aware robotic multi-sided assembly line balancing problem,[76940],211,"[72, 59, 74]",1757,Analytics and the link with stochastic dynamics III,17,9,31,Analytics,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,54 [building - 208],"['Mathematical Programming', 'Industrial Optimization', 'Metaheuristics']",TC-31
"We study the concept of cone-compactness for a subset of a normed vector space from the perspective of a sequential characterization under a separability assumption. Then we show that this concept is naturally involved in the investigation of several issues related to set optimization such as stability problems, conic cancellation laws, subdifferential calculus and optimality conditions.",Cone-compactness and applications in set optimization,"[13487, 50394]",49,"[19, 0]",1759,Vector and Set Optimization I,33,2,41,Vector and Set Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,97 [building - 306],['Continuous Optimization'],MA-41
"Incentive contracts are a primary means to align subordinates' behavior with the organization's objectives. Among the difficulties incentive contracting encounters is measuring the performance in all task elements assigned to a subordinate, which may result in incomplete contracts. Then, mutual trust among the contracting parties is crucial - with an incomplete contract, the superior has some discretion over the subordinate's rewards. Hence, the subordinate's trust in the superior affects the willingness to incur effort related to the not-contracted task elements. On the other hand, the superior's trust in the subordinate is relevant to the rewards provided. 

Building on Brower et al.’s [2000] theoretical model of trust in organizations, the paper translates the situation above of incomplete incentive contracts into an agent-based simulation model to study the emergence of reciprocal trust and the performance effects thereof.

The results suggest that neither the contracting parties’ trust universally increases with the superior's propensity to reciprocate the perceived subordinates’ trust, nor does performance generally increase with higher trust levels. Among the reasons is that the superior's trust in the subordinate might be unjustified from the subordinate's perspective. The results indicate that intra-organizational interdependencies, in conjunction with the superior's propensity to trust, subtly affect the trust-building of the contracting parties.
",When Reciprocity of Trust Backfires - Results of an Agent-based Simulation of Incomplete Incentive Contracts ,[25320],563,"[3, 68, 10]",1761,Simulation of organizations I,77,2,43,"Agent-based Models in Management, Economic and Organisation Sciences","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,99 [building - 306],"['Agent Systems', 'Managerial Accounting', 'Behavioural OR']",MA-43
"As one of the largest humanitarian logistics supply chains, the task for United Nations Humanitarian Response Depot [UNHRD] is how to allocate relief aid to save more people who suffered from disasters. This task is particularly challenging in areas like South Asia, where food aid efforts are confronted with complex transportation conditions, significant economic disparities, and the frequent occurrence of disasters, not to mention that relief aid resources are often scarce. In this research, we develop a novel Multi-stage Stochastic Programming [MSP] model to help UNHRD support critical decisions regarding site selection and aid allocation. Differently from the main literature where these decisions are often made within a two-stage paradigm, our three-stage perspective takes into account relief aid donation campaigns that are triggered depending on the disaster impact and its effects. Our objective function maximizes coverage weighted by the population profile, leveraging suitability indicators from locations to enhance strategic decisions on aid distribution. The results overall highlight the role of donated food aid in settings with severe constraints, demonstrating the MSP model's effectiveness in enhancing strategic response through staged information processing. This approach not only illustrates the model's adaptability in complex disaster scenarios but also its capacity to mitigate uncertainties in aid demand coverage, even when increases in coverage are marginal.",United Nations Humanitarian Response Depot’s Food Support in South Asia via Multi-stage Stochastic Programming,"[77400, 58203, 61420]",552,"[136, 58, 72]",1762,Site selection and aid allocation in humanitarian operations,38,5,21,OR in Humanitarian Operations [HOpe],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,49 [building - 116],"['Stochastic Optimization', 'Humanitarian Applications', 'Mathematical Programming']",MD-21
"We address the maintenance planning for rotor blades on onshore wind turbines at a tactical level, motivated by the case of a German onshore wind turbine manufacturer. The goal is to select maintenance teams from external service providers and assign maintenance tasks for a maintenance season while minimizing the total costs. The operative scheduling of the maintenance tasks and routing of the maintenance teams are anticipated to determine the capacity needs. We consider team-specific work time regulations, locations, qualifications, cost rates, means of transport, task-specific time windows, and weather conditions. However, planning such a maintenance season is very challenging.
For this reason, we formulate a mixed-integer linear problem for routing and scheduling the heterogeneous teams to perform maintenance tasks at different locations with minimum total costs. We propose a branch-and-price algorithm approach based on a Dantzig-Wolfe decomposition to solve it. The pricing subproblems are solved using a labeling algorithm. The approach is implemented using the SCIP framework. We present the underlying problem decomposition, implementation, and first numerical results using a state-of-the-art commercial solver as a benchmark.
",A Branch-and-Price-Algorithm for Tactical Maintenance Planning for Rotor Blades on Onshore Wind Turbines,[71905],758,"[13, 145, 129]",1763,Real-Life Applications in Routing,5,5,58,VeRoLog - Vehicle Routing and Logistics,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S07 [building - 101],"['Column Generation', 'Vehicle Routing', 'Scheduling']",MD-58
"Free-floating vehicle sharing systems such as car sharing systems offer customers the flexibility to pick up and drop off vehicles at any location within the business area. However, this flexibility comes with the drawback that vehicles tend to accumulate at locations with low demand. To counter these imbalances, pricing has proven to be an effective and cost-efficient means. The fact that modern systems rely on mobile applications for their communication with customers, combined with the fact that providers know the exact location of each vehicle in real-time, offers new opportunities for pricing. We develop a profit-maximizing dynamic pricing approach which is customer-centric, meaning that, whenever a customer opens the mobile application, the price optimization incorporates the customer’s location as well as the customer’s choice behavior. In particular, it considers the effects of prices and walking distances to available vehicles on the customer’s rental decision. Further, the approach anticipates future vehicle locations, rentals, and profits. More specifically, we propose an approximate dynamic programming-based solution approach with nonparametric value function approximation. It allows direct application in practice, because historical data can readily be used and key parameters can be precomputed such that the online pricing problem becomes tractable.",Customer-Centric Dynamic Pricing for Free-Floating Vehicle Sharing Systems,"[59177, 56420, 22994, 16305]",700,"[124, 143]",1764,Pricing and applications 2,11,12,59,Pricing and Revenue Management,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S08 [building - 101],"['Revenue Management and Pricing', 'Transportation']",WA-59
"Given the increasing importance of sustainability in the aviation sector due to climate change, electrifying ground vehicles on airport aprons is one way to reduce emissions. Dynamic inductive charging, which wirelessly charges vehicles while moving, is especially suitable for airport apron vehicles as it eliminates downtime for charging electric vehicles compared to conductive charging. We focus on scheduling electric passenger buses on airport aprons that use an inductive charging infrastructure to charge their batteries. Specifically, we investigate which vehicle should perform each service trip, whether it is transporting passengers from a gate to an airplane or from an airplane to a gate. We aim to ensure the reliable operation of these buses and avoid delays and breakdowns due to empty batteries. We present a formulation of the problem using a mathematical model. With this model, we want to evaluate different inductive charging infrastructures.",Planning of Inductive Charged Passenger Buses on Airport Aprons ,[71907],763,"[145, 129, 119]",1765,Public Transport,5,8,58,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S07 [building - 101],"['Vehicle Routing', 'Scheduling', 'Public Local Transportation Systems']",TB-58
"Despite being an essential technology in industrial decarbonization scenarios, Carbon Capture and Storage [CCS] struggles to reach large-scale deployment. To tackle this issue, public entities have set up numerous funding mechanisms to kickstart the adoption of this technology. However, these mechanisms generally support only the capture stage of the CCS value chain, thus overlooking support for dedicated transport and storage providers. In contrast, this paper develops a distribution rule to allocate a subsidy budget to the entire CCS value chain. First, we derive a cooperative game based on the cost-minimizing pipeline network connecting emitters to storage sites. We find that the Shapley value, typically computationally challenging, is attainable in polynomial time due to the specific structure of our problem. We then define each emitter's subsidy claim from public entities by combining the emitters' network cost shares [i.e., location] with their cost of capturing CO2 [i.e., volumes]. Because public entities may lack sufficient resources to cover all emitters' claims, we turn to bankruptcy theory and apply a division rule known as the Constrained Equal Awards rule, which we normatively justify by axioms relevant to our context. We prove that the spatial dimension substantially impacts the results of the distribution rule. From a policy perspective, our paper's findings suggest that current approaches should not overlook the location of industrial emitters.",Location- and Volume-Based Subsidies for CCS - A Game-Theoretical Approach,"[77274, 67540]",711,"[50, 37]",1766,Environment and climate change,21,14,22,Energy Management,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,81 [building - 116],"['Game Theory', 'Energy Policy and Planning']",WC-22
"Classifiers in today's data-driven domain face myriad challenges - overfitting, computational overheads, diminished accuracy, imbalanced datasets, and the opaque black box issue. Conventional classifiers struggle with managing noisy and missing features. This study introduces classification methods leveraging hypercuboid learning and outranking measures.
Our framework autonomously discerns hypercuboid features from the training set, eliminating the need for prior domain expertise. These hypercuboids capture essential dataset patterns, while outranking measures mitigate noise and uncertainty. Empirical evaluations across diverse datasets from the UCI repository compare the proposed classifiers with established models like k-NN, SVM, Random Forest, Neural Networks, and Naive Bayes. Results demonstrate our classifiers' robustness against imbalanced data and extraneous features, achieving comparable or superior performance to benchmarks. Moreover, our models offer interpretability without sacrificing predictive accuracy.",Outranking hypercuboid learning approach for classification problems,[2134],517,"[66, 42, 18]",1771,"Advancements of OR-analytics in statistics, machine learning and data science 8",16,12,28,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,065 [building - 208],"['Machine Learning', 'Expert Systems and Neural Networks', 'Computer Science/Applications']",WA-28
"Chronic patients affected by Non-Communicable Diseases live at home, but often book appointments at a hospital for regular visits, tests or treatments according to their care pathway, which can integrate the medical protocols for several morbidities. It is good practice to schedule multiple services for the same patient on the same day, for the sake of a holistic approach to patient care.
The services have a known duration and are partitioned by medical specialty; the operators are also partitioned and can provide all and only the services of their specialty.
Services can be provided in any order, each operator can see only one patient at a time, and each patient can receive only one service at a time, without preemption. 
Since resources are limited, both in terms of operators and shifts, the goal of the problem is to schedule as many services as possible.
However, the duration of a service is only an approximation and unexpected events, such as equipment failures or extra care for fragile patients, can delay its end time.
Given a schedule, a delay propagates forward to all the subsequent services of the operator providing the service, as well as to the subsequent activities of the patient involved.
Our main contributions are the formalization of a scheduling problem with limited operator shifts, and the methodological approach developed to deal with the uncertainty in the duration of services.",Robust Scheduling of Medical Appointments for Outpatients,"[3379, 72973, 13306, 36325, 72972]",874,"[56, 0]",1773,Applications of combinatorial optimization II,64,8,25,Combinatorial Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,011 [building - 208],['Health Care'],TB-25
"GAMSPy is a powerful mathematical optimization package which integrates Python's flexibility with GAMS's modeling performance. This combination opens doors to previously challenging applications, notably in bridging the worlds of machine learning [ML] and mathematical modeling. While GAMS excels in indexed algebra, ML predominantly relies on matrix operations. To enable applications in ML, our work introduces essential ML operations such as matrix multiplications, transpositions, and norms into GAMSPy. In this talk, we showcase the use of these additions by generating adversarial images for an optical character recognition network using GAMSPy. We highlight GAMSPy's versatility and its potential to be used in ML research and development. We delve into future prospects, show how GAMSPy's approach differs from existing alternatives and discuss innovative methods where mathematical modeling intersects with machine learning.",Integrating Machine Learning with GAMSPy,[77405],432,"[72, 66, 134]",1774,Modeling tools,76,9,30,Software for Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,53 [building - 208],"['Mathematical Programming', 'Machine Learning', 'Software']",TC-30
"Reserved product pricing [RPP] is a strategy used by firms selling two [or more] products to consumers.  Under this strategy, products are offered for sale individually and consumers who purchase only one of the products is later offered a discount on the other product.  Prior analysis has shown that RPP can be more profitable than other bundling strategies under the common assumption of independent and uniformly distributed reservation prices.  We use a simulation model to investigate the performance of RPP with more general assumptions about reservation prices.  The results show that RPP remains an attractive strategy provided not too many customers anticipate the discount and modify their behaviour to purchase only one product initially.  The main finding is that the optimal RPP strategy is relatively robust to the correlation in reservation prices.  
",Reserved product pricing with dependent and normally distributed reservation prices,[356],697,"[71, 124, 131]",1776,Pricing and Capacity Management,11,8,59,Pricing and Revenue Management,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S08 [building - 101],"['Marketing', 'Revenue Management and Pricing', 'Simulation']",TB-59
"In various research fields, understanding the relationship between predictors and a response variable involves curve estimation with specific characteristics. For instance, isotonic regression estimates mortality rates, assuming an increasing relationship between older age groups and mortality risk. Similarly, efficiency analysis can be reframed as a shape-restricted regression problem, aiming to estimate a non-decreasing and concave function that envelopes the observed data points. In this context, Data Envelopment Analysis [DEA] is commonly used for nonparametric production frontier estimation. However, DEA is susceptible to overfitting, resulting in overly optimistic efficiency estimates.

Recently, an adaptation of the Multivariate Adaptive Regression Splines [MARS] algorithm was introduced for production function estimation, addressing overfitting concerns. Our work builds upon this methodology, with three primary objectives. First, we propose a method to incorporate variable interaction during model fitting while maintaining shape constraints for production functions, enhancing predictive capacity. Second, we enhance robustness by randomizing data and input variables during model construction, drawing inspiration from the Random Forest [RF] methodology. Finally, within the RF framework, we can identify the most relevant inputs related to output prediction.
",Adaptive Constrained Enveloping Splines and Random Forest for Technical Efficiency Measurement,"[77407, 14048, 77412]",391,"[24, 66]",1777,DEA and its application,89,2,48,Data Envelopment Analysis and its Application,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,60 [building - 324],"['Data Envelopment Analysis', 'Machine Learning']",MA-48
"Scheduling of the sequential production stages Continuous Casting and Hot Rolling in steel production for a time horizon of about 7 days is a complex problem that requires considering technical constraints at both stages as well as limited capacities of upstream facilities [steelmaking and refining] and intermediate stages [slab processing and heat preservation pits]. Due to this complexity, decomposition is required to handle the problem. Based on the case of an Austrian steel plant we elaborate on the decomposition principles applicable, mainly temporal decomposition and two-stage block planning, where the former seems to be closer to practical applicability. In our research we pursue the latter. We show that the technical characteristics of the two stages lead to a combined two-stage lot sizing and slab assignment problem that can be solved by a fix-and-relax heuristic using an MILP solver. The sequencing problem on the continuous casters and the determination of start and completion times of the casts and of the rolling turns [timing] must be solved subsequently due to the problem decomposition. Numerical experiments indicate that the sequencing problem can be solved well by established methods provided that appropriate constraints are formulated at the lot sizing/assignment stage. The timing problem can be solved by resource-constrained project scheduling methods, but the capacity constraint of slab processing requires extension of the scheduling method we selected.",Integrated Scheduling of Continuous Casting and Hot Rolling in Steel Production - Modeling as a Two-stage Lot Sizing/Assignment Problem,"[2044, 61176, 77411]",804,"[129, 84, 111]",1779,Integrated lot-sizing problems,32,3,49,"Lot Sizing, Lot Scheduling and Production Planning","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,M1 [building - 101],"['Scheduling', 'Optimization Modeling', 'Programming, Mixed-Integer']",MB-49
"This research develops a novel three-stage stochastic location-allocation model for a preparedness-response disaster relief problem within a scenario-based stochastic programming setting. In the mainstream academic literature, preparedness-response activities are commonly represented within a two-stage stochastic programming framework assuming there is a single moment when all uncertain parameters are revealed. Grounded on practical disaster operations, our approach considers two moments within the disaster timeline in which different related uncertain parameters are unveiled - victims’ needs and donation. The main rationale is to represent practical disaster aftermath situations in which right after the disaster strikes a given region, victims’ needs become known and donation campaigns start collecting relief items. The precise and available donation amount is finally revealed when the donation campaign ends and it is very much dependent on the victims’ needs. In this three-stage model, the initial stage involves facility location decisions about establishing distribution centres and relief prepositioning, while the next two stages involve last-mile distribution decisions about prepositioned and donated supplies. And we fully account for deprivation costs arising from unmet victims’ needs in our modelling. Our results are based on real disasters in Rio de Janeiro, Brazil.",Equitable location-allocation in multistage humanitarian settings applied to Brazilian disasters,"[77408, 58203, 61420]",775,"[117, 58, 30]",1780,"Efficiency, equity and fairness in humanitarian operations",38,9,21,OR in Humanitarian Operations [HOpe],"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,49 [building - 116],"['Programming, Stochastic', 'Humanitarian Applications', 'Disaster and Crisis Management']",TC-21
"Adaptive gradient methods are popular in optimizing modern machine learning models, yet their theoretical benefits over vanilla Stochastic Gradient Descent [SGD] remain unclear. This presentation examines the convergence of SGD and adaptive gradient methods when optimizing stochastic nonconvex functions without the need for setting algorithm hyper-parameters based on problem-specific knowledge. First, we explore smooth functions and compare SGD to well-known adaptive methods like AdaGrad, Normalized SGD with Momentum [NSGD-M], and AMSGrad. Our findings reveal that while untuned SGD can reach the optimal convergence rate, it comes at the expense of an unavoidable catastrophic exponential dependence on the smoothness constant. Adaptive methods, on the other hand, eliminate this reliance without needing to know the smoothness constant in advance. We then look at a broader group of functions characterized by [L0, L1] smoothness. Here, SGD is shown to fail without proper tuning. We present the first instance of tuning-free convergence with adaptive methods in this context, specifically with NSGD-M, achieving near-optimal rate despite an exponential dependence on the L1 constant. We also demonstrate that this dependency is unavoidable for a family of normalized momentum methods.",Unveiling the Power of Adaptive Methods Over SGD - A Parameter-Agnostic Perspective,[77014],360,"[136, 19, 66]",1781,Adaptive and Polyak step-size methods,84,12,32,Advances in large scale nonlinear optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,41 [building - 303A],"['Stochastic Optimization', 'Continuous Optimization', 'Machine Learning']",WA-32
"We propose a new mixed integer formulation and an efficient metaheuristic for the Bi-objective Cumulative Capacitated Vehicle Routing Problem considering Priority Indexes [BCCVRP-Pr], a variant of the classical Capacitated Vehicle Routing Problem in which customers are served according to a certain level of preferences by a fixed fleet of heterogeneous vehicles. In this problem, two objectives are minimized - the total latency and the total tardiness of the system. The proposed mathematical formulation [a multilevel network approach] showed its effectiveness by outperforming the previous models presented in the literature, reporting efficient Pareto Fronts for instances up to 25 nodes. Regarding the metaheuristic, we developed a Non-Dominated Sorting Genetic Algorithm [NSGA-II] capable of dealing with larger instances [up to 100 nodes]. To evaluate and compare the performance of the metaheuristic procedure against the results obtained by the proposed exact model, we used four performance metrics - the quantity of non-dominated points, the hypervolume, the coverage of two sets, and the elapsed computational time. According to the results, the algorithm showed a compelling performance by providing high-quality Pareto fronts at competitive computational times. Our research provides valuable insights into solving the Cumulative Capacitated Vehicle Routing Problem with priority indexes, suitable for practical applications in industries that require prioritizing customer service.",A mixed integer formulation and an NSGA-II for the Bi-objective Cumulative Capacitated Vehicle Routing Problem with Priority Indexes,"[77410, 54921, 46774, 77413]",725,"[65, 14, 74]",1782,Vehicle Routing II,64,5,29,Combinatorial Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,157 [building - 208],"['Logistics', 'Combinatorial Optimization', 'Metaheuristics']",MD-29
"The “Data Science Gymkhana” is a yearly activity organized with the aim of promoting critical thinking and the development of the scientific method among secondary school students. It was born in 2020 as one of the activities of the program STEM4Girls developed at Universidad Carlos III de Madrid [Spain] to promote scientific and technological vocations among children and young people and, in particular, girls. The Gymkhana seeks to make visible the importance of data science as a driver of change and social transformation in achieving the Sustainable Development Goals [SDG]. Each edition is focused on one SDG and top researchers are invited to give a talk about her work. In this edition an Operational Researcher has been invited in connection to the SDG7 - Affordable and clean energy. Students have to analyze a data set related to the selected SDG, solve some riddles and present their analysis in relation to the actions we can make based on data. In this presentation, we describe the activity and discuss some of its outcomes and impact.",Data-driven decision making for secondary school students under the framework of Sustainable Development Goals,[46717],455,"[92, 7]",1783,Experienced Routes for the Teaching Students' Problem,48,4,16,OR Education,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,19 [building - 116],"['OR in Education', 'Analytics and Data Science']",MC-16
"The capacitated multi-item lot-sizing problem with setup times is known to be NP-hard. In our recent work, we proposed the 2-step construction heuristic [2-SCH], which delivers competitive solution quality within a short computational time and can be applied to various problem extensions with minimal modifications. It was shown that problem instances with high capacity utilization and high average time between orders are particularly challenging and, in the worst case, only solutions that use overtime capacity could be constructed. Leveraging the structure of the 2-SCH, we explore opportunities to enhance solution quality as well as counteract the possible use of overtime by incorporating the 2-SCH into a metaheuristic and designing an improvement heuristic that relies on information from the 2-SCH solutions.","Solving capacitated lot sizing problems using a combination of fast construction heuristic, metaheuristic and improvement heuristic","[68869, 13889]",837,"[74, 105, 69]",1786,Production planning problems,32,10,49,"Lot Sizing, Lot Scheduling and Production Planning","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,M1 [building - 101],"['Metaheuristics', 'Production and Inventory Systems', 'Manufacturing']",TD-49
"Open Banking is an innovative data-sharing framework designed for data interoperability among diverse financial and non-financial organizations, operating within a country’s regulatory confines. Interoperability works through the deployment of application programming interfaces [APIs], which enable a two-way exchange of banking data with customers’ consent. This scenario is poised to reshape the decision-making process of financial institutions, necessitating the refinement of strategies to implement customer-centric business models and augment profitability through the exploration of novel revenue streams. Facing the complexity of the banking market it's paramount to structure the decision context considering the multiple stakeholders and variables involved. This research presents a case study on a retail bank where stakeholders encountered challenges in structuring action plans for digital transformation following the launch of Open Banking. It introduces a Hierarchical Multi-criteria Decision-Aid [H-MCDA] framework for engaging stakeholders at various levels and proposes a Multi-objective Optimization [MOO] approach to assess and compare the effectiveness of multiple strategic plans, which aligns with technological innovation, customer interests, and the bank’s long-term goals. Our findings present a comparison between two strategic action plan scenarios - the bank’s intrinsic in-house technological developments and the outsourcing of technical expertise.",A Multicriteria and Multi-Objective Framework for In-House and Outsourced Technological Developments by Open Banking,"[76291, 70640, 77419, 77420]",603,"[25, 77, 44]",1787,Applications of Multiobjective Optimization,34,10,37,Multiobjective Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,33 [building - 306],"['Decision Analysis', 'Multi-Objective Decision Making', 'Finance and Banking']",TD-37
"Resources in warehouses include both workforce and material handling equipment and still remain a persistent challenge in warehouse planning operations. Since the productivity of any warehouse process increases with the speed at which the products are moved within the storage facility, optimizing resource planning is crucial for improving productivity and minimizing operational cost. An approach for optimizing resources is via simulation modelling. Indeed, Discrete-event simulation [DES] is one of the popular modeling techniques which has been used for warehouse process and resource optimization. To this end, this article, presents multiple simulation models with the aim to identify the optimal resources [i.e. workforce, docking stations, material handling equipment] needed for receiving, put-away, picking, packing and dispatching processes in a given case study with the aim to complete all processes in certain time horizon. For each process, various scenarios are developed that evaluate the impact of adopting different planning strategies. The models developed identify the mix of workforce and equipment that provide the best process completion time.  The results are encouraging, showing that the use of simulation models may identify the selection of the best workforce-equipment mix and subsequently improve warehouse operations. ",Simulation-based scenarios for optimizing resource planning in warehouse operations - A practical approach,"[76250, 61446]",657,"[65, 131, 43]",1791,Simulation and Modelling for Decision Support,45,14,45,Decision Support Systems,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,30 [building - 324],"['Logistics', 'Simulation', 'Facilities Planning and Design']",WC-45
"The increasing amount of waste generated, combined with a lack of proper management, has resulted in the development of policies to encourage the recycling of end-of-life [EOL] products. Extended producer responsibility [EPR] has been one of the most common regulations, holding producers accountable for the management of the products they manufacture. End-of-life tires [ELTs] and batteries are two of the most common EOL products covered by EPR laws. Different supply chain designs have been proposed in the literature to address the treatment of both products, but they address them separately. 
The study presents a multi-objective optimization model for the development of an integrated supply chain considering three different objective functions to assess economic, environmental, and social perspectives. The proposed model is applied to a case study in Chile's Metropolitan Region. The e-constraint methodology yields a Pareto front used to calculate a compromise solution. The supply chain exhibits a unique distribution compared to previous mono-objective iterations, featuring a mix of different technologies installed at both the centers and reprocessing plants. The system processes 33.17% of the total available EOL products. This total fulfills the minimum requirements established by the EPR law in Chile of 3% for batteries and 25% for ELTs. 
",Mixed sustainable supply chain design - a case of the recovery of end-of-life tires and batteries,"[50658, 77424, 77423]",923,"[77, 125, 28]",1794,Optimization for the Circular Economy,18,9,23,"Circular Economy, Remanufacturing and Recycling ","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,82 [building - 116],"['Multi-Objective Decision Making', 'Reverse Logistics / Remanufacturing', 'Developing Countries']",TC-23
"The segmentation of Magnetic Resonance Images [MRIs] is a challenging task due to the artifacts introduced by the acquisition process, namely intensity inhomogeneity [also known as bias field] and noise [which follows a Rician distribution].
In this work we introduce a model called MICCT, based on the Multiplicative Intrinsic Components framework and Cartoon-Texture decomposition techniques, to perform simultaneous denoising and bias-field correction of an MRI.
The output of MICCT can be then be segmented with any state-of-the-art segmentation strategy.
We introduce an ADMM strategy to solve the nonlinear and nonconvex model associated with MICCT and analyse its theoretical convergence properties.
Finally, we present some numerical experiments showing the effectiveness and the competitiveness of the proposed approach [combined with a K-means strategy] in the segmentation of noisy MRI from the BrainWeb database.",A multiplicative components framework for joint correction and segmentation of magnetic resonance images,"[50552, 77427, 62357]",421,"[113, 63, 9]",1795,Optimization and learning for data science and imaging [Part I],84,2,34,Advances in large scale nonlinear optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,43 [building - 303A],"['Programming, Nonlinear', 'Large Scale Optimization', 'Auctions / Competitive Bidding']",MA-34
"Directed acyclic graphs [DAGs] can be used to represent conditional
independence relations in joint probability distributions, and, with
suitable assumptions, can additionally represent causal relations
between the variables in a distribution. The problem of learning the
'best' DAG from data is often cast as a discrete optimisation problem
- which is known to be NP-hard. Encoding this problem as an integer
linear program [ILP] is not hard, but solving large instances requires
a careful combination of cutting [adding linear constraints during
solving] and pricing [adding new ILP variables during solving]. In
this talk I will discuss 'lessons learned' in tackling the
DAG-learning problem with ILP and discuss the more ambitious task of
learning models [causal and non-causal] which allow for latent variables.",Learning Directed Acyclic Graphs using Integer Linear Programming,[76383],207,"[14, 66, 13]",1797,Combinatorial Optimization for Machine Learning,64,4,26,Combinatorial Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,012 [building - 208],"['Combinatorial Optimization', 'Machine Learning', 'Column Generation']",MC-26
"The Xpress Mosel language is used for implementing large optimization and analytics projects. Many such projects are deployed as Xpress Insight apps, either on individual software installations or in a cloud-based environment.
To develop, build, deploy, and maintain such a large code base a team of OR specialists typically works in collaboration with other experts [UI specialists, problem domain experts]. The availability of a suitable toolset for building, analyzing and testing the code in a largely automated way is a must nowadays. The Mosel distribution comes with a set of open-source tools supporting these development tasks. 
In this talk we review the roles and usage of the different tools to support the operation of enterprise-grade OR applications. We start with the build process [including the generation of online documentation], we then move on to the topics of testing and code analysis [profiling and coverage], and finally debugging. We consider in particular the case of Insight apps.","Building, testing, analyzing, and debugging FICO Xpress Mosel projects","[21574, 31389]",240,"[76, 148, 134]",1798,Modeling Languages,76,8,30,Software for Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,53 [building - 208],"['Modeling Systems and Languages', 'Web-based Information Systems', 'Software']",TB-30
"Cervical cancer, responsible for over 300,000 deaths in 2020, ranks as the fourth most common cancer. In 2018, the World Health Organization introduced a strategy to partially eradicate it, emphasizing national screening programs to identify the human papillomavirus [HPV], the primary risk factor. A region in Spain have planned to implement comprehensive population-wide screening. This study examines optimization challenges within this program, including HPV detection tests and cytology screenings. While HPV tests are conducted in hospitals, samples and cytologies are collected at community health centers [CHCs] or homes. Midwives at CHCs may need additional hours to attend to patients. Critical resources in this program are the machines in hospitals to process the tests and the midwives. The optimization problem can be modeled in three phases. The first phase selects the hospitals to process the tests, assigns each CHC to a hospital, and calculates the extra cost of midwives in each CHC. The second phase determines the number of tests each CHC sends weekly to their hospital and establishes the program's start and end dates in each CHC. Finally, the third phase calculates how many patients to contact weekly in each CHC to meet the conditions of the second phase. Computational tests using real data validate the models.",Optimising Resource Allocation for a Cervical Cancer Screening Program,"[1085, 13449, 1081, 79661]",973,"[56, 72, 12]",1799,Capacity and treatment planning in healthcare,3,14,10,OR in Health Services [ORAHS],"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,11 [building - 116],"['Health Care', 'Mathematical Programming', 'Capacity Planning']",WC-10
"This work provides a practical algorithm for solving the generation expansion planning problem for both transmission and distribution systems. Conventional and wind-based generators are eligible to be installed under a set of scenarios that captures the uncertainty arising from different operating conditions, demand fluctuations and wind dynamics. Assuming there is one planner at the transmission level and distinct planners for each of the analyzed distribution systems, an optimal expansion plan for the whole power system is obtained. The resulting problem, which is formulated as an instance of scenario-based linear programming, is solved here in a coordinated but distributed fashion by using the Alternating Direction Method of Multipliers [ADMM]. Unlike other works that also facilitate transmission and distribution planners’ coordination, the proposed solution methodology not only guarantees the finite convergence to optimality but also the privacy of information related to each planner’s assets. Privacy is achieved by solely exchanging information on the injections at the interface nodes among neighboring transmission and distribution systems. The algorithm’s performance is tested on small- and medium-sized systems, revealing a reasonable tradeoff between solution quality and computation times for a practical setting.",Coordinated Generation Expansion Planning for Transmission and Distribution Systems via ADMM,"[76923, 77430, 77431, 27748]",247,"[110, 117, 37]",1802,Planning problems in electrical energy systems,23,2,21,OR in Energy,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,49 [building - 116],"['Programming, Linear', 'Programming, Stochastic', 'Energy Policy and Planning']",MA-21
"Research on behavior and soft systems emphasizes the role of 'socio-emotional demands' in decision-making processes. Causal loop diagrams, which represent the complex mental models of decision-makers, play a crucial role in achieving a shared understanding of the system elements and boundaries. However, it is not fully understood which elements might be overlooked in the process of reaching a common understanding, and why this occurs. Drawing from critical systems thinking, we propose that during the creation of causal loop diagrams intended to aggregate diverse mental models, certain variables and links might be unintentionally neglected. We introduce the concept of two types of boundaries in these diagrams - structural [the interconnections between system elements] and emotional boundaries [how stakeholders perceive the variables with varying valence and activation]. The complex interaction between these boundaries can lead to parts of the system being ignored. Through a case study on sustainable urban development, our research deepens the understanding of how boundaries are compared and elements of the system are marginalized. Our goal is to encourage the integration of socio-emotional and cognitive perspectives in analyzing causal loop diagrams in complex systems.",Integrating Socio-Emotional and Cognitive Perspectives in Causal Loop Diagrams Analysis,"[77207, 46682, 68042]",131,"[133, 140, 10]",1803,OR Innovations in Policy Making - A,26,3,13,Soft OR and Problem Structuring Methods,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,15 [building - 116],"['Soft OR', 'System Dynamics and Theory', 'Behavioural OR']",MB-13
"As a promising sustainable transportation solution, Electric Vehicles [EVs] require efficient strategies for their refueling to complete their journey. This study aims to address the charging station location problem [CSLP], considering multiple charging options and varied driver behaviours. We first integrate the Multi-Path Refueling Location Model [MPRLM] with various charging types, including slow, fast, and battery-swapping charging stations. This model considers different charging rates of multiple charging stations, assuming a linear recharging rate over time. To balance the investment cost of charging stations and the service level for EV users, our objective involves minimizing the total cost for operators and the overall charging time for EV users. Experimental tests on a 25-node network indicate the extended MPRLM outperforms the original model by better balancing system costs, charging time, and station utilization. Then, the Integrated Behavioral Multipath Refueling Location Model [IB-MPRLM] is introduced to incorporate varied charging profiles of EV users. We categorize private EV drivers based on their sensitivity levels to range anxiety, charging costs, and charging time. For each EV driver category, the satisfaction function and satisfaction threshold are established. In this IB-MPRLM, the charging station located at a certain node can only refuel an EV when the user's satisfaction is higher than the threshold of the EV user.",Strategic Locations of EV Charging Stations - Catering to Various Charging Types and Diverse Driver Profiles,"[76621, 11678, 47655]",580,"[64, 93, 139]",1804,Location of Alternative Fuel and Charging Stations,29,3,61,Locational Analysis,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S10 [building - 101],"['Location', 'OR in Energy', 'Sustainable Development']",MB-61
"Carbon policies are often limited to specific regions. Therefore, carbon leakage leads to the relocation of production to regions without carbon policies. To prevent carbon leakage, the European Commission recently adopted the Carbon Border Adjustment Mechanism. It imposes carbon tariffs on carbon emissions imported into regulated regions. 
We present a new mixed-integer model formulation for global and multi-period supply network design subject to carbon tariffs and location-specific carbon policies [SND-CT-CP]. Consequently, locations differ in whether and how carbon policies are active at the respective location. Here, we include carbon tax, carbon cap-and-trade and carbon caps as location-specific carbon policies. In contrast, carbon tariffs are imposed on emissions imported from unregulated locations. 
As we propose a strategic planning problem with a long-term planning horizon, we include uncertainties in the SND-CT-CP. These uncertainties focus on stochastic carbon caps, which have a particular influence on the supply network design.
Within the presented SND-CT-CP, we consider a four-layer supply network. Accordingly, the locations of manufacturing plants and distribution centers are planned. Suppliers are selected based on resilience considerations. In addition, the demand includes price-dependent levels. Thus, the objective of the SND-CT-CP is to generate a supply network configuration that maximizes the expected net present value.
",Green supply network design with carbon tariffs and location-specific carbon policies,"[73544, 13866]",484,"[64, 79, 137]",1806,Sustainable Supply Chain Design,19,2,24,Sustainable Supply Chains,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,83 [building - 116],"['Location', 'Network Design', 'Strategic Planning and Management']",MA-24
"In this work we address the problem of deciding where to install service points when it is certain that the total demand is greater than the capacity that the installed system can achieve. The starting point is a set of potential locations for service points and a set of locations where customers with known demands are. There is a budget to install p service centers with different capacities, although the sum of these capacities is less than the sum of the demands. The problem is deciding where to install these p centers using allocation criteria from conflicting claims problems. Equal losses, equal awards or proportional allocation approaches are considered. All the optimization models we introduce are mixed linear, their optimal solutions illustrate the effect of scarce resource management on the service centers location. Likewise, in addition to including different fairness criteria in the objective function of the optimization models, we also analyze different families of constraints that allow us to guarantee other desirable properties such as that two customers with the same demand have similar services. We check the conditions under which the solutions of our models are efficient in the sense that all capacity is allocated. Finally, we illustrate the behavior of the optimization models by solving instances of different sizes.",Capacitated service location when fairness principles are significant,"[77432, 1174, 58522, 1313]",43,"[84, 111, 64]",1807,Advances in Location Analysis ,29,2,61,Locational Analysis,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S10 [building - 101],"['Optimization Modeling', 'Programming, Mixed-Integer', 'Location']",MA-61
"In this talk we present some sharp functional [Sobolev, log-Sobolev] inequalities on metric measure spaces that are curved in the sense of Lott-Sturm-Villani, by using optimal transport theory. As an application, we provide a sharp weighted hypercontractivity estimate for the Hopf-Lax semigroup related to the Hamilton-Jacobi equation. ",Sharp functional inequalities on metric measures spaces via optimal transport theory,[57068],86,"[81, 0]",1808,Optimization on Geodesic Metric Spaces II - Nonsmooth case,69,7,41,Optimization on Geodesic Metric Spaces - Smooth and Nonsmooth,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,97 [building - 306],['Non-smooth Optimization'],TA-41
"Gurobipy makes it very easy to build optimization models that are naturally 
formulated with multi-dimensional constraints.  In particular you can use
familiar concepts from NumPy like dimensions, shape, vectorization and
broadcasting in combination with gurobipy's matrix-friendly objects to
construct your optimization model.  In this talk  we will walk you through
the functionality, discuss performance aspects, and present best practice
code patterns.",Matrix-friendly modeling with gurobipy,[59783],703,"[76, 0]",1809,Python Modeling Tools,76,10,30,Software for Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,53 [building - 208],['Modeling Systems and Languages'],TD-30
"Shelf space is a fundamental resource in retail and thus it is vital to manage it efficiently. In traditional shelf space planning, the number of facings is defined in advance and it is assumed that this number will not be changed during the operational replenishment process in the store. In operational practice, however, it is often beneficial to deviate from this tactical shelf plan to reduce in-store replenishment costs. Increasing the facing number of a product decreases the required effort for replenishing this product as more items fit in the showroom shelf space. However, due to limited shelf space, it will be necessary to reduce the facing number of other products which in turn may lead to additional in-store replenishment effort. Adapting the shelf layout further causes rearrangement costs, most notably facing elimination and shifting costs. This paper introduces the flexible shelf space refilling problem in which total in-store replenishment and rearrangement costs are minimized. We illustrate in detail the current process of adapting the number of facings during the refilling process and give some empirical insights to demonstrate the practical relevance of the problem. To provide decision support, a first seminal mathematical model is presented and its suitability to improve the operational shelf space refilling process is demonstrated based on a first case study. Lastly, we give an overview on possible future research directions in this new area of research.",The flexible shelf space refilling problem,"[49639, 72311, 25668, 1131]",480,"[65, 149, 72]",1810,Retail Inventory Management II,30,4,50,Retail Operations,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,M2 [building - 101],"['Logistics', 'Problem Structuring', 'Mathematical Programming']",MC-50
"Meet your peers in a friendly, informal way, through discussions in groups of 2-3 persons with pairings changing after a fixed time span of a few minutes.
OR is a team business, and knowing people you can turn to for ideas, feedback and support makes all the difference. But it is not always easy building your network, especially if you are shy or feel that you are an outsider. This welcoming session is a way of overcoming the barriers to networking, and enjoying yourself while you do it.

You may find it easier if you come with an idea of how you can introduce yourself to others in just two minutes. But don't worry if you don't have time to prepare – you'll soon pick it up. 

If you have business cards, do bring them along; if not, we’ll have plenty that you can use if you choose. 
",Speed Networking,[46961],491,"[151, 0]",1812,Speed Networking,40,3,46,Making an Impact,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,40 [building - 324],['Practice of OR'],MB-46
"This talk investigates a long-step interior-point framework for solving linear complementarity problems. The algorithmic framework combines the approach of Ai and Zhang and the algebraically equivalent transformation technique proposed by Darvay. We investigate a set of sufficient conditions on the transformation function applied in the algebraically equivalent transformation technique, under which the general algorithmic framework's convergence and best known iteration complexity can be proved.
We analyze the theoretical and practical role of different neighborhood definitions. As expected, the proposed conditions depend on the definition of the neighborhood of the central path.
",Ai-Zhang-type interior-point algorithms for solving linear complementarity problems,"[57860, 38402]",139,"[60, 72]",1814,Interior point methods,68,3,38,"Conic Optimization - Theory, Algorithms, and Applications","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,34 [building - 306],"['Interior Point Methods', 'Mathematical Programming']",MB-38
We study a reinforcement learning approach to portfolio selection when the underlying asset price process follows a regime-switching diffusion model. An analytical solution to the HJB equation for the exploratory mean-variance model under regime switching  is derived and implementation in practice employing a deep neural network will be discussed. Empirical results investigating robustness with respect to model mis-specification will also be presented.,Reinforcement Learning for Portfolio Optimization in a Regime-Switching Model,[71314],436,"[45, 126, 20]",1815,Novel Optimization Models in Finance,4,12,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S14 [building - 101],"['Financial Modelling', 'Risk Analysis and Management', 'Control Theory']",WA-63
"The Pickup and Delivery Routing Problems [PDRPs] are challenging combinatorial optimization problems with significant real-world applications. This paper proposes new Reinforcement Learning [RL] methods to address this type of routing problems.  The typical mathematical programming formulations and the existing solution methods are first presented. Then, the concepts of Reinforcement Learning [RL] and Deep Learning are introduced, and the PDRP is reformulated as a RL problem. Several RL algorithms including new proposals are discussed and their performance is compared to an exact solution. This research contributions include employing RL to solve PDRP and comparing various RL algorithms including new proposals.",New Reinforcement Learning Algorithms for Pickup and Delivery Routing Problems,"[76697, 11924, 6900, 51368]",786,"[145, 66]",1816,Heuristics for Vehicle Routing 3,5,3,58,VeRoLog - Vehicle Routing and Logistics,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S07 [building - 101],"['Vehicle Routing', 'Machine Learning']",MB-58
"Prematurity [birth before the 37th week of gestation] is a leading cause of neonatal mortality and heightened complications in newborns. In Brazil, the annual cost of premature births is approximately R$8 billion, with newborns spending an average of 51 days in intensive care, totalling a yearly cost exceeding R$15 billion, so improving the health system for mothers and premature births is essential. Data Envelopment Analysis [DEA] is a vital tool to evaluate healthcare system efficiency across territories, but measuring efficiency over time requires re-implementing DEA models. Consequently, Machine Learning [ML] emerges as a viable solution for predicting efficiency scores through supervised learning. Therefore, this study integrates DEA, specifically the Constant Returns to Scale [CRS] model with input orientation, with ML techniques to develop a predictive model for healthcare system efficiency based on prematurity and incorporating indicators such as the number of physicians, beds, healthcare establishments, and per capita health expenditure [R$] in Brazil's microregions. The DEA analysis assesses each microregion's readiness to care for premature newborns, providing inverted efficiency indicators where Decision-Making Units [DMUs] with values near or equal to one suggest lower performance. This approach enables determining efficiency for new DMUs through ML, based on DEA indicators and results, while identifying sectors needing improvement in the healthcare system.",Data envelopment analysis with machine learning in healthcare efficiency - a prematurity study,"[1667, 77429, 77434, 77438, 77439, 77440]",515,"[24, 66, 56]",1817,"Advancements of OR-analytics in statistics, machine learning and data science 7",16,10,28,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,065 [building - 208],"['Data Envelopment Analysis', 'Machine Learning', 'Health Care']",TD-28
"Produced from renewables, green hydrogen can be used as a clean fuel in the transportation and power generation sectors. According to the International Energy Agency, the annual production of low-emission hydrogen could reach 38 million tons in 2030. However, hydrogen transportation across a large system produces significant costs and emissions, undermining its decarbonization purpose. This talk will present a supply chain optimization model that addresses variability and uncertainty in green hydrogen and ammonia production. Specifically, the optimization model will consider the production and storage of green hydrogen and ammonia and their transportation to different destinations via tube trucks, trucks, rail, or ships. The uncertainty-aware formulation will model the variability and uncertainty of renewables during hydrogen and ammonia production.",Will hydrogen remain green?  Supply chain optimization for green hydrogen transportation considering production uncertainty,"[77226, 76965]",471,"[93, 138, 136]",1819,Stochastic models in energy systems planning and operations,21,9,22,Energy Management,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,81 [building - 116],"['OR in Energy', 'Supply Chain Management', 'Stochastic Optimization']",TC-22
"Effective production scheduling is essential for optimizing productivity and meeting demand within dynamic production systems. This study conducts a comparative analysis of two modeling techniques, Process Network Synthesis [PNS] and Time-Constrained Process Network Synthesis [TCPNS], in the context of mass production. Both techniques, rooted in P-graphs, excellent in managing complex and flexible recipes.
The precedence-based resource scheduling model employing TCPNS, provides high precision in schedule generation and effective handling of complex changeover times. However, the required computational effort may exceed practical limitations. In contrast, the discrete time process flow model using PNS offers a fast computation through time discretization and a series of combinatorial techniques to reduce the complexity of the mathematical model of the initial problem and sub-problems.
In addition to the comparative analysis, this study examines algorithmic model generation for both precedence and discrete-time formulation, with multiple resolutions in the latter case. This nuanced approach provides an enhanced understanding of the practical applicability and computational efficiency of PNS and TCPNS methodologies, particularly in real-life furniture manufacturing scenarios.",Comparison of Discrete Time and Precedence Based MILP Formulations of Production Scheduling for Furniture Manufacturing,"[77443, 73942, 75411]",808,"[129, 14]",1821,Machine scheduling problems,32,14,49,"Lot Sizing, Lot Scheduling and Production Planning","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,M1 [building - 101],"['Scheduling', 'Combinatorial Optimization']",WC-49
"Despite the significant advancements in optimization models for route planning and their implementation in planning tools, planners often deviate from the recommended solutions offered by these tools. To detect the factors that influence such deviations, we analysed field data obtained from six small-sized transportation companies in the Netherlands. A meta-analysis across these companies suggests that [i] the prioritization of important customers and [ii] the level of complexity of the planning problems affect the extent to which planners deviate from the recommended routes. Planners learn which customers are more important using customers’ history of orders and take that into consideration in route planning. However, the impact of customer priority on routing decisions varies depending on the complexity of the planning task. Planners increasingly adhere to the recommended routes when faced with more complex routing problems. This can possibly be explained by the fact that prioritizing important customers, and simultaneously minimizing the distance of the route, requires more cognitive load. As a result, planners deviate less from the tool's recommendations for complex routing problems, even when such routes serve a high number of important customers. ",The effect of customer priority and planning complexity on deviations from model-generated recommendations in route planning,"[70499, 61931, 55094]",94,"[10, 145]",1822, Behaviour and decision processes ,13,12,07,Behavioural OR,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1019 [building - 202],"['Behavioural OR', 'Vehicle Routing']",WA-07
"The $882 billion textile trade in 2021 raises environmental concerns, emphasizing the importance of promoting a circular economy for sustainable textiles. As a result, policies must prioritize textile recycling, especially in developing countries, as well as information sharing across the value chain. Thus, the purpose of this study was to investigate the potential environmental benefits of two industrial recycling processes for mixed textile residues. We conducted a life cycle assessment using the ReCiPe method at the midpoint and endpoint levels, with a focus on generating significant data availability and a broader assessment than existing literature to support decision-making related to textile residue recycling.
The results of the textile residue recycling process to obtain stripes [R1] and replace sawdust to fill pushing balls demonstrate that it would produce environmental benefits regardless of location in several midpoint categories. In terms of endpoint results, the DALY savings are primarily due to avoiding final disposal, while the savings in ecosystem impacts are generated by avoiding landfill and sawdust production. In the recycling process to obtain recycled yarn and fill [R2], endpoint results show that the DALYs of all avoided processes are 1.5 times those of all R2 recycling processes, owing primarily to the avoidance of virgin yarn production. Thus, both recycling methods are recommended.

",Life Cycle Assessment of Different options for Mixed Industrial Textile Recycling,"[77435, 50658, 74154]",926,"[139, 40]",1823,Recycling,18,13,23,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,82 [building - 116],"['Sustainable Development', 'Environmental Management']",WB-23
"The increasing demand for healthcare services poses significant challenges in effectively managing patient flow, particularly concerning patients classified as Alternative Level of Care [ALC]. These patients, although no longer in need of acute care, often encounter obstacles to discharge and cause several issues such as hospital overcrowding and compromised health outcomes. This study uses administrative health data from Canadian hospitals and proposes using machine learning models to identify potential ALC patients and estimate their hospital length of stay as early as their admission time. The findings show the efficacy of the eXtreme Gradient Boosting algorithm in accurately predicting potential ALC patients, while the Random Forest regression model surpasses others in forecasting the length of stay for ALC patients. To understand how the predictions are made from the features of the dataset, the Shapley values were analyzed and used to identify the most important features of the dataset and their impact on machine learning outcomes.  Using the most important features, two sets of user-friendly and easy-to-follow guidelines were developed for the hospital staff to proactively identify the ALC patients and estimate their length of stay, and mitigate the patient flow challenges posed by the ALC patients. Hospitals can use such decision-making tools to optimize resource allocation, enhance operational efficiency, and ultimately improve patient care outcomes.",Predicting delayed discharge from hospitals - A Decision support system using machine learning,"[56720, 77444, 52917]",611,"[56, 66, 7]",1825,Machine learning and game theory in healthcare,3,2,15,OR in Health Services [ORAHS],"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,18 [building - 116],"['Health Care', 'Machine Learning', 'Analytics and Data Science']",MA-15
"This paper addresses the dynamic stochastic purchasing routing problem faced by a purchaser in negotiating with multiple suppliers to meet its demand. The purchaser contacts suppliers sequentially and must decide whether to buy based on purchase prices, available quantities, and routing costs. Each supplier can be contacted only once, with uncertain price and quantity information revealed upon contact. The purchaser faces a maximum number of contacts within a planning horizon. This problem is challenging because of the dynamic stochastic components of the negotiation process. The purchaser must weigh the information revealed by the current supplier against the remaining suppliers' behaviors and route consolidation to make decisions. We propose a rollout algorithm that combines dynamic programming with a perfect information model to determine whether to purchase from the current supplier and which supplier to contact next. Our method samples the supplier behavior scenarios and solves independent deterministic problems to estimate the opportunity cost of not purchasing from remaining suppliers or the impact of purchasing from the current supplier. We introduce a reverse discount factor to reduce the optimism in perfect information model estimation. Our method is validated against benchmark policies using real procurement data from an e-commerce platform. Results confirm the superior performance of the rollout algorithm and provide insights into benchmark policy effectiveness.",A Rollout Algorithm for Dynamic Stochastic Purchasing Routing With Perfect Information Model Estimation,"[58766, 29757, 46258, 58784]",745,"[145, 108, 65]",1827,Dynamic Vehicle Routing 1,5,9,64,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S16 [building - 101],"['Vehicle Routing', 'Programming, Dynamic', 'Logistics']",TC-64
"Sparse optimization seeks an optimal solution among vectors with at most k nonzero coordinates.
This constraint is hard to handle, and a strategy to overcome that difficulty
 amounts to adding a norm penalty term to the objective function. The most widely used penalty
 is based on the l1-norm which is recognized as the archetype of sparsity-inducing norms.
 In this talk, we present generalized k-support norms, generated from a given source norm,
 and show how they contribute to induce sparsity via support identification.
 In case the source norms are the l1- and the l2-norms, we analyze the faces and normal cones 
 of the unit balls for the associated k-support norms and their dual top-k norms.
",The Geometry of Sparsity-Inducing Balls,"[67932, 12378, 77451, 77452]",294,"[19, 21]",1831,Variational Analysis and Subdifferential techniques,82,9,42,Variational Analysis and Continuous Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,98 [building - 306],"['Continuous Optimization', 'Convex Optimization']",TC-42
"Several important open problems in chemical physics, mathematics, and operations research can be posed as nonconvex global optimization problems involving the Euclidean norm. For example, pairwise potential energy minimization, object packing and cutting, the kissing number problem, and planar facility location problems admit a representation of this form. We present new formulations and convexification strategies for problems involving the Euclidean norm and use them to solve several instances to global optimality for the first time. ",Convexification of optimization problems involving the Euclidean norm,"[57131, 77454]",720,"[11, 52]",1835,Mixed Integer Nonlinear Programming and Nonconvex Optimization ,86,13,04,MINLP,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1001 [building - 202],"['Branch and Cut', 'Global Optimization']",WB-04
"BARON, under development since the early 1990s, has evolved into a robust computational system for solving continuous and discrete optimization problems to global optimality. The current generation of BARON focuses on enhancements in hybrid relaxations, presolve methods, convexification techniques, heuristics, and robustness improvements. Our experiments underscore the importance of all algorithmic components within the branch-and-reduce framework. Computational comparisons against state-of-the-art global and local solvers highlight BARON's superior performance.",Global optimization of continuous and discrete nonlinear programs with BARON,"[70666, 57131]",238,"[52, 0]",1836,MINLP Solvers,76,3,30,Software for Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,53 [building - 208],['Global Optimization'],MB-30
"While electrification of urban transit systems is underway worldwide, it is essential to integrate it with renewable energy sources to make it fully sustainable. The intermittent nature of renewables poses a challenge in deciding the required solar panels and battery storage capacity at charging locations. To address these challenges, we propose a two-stage stochastic programming model considering uncertainties in solar energy generation and bus energy consumption under dynamic time-of-use electricity prices. Specifically, we formulate the problem as a multi-scenario linear program and employ a Benders’ decomposition approach where the first stage [long-term] master variables determine the contracted power grid capacity, battery storage capacity, and the area of solar panels installed at each charging location. The second stage variables [scenario-specific] associated with each child problem prescribe the energy transferred to buses from the grid or solar-based power systems during layovers. We present a case study on the Arlington bus network, US, for 52 scenarios with 3337 trips, where 145 buses and 26 charging locations are required as per a concurrent scheduler algorithm. The solar energy generation data is collected from the National Renewable Energy Laboratory database. Our results reveal that the Benders’ decomposition is faster compared to the Simplex algorithm, and the scenario-based schedule adapts better to the uncertainties than the average scenario schedule.",Charge Scheduling with Renewables for Electric Buses Using Stochastic Optimization ,"[65659, 77462, 77463, 66067]",815,"[136, 84, 119]",1843,Electric Busses,85,8,51,Public Transport Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,M5 [building - 101],"['Stochastic Optimization', 'Optimization Modeling', 'Public Local Transportation Systems']",TB-51
"Ensuring affordable and reliable access to energy in rural communities remains a critical challenge for achieving Sustainable Development Goal 7 [SDG7]. Minigrids [MGs] powered by renewable energy [RE] sources offer a potential solution, but their long-term financial sustainability remains a hurdle. This paper proposes a novel approach employing strategic optimization to enhance MG financial viability through the integration of productive use of energy [PUE]. The framework meticulously sizes MGs and generation technologies [solar PV, biomass, etc.] to align with varying seasonal consumer demands. This optimization approach aims to match periods of low-capacity utilization with peak energy processing activities, consequently reducing the levelized cost of electricity [LCOE] for MG users. The benefits are twofold - increased capacity utilization and reduced operational costs, leading to significantly lower consumer tariffs. Through a comprehensive case study, this research demonstrates the effectiveness of strategic optimization in meeting energy needs while ensuring the long-term financial viability of MGs. Notably, the findings emphasize the critical role of improved capacity utilization in fostering MG financial sustainability. This research offers valuable insights for policymakers, practitioners, and stakeholders in the RE sector, demonstrating the potential of optimally designed MGs to drive lasting positive change in rural communities.",A Strategic Framework for Maximizing Utilization of Minigrids in India - Achieving Financial Sustainability and Productive Energy Integration,"[72195, 77470]",712,"[37, 139, 93]",1844,Empowering Energy Access,21,15,22,Energy Management,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,81 [building - 116],"['Energy Policy and Planning', 'Sustainable Development', 'OR in Energy']",WD-22
"In 1960, under the simple title “An automatic method of solving discrete programming problem”, the method for solving mixed-integer-programming problems was introduced. This Branch-and-Bound framework, with enhancements, remains at the core of discrete optimization software to this day, more than 60 years later. We will discuss the many pioneering ideas of this paper, and also the two women pioneers of operational research, Ailsa Land and Alison Doig Harcourt, who co-authored this landmark paper.",The origin of the Branch-and-Bound Method,[3301],19,"[88, 111]",1845,Moments in the history of OR  1,27,13,20,Moments in the history of OR,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,45 [building - 116],"['OR History', 'Programming, Mixed-Integer']",WB-20
"Vehicle electrification represents one of the most promising solutions to significantly reduce road transport emissions, paying attention to the full energy cycle. However, truck electrification has downsides such as economic investments, limited battery life and charging time requirements. Cooperation is another lever for increasing sustainability - sharing transport orders allows the reduction of the number of trucks needed to meet the transport demand, resulting in increased profits and reduced CO2 emissions. 
This paper proposes an optimization model for the cooperative planning of road transport operations, matching trips owned by different carriers and assigning them to a fleet of diesel and electric trucks. The final objective is to maximize the total carriers’ profit while reducing emission costs. Time windows, haulers’ driving constraints, maximum duration of electric batteries, charging times and emissions costs are considered. A compensation mechanism is introduced to motivate carriers to share their trips. The problem is solved using exact approaches and evaluated using a real data-set related to the port of Genoa and its hinterland. An experimental campaign is carried out, making sensitivity analysis on some significant factors of the problem. A numerical analysis is realized to evaluate the computational effort necessary to solve large-size case studies and an appropriate heuristic approach is designed for large problem instances.
",Port-hinterland transport - addressing sustainability with cooperation and electric trucks,"[65430, 77464, 24097, 75770]",167,"[143, 84, 110]",1846,Sustainable freight transportation,52,10,62,OR in Port Operations,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S12 [building - 101],"['Transportation', 'Optimization Modeling', 'Programming, Linear']",TD-62
"We consider equitable linear optimization problems [ELOP], which are multiobjective optimization problems, with each objective representing the benefit that one entity receives. In such problems, the concept of dominance is replaced by equitable dominance. The aim is to find the set of equitably nondominated points, which can be done by solving ordered weighted averaging [OWA] scalarization. Each solution corresponds to a different set of weights that make the solution optimal, hence a different degree of inequity aversion. We discuss a novel use of the parametric simplex algorithm to address ELOP. The algorithm not only provides the set of equitably nondominated solutions but also yields the corresponding weight space decomposition. We also propose an alternative method based on geometric duality to compute the weight space decomposition given the set of nondominated solutions.",Weight Space Decomposition for Multiobjective Linear Programming in the Context of Equitable Optimization,"[14206, 56337]",12,"[112, 110, 77]",1847,Objective Space-Based Approaches in Multiobjective Optimization,34,7,37,Multiobjective Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,33 [building - 306],"['Programming, Multi-Objective', 'Programming, Linear', 'Multi-Objective Decision Making']",TA-37
"Motivated by the practical aircraft routing problem in civil flight planning, we consider a variant of the shortest path problem with inclusionary and exclusionary constraints, specifying whether node Y must or must not be visited after node X is visited.
The problem is defined on a directed graph G, with a source node S and a sink node T. We introduce an algorithm that generates an augmented graph G’ from the original graph G, ensuring compliance with the inclusionary and exclusionary constraints. This transformation reduces the constrained shortest path problem in G to a classic shortest path problem in the augmented graph G’. For each inclusionary constraint, we modify the original graph to ensure that all paths from X to T pass through Y. For each exclusionary constraint, we modify the original graph so that all paths from X to Y are redirected to a duplicate node Y', which has no outgoing arcs, ensuring that no path extended from X to T through Y/Y’.
It need to be proved that - [1] There are no paths violating the constraints in G’; [2] For each valid path in G, there exists a corresponding path in G’; and [3] For each path in G’, there is a corresponding path in G. We demonstrate the algorithm's validity through proofs by contradiction.
Numerical experiments demonstrate the efficiency of our algorithm. Furthermore, the algorithm has been successfully integrated into our flight planning system, indicating its practical utility.",The shortest path problem with path constraints,"[57851, 77024, 79713, 80031]",516,"[4, 143, 5]",1848,"Advancements of OR-analytics in statistics, machine learning and data science 6",16,9,28,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,065 [building - 208],"['Airline Applications', 'Transportation', 'Algorithms']",TC-28
"The ability to guide vehicles through the best routes to their destinations according to dynamic traffic information is called for in transportation and logistics operations. However, it is well known that the dynamic routing problem is very difficult to tackle due to dynamic nature of road networks, traffic evolution, and time-sensitive delivery constraints. It is noted that existing route planning methods often fail to adapt to non-recurrent traffic congestion and especially unexpected events such as accidents. That is, effective route planning methods must be equipped with dynamic strategies for dynamic adaptation to traffic changes. Therefore, our research intends to develop a dynamic route planning method by leveraging dynamic traffic data and predictive modeling techniques. More specially, this method timely adjusts vehicle routes based on changing traffic conditions and time constraints. Note that the proposed dynamic route planning method focuses on signalized urban road networks. We use simulation technique to verify the proposed method and carry out sensitivity analysis.",Dynamic Routing Problem With Time Windows in a Signalized Road Network,"[76806, 11924, 77528]",784,"[145, 65, 143]",1851,Dynamic Vehicle Routing 2,5,10,64,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S16 [building - 101],"['Vehicle Routing', 'Logistics', 'Transportation']",TD-64
"In this study, we propose novel multicriteria product prioritization and classification methods that use criteria from different functions of a manufacturing firm. We transform the scores for each criterion using a quantile transformer. In the prioritization study, the goal is to choose products for pre-build inventory and cycle-stock inventory. Criteria weights are determined with the Analytic Hierarchy Process [AHP] approach. In the classification study, the goal is to develop customized production and inventory policies for different groups of products. To this end, we use Principal Component Analysis [PCA] and Fuzzy C-Means [FCM] clustering. We illustrate the use of the methods with two case studies from a leading tire manufacturer. The methods are transparent and easily understood by company managers. Being structured, data-based as well as flexible, these methods can ease the decision burden on managers, and contribute to quick and well-tailored operational decisions. ",Multicriteria Product Prioritization and Classification Methods  to Support Production and Inventory Decisions,[25298],894,"[105, 66, 6]",1852,Pairwise comparisons and preference relations 4,44,13,44,Multiple Criteria Decision Analysis,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,20 [building - 324],"['Production and Inventory Systems', 'Machine Learning', 'Analytic Hierarchy Process']",WB-44
"In optimization, many solvers are based on iteratively building a local model of the objective function and then minimizing the model instead of the original function. In the smooth case, such models can be derived from the Taylor expansion based on derivatives of different orders. In the nonsmooth case on the other hand, models are more challenging to construct - The first issue is the lack of a Taylor expansion. While replacing the gradient in smooth first-order methods by the Clarke subdifferential from nonsmooth analysis yields a way to characterize descent directions, there is no simple way to derive higher-order models. The second issue is that generalized derivatives like the Clarke subdiff. are difficult to work with in practice, since they can be unstable and impossible to evaluate in the general case.
In this talk, I will present two models for [unstructured] locally Lipschitz continuous functions. The first model is a simple first-order model, which is based on approximating the Clarke subdiff. by the Goldstein eps-subdiff. which, in turn, can be approximated in practice by a deterministic gradient sampling approach. The second model is based on the idea of sampling Hessian matrices in addition to the gradient. More precisely, it is defined as the maximum of [existing] second-order Taylor expansions in a neighborhood of a point. After introducing each model, I will present ways to generate them in practice and discuss the behavior of the resulting descent methods.",First- and second-order models for nonsmooth functions based on derivative sampling,[61859],509,"[81, 0]",1854,Structured nonconvex optimization ,70,13,41,Nonsmooth Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,97 [building - 306],['Non-smooth Optimization'],WB-41
"This study addresses the tactical planning aspect of a dynamic technician routing and scheduling problem spanning several days and with home depots. The challenge involves pre-known tasks and dynamically arriving tasks, requiring efficient scheduling to minimize overall driving distance. Technicians, each possessing specific skills, are dispatched to locations with tasks having time windows and skill requirements. The goal is to minimize driving distance while maximizing task completion. The extended model introduces substitution skills, allowing less qualified technicians to serve tasks with a time penalty. The study presents both an MIP formulation and an ALNS algorithm for optimization. Results, derived from real-life data, are compared against routes executed in an actual company, where technicians can only handle tasks matching their skill sets.",Workforce scheduling and routing with substitution skills,"[71608, 64035]",233,"[145, 129, 74]",1855,Novel topics and recent advances in solution approaches in scheduling,64,3,26,Combinatorial Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,012 [building - 208],"['Vehicle Routing', 'Scheduling', 'Metaheuristics']",MB-26
"The purpose of the line planning problem in railways is to determine a set of lines with their route, stops and frequency. This line plan is an important aspect of the quality of the service that a railway undertaking [RU] provides to its passengers. For example, it determines which origin-destination pairs are connected by a direct trip or need a transfer between lines. Although the railway demand is varying throughout the day in volumes and directions, the line plan is often constant throughout the day. To better match the supply with this varying demand, we present a mixed-integer linear programming model for multi-period line planning. With this model, we intent to determine a line plan for each period with different demand, that minimizes the generalised journey time [GJT] of the passengers. To meet the varying demand, we allow the model to make changes to the selected routes in the network, the stopping pattern, and the frequency in each period. Furthermore, we include the possibility of having asymmetric lines to deal with spatially unbalanced [peak hour] demand. Although changing the line plan during the day has a benefit for the passengers in terms of reduced GJT, it also comes with costs - RUs must create multiple plans and passengers need to adapt to multiple plans per day. Therefore, the ε-constraint method is used to create Pareto optimal solutions. We use Gurobi to solve the proposed model for a case study based on real data of part of the Dutch railway network.","Multi-period line planning for varying railway demand considering stops, frequencies and asymmetric lines","[77467, 68555, 77469, 31981]",376,"[122, 111, 112]",1857,Demand-responsive public transport 1,85,13,54,Public Transport Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S01 [building - 101],"['Railway Applications', 'Programming, Mixed-Integer', 'Programming, Multi-Objective']",WB-54
"Smooth progression of platoons of vehicles along urban arterials can significantly contribute to environmental objectives, especially when heavy-duty vehicles are involved. Generally, signal controls such as fixed-time, vehicle-actuated and adaptive aim to minimise delays, which may conflict with reducing the number of stops to improve air quality and GHG emissions. Overcoming this challenge requires dynamically synchronising the signal timings of a series of intersections with approaching vehicles to simultaneously reduce delays and the number of stops. A fundamental prerequisite of such controls is real-time prediction of platoon dynamics including forming, dispersion, stopping and arrival after departing from upstream. Existing coordination methods usually focus on flow estimation at aggregate levels, losing critical information for multi-criteria signal operations. Our study introduces a novel method for predicting platoons’ dynamics with high resolution under mixed-traffic environments. Utilizing various data sources, including loop detectors, vehicle-to-infrastructure [V2I] communication and floating car data, we employ attention-based graph neural networks to predict platoon arrival and stop times while considering current and anticipated signal phases. Experiments conducted on real-world datasets from an arterial in Enschede, the Netherlands, reveal the suitability of our approach for coordinated signal controls to reduce the number of stops and enable green waves.",High-resolution Platoon Prediction for Coordinated Traffic Control along Urban Arterials,"[70832, 62655, 9109]",547,"[143, 66, 84]",1858,Advancing mobility towards sustainable solutions II,6,10,56,Transportation,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S04 [building - 101],"['Transportation', 'Machine Learning', 'Optimization Modeling']",TD-56
"Machine learning tasks rely heavily on the quality of input data, yet acquiring adequate datasets can often be challenging. Useful datasets are typically distributed amongst various owners who may, in practice, be competitors in downstream markets, making them reluctant to share information. In contrast to existing frameworks that address distributed and privacy-preserving [incentive-free] learning, we explore here a novel market-based framework, called regression markets, to provide financial incentives for data sharing. Agents aiming to improve their forecasts can post a regression task, for which others can contribute by sharing their data and get monetarily rewarded for it. We introduce the market design, provide its desirable properties, and discuss some of the open challenges with practical applications of treating data as a tradeable good.",Regression Markets - Incentivizing Data Sharing for Forecasting,"[76632, 71120, 78463]",430,"[47, 93, 50]",1859,Data Valuation from Data-driven Optimization,49,12,35,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,44 [building - 303A],"['Forecasting', 'OR in Energy', 'Game Theory']",WA-35
"PyPSA [Python for Power System Analysis] is an open-source toolbox to simulate and optimise modern power and energy systems. It encompasses various functionalities, such as conventional generators and links with unit commitment, variable wind and solar generation, storage units, coupling to other energy sectors, and mixed alternating and direct current networks. PyPSA is graph-based and designed to scale well with large networks and long time series.
 
PyPSA is used in several projects, such as PyPSA-Eur, which focuses on the European energy system. PyPSA-Eur automates the creation of highly detailed models of the European transmission networks of several energy carriers, incorporating renewable energy potentials, conventional power plants, and demand data across different sectors. This enables researchers and policymakers to conduct extensive analyses of network expansion, storage needs, and sector coupling strategies essential for achieving carbon neutrality.

This presentation will explore the functionalities, applications, and recent advancements of PyPSA and PyPSA-Eur. We will explore case studies demonstrating their use in optimising energy systems for cost-effectiveness and sustainability. Attendees will gain insight into the potential uses of these software tools in energy system planning and operational research.","An Introduction to PyPSA - Exploring Open-Source, High-Resolution Energy Planning","[77415, 70705]",703,"[84, 37, 134]",1861,Python Modeling Tools,76,10,30,Software for Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,53 [building - 208],"['Optimization Modeling', 'Energy Policy and Planning', 'Software']",TD-30
"Planning in logistics and transportation is oftentimes complicated by a high degree of uncertainty about the actual travel distances, times, or costs. While stochastic optimization is concerned with making optimal decisions under such uncertainty, it disregards that in many practical applications, uncertainty can be reduced upfront through research and tests, also known as explorable uncertainty. However, while uncertainty reduction through exploration can improve decision-making, it oftentimes comes at a cost, and one needs to balance exploration costs and solution quality. We study the vehicle routing problem with time windows [VRPTW] and stochastic travel times where uncertainty about travel times can be reduced by making queries to a traffic data provider while respecting an overall querying budget. This converts the stochastic VRPTW into a partially deterministic problem that we solve via point-based approximation and sample average approximation to deal with the remaining uncertainty. We present different methods to make good and fast querying decisions based on statistical features and learning and show their effectiveness in an extensive numerical study. By assuming different degrees of uncertainty, correlations, and time-window restrictions, we give detailed insights into the value of uncertainty exploration in routing.",Explorable Uncertainty in Routing,"[75198, 77473, 9112]",162,"[145, 135, 66]",1862,Stochastic Models in Logistics,50,8,39,Stochastic Modelling,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,35 [building - 306],"['Vehicle Routing', 'Stochastic Models', 'Machine Learning']",TB-39
"Our society is currently facing various sustainability challenges such as climate change, water pollution, and biodiversity loss. Decision-making problems aimed at solving these challenges are associated with considerable uncertainty. Uncertainty can arise from various sources such as measurement errors, difficulties to establish cause-effect relationships, disagreements between experts, or future unpredictability. Not only is the nature of uncertainty extensive, but other explanatory factors for effective uncertainty communication such as individual differences between recipients and the context of the situation must also be taken into account. Navigating the many factors essential to effective uncertainty communication makes this a challenging task. In our work, we examine the critical role of uncertainty communication from a decision analytical, economic, and psychological perspective. We address the definition, interpretation, and diverse communication formats for uncertainty, with a specific focus on enhancing understanding within multi-criteria decision analysis [MCDA] projects. It is crucial that stakeholders have an adequate understanding of uncertainty to identify their preferences and appropriate options. Therefore, our contribution is valuable for finding options to solve pressing sustainability problems and managing uncertainty in decision-making.",The critical role of uncertainty communication in informed decision-making,"[77282, 77474, 77475, 31727]",570,"[25, 10, 126]",1864,Behavioral Decision Analysis III,13,5,11,Behavioural OR,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,12 [building - 116],"['Decision Analysis', 'Behavioural OR', 'Risk Analysis and Management']",MD-11
"Despite recent progress in semi-autonomous vehicle [semi-AV] development, achieving an optimal autonomous traffic system faces challenges due to the low penetration rate of semi-AVs and their need for human oversight. To improve traffic flow in such environments with human-driven vehicles [HVs] and semi-AVs, we advocate the use of dedicated lanes and develop an innovative framework to optimize the network performance by planning the mode choice of semi-AVs [either human-driven or auto-driven]. Specifically, we introduce a three-dimensional Macroscopic Fundamental Diagram [3D-MFD] to characterize the complex flow dynamics in mixed traffic. Our analysis identifies that an optimal proportion of human-driven and auto-driven vehicles can significantly maximize network flow. Inspired by this key finding, we construct an optimization model to regulate semi-AVs’ mode choice, aiming to optimize vehicle proportions within mixed traffic networks. Although solving the model presents challenges due to its nonlinearity and integer constraints, especially in large networks with numerous intersections, the unique structure of the proposed 3D-MFD enables us to derive an analytical solution. This solution significantly reduces the computational complexities of the optimization model, allowing the resulting control strategy to be implemented in real time and substantially improves network performance.",Analytical Strategy for Optimizing Mixed Traffic Networks with Semi-Autonomous Vehicles,"[77141, 77123, 78758]",505,"[150, 113, 131]",1866,Traffic flow modeling ,6,4,56,Transportation,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S04 [building - 101],"['Network Flows', 'Programming, Nonlinear', 'Simulation']",MC-56
"Building and understanding powerful Machine Learning algorithms requires statistical thinking supported by accessible and powerful tools. JMP Pro brings advanced predictive modeling into the classroom, without any need for coding. 

As teaching examples, automated Model Screening and state-of-the-art Torch Deep Learning will be demonstrated from a student's perspective. We will discuss course use and teaching best practices. ",Teaching Predictive Analytics without Coding,[38380],455,"[66, 92, 134]",1867,Experienced Routes for the Teaching Students' Problem,48,4,16,OR Education,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,19 [building - 116],"['Machine Learning', 'OR in Education', 'Software']",MC-16
"Most e-commerce companies design combined delivery-pickup logistics systems, where the collection of unsatisfactory items is ensured along with the traditional distribution of products to customers. In this context, a successful option is to solve a Vehicle Routing Problem with Divisible Deliveries and Pickups [VRPDDP] where each customer requiring both a pickup and a delivery service may be served, if beneficial, in two separate visits. In the version of the problem analyzed in our study, there are mandatory delivery and pickup demands that must be fulfilled by a fleet of homogeneous vehicles. In addition, it is needed to ensure that a percentage of optional pickups is served. Optional pickup demands are affected by uncertainty and random variables are used for their representation. In this sense, we refer to our problem as stochastic VRPDDP and model it as a two-stage stochastic program with recourse. More specifically, a set of routes is designed at the first stage. At the second stage, when uncertainty is revealed, it may be impossible to implement the solution as planned at the first stage. Then, two different recourse actions are carried out. The computational experiments confirm the usefulness of using stochastic programming in realistic setting.",A Vehicle Routing Problem with Divisible Deliveries and Pickups under Uncertainty,"[13546, 77477, 42989]",273,"[145, 136, 125]",1869,Urban Logistics and sustainable TRAnsportation - OPtimization under uncertainTY and MAchine Learning,49,4,35,"Stochastic, Robust and Distributionally Robust Optimization","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,44 [building - 303A],"['Vehicle Routing', 'Stochastic Optimization', 'Reverse Logistics / Remanufacturing']",MC-35
"This paper examines the optimal behavior of carbon-intensive companies operating under the European Union Emission Trading System [EU ETS], wherein they are obligated to purchase emission permits on the secondary market if their emissions exceed their allowance. Specifically, we consider the scenario where these companies, typically required to procure permits at market prices, are endowed with the [real] option to invest in Carbon Capture and Storage [CCS] technology to mitigate their emissions and, thus, permit expenditures. The central challenge is the determination of the optimal time for investment in CCS within a stochastic framework characterized by uncertainty in EU ETS permit prices. To enhance realism, we address the problem for a heterogeneous group of companies distinguished by their respective costs of implementing CCS systems, reflecting varying degrees of environmental consciousness across industrial sectors. Furthermore, we incorporate an increasing floor for permit prices to mirror policy efforts aimed at promoting green transition by elevating emission costs. We solve this problem analytically and through numerical simulations calibrated with real market data. In addition to offering insights into individual company behaviors, the findings can inform decision-makers in refining environmental policies, particularly regarding the management of permits price floor and its potential to expedite the green transition.",Equilibrium Carbon Permit Supply and CCS Investment under Uncertainty,"[67961, 61089, 61552, 61205]",854,"[33, 37, 82]",1870,Dynamics of the Firm II,90,4,33,Optimal Control Theory and Applications,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,42 [building - 303A],"['Economic Modeling', 'Energy Policy and Planning', 'Optimal Control']",MC-33
"We consider the problem of allocating wavelengths of an optical network in order to serve a set of connection requests. In order to provide a robust allocation, communication must be ensured even in case of failure of some link, which is obtained by defining, for each connection request, two disjoint paths in the network. Each connection uses its working path by default, and is rerouted to its unique recovery path in case the working path is unavailable. As uniquely assigning both paths to a single connection may be highly inefficient, we adopt a shared protection policy, in which the same wavelength and path can be shared among different connections, provided that no collision arises. We discuss alternative relaxations for the problem based on Mixed-Integer Linear Programming formulations, and introduce a metaheuristic algorithm based on the Ruin-and-Recreate paradigm. Computational experiments on realistic data show that our algorithms are able to produce feasible solutions with tight optimality gap.",Lower and upper bounds for resources allocation in lightpath communication networks,"[7400, 77478, 77476, 2813, 24902]",206,"[14, 53, 141]",1871,Graph and network optimization,64,9,25,Combinatorial Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,011 [building - 208],"['Combinatorial Optimization', 'Graphs and Networks', 'Telecommunications']",TC-25
"In this study, we propose a novel coordinated supply contract scheme integrating bargaining power [BP] considerations with capacity planning. Our scheme empowers manufacturers to design supply contracts conducive to channel coordination under arbitrary BP scenarios. It comprises two supply contracts - capacity cost sharing [CCS] and surplus capacity compensation [SCC] contracts. In the CCS contract, the manufacturer shares a portion of the supplier's capacity investment cost in a revenue-sharing framework. Conversely, the SCC contract entails the supplier bearing the cost of unused capacity. These contracts incentivize the supplier to invest in production capacity in alignment with the overall supply chain capacity, regardless of the BP scenarios. 
We demonstrate that our contract scheme can satisfy any BP structure. Specifically, the SCC contract facilitates channel coordination under higher supplier BP, while the CCS contract leads to coordination under lower supplier BP. We also analyze the interrelation between contract parameters for channel coordination and establish the relationship between bargaining power and optimal supply contracts. Our findings reveal that revenue sharing and capacity cost sharing fractions are positively proportional to each other in the coordinated CCS contract, and the compensation rate for the surplus capacity increases as capacity cost sharing rate increases to achieve a coordinated SCC contract.
",Supply contracts with investment cost sharing under arbitrary bargaining powers,"[64766, 78728, 44625]",810,"[138, 65, 61]",1872,Lot-sizing with game theory aspects,32,9,49,"Lot Sizing, Lot Scheduling and Production Planning","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,M1 [building - 101],"['Supply Chain Management', 'Logistics', 'Inventory']",TC-49
"The circular economy encourages a move away from the traditional linear corporate approach to production and consumption in favor of integrated systems with external connections and in which materials circulate in a closed circular loop leading to substantial improvements in resource efficiency, energy savings and achievement of sustainability goals. In this transitional process, access to bank financing for firms is a key challenge not only in the perspective of circular businesses but also in the perspective of banks which must consider new factors that may influence the creditworthiness evaluation process with an impact on risk management. Consequently, circular businesses and banks are linked in a new systemic and complex approach both in practice and in academic literature. The aim of this research is to explore this twofold area and understand its main characteristics in order to develop future research on this topic. To achieve this objective we developed a conceptual framework that guided a literature review based on a selection of 42 articles explored through a bibliometric and thematic analysis. Our results confirm the complexity of the relationship between circular businesses and banks - in the implementation process circular businesses are mainly faced with financial barriers; banks do not consider the impact of circularity factors in assessing credit risk and default probability of circular businesses; there is a need for banks to shift from a shareholder vision.",Financing the circular economy - effects on bank risk management,"[77482, 50941, 80281]",386,"[44, 126, 100]",1874,New Challenges for Risk Management ,4,10,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S14 [building - 101],"['Finance and Banking', 'Risk Analysis and Management', 'OR in Sustainability']",TD-63
"This paper proposes a new approach to modeling of vehicle routing problems with a MinMax objective. At the core of the approach, a Set Partitioning model is formulated and used for minimizing infeasibility of the considered vehicle routing problem for a given upper bound on the objective value. Generally, the optimal value of the MinMax objective is obtained as the smallest upper bound for which the Set Partitioning model obtains a total infeasibility of zero. Computational experience shows very promising results.",A Set Partitioning Model for MinMax Vehicle Routing,"[6803, 36026]",755,"[145, 0]",1875,Column Generation for Vehicle Routing,5,4,58,VeRoLog - Vehicle Routing and Logistics,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S07 [building - 101],['Vehicle Routing'],MC-58
"Green bonds are widely utilized financial instruments designed for the purpose of funding environmentally friendly projects. Positive environmental effects of green bonds have been analyzed previously. These investigations largely rely on the Difference-in-difference models to gauge the overall impacts. We investigate the influence of corporate green bonds on the environmental performance of issuers at an individual level and employ a Controlled Interrupted Time Series Model. When statistically significant effects on the issuers’ environmental performance, measured by the E score of the issuers’ ESG, are determined, two sequential experiments are performed - firstly, we probe into the factors that can influence the issuance of green bonds. Secondly, we examine the interrelations between company characteristics, issuer characteristics, and the magnitude of the effects released by green bonds. To address the first and second experiment, we build a random forest and a generalized additive model, respectively. Our findings indicate that the environmental performance of most issuers improves following the issuance of green bonds. While both bond and company characteristics influence the impact of green bonds, it is the company’s characteristics that play a more pivotal role.",Green Bonds and Environmental Scores - analysing impact and factors of environmental performance,[76128],246,"[66, 139, 45]",1876,"AI in Eco-Finance - Time, Space, and Networks",53,3,08,AI & Innovation in Sustainable Finance,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,1020 [building - 202],"['Machine Learning', 'Sustainable Development', 'Financial Modelling']",MB-08
"GAMS Engine SaaS is a cloud-based service that allows users to run GAMS jobs on a scalable and flexible infrastructure, currently provided by Amazon Web Services [AWS]. It was launched in early 2022 and has since attracted a variety of customers who benefit from its features, such as horizontal auto-scaling, instance sizing, zero maintenance, and simplified license handling. GAMS Engine SaaS is especially suitable for workloads that require large amounts of compute power and can be adapted to many different scenarios. In this presentation, we show a case study of a large international consultant agency that uses GAMS Engine SaaS to run Monte-Carlo simulations of a large energy system model in response to varying climate change scenarios. We describe how they leverage the GAMS Engine API to submit and monitor their jobs, how they select the appropriate instance type for each job, and how they can use custom non-GAMS code on Engine SaaS. We also discuss the challenges and benefits of using GAMS Engine SaaS for this type of application, and provide some insights into the future development of the service.",GAMS Engine SaaS - A Cloud-Based Solution for Large-Scale Optimization Problems,[56010],240,"[134, 151]",1877,Modeling Languages,76,8,30,Software for Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,53 [building - 208],"['Software', 'Practice of OR']",TB-30
"Proposals to include adjustments such as brown penalising and green supporting factors in the prudential regulation are meant to direct bank lending towards environmentally friendly projects. However, such adjustments can blur the lines between prudential credit risk assessment and environmental objectives. Favouring green projects, although clearly socially responsible in the long term, may channel bank lending towards excessively risky assets in the short–term and provide a distorted picture of the true financial health of the bank. We adopt a principal–agent approach to formalise this trade–off and highlight its impact on bank lending. We also show that banks could also decide to redirect lending towards green projects without a direct intervention of the regulator. For instance, investor pressures.","Green-Supporting Factors, Brown-Penalising Factors and the Prudential Framework -  A Theoretical Approach","[77486, 77484]",275,"[45, 40]",1880,Optimization Model for Novel Risks in Finance and Climate,4,5,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S14 [building - 101],"['Financial Modelling', 'Environmental Management']",MD-63
"Influence diagrams are an intuitive representation of decision problems with dependencies between uncertain events, decisions, and consequences. Recently, two frameworks that transform influence diagrams into mixed-integer-linear programs [MILP] have been developed. Decision Programming [Salo et al., EJOR 299/2, 2022] enforces constraints on different possible realizations of the whole problem. This exposes the full probability distribution of consequences to be used in the optimization model, which in turn allows risk to be explicitly constrained or optimized. Rooted junction tree approach [Parmentier et al., Informs Journal on Optimization, 2/3, 2020] decomposes an influence diagram into clusters, that are subsets of nodes, and finds the probability distributions of these clusters with a MILP. Compared to decision programming, the rooted junction tree model offers better computational performance, but risk cannot be straightforwardly represented in the optimization model. We present approaches and conditions that combine the computational efficiency of rooted junction trees and the modeling flexibility of decision programming. More specifically, we highlight how risk can be directly represented in rooted junction tree models with examples. ",Risk averse mixed-integer linear programming models for influence diagrams,"[77261, 67611, 57798]",483,"[136, 25, 126]",1882,Decision problems represented as influence diagrams,49,12,34,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,43 [building - 303A],"['Stochastic Optimization', 'Decision Analysis', 'Risk Analysis and Management']",WA-34
"In this talk, we introduce large-scale mathematical optimization problems that arise within the complex landscape of the information and communication industry. Our focus spans several practical scenarios, including resource allocation in large scale data networks, traffic planning in optical networks, as well as task allocation and scheduling in deployment operations. These optimization challenges predominantly fall into the categories of LP or MILP. The scale of these problems varies significantly, ranging from millions to billions of variables and constraints. Some medium-sized problems necessitate real-time decision-making, with solving times in milliseconds to seconds. The larger problems demand hours of computational effort for offline planning. The stringent requirement poses considerable challenges, especially given the constraints of single-server computational resources. We aim to strike a balance between customized and generic design approaches across multiple dimensions, including optimality, efficiency, robustness, and generality. Leveraging Huawei’s OptVerse optimization solver, the design solution directly translates to increased revenue or reduced costs. Additionally, some contributions further enhance OptVerse’s generic problem-solving performance. Lastly, we explore the concept of co-optimization of software and hardware—a new dimension that promises innovative solution design, novel development modes, and potential business models for future solvers.",Recent Advances in Solving Large-Scale Optimizations from an Industry Perspective,"[70665, 77683, 77686, 77461, 77692, 77465, 77693, 78539]",297,"[63, 38, 72]",1883,Advances in Optimization for Industrial Applications,64,12,29,Combinatorial Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,157 [building - 208],"['Large Scale Optimization', 'Engineering Optimization', 'Mathematical Programming']",WA-29
"The radius of robust feasibility provides a numerical value for the largest possible uncertainty set that guarantees feasibility of a robust counterpart of a mathematical program with uncertain constraints. The objective of this review of the state-of-the-art in this field is to present this useful tool of robust optimization to its potential users and to avoid undesirable overlapping of research works on the topic as those we have recently detected. In this paper we overview the existing literature on the radius of robust feasibility in continuous and mixed-integer linearly constrained programs, linearly constrained semi-infinite programs, convexly constrained programs, and conic linearly constrained programs. We also analyze the connection between the radius of robust feasibility and the distance to ill-posedness for different types of uncertain mathematical programs.",A review on the radius of robust feasibility of uncertain mathematical programs,"[18816, 4799, 77491, 42455]",266,"[127, 0]",1885,Infinite Optimization - stability and duality,82,12,42,Variational Analysis and Continuous Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,98 [building - 306],['Robust Optimization'],WA-42
"The moment-SOS hierarchy provides a sequence of lower bounds on the minimum of a polynomial f on a semialgebraic set S. These bounds can be computed by solving semidefinite programs of increasing size, corresponding to sum-of-squares representations of increasing degree. As a consequence of powerful Positivstellensätze from real algebraic geometry, the bounds are known to converge to the true minimum of f under mild assumptions on the feasible region S. In this talk, we discuss some recent progress on the asymptotic behaviour of the SOS-hierarchy for certain special choices of S, such as the unit hypercube.
",Convergence analysis of the sum-of-squares hierarchy for polynomial optimization,[77488],155,"[115, 0]",1886,Applications of polynomial optimization,68,4,38,"Conic Optimization - Theory, Algorithms, and Applications","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,34 [building - 306],"['Programming, Semidefinite']",MC-38
"For semidefinite programs where the solution is nonunique but the dimension of the optimal face is smaller than the dimension of the affine space, it is not immediately clear how to round the numerical output of an interior-point solver to an exact optimal solution. Rounding each entry of the solution matrices often does not work since the analytical center of the optimal face is complicated to describe. Projecting into the affine space also does not work, since the projection will not be positive semidefinite. In [Dostert, de Laat, Moustrou 2021] a rounding approach using the LLL algorithm is given. I will explain our improved method, using the LLL algorithm in different ways, which is much faster for large semidefinite programs. This allows us to prove the optimality of several spherical codes using semidefinite programming bounds.",From approximate to exact optimal solutions of large semidefinite programs,"[75160, 77492, 76625]",378,"[115, 5, 60]",1888,Recent advances in LP and SDP for discrete optimization problems,68,12,38,"Conic Optimization - Theory, Algorithms, and Applications","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,34 [building - 306],"['Programming, Semidefinite', 'Algorithms', 'Interior Point Methods']",WA-38
"Recently, deep reinforcement learning [DRL] has been applied to solve scheduling problems in real-time. These methods involve generating a policy through interaction with thousands of random instances, leading to enhanced solutions by maximizing a reward. However, real-world instances often exhibit repetitive patterns and represent only a small subset of all possible scenarios. Additionally, obtaining a large number of examples is uncommon in real-world applications. In this paper, we propose a methodology to learn an optimized dispatching rule tailored to a specific set of instances. To do this, we model the scheduling problem as a Markov process, employ graph neural networks to represent instances, and utilize behavioral cloning alongside optimal solutions to simpler instances to determine the best policy. Given the challenge of training with limited instances, we suggest initially training a general scheduler using diverse instances. Subsequently, this model undergoes retraining to adapt to specific distributions, involving fine-tuning of its last layers for efficient adaptation. To validate our approach, we conduct experiments on both the job-shop scheduling problem [JSSP] and the flexible JSSP in several public scheduling benchmarks. Results demonstrate that our method, trained using behavioral cloning, outperforms DRL-trained models. Additionally, we show that the retraining strategy enables superior performance compared to a general model trained with random instances.",Solving scheduling problems in real-time through deep learning methods,"[76823, 63773, 63726]",517,"[14, 8, 129]",1889,"Advancements of OR-analytics in statistics, machine learning and data science 8",16,12,28,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,065 [building - 208],"['Combinatorial Optimization', 'Artificial Intelligence', 'Scheduling']",WA-28
"The goal of this talk is to provide sharp spectral gap estimates for problems involving higher-order operators [including both the clamped and buckling plate problems] on Cartan-Hadamard manifolds. The proofs are symmetrization-free -- thus no sharp isoperimetric inequality is needed -- based on two general, yet elementary functional inequalities. The spectral gap estimate for clamped plates solves a sharp asymptotic problem from Cheng and Yang [Proc. Amer. Math. Soc., 2011] concerning the behavior of higher-order eigenvalues on hyperbolic spaces, and answers a question raised in Kristály [Adv. Math., 2020] on the validity of such sharp estimates in high-dimensional Cartan-Hadamard manifolds. As a byproduct of the general functional inequalities, various Rellich inequalities are established in the same geometric setting.",Sharp spectral gap estimates for higher-order operators on Cartan-Hadamard manifolds,[61877],904,"[81, 0]",1890,	Optimization on Geodesic Metric Spaces I - Smooth case,69,5,41,Optimization on Geodesic Metric Spaces - Smooth and Nonsmooth,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,97 [building - 306],['Non-smooth Optimization'],MD-41
"Imbalances between demand and capacity at the world’s busiest airports cause air traffic delays, which can have serious financial and environmental consequences. To alleviate the burden on runway capacity, various types of interventions are possible. At the pre-tactical stage [up to a few hours before a day of operations], the schedule can be adjusted by imposing ground delays on flights in order to avoid “hotspots” of delay occurring at peak times. At the tactical stage [during the day of operations itself], sequencing of take-offs and landings can be optimised in order to minimise average time separations between runway movements.

This talk will discuss the progress of an EPSRC-funded project [EP/X039803/1] aimed at integrating pre-tactical and tactical interventions in order to reduce air traffic delays at Heathrow Airport. We formulate a stochastic dynamic optimisation problem in which the “system state” at any given time includes hundreds of variables evolving via continuous-time stochastic processes. Solution approaches via approximate dynamic programming are possible in theory, but very difficult to implement in practice. Instead we consider an approach based on the emerging field of “simheuristics”, which involves continuously simulating possible trajectories of future random events and using a ranking and selection method to jointly optimise both runway sequencing and ground holding delay decisions.",Stochastic Dynamic Models for Reducing Air Traffic Delays at Heathrow,"[67675, 47772]",331,"[143, 135, 131]",1891,Airline Applications I,6,4,55,Transportation,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S02 [building - 101],"['Transportation', 'Stochastic Models', 'Simulation']",MC-55
"Healthcare institutions, including hospitals and retirement homes, face various challenges. They struggle with tight budgets due to government savings and rising costs. Moreover, they deal with a shortage of care staff due to high workloads, forcing them to make an effort to relieve non-care tasks [i.e., logistics tasks]. These challenges put significant pressure on healthcare institutions. To address these challenges, hospitals aim to reduce costs and improve healthcare logistics while ensuring high-quality care. One key opportunity to improve healthcare logistics is consolidating inventory across healthcare institutions. This entails healthcare institutions within a network pooling their inventory from individual warehouses into one or a few central care hubs. Decision support for inventory pooling is facilitated by integrated decision-making on location and inventory. Adopting this integrated decision-making can cut costs and enhance healthcare logistics while at least maintaining quality of service. This research aims to develop innovative models and solution algorithms, providing decision support to improve healthcare logistics by integrating location and inventory decisions. This talk will discuss key findings from a new literature review on integrating these decisions, covering 44 papers across contexts like manufacturing, healthcare, spare parts, etc. Moreover, a first mathematical model focusing on integrating these decisions in a healthcare context will be proposed.",Integrating location and inventory decisions in healthcare supply chains - review and first model,"[75024, 23971, 23979]",768,"[65, 64, 61]",1892,Location in Logistics and Supply Chain Management,29,10,61,Locational Analysis,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S10 [building - 101],"['Logistics', 'Location', 'Inventory']",TD-61
"In this paper, we propose to determine optimal portfolios using the Herd Behavior Index [HIX, Dhaene et al. [2012]]. The HIX is a diversification measure that provides information about the extent to which stock prices move together in the same direction. The optimal minimum-HIX, as well as mean-HIX, portfolio can be seen as the most diversified one since it has the lowest degree of co-movement. We make use of a reformulation method for determining the minimum-HIX, and mean-HIX, portfolios, as their closed-form expressions are not readily available in a general setting. This also allows us to study the mean-HIX efficient frontier. We prove the existence of the minimum-HIX, and mean-HIX, portfolios in the general setting, and provide their closed-form expressions in the two-stock case. We also study how to determine the minimum-HIX, and mean-HIX, portfolios when short-selling is allowed. This requires us to generalize the definition of HIX in order to ensure that the [theoretical] comonotonic portfolio always exceeds the [actual] portfolio in convex order.  

[1] Dhaene, J., Linders, D., Schoutens, W. & Vyncke, D. [2012], ‘The herd behavior index - A new measure for the implied degree of co-movement in stock markets’, Insurance - Mathematics and Economics 50[3], 357–370. 
",Portfolio selection based on the Herd Behavior Index,"[77001, 77496, 77497]",571,"[45, 83]",1894,Methodology in asset allocation and banking,74,5,57,Modern Decision Making in Finance and Insurance,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S06 [building - 101],"['Financial Modelling', 'Optimization in Financial Mathematics']",MD-57
"Demand-responsive multimodal transit offers opportunities to complement existing public transport systems and provide a better service level to passengers while simultaneously making better use of the resources. This study optimizes the capacity of such a system by strategically sizing the required fleet and allocating it to the operating services. We formulate a two-stage stochastic optimization model that plans the transit system and the required fleet in the first stage and optimizes the demand-responsive operations in the second stage. We develop a decomposition-based method that exploits the network-based formulation of the second stage, allowing us to solve practical instances. Results from a case study in Zurich show that jointly designing and operating public transport and on-demand systems can benefit transport operators and passengers. By allocating the system capacity more efficiently, operators reduce operational costs while maintaining or improving the travel experience for passengers.",Dynamic capacity planning for demand-responsive multimodal transit,"[65221, 58406]",587,"[119, 136, 63]",1897,Demand-responsive public transport 2,85,14,54,Public Transport Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S01 [building - 101],"['Public Local Transportation Systems', 'Stochastic Optimization', 'Large Scale Optimization']",WC-54
" In this project we help a retailer increase his revenue and decrease his waste by introducing a new selling strategy. Instead of offering his original, fixed priced assortment, he will additionally sell an ‘opaque product’ with a dynamic price as he sees fit. As he does not now how his clientele will respond to this new product and its price, we help him to find the right balance between earning [by optimizing his revenue] and learning [by gathering enough information to be able to estimate the effect of the new product]. An interesting problem with many mathematical challenges.
","Good stuff - Matching supply and demand by introducing an opaque product - Pricing, learning, fun.",[77304],696,"[124, 108, 135]",1898,Learning and pricing,11,7,59,Pricing and Revenue Management,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S08 [building - 101],"['Revenue Management and Pricing', 'Programming, Dynamic', 'Stochastic Models']",TA-59
"The Ubiquity Generator framework [UG] was originally designed to parallelize powerful state-of-the-art branch-and-bound based solvers [we call these “base solvers”]. Two of the most intensively developed parallel solvers are FiberSCIP [for a shared memory computing environment] and ParaSCIP [for a distributed computing environment]. Both of which use SCIP as the base solver. Since debugging on distributed environments is inefficient, FiberSCIP was developed, which has the same parallelization algorithms as ParaSCIP [since UG abstracts the parallelization library] but can run on a single PC. Due to the major debugging effort of UG and SCIP via FiberSCIP, ParaSCIP could solve more than 20 open instances from MIPLIB by using up to 80,000 cores and none of these results has been proven wrong so far. Since UG provides a systematic way to parallelize a state-of-the-art sequential or multi-threaded solver to run on a large-scale distributed memory environment, with version 1.0, UG is generalized to a software framework for high-level task parallelization. That is, with version 1.0, UG will not only parallelize the tree search of branch-and-bound-based solvers but allow the parallelization of other kinds of solvers. In this talk, we present the latest status for exactly solving previously unsolved instances of combinatorial optimization problems.","Finding optimal solutions to previously unsolved combinatorial optimization problems by using more than 100,000 cores",[29543],191,"[102, 14, 134]",1900,Parallel Solvers,76,7,30,Software for Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,53 [building - 208],"['Parallel Algorithms and Implementation', 'Combinatorial Optimization', 'Software']",TA-30
"The rising demand for materials such as wood undeniably contributes to the depletion of natural resources and global warming. Recycling our wood waste intelligently could help to curb this phenomenon and have a significant ecological impact. This wood waste can be in the form of beams or pallets and could be considered as wood slats. They could be combined, assembled, and glued to create Cross-Laminated Timber [CLT] panels for the construction industry.

We aim to develop optimization techniques to recycle raw wood waste by providing the layout schemes to create CLT panels. The objective can be either to maximize the CLT panel production or to minimize the wood surplus when a given set of panels should be built.

We conducted a literature review to identify relatively similar problems in the operations research field. The skiving stock problem and the dual bin packing problem are the two closest problems to our. The second’s name is quite misleading since our problem is technically not a dual version of the
cutting stock/bin packing problem, but a problem on its own as shown in the literature.

In this work, we propose a clear description of our problem and different mathematical formulations with cuts for the variants. The results of various numerical experiments with field data from the wood industry are presented. As it is an NP-hard problem, we identify the limit size of the instances for which the problem can still be solved in a reasonable amount of time.",Mathematical formulations for wood recycling optimization,"[76755, 36600]",226,"[109, 23, 14]",1901,Applications of combinatorial optimisation in industry and services I,64,7,29,Combinatorial Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,157 [building - 208],"['Programming, Integer', 'Cutting and Packing', 'Combinatorial Optimization']",TA-29
"Online food ordering is well-developed nowadays, with many people choosing to order their meal online and waiting at home instead of going to a restaurant. We consider a dynamic online restaurant meal delivery problem [ORMDP] where a pool of drivers deliver food from multiple restaurants to ordering customers. The objectives are to reduce both delivery delays and unfairness for drivers to satisfy those two main stakeholders in the meal delivery process. In practice, order delays increase significantly when the demand for orders is high relative to the number of available drivers. To address this issue, we consider the ORMDP as two sub-problems, order bundling and order assignment. First, we implement a policy-based order bundling algorithm that gather orders into groups within drivers’ capacity.  Second, we propose an order assignment algorithm to allocate the groups to drivers considering both delay reduction and fairness to drivers. In order to enhance realism, we emulate the behavior of drivers as they await assignments by employing an Iterative K-means Clustering approach [IKC]. This method entails clustering proximate restaurants into zones, from which drivers opt for selections in accordance with probability distributions. Finally, we developed a discrete-time simulation model to test our methods using public data. The experimental results show that order bundling and fair assignment can effectively improve the delivery service in ORMDP.",Online Restaurant Meal Delivery Problem - Order Bundling and Fair Assignment,"[77262, 48934, 9228]",789,"[65, 145, 143]",1904,Logistics 3,5,15,58,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S07 [building - 101],"['Logistics', 'Vehicle Routing', 'Transportation']",WD-58
"The mirror descent algorithm is known to be effective in situations where it is beneficial to adapt the mirror map to the underlying geometry of the optimization model.
However, the effect of mirror maps on the geometry of distributed optimization problems has not been previously addressed.  In this paper we study an exact distributed mirror descent algorithm in continuous-time under additive noise.
We establish a linear convergence rate of the proposed dynamics for the setting of convex optimization.
Our analysis draws motivation from the Augmented Lagrangian and its relation to gradient tracking. 
To further explore the benefits of mirror maps in a distributed setting we present a preconditioned variant of our algorithm with an additional mirror map over the Lagrangian dual variables. This allows our method to adapt to both the geometry of the primal variables, as well as to the geometry of the consensus constraint. 
We also propose a Gauss-Seidel type discretization scheme for the proposed method and establish its linear convergence rate.
For certain classes of problems we identify mirror maps that mitigate the effect of the graph's spectral properties on the convergence rate of the algorithm. 
Using numerical experiments we demonstrate the efficiency of the methodology on convex models, both with and without constraints. Our findings show that the proposed method outperforms other methods, especially in scenarios where the model's geometry is not cap",Stochastic Mirror Descent for Convex Optimization with Consensus Constraints,[39009],447,"[19, 0]",1907,Preconditioning for  Large Scale Nonlinear Optimization,84,5,34,Advances in large scale nonlinear optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,43 [building - 303A],['Continuous Optimization'],MD-34
"In case of disturbed railway operations, traffic management can apply rescheduling measures to resolve conflicts while minimising delay propagation. This can be optimised by conflict detection and resolution [CDR] models. Usually based on alternative graph or mixed integer linear programming [MILP] formulations, existing models mostly refer to conventional signalling systems in which the track is discretised into blocks. Only at block entries, trains can receive a brake indication. Hence, train separation distances are based on a number of blocks. With the implementation of continuous braking curve supervision in distance-to-go [DTG] signalling systems such as the European Train Control System [ETCS], train separation is based on absolute braking distances. Consequently, an explicit relation between speed and train separation is required in CDR models for DTG signalling. Recently, we enhanced the CDR model RECIFE-MILP to DTG operations. The enhancements relate to track discretisation and speed-dependent train separation. In this research, we investigate the effect of track discretisation granularity on the performance of the enhanced model for next-generation DTG signalling systems - ETCS Level 3 Fixed Virtual Block and Moving Block. The performance is assessed regarding total delay and rescheduling decisions. The results indicate that, depending on the track and traffic scenario, a finer granularity can lead to different rescheduling decisions due to shorter train separation.",Track discretisation in railway traffic rescheduling models for next-generation distance-to-go signalling,"[77287, 36073, 77502, 31981, 27630]",816,"[122, 111, 26]",1908,Railway Traffic Management,85,13,51,Public Transport Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,M5 [building - 101],"['Railway Applications', 'Programming, Mixed-Integer', 'Decision Support Systems']",WB-51
"When analyzing a data set, typical questions are - ‘Do underlying relationships exist?’, ‘Are some variables redundant?’, and ‘Is some target variable Y highly or weakly dependent on variable X?’. Interestingly, despite the evident need for a general-purpose measure of dependency between RV’s, common practice of data analysis is that most data analysts use the Pearson correlation coefficient [PCC] to quantify dependence between RV’s, while it is well-recognized that the PCC is essentially a measure for linear dependency only. Although many attempts have been made to define more generic dependency measures, there is yet no consensus on a standard, general-purpose dependency function. In this talk, we will discuss and revise the list of desired properties and propose a new general-purpose dependency function provides data analysts a powerful means to quantify the level of dependence between variables. ",Beyond the Pearson’s correlation coefficient - a general-purpose measure for quantifying the dependency between random variables,"[39439, 77500, 77115, 47048]",515,"[7, 66, 47]",1909,"Advancements of OR-analytics in statistics, machine learning and data science 7",16,10,28,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,065 [building - 208],"['Analytics and Data Science', 'Machine Learning', 'Forecasting']",TD-28
"This study introduces the concept of task-splitting into home healthcare routing and scheduling, focussing on the design of routes and time-tables for caregivers providing services at patients their homes. Task-splitting entails the division of a [long] patient visit into two separate visits, which can be performed by different caregivers at different times. Splitting a visit can increase the planning flexibility of home healthcare [HHC] providers, as the resulting split tasks may have lower requirements related to the caregiver qualifications, relaxed visiting time-windows, or a shorter combined duration. However, the inclusion of task-splitting together with the temporal dependencies that arise between the split parts also presents a computational challenge.
To support the incorporation of task-split decisions into the planning process and explore the impact for HHC providers, we first introduce different mixed integer linear programming formulations. Next to deciding which tasks to split, these formulations are capable of handling various types of synchronisation constraints between the visits. Subsequently, we propose pre-processing routines and heuristic procedures to improve the computational performance of the resulting solution algorithm. Finally, we present results from a computational study that demonstrates the potential benefits of task-splitting, including a reduction on the staff requirements and decrease of operations costs among the instances considered.",Home healthcare routing and scheduling with task-splitting,"[77037, 12046, 67397]",598,"[56, 111, 145]",1911,Home Health Care and Operating Room Scheduling,3,12,15,OR in Health Services [ORAHS],"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,18 [building - 116],"['Health Care', 'Programming, Mixed-Integer', 'Vehicle Routing']",WA-15
"The container terminal comprises two modules - the yard and the terminal apron, where vehicles engaged in loading/discharging operations are shuttled between the two modules using holding buffers to mitigate congestion and delays. However, uncertainties such as container retrieval locations in the yard, vehicle speed, and potential waiting times due to congestion or incidents, introduce unpredictability in container transit time and may lead to delayed arrivals at the terminal apron. This unpredictability presents challenges in estimating the makspan for handling vessels, particularly for loading operations. This study addresses the integrated optimization of vehicle positioning and scheduling problem at the terminal apron, considering uncertain transit times. The research aims to enhance the tolerance of the terminal apron system to delays in the completion of loading operations in an uncertain environment, striving to develop an efficient and robust vehicle scheduling plan. Stochastic programming is utilized to model the ambiguous transit times, and a target-oriented robust optimization [RO] model is introduced to maximize the system's tolerance to delays. Three approaches are utilized to address the RO, and the performance RO is evaluated across four dimensions - mean, standard deviation, target completion probability, and value at risk. Finally, the RO exhibited strong performance in numerical experiments constructed using real data in Tianjin Port in China.",A Target-oriented Robust Optimization Approach for Vehicle Positioning and Scheduling Problem at Terminal Apron,"[77271, 61927, 77510, 65825]",174,"[127, 70, 65]",1912,Container Stacking and Yard Planning I,52,3,62,OR in Port Operations,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S12 [building - 101],"['Robust Optimization', 'Maritime applications', 'Logistics']",MB-62
"Survival for out-of-hospital cardiac arrest [OHCA] can be significantly improved through bystander efforts. To shorten the time to good-quality cardiopulmonary resuscitation, some emergency call centers use mobile phone technology to rapidly locate and alert nearby trained volunteers. Several such community first responder [CFR] systems are active worldwide, for example GoodSAM, which operates in the UK, Australia and New Zealand. 

GoodSAM sends so-called phased alerts - they notify increasingly many volunteers with built-in time delays. The policy that defines these delays affects [1] response times - which have a direct relation to survival - [2] CFR workload and [3] the number of redundant CFR arrivals. We start by comparing policies through Monte Carlo Simulation, in which we use bootstrapped values from historical GoodSAM responses, estimating the three KPIs above. CFR app managers can use those results to identify a policy that displays a desirable trade-off between the performance measures.

We continue by using machine learning to predict the best policy to use, given where the volunteers are observed in relation to the patient. We do this by formulating the problem as a multiclass classification problem, for which we train a tree on the results from the simulations above. We compare the performance of the tree against a policy designed by dynamic programming. Finally, we look into optimal trees which go beyond the heuristic nature of machine learning algorithms.",How should volunteers be dispatched to out-of-hospital cardiac arrests?,[64020],336,"[135, 7, 131]",1914,Analytics for Combinatorial Problems from Health Care to the Food Industry,17,10,31,Analytics,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,54 [building - 208],"['Stochastic Models', 'Analytics and Data Science', 'Simulation']",TD-31
"In this talk, a new approach is presented for obtaining guaranteed upper and lower bounds on the true optimal value of stopping problems by nonparametric regression. Unlike existing simulation-and-regression approaches, such as those based on least squares Monte Carlo and information relaxation, whose estimates depend on the sampled paths of the underlying stochastic process and are therefore stochastic in nature, our guaranteed bounds allow replacing the sampling error with a pre-specified confidence level. The key to our approach is the use of non-asymptotic uniform confidence bands based on a kernel ridge regression [KRR] estimator to over- and underestimate the conditional expected value from  continuation. Since our high- and low-biased KRR estimators are guaranteed to over- and underestimate, respectively, the true but unknown conditional expectation at every step of the backward recursive procedure with a pre-specified confidence level, we obtain probabilistically guaranteed bounds on the problem's true optimal value. We derive closed-form formulas for both the fixed design and the random design case, which makes our approach readily applicable in both simulation and data-driven situations. We illustrate the applicability of the proposed bounding procedure by valuing a Bermudan-style put option.",Guaranteed bounds for optimal stopping problems using kernel-based non-asymptotic uniform confidence bands,"[56955, 48981, 3122]",301,"[136, 117]",1917,Optimization under uncertainty - theory and solution algorithms,49,7,35,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,44 [building - 303A],"['Stochastic Optimization', 'Programming, Stochastic']",TA-35
"We study single-machine scheduling problems with step-learning, where an improvement in processing time is experienced if a job is started after a job-dependent learning-date. We consider minimizing two functions - the number of late jobs and the total late work, and we show that when at least a common due-date or common learning-date is assumed, the problem is NP-hard in the ordinary sense; however, when both are arbitrary, the problem becomes strongly NP-hard. For each of the problems where at least one of the dates is assumed to be common, we analyze the structure of an optimal job schedule with and without idle time and propose pseudo-polynomial time dynamic programming algorithms. We also show that the problem of minimizing the weighted number of late jobs with step-learning can be solved with a minor change to the algorithms for the unweighted case. In addition to this, we show that when a common due-date is assumed and no idle time is allowed, the problem of minimizing the total late work is equivalent to that of minimizing the makespan. Furthermore, we provide a more efficient algorithm to solve the problem of minimizing makespan under the assumption of a common learning-date than the one in the existing literature.",Minimizing the number of late jobs and total late work with step-learning,"[77450, 2881]",932,"[129, 14, 108]",1918,Machine Learning in Machine Scheduling,35,14,60,Project Management and Scheduling,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S09 [building - 101],"['Scheduling', 'Combinatorial Optimization', 'Programming, Dynamic']",WC-60
"Limiting climate change demands an extensive expansion of variable renewable energy technologies. Capacity expansion models estimate the necessary installations of these technologies. However, a major challenge of energy systems based on these technologies are interannual weather fluctuations across weather years leading to uncertainties in capacity expansion as well as operation to match supply and demand. The presented study investigates the 38 government districts of Germany using 40 years of weather data with hourly resolution to calculate energy systems able to supply energy in all cases. Based on computational experiments with weather data from multiple years, we identify critical dark lull-patterns in weather years. We showcase how such patterns can be incorporated into arbitrary reference years by manipulating time-series data. The developed methodology takes uncertainties in the capacity factor based on the weather data into account and weighs them relative to the demand. Large discrepancies between capacity factors and demand indicate time periods with potential gaps between supply and demand. The resulting energy systems based on this manipulated weather year are then taken as input for cross-validation to proof feasibility of the resulting systems. The results show that already the inclusion of only a few, well selected time periods leads to robust energy system models with minor optimality gap and can ultimately to a substantial decline in calculation time.",Applying Gamma Robustness for Weather year uncertainty to generate Robust Energy Systems,[75209],340,"[12, 37]",1920,Enhanced statistical methods for energy challenges,22,7,14,Energy Markets,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,16 [building - 116],"['Capacity Planning', 'Energy Policy and Planning']",TA-14
"Endogenous, or decision-dependent, uncertainties pose significant challenges for decision making under uncertainty. Various methods have been developed for different types of endogenous uncertainty, including influence diagrams [ID] for intuitively representing decision-dependent probability distributions. If the problem instead has a decision-dependent information structure, that is, some of the information is revealed conditionally, conditional non-anticipativity constraints can be added to a stochastic programming model representing the problem. However, combining these two types of uncertainty has proven challenging, and no framework existed for such problems until recently. 
This presentation aims to shed light on two approaches for reformulating an influence diagram as a mixed-integer linear problem [MILP], enabling us to simultaneously consider different types of endogenous uncertainty and risk measures. A framework called Decision Programming directly reformulates the ID into a MILP, but it has been shown that first reformulating the ID into a rooted junction tree can greatly enhance computational performance and result in asymptotically smaller MILPs. Our discussion is focused on a Julia package implementing the framework along with possible improvements, and an illustrative climate change mitigation case study. Additionally, we mention some other application areas where the methods have been applied to give the audience an overview of the possibilities. 
",Mixed-integer reformulations for influence diagrams with conditional information structures,"[67611, 57798, 61476]",483,"[136, 25, 134]",1922,Decision problems represented as influence diagrams,49,12,34,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,43 [building - 303A],"['Stochastic Optimization', 'Decision Analysis', 'Software']",WA-34
"With safer, cheaper and sustainable transportation of goods attracting major interest in recent years, rail freight is becoming increasingly attractive compared to road freight transport. For railway timetabling, this requires a better utilization of the infrastructure's capacity. However, the unpredictable nature of this growing rail freight demand poses numerous challenges to infrastructure managers, one of them being the short-term scheduling of additional freight trains within existing timetables. Indeed, residual capacity management is a good lever to accommodate freight transport requests, while ensuring safety and minimal disruption of passenger trains. Despite its significant relevance, this problem has received relatively limited attention in the literature. This presentation aims to describe and formalize the problem, to position it within the literature and to introduce a first solution approach. Computational experiments and their numerical results will be discussed.",Optimized Short-Term Insertion of Additional Trains Into Existing Timetables,"[76750, 16596, 16259, 77513]",182,"[122, 143, 142]",1923,Europe's Rail MOTIONAL - Algorithms for railway planning,85,5,54,Public Transport Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S01 [building - 101],"['Railway Applications', 'Transportation', 'Timetabling']",MD-54
"We study the facility location problem under uncertainty where the decisions on where to locate facilities often require estimating different uncertain parameters associated with the problem, for example costs, supply, demand, distances, etc. which may fluctuate with time and circumstances. The recent body of literature has been towards incorporating this uncertainty at various levels. 
Following and extending this branch of literature, our research aims at developing facility location procedures which account for uncertainty in the model and in the data. To do so, we extend Bayesian optimisation methods for combinatorial structures[BOCS] to solve the facility location problems.
Our research presents a novel approach by extending the BOCS method to account for constraints, uncertainty, grid structure and interaction of the allocation nodes. Incorporating constraints in combinatorial domain within the framework of BOCS is challenging. Hence, we propose an extension to the framework by introducing probabilistic reparametrisation for the decision variable, which allows optimisation of the acquisition function on a continuous space.
By employing Bayesian optimisation we incorporate uncertainty in the optimisation procedure itself, while allowing for an integrated framework in which the estimation of optimisation variables is data driven. We demonstrate the performance of our method in a simulated study.",Bayesian optimisation for facility location problems,"[73304, 68848]",530,"[43, 7, 14]",1924,"Advancements of OR-analytics in statistics, machine learning and data science 11",16,15,28,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,065 [building - 208],"['Facilities Planning and Design', 'Analytics and Data Science', 'Combinatorial Optimization']",WD-28
"The aim of this methodological proposal is to assess the fairness of healthcare services for various age groups, with particular attention to older individuals and their unique requirements. The significance of pharmacies, hospitals, and para-pharmacies as crucial providers of specialized services underscores the issue of pedestrian access to healthcare services, especially for older adults. Expanding the scope of this methodological approach to incorporate walkability and economic indices is another objective of this study. Fairness and walkability play pivotal roles in establishing an age-friendly environment, particularly for individuals with specific needs such as older pedestrians. This aspect is particularly significant for gaining insights into the accessibility of essential services. In our methodology, we acquired data regarding the distribution of the Italian population across diverse geographical areas and age groups. We then integrated this information with the distribution of health services in Italy. By leveraging the concept of convolution between matrices, we have developed a novel, generalizable methodological approach to analyze the fairness in the distribution of services across various territorial entities, including municipalities, provinces, and regions. We evaluate the concept of fairness, focusing on older people, through the combination of kernel convolution and economic indices for service inequalities [i.e., Gini coefficient].",Evaluate the Fairness of the Distribution of Health Services for Older People in Italy - A Methodological Approach,[69151],708,"[64, 56]",1929,"Sustainability and Equity in Ecosystems, Ecology and Food",80,14,53,Sustainable and Resilient Systems,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,8007 [building - 202],"['Location', 'Health Care']",WC-53
"A rational linear system is totally dual integral [TDI] if for every integer linear function, the associated dual problem has an integer optimal solution whenever the optimum is finite. Following the definition, TDI systems with integer right-hand sides characterize integer polyhedra. A TDI system is box-TDI if adding any rational bounds on the variables [i.e., boxes] preserves its TDIness. Classical examples of box-TDI systems are those involved in the Max Flow-Min Cut Theorem of Ford and Fulkerson. In general, box-TDI systems are systems that yield strong min-max relations.

In the early 80s, W.J. Cook proved that any TDI system describing a polyhedron allowing a box-TDI description is also box-TDI. Thus, box-TDIness is a geometrical property, and any polyhedron that can be described by a box-TDI system is called a box-TDI polyhedron.

Recently, a graph characterization of the box-TDIness of the matching polytope has been released. However, this characterization is too restrictive for the perfect matching polytope [PMP]. In this talk, we highlight the differences between the two cases and provide insights for tackling the problem of characterizing the box-TDIness of the PMP. We also show that deciding whether the PMP of a graph is box-TDI is equivalent to testing the box-TDIness of its affine hull, which implies that Recognizing whether the perfect matching polytope of a given graph is box-TDI can be done in polynomial time.",On the box-total dual integrality of the perfect matching polytope,"[71642, 77516, 77515]",866,"[103, 0]",1930,Topics in Integer Programming II,64,13,25,Combinatorial Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,011 [building - 208],['Polyhedral Combinatorics'],WB-25
"We provide optimality conditions for general convex infinite optimization problems in absence of compactness assumptions. Hence, no prerequisites are considered on the constraints index set and no continuity-like behavior is assumed on the dependence of these constraints with respect to the
parameters. The resulting KKT and Fritz-John conditions involve only the objective and the constraint functions, enlightening the different roles played by the [almost] active and non-active constraints. Namely, the Lagrange multipliers associated with non-active constraints can be made very small. The main ingredient is new representations of
both the subdifferential of the supremum function and the normal cone of its efective domain.",Non-active constraints in convex infinite optimization,[39025],294,"[21, 19, 107]",1931,Variational Analysis and Subdifferential techniques,82,9,42,Variational Analysis and Continuous Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,98 [building - 306],"['Convex Optimization', 'Continuous Optimization', 'Programming, Constraint']",TC-42
"We are considering a scheduling problem which consists of both scheduling activities in one or more projects, and the allocation of personnel and transportable equipment demanded to complete the activities. The activities are multi-modal and discretely preemptive, and there are precedence constraints that need to be respected.

Due to the challenges of scalability and efficiency when scheduling using direct mixed-integer programming [MIP] formulation and a MIP solver, we investigate the benefits of decomposing the integrated problem using a Dantzig–Wolfe reformulation. This method decomposes the problem into a master problem and one or several subproblems to find improving solutions.

The subproblem involves scheduling the activities of individual projects, and its solution is fed to the master problem as a new column. The master problem uses the project-schedule columns generated by the subproblems to find the best project schedules to use so that personnel and equipment are allocated the most cost-efficiently. By decomposing the problem into project-specific subproblems that are quick to solve, the efficiency of the overall solution method is significantly increased. Our approach demonstrates promising performance in terms of computational time and solution quality when compared to solving the MIP formulation directly using a MIP solver.",Solving an integrated project and personnel scheduling problem using Dantzig–Wolfe decomposition,"[70847, 9655, 41439]",348,"[118, 13, 129]",1932,Project scheduling under uncertainty,35,4,60,Project Management and Scheduling,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S09 [building - 101],"['Project Management and Scheduling', 'Column Generation', 'Scheduling']",MC-60
"We investigate the benefits of using contextual information in data-driven demand prediction to solve the robust capacitated vehicle routing problem with time windows. Instead of estimating the demand distribution or its mean, we introduce contextual machine learning models that predict demand quantiles even when the number of historical observations for some or all customers is limited. We investigate the use of such predicted quantiles to make routing decisions. We also evaluate the efficiency and robustness of the decisions obtained by both exact and heuristic methods. Our extensive computational experiments show that using a robust optimization model and predicting multiple quantiles is promising when extensive historical data is available. In scenarios with limited demand history, using a deterministic model with only a single quantile shows greater potential. Interestingly, our results also indicate that using appropriate quantile demand values within a deterministic model leads to solutions with robustness levels comparable to those of robust models. This is important because, in most applications, practitioners use deterministic models as the industry standard, even in an uncertain environment. In addition, because they are less computationally demanding and require only a single demand value prediction, deterministic models coupled with an appropriate machine learning model hold the potential for robust decision making.",Leveraging Contextual Information for Robustness in Vehicle Routing Problems,"[66107, 67797]",431,"[7, 145, 127]",1933,Interpretable Optimization Methods and Applications,14,13,03,Data Science Meets Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1005 [building - 202],"['Analytics and Data Science', 'Vehicle Routing', 'Robust Optimization']",WB-03
"The Rank Pricing Problem is a challenging mixed-integer optimization problem. It aims to determine the optimal pricing strategies of a set of products ranked by customer preferences. Given its NP-hard nature, existing literature offers various exact methodologies. However, these approaches can be intricate to formulate and computationally intensive. In contrast, in this talk, we propose a novel data-based methodology that is simple but effective. Even though our heuristic proposal cannot guarantee to obtain the optimal solution, the numerical results in different instances show its capacity to deliver high-quality results, providing a pragmatic alternative within a short computational timeframe.",A data-based approach for solving the Rank Pricing Problem,"[43618, 18518, 39196]",141,"[111, 66, 14]",1934,Machine Learning for and with Mathematical Optimization,15,15,27,Mathematical Optimization for XAI,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,047 [building - 208],"['Programming, Mixed-Integer', 'Machine Learning', 'Combinatorial Optimization']",WD-27
"Research in energy markets often requires jointly considering multiple sources of uncertainty. Among all players in the energy markets, typical sources of profit uncertainty are supply costs, weather conditions, sale prices, and demand levels. Typically, these components are not independent of one another. Moreover, the dependencies are often non-linear. In turn, this makes investment and management decisions and risk assessment complex.
In the methodological field, an approach that has proven particularly effective in modelling linear and non-linear dependencies among multiple sources of uncertainty was advanced in Cerqueti et al. [2017]. The method is based on the approximation of stochastic processes through Markov chains of order k, with k belonging to the set {1,2,...}.
The present work is concerned with modelling the quadrivariate stochastic process of natural gas price, electricity demand, electricity price, and solar radiation based on the method of in Cerqueti et al. [2017]. The novel aspect of the present method compared to its previous applications consists in the management of the different temporal frequencies of the four components [gas prices have a daily frequency, while the others have an hourly frequency].
The method, based on the choice of a few parameters compared to other methods, allows us to bootstrap and simulate the quadrivariate stochastic process. Statistical tests are applied to the generated scenarios to assess their goodness of fit.",Markov Chain Bootstrapping and Simulation for a Quadrivariate Stochastic Process in Energy Market Scenario Generation,"[3654, 24914, 2991, 77002]",450,"[131, 135, 36]",1935,Analysis of Stochastic Models I,50,10,39,Stochastic Modelling,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,35 [building - 306],"['Simulation', 'Stochastic Models', 'Electricity Markets']",TD-39
"We study a Multi-Hospital Home Hospitalization Allocation-Routing Problem [MH-HHARP] to provide a useful tool for a healthcare provider in Portugal for evaluating the impact of the opening of a new Home Hospitalization [HH] unit. Given the complexity of HH systems, where specialized nurses and doctors allocate their time between inpatient and outpatient systems, it is critical to efficiently manage their time and workload. For this purpose, we introduce a Mixed-Integer Linear Programming formulation that aims at minimizing routing costs, alongside introducing two other functions - workload balancing among nurses and the minimization of employed doctors. The model is validated using benchmark instances derived from the provider’s historical data. Through single-objective resolutions, we provide an economic analysis that assesses the cost implications and the benefits of opening a new HH unit at the variation of objective function. Furthermore, we explore bi-objective problems through an epsilon-constraints method, presenting significant Pareto frontiers to facilitate the provider's decision-making process regarding the trade-offs between different objectives. A TOPSIS analysis is subsequently applied to identify the optimal solution for the provider according to varying objectives weights.",Allocation-Routing Problem in a Multi-Hospital Home Hospitalization System - A Case Study of a Healthcare Provider in Portugal,"[67518, 67738, 46526, 71267, 48740]",597,"[56, 145, 111]",1940,Mobility and transportation in healthcare,3,4,10,OR in Health Services [ORAHS],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,11 [building - 116],"['Health Care', 'Vehicle Routing', 'Programming, Mixed-Integer']",MC-10
"Mobility-as-a-Service [MaaS] is an innovative user-based mobility system that offers users a one-stop personalized service by seamlessly integrating multi-modal transport services. MaaS has become increasingly popular and is reshaping travel behavior. However, MaaS is essentially a decentralized user-centric system, which is challenging to operate since both service providers and travelers are self-interested. We address this problem by studying a MaaS system that integrates one-to-many ridesharing and public transit through a digital platform that flexibly matches riders with drivers and/or public transit. We generalize the classic one-to-many stable matching concept to incorporate the users' preferences in the planning of daily operations. The stable matching problem formulated as an integer program typically includes a huge number of matches and complex stability constraints. We develop a computationally efficient algorithm that integrates Lagrangian relaxation with column-and-row generation procedure to solve the model. Theoretical properties of the model and algorithm are investigated. We also test the algorithm on a real-world case and show that the algorithm converges quickly. Extensive computational experiments reveal various new managerial insights into social welfare, transit usage, users' savings, matching rates, and travel detour.",One-to-Many Stable Matching for Integrating Ridesharing and Public Transit on a Mobility-as-a-Service Platform,[77455],352,"[143, 13, 63]",1941,Advancing mobility towards sustainable solutions I,6,9,56,Transportation,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S04 [building - 101],"['Transportation', 'Column Generation', 'Large Scale Optimization']",TC-56
"This paper introduces an enhanced algorithm for computing generalized Nash equilibria in multiple-player nonlinear games. The algorithm degenerates into a gradient algorithm for single-player games [i.e., optimization problems] or potential games [i.e., equivalent to minimizing the respective potential function], similar to the Rosen gradient algorithm. Analytical examples demonstrate that it has similar theoretical guarantees of finding a generalized Nash equilibrium when compared to the relaxation algorithm, while numerical examples show that it is faster. Furthermore, the proposed algorithm is as fast as, but more stable than, the Rosen gradient algorithm, especially when dealing with constraints and non-convex games. The proposed strategy is tested in benchmarking game theory problems and real-world applications.",An enhanced gradient algorithm for computing generalized Nash equilibrium,"[32500, 24811, 24809, 24812]",645,"[50, 26, 36]",1942,"Game Theory, Solutions and Structures VII",88,9,36,"Game Theory, Solutions and Structures","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,32 [building - 306],"['Game Theory', 'Decision Support Systems', 'Electricity Markets']",TC-36
"Money laundering presents a pervasive global challenge, placing a burden on society through the financing of illegal activities. To combat this phenomenon, the inclusion of network information in the modelling pipeline is increasingly being explored for more effective detection, exploiting the fact that money laundering necessarily interconnected parties. This evolution has resulted in a strong growth of the literature on social network analytics [SNA] for anti-money laundering [AML] in the past few years. At the same time, the increasing attention for network approaches for AML in the literature has created important challenges for researchers and practitioners. There is no clear overview of the existing work, there exist limited insights into how different methods compare, and it is not clear which approach works best in practice.The main objective of this paper is to construct a taxonomy of SNA for AML using a systematic literature review. This review is supplemented by an extensive experimental evaluation of graph learning methods, going from manual feature engineering, over shallow representation learning to graph neural networks. These are evaluated on two money laundering datasets, a crypto-currency and a proprietary transaction data set. The results highlight the strengths and weaknesses of the different graph learning methods in the specific context of money laundering. ",A Systematic Review and Experimental Evaluation of Social Network Analytics for Anti-Money Laundering,"[77280, 69331, 46180, 10234, 67533]",62,"[132, 66, 44]",1944,Crime Analytics,17,3,31,Analytics,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,54 [building - 208],"['Social Networks', 'Machine Learning', 'Finance and Banking']",MB-31
"Stochastic simulation models, while effective in capturing the dynamics of complex systems, are often too slow to run for real-time decision-making. Metamodeling techniques are widely used to learn the relationship between a summary statistic of the outputs [e.g., the mean or quantile] and the inputs of the simulator, so that it can be used in real time. However, this methodology requires the knowledge of an appropriate summary statistic in advance, making it inflexible for many practical situations. In this paper, we propose a new metamodeling concept, called generative metamodeling, which aims to construct a“fast simulator of the simulator”. This technique can generate random outputs substantially faster than the original simulation model, while retaining an approximately equal conditional distribution given the same inputs. Once constructed, a generative metamodel can instantaneously generate a large amount of random
outputs as soon as the inputs are specified, thereby facilitating the immediate computation of any summary statistic for real-time decision-making. Furthermore, we propose a new algorithm—quantile-regression-based generative metamodeling [QRGMM]—and study its convergence and rate of convergence. Extensive numerical experiments are conducted to investigate the empirical performance of QRGMM, compare it with other state-of-the-art generative algorithms, and demonstrate its usefulness in practical real-time decision-making.",Learning to Simulate - Generative Metamodeling via Quantile Regression,[77165],502,"[131, 66, 26]",1945,Stochastic Modelling,47,5,40,Advances in Stochastic Modelling and Learning Methods,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,96 [building - 306],"['Simulation', 'Machine Learning', 'Decision Support Systems']",MD-40
"Recently, the literature has become too focused on variations of the fixed charge transportation problem [FCTP], such as solid, step, capacitated or multi-stage, despite a significant number of gaps remaining in the research for pure FCTPs. The research presented is focused on developing ruin-and-recreate [R&R] heuristics specifically for pure FCTPs, which have not been implemented before. R&R heuristics are new methods applied to combinatorial optimisation problems [specifically vehicle routing problems] in which a solution is destructed [possibly reaching an infeasible solution] and rebuilt in an intelligent manner to maintain feasibility. This is done repeatedly until a desired solution is reached.

Four ruin methods utilising the tree structure of FCTP basic feasible solutions, seven recreate methods divided into single-cell and multi-cell heuristics, and various metrics to determine entering and leaving variables in each iteration of a ruin or recreate method, are developed. Different variations of the algorithms are executed on a set of existing and newly generated benchmarks for algorithmic comparisons. Different characteristics of a problem, such as the problem size, fixed charge to variable cost ratio, average units supplied/demanded and arc density, are reported on to determine strong performing algorithms in given situations. Lastly, the convergence of the algorithms as well as the trade-off between solution quality and computational time are explored.",Ruin-and-recreate heuristics for the fixed charge transportation problem,"[77524, 8721]",329,"[143, 14, 53]",1947,ML & OR Applications in Transport Modelling,6,7,55,Transportation,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S02 [building - 101],"['Transportation', 'Combinatorial Optimization', 'Graphs and Networks']",TA-55
"This paper is concerned with how different team structures affect task performance. By using a Case Study of the maintenance department of a large utility company as a starting point, the study creates an agent-based computational simulation model to compare four different structures - [a] strict hierarchy, [b] relaxed [or loose] hierarchy, [c] anarchy, and [d] hybrid [or baseline]. The model is built by taking ethnographic observations from the Case Study to develop the basic layout for the model. The baseline case is the hybrid structure, from which the other structures have been created by reference to the literature. In accordance with the literature on distributed cognition in organizations, the main characteristic of the agents in the model is competence - i.e., their knowledge and practical expertise as they are functionally constituted in relation to their taking on of tasks as well as their subsequent completion. The simulation reveals that the effect of competence changes depending on the network of relations [relationship range in the model] among employees allowed by the team structure as well as according to cognitive dispositions [docility in the model]. These findings indicate that, despite an overwhelming individualistic characterization of competence, a more accurate representation would be for it to be considered distributed rather than solely attributable to the individual. ",Distributed Competence - How Team Structure Affects Task Performance,[77525],563,"[15, 57, 131]",1948,Simulation of organizations I,77,2,43,"Agent-based Models in Management, Economic and Organisation Sciences","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,99 [building - 306],"['Complex Societal Problems', 'Human Resources Management', 'Simulation']",MA-43
"In this work, we elaborate on the concept of assortative mixing. The role of assortativity contagions has been widely studied - in the spread of epidemics, in systemic risk, and in the resilience of networks, just to point out a few. The classic definition of assortativity through the Pearson correlation coefficient among the degrees of the nodes at the end of each edge captures the linear dependencies among the degrees of the connected nodes. We are going to push forward the concept of assortative mixing allowing for non linear dependencies. In order to achieve this task, we fix the marginal distributions as the most detected from the empirical data sets and we base on the generation of complex networks through copulas. The approach is different from the classic configuration model and its extensions which allow to build a network with a target level of correlation.
It is worth mentioning that the application of the technique goes beyond the mere result of simulating networks. 
Actually, the possibility to generate networks with specific properties allows to have a null model for investigating the role of topological structures in specific problems.
For instance, the generation of synthetic data keeping the statistical distribution of real data sets is currently successfully used in machine learning to enlarge the training set.
",Copula-based synthetic networks generation,"[7149, 7142]",188,"[53, 45]",1950,Applications in Finance and Economics,4,2,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S14 [building - 101],"['Graphs and Networks', 'Financial Modelling']",MA-63
"This study introduces a new variant of the Multi-Mode Resource-Constrained Project Scheduling Problem [MRCPSP], integrating two common real-world aspects - time-dependent resource costs and time-varying resource capacities. Time-dependent resource costs imply that costs vary based on resource utilization time, while time-varying resource capacities involve resource availability across different planning horizon periods. Employing a multi-objective framework, the study aims to minimize both the makespan and the total resource usage costs. A mathematical formulation is presented, and attempts are made to identify the Pareto front [PF] using an exact method. However, due to the problem complexity, a metaheuristic approach is developed to approximate the PF. We propose a new NSGA-II based metaheuristic, designing a novel solution encoding and the corresponding genetic operators to accommodate the new problem elements. Furthermore, we propose a fitness function to handle infeasible individuals within the multi-objective context. Computational experiments are conducted using instances derived from existing literature, demonstrating the algorithm's strong performance.",Exploring New Insights into the Multi-Mode Resource-Constrained Project Scheduling Problem,"[76930, 6053, 59505]",960,"[129, 74, 77]",1951,RCPSP and extensions,35,5,60,Project Management and Scheduling,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S09 [building - 101],"['Scheduling', 'Metaheuristics', 'Multi-Objective Decision Making']",MD-60
"Many real-world optimization problems depend on several criteria. The computational effort of approximating the nondominated set of these multiobjective optimization problems increases quickly with the number of objectives. For a large number of objectives, computing a good approximation of the nondominated set may exceed the available computation budget.

To compute approximations of high-dimensional nondominated sets, we incorporate dimension reduction techniques into the approximation process. We analyze the problem to identify strongly-correlated directions of the nondominated set and reduce the objective space dimension accordingly. After computing an approximation in the lower-dimensional space, we project the approximation back to the original full-dimensional space.
We relate the approximation of the nondominated set constructed by this algorithm to the approximation of the full-dimensional set and develop criteria under which the approximation in lower-dimensional space still yields a good approximation when projected back to the full-dimensional space.

Finally, we demonstrate the usefulness of the method on examples from different fields of application. 
",Dimension reduction in multiobjective optimization,"[72481, 8814]",12,"[112, 5]",1952,Objective Space-Based Approaches in Multiobjective Optimization,34,7,37,Multiobjective Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,33 [building - 306],"['Programming, Multi-Objective', 'Algorithms']",TA-37
"Refineries are complex industries that take crude oil then fractionate and crack it into valuable products. Feed continuity and quality are very crucial for the production, so the petroleum scheduling is a key step for a stable operation. This study aims to develop an optimization tool addressing operational limits encountered practical contexts to facilitate adept decision-making in crude oil scheduling. To this end, a scheduling model was devised, covering the unloading of crude oil received via pipeline to multiple storage tanks, intra-tank crude oil mixing, and charging the resultant mixture to the crude distillation unit. An event-based continuous-time Mixed Integer Linear Programming formulation was created to effectively navigate the intricacies of the scheduling problem. Given the nonlinear problem stemming from the homogenous crude oil mixture in the tanks, an iterative solution approach was designed. This approach breaks down the problem into manageable sub-problems based on periods of constant tank compositions. The viability of the proposed model has been evaluated through empirical assessments conducted on real refinery cases. The findings revealed satisfactory results within a brief interval, especially given the industrial scale and complexity of the problem covering 4 tanks with around 10 crude oil types. In this regard, the model aims to notably shorten scheduling time while enhancing efficiency through steady operations facilitated by prompt decision-making.",An Iterative Optimization Framework for Streamlining Crude Oil Scheduling in Refinery Operations,"[76883, 1564, 77520, 77530, 77532, 77534]",603,"[59, 109, 129]",1955,Applications of Multiobjective Optimization,34,10,37,Multiobjective Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,33 [building - 306],"['Industrial Optimization', 'Programming, Integer', 'Scheduling']",TD-37
"Petroleum refineries are intricate industrial facilities, and effective management of maintenance and repair activities is crucial for uninterrupted production processes. The inter-dependency of maintenance tasks and their predetermined start and end dates is the focus of this study, and the use case is selected from an oil refinery. An optimization model is proposed to minimize the inefficiencies in the form of idle machinery, underutilized teams, and relocation of maintenance technicians between the given tasks. A three-step algorithmic approach is suggested. Firstly, an unconstrained binary knapsack model is employed to reduce the problem size. Secondly, a constrained knapsack problem is solved via mathematical programming to determine maintenance schedules. Finally, a Time-Expanded Network Flow Algorithm is used to assign maintenance teams to the scheduled maintenance tasks, minimizing the number of workers used to execute the scheduled maintenance tasks. The proposed algorithm will not only shorten the time required for planning experts to create a weekly plan but also enhance the utilization of technicians. By streamlining the maintenance and repair processes, the algorithm aims to optimize resource allocation and improve overall operational efficiency in petroleum refineries.",Optimizing Refinery Maintenance Planning Processes with Three-step Algorithm Approach,"[77520, 77529, 66414, 77530, 77531, 77533, 77535]",878,"[14, 59, 53]",1956,Heuristic Algorithms for Combinatorial Optimization Problems II [Contributed],64,15,52,Combinatorial Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,8003 [building - 202],"['Combinatorial Optimization', 'Industrial Optimization', 'Graphs and Networks']",WD-52
"Due to the energy transition towards renewables, renewable alternatives will likely replace natural gas-based heating in the medium term. Such alternatives include renewable methane and hydrogen as well as heat pumps. Currently, the expectation is that natural gas distribution grids will therefore face declining usage in many areas.

To consider this development in automated energy grid-planning approaches, an existing, pandapower-based power grid expansion algorithm is extended to natural gas networks using pandapipes and enhanced to also cover partial decommissioning of natural gas pipelines.

The decline of gas consumers in the gas grid reduces the flow rate in pipelines in some areas, possibly leading to violations of the network operational constraints while also rendering a number of assets redundant. The algorithm uses Iterated Local Search with either Hill Climbing or Late Acceptance Hill Climbing and cures violations by shutting down or adding selected pipelines or pressure controller to the grid model. If a pipe shut down results in unsupplied sinks, the algorithm uses the Delauney Triangulation to generate potential new paths to reconnect the sink.

A special optimization problem occurs if numerous natural gas consumers intend to switch to hydrogen instead. In this case, the algorithm’s task is to design a hydrogen network while leveraging existing pipelines through conversion and observing all relevant operational restrictions.
",Decommissioning and Conversion Optimization for Automated Natural Gas Distribution Grid Planning,"[77135, 77615, 77537, 28733, 77903]",252,"[36, 53, 37]",1957,Impacts of transitioning to green gases,22,2,14,Energy Markets,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,16 [building - 116],"['Electricity Markets', 'Graphs and Networks', 'Energy Policy and Planning']",MA-14
"In this study, we analyze the reneging behavior of customers in a Markovian M/M/1 queue with an alternating service process, representing a normal operation mode and a working repair mode with reduced service speed. The customers are strategic and, as they continuously observe the system’s state, they decide whether to stay or to renege. For this model, we derive equilibrium customer reneging strategies and the corresponding performance measures of the system, considering both the baseline model with discrete units of customers and its fluid counterpart. To capture the effect of reneging, we compare their performance with similar systems, operating under a no-reneging policy. Both theoretical findings, as well as numerical experiments, reveal that allowing reneging significantly affects system performance. Finally, to assess the quality of the fluid approximation we conduct a numerical comparison between the fluid and the baseline version of the model.",Strategic reneging in service systems with working breakdowns,"[66984, 68598, 48062]",884,"[121, 130]",1958,Queueing Models with Strategic Customers,47,7,40,Advances in Stochastic Modelling and Learning Methods,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,96 [building - 306],"['Queuing Systems', 'Service Operations']",TA-40
"Previous literature show that emission benefits of electric passenger vehicles can vary a lot when regional differences such as grid mix, ambient temperature, patterns of vehicle miles travelled and driving profiles are considered. In this study, we propose a simulation model which can be used to extend the analysis to estimate GHG emissions of electric delivery trucks considering all these differences. We first develop an energy consumption model for delivery trucks based on real world usage data. Our model can estimate energy consumption per unit distance using average speed, ambient temperature, trip duration and acceleration characteristics. We then create a simulation framework which can be used to estimate the energy consumption from a fleet for a certain timeframe and calculate the emissions due to charging. Our framework can incorporate regional differences and it can create charging profiles [time and duration of charging] under different strategies such as immediate charging after the final trip, convenience charging or delayed charging. The results are combined with emissions factors from regional electricity grid to estimate use-phase emissions due to charging. As a case study, we use our simulation framework to quantify the effect of ambient temperature on CO2 emissions of electric delivery trucks in the United States and present the regional implications. ",Estimating greenhouse gas emissions of electric delivery trucks,"[58102, 77718, 77681]",526,"[143, 131, 139]",1960,Methods and models for sustainable transport solutions,6,7,56,Transportation,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S04 [building - 101],"['Transportation', 'Simulation', 'Sustainable Development']",TA-56
"A Variable Annuity [VA] differs from traditional life insurance products because, when the insurer invests the policy premium in a risky portfolio, the policyholder bears prots or losses according to the investment performances. In general, the dynamics of such a portfolio are defined via a geometric Brownian motion with constant interest rate and volatility, see e.g. Shen et al. [2016] and De Angelis et al. [2022]. However, such a choice might be unrealistic, since it fails to incorporate well-known stylized facts observed in the financial market, such as the volatility smile/skew, see e.g.Heston [1993].
Moreover, the presence of an American option embedded in the structure of a VA makes it unfeasible to determine a closed-form formula for such a contract, regardless of the market model. Therefore, we have to resort to ad-hoc numerical techniques, such as finite difference methods for the associated PDE, see e.g. Shen et al. [2016]. Another possible route is to refer to the integral equation characterizing the early exercise boundary, as shown in Kim [1990].
Within the latter setting, and along the lines of Adolfsson et al. [2013], in this paper, we propose a novel algorithm to solve integral non-linear two-dimensional Volterra equations in a stochastic volatility framework by exploiting specific features of the involved Fourier-type integrals.",A new method to solve Volterra integral equations for variable annuities evaluation with stochastic volatility,"[71017, 73219, 77542]",798,"[5, 45, 135]",1965,Financial Modelling,50,14,39,Stochastic Modelling,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,35 [building - 306],"['Algorithms', 'Financial Modelling', 'Stochastic Models']",WC-39
"An efficient allocation of space in Work-in-Progress [WIP] areas is a key factor for a smooth flow in production, as inadequate planning can lead to bottlenecks, machine idle time and delays. This is especially critical in situations of a high occupation rate as well as limited space. We present an optimization approach for pallet-like production processes illustrated by the working flow in a plant. Our approach is based on mixed-integer linear programming [MILP] and the model allocates order blocks to lanes of varying sizes, while considering the production plan of machines before and after the WIP areas, coupling possibilities of lanes as well as different sequence requirements on lanes. The allocation is optimized according to plant-specific needs, including suitability of the lanes for the orders, lanes kept free for potential changes and blocks of the same order being assigned near each other. We illustrate our approach using synthetic data to mimic the production process of a real plant.  ",Optimizing the Work-in-Progress Area Allocation of pallet-like production process - A Mixed-Integer Linear Programming Approach ,[77541],803,"[105, 84, 65]",1967,Lot-sizing with joint replenishment and routing decisions,32,2,49,"Lot Sizing, Lot Scheduling and Production Planning","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,M1 [building - 101],"['Production and Inventory Systems', 'Optimization Modeling', 'Logistics']",MA-49
"Scheduling patients for radiotherapy within the oncology department is vital for ensuring timely and effective treatment. The patient’s therapy consists of a sequence of pre-treatment consultations, i.e., preliminary investigations to define the required treatment, followed by the radiotherapy treatment sessions on linear accelerators. Timely scheduling of pre-treatment consultations is crucial to prevent unnecessary delays in radiotherapy treatment, thereby impacting patient health. Therefore, we suggest integrating both scheduling problems into one mathematical model formulation. We employ an online stochastic scheduling technique relying on simulation-based optimisation to solve the model. Additionally, we compare our proposed model with benchmark models utilizing a batch scheduling algorithm and ASAP heuristic. A comparison is made between our model and a sequential optimisation approach that subsequently solved the pre-treatment and treatment phase. We use information on the future arrivals of patients to provide a precise overview of anticipated patient flow. The input data was collected from a real-life case study. Our findings highlight the importance of integrating both phases in the scheduling process.",An Integrative Approach to Scheduling the Pre-Treatment and Treatment Radiotherapy,"[77538, 19342]",967,"[129, 136, 56]",1969,Integrated planning in healthcare,3,13,10,OR in Health Services [ORAHS],"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,11 [building - 116],"['Scheduling', 'Stochastic Optimization', 'Health Care']",WB-10
"We present new constraint qualification conditions for nonlinear conic programming that extend some of the constant rank-type conditions from nonlinear programming. Specifically, we propose a general and geometric approach, based on the study of the faces of the cone, for defining  extensions of [relaxed] constant rank CQ and constant rank of subspace components condition to the conic context. We then compare these new conditions with some of the existing ones, including the nondegeneracy condition and Robinson’s constraint qualification. The main advantage of the latter is that we are able to recast the strong second-order properties of the constant rank condition in a conic context. In particular, we obtain a second-order necessary optimality condition that is stronger than the classical one obtained under Robinson’s constraint qualification, in the sense that it holds for every Lagrange multiplier, even though our condition is independent of Robinson’s condition. ",Extensions of Constant Rank Qualification Constraint Condition to Nonlinear Conic Programming,[30295],771,"[19, 115]",1970,Variational techniques in conic optimization and mean field games,82,10,42,Variational Analysis and Continuous Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,98 [building - 306],"['Continuous Optimization', 'Programming, Semidefinite']",TD-42
"In a business with integrated online and offline storefronts, the management and control of stock for end-of-sales-life products facing uncertainty and decline in demand poses important operational challenges. The retailer sells items through its stores and its warehouse. The stores satisfy demand from its available stock, while the warehouse fulfils online orders as well as a proportion of excess customer demand if the stores did not have enough stock on hand; with remaining unsatisfied demand at the store becoming lost-sales.

We develop a two-echelon divergent, periodic-review inventory model, with partial lost-sales at the store level, to determine order quantities during the end-of-sales-life period. We solve small instances to optimality using dynamic programming techniques. Due to the curse of dimensionality, we develop novel heuristics to make ordering decisions that balance accuracy and interpretability for larger instances. 
",Inventory control of end-of-sales-life products with integrated online and offline sales channels,"[77540, 42203, 77624]",480,"[61, 138]",1974,Retail Inventory Management II,30,4,50,Retail Operations,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,M2 [building - 101],"['Inventory', 'Supply Chain Management']",MC-50
"In this talk, we will present a regularized version of the primal-dual Interior Point Method [IPM] for the solution of Semidefinite Programming Problems [SDPs]. Leveraging on the proximal point method, a novel Proximal Stabilized Interior Point Method for SDP [PS-SDP-IPM] is introduced. The method is strongly supported by theoretical results concerning its convergence - the worst-case complexity result is established for the inner regularized IPM solver. Moreover, the new method demonstrates an increased robustness when dealing with problems characterized by ill-conditioning or linear dependence of the constraints. Extensive numerical experience is reported to illustrate the advantages of the proposed method when compared to the state-of-the-art solver.",Proximal-Stabilized Semidefinite Programming,"[68601, 9890]",270,"[115, 60, 19]",1975,"Semidefinite Programming and implementations, Quantum Information Theory and other applications",68,10,38,"Conic Optimization - Theory, Algorithms, and Applications","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,34 [building - 306],"['Programming, Semidefinite', 'Interior Point Methods', 'Continuous Optimization']",TD-38
"The rapid growth of online retailing necessitates flexible warehouse management strategies to adapt to this evolving landscape. One of the key challenges in this area is to reduce the order-picking travel distance. This travel distance is highly affected by the Storage Location Assignment [SLA] decision, which determines how products are allocated to locations in the warehouse. This study aims to develop a mixed SLA strategy which tries to adopt different SLA strategies to some degree that is tailored to the customer order pattern. To do so, four criteria are defined to assess the SLA state - [a] Scatteredness - increases the accessibility of each Stock Keeping Unit [SKU] by spreading its units through the storage locations. [b] Integrity - avoids collecting a single order-line from multiple locations by keeping sufficient number of each SKU in its storage location[s]. [c] Association - stores correlated SKUs close to each other. [d] Depot adjacency - stores high-demand SKUs near the depot[s]. To address the dynamic nature of business needs, a data-driven approach is introduced to weigh each criterion. Then, A multi-objective mathematical model, incorporating contextual constraints and these weighted measures, is proposed to optimize the SLA. As the ultimate goal is to reduce the order-picking travel distance, the proposed model and hypothesis will be validated for this goal under various business environments.",Let Customers Scatter the Inventory - A Multi-Objective Storage Location Assignment in Warehouses,"[77545, 36613, 23971, 47411]",761,"[146, 77, 72]",1977,Warehouse Operations,5,7,58,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S07 [building - 101],"['Warehouse Design, Planning, and Control', 'Multi-Objective Decision Making', 'Mathematical Programming']",TA-58
"Graphs in which the arc set changes over time [in discrete steps], are called temporal graphs. Given a directed temporal graph, a start node s, and a constant number of objectives, the task in the single-source multiobjective temporal shortest path problem consists of computing the set of nondominated images of temporal paths from s to every other node as well as one corresponding efficient path for each of these images. This problem generalizes both the multiobjective shortest path problem in static graphs and the singleobjective temporal shortest path problem.

This talk presents a general label setting algorithm for the single-source multiobjective temporal shortest path problem that can handle a large variety of different objectives. The only condition imposed on the objectives is a monotonicity property that generalizes the nonnegativity of the arc costs required for the well-known label setting algorithm for solving the static single-source shortest path problem in both the singleobjective and the multiobjective case. The worst-case running time of our algorithm is polynomial in the sum of the input size of the problem instance and the number of nondominated images, which means that it runs in polynomial time for all tractable versions of the problem.

To complement this result, we provide a complete classification into tractable and intractable problems for all single-source multiobjective temporal shortest path problems involving a large variety of objectives.",A General Label Setting Algorithm for the Multiobjective Temporal Shortest Path Problem,"[36955, 62688, 77547, 63337]",117,"[77, 53, 5]",1981,Discrete Multiobjective Optimization,34,13,37,Multiobjective Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,33 [building - 306],"['Multi-Objective Decision Making', 'Graphs and Networks', 'Algorithms']",WB-37
"Dynamic programming [DP] has been consistently employed in inventory management, especially in multi-period formulations, contributing to our understanding of the field. Despite the challenges posed by the curse of dimensionality, DP-based formulations have evolved over time to alleviate its effects, especially as inventory problems become more complex. This review aims to analyze the use and evolution of DP-based methods in inventory management, encompassing 287 articles. To provide a comprehensive overview of DP-based formulations for inventory problems, we propose a typology classification, centered inventory-  and DP-based criteria, by categorizing inventory problems based on the types of decisions explored and the main characteristics of the problem investigated, and by identifying DP approaches employed. We emphasize the practical implications of DP approaches in industry, specify limitations in existing studies, and suggest future research directions. In particular, we stress the relevance of considering uncertainties beyond demand-related factors, integrating sustainability aspects into DP approaches such as incorporation of government subsidy levels and regulations, and exploring inventory problems in reverse logistics and remanufacturing. Furthermore, we highlight the need for further development of applied studies utilizing Approximate and Hybrid DP in combination with data-driven methodologies to address challenges in real-life inventory scenarios.",Dynamic Programming-based formulations and algorithms applied to inventory management - A systematic review,"[77511, 19453, 77795, 41201]",339,"[61, 108, 18]",1983,Optimal control in supply chain management,90,2,33,Optimal Control Theory and Applications,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,42 [building - 303A],"['Inventory', 'Programming, Dynamic', 'Computer Science/Applications']",MA-33
"Our research addresses the loading and unloading problem for Roll-on/Roll-off [RoRo] ships, a critical aspect of maritime logistics. Embracing the complexity of real-world uncertainties, this study explores the impact of varying levels of data availability, facilitated by new telecommunications technologies. Realistic real-world problems face the challenges of stochastic and dynamic aspects, such as uncertainties regarding the type of cargo or process times.

To solve this high-dimensional decision problem, we model it as a sequential decision process and use a hierarchical solution method similar to classification and regression trees [CART]. The proposed solution approach yields solutions of short turn-around times but also provides explainability in decision-making. We test our approach against myopic decision policies using real-world scenarios from the Port of Kiel. Subsequently, we apply our methodology to illustrate the influence of different data availability levels, providing specific recommendations regarding the benefits of additional data.",Explainable Decision-Making for Stochastic and Dynamic RoRo Ship Operations,"[77546, 72606, 13086, 19297]",166,"[70, 151, 26]",1985,Machine Learning and Optimization in Ports II,52,13,62,OR in Port Operations,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S12 [building - 101],"['Maritime applications', 'Practice of OR', 'Decision Support Systems']",WB-62
"In this work, we study consumer learning in assortment planning, exploring the impact of online reviews on choice, convergence, and bias. In a utility-maximizing customer scenario, we analyze feedback mechanisms used by retailers and consumers to estimate product qualities.
When consumers are myopic, estimating mean qualities without considering selection bias, product quality estimates converge with an asymptotic bias. This extends findings in the literature for assortment planning settings.
Forward-looking consumers, anticipating and accounting for bias, prevent quality estimates from converging, hindering consumer learning. However, when a subset remains myopic, forward-looking consumers can disentangle bias, enabling convergence in quality estimates for both types.
Illustrating implications for assortment planning, we show how incorporating consumer feedback influences revenue and consumer surplus. In online commerce, real-time consumer information allows personalized assortment decisions based on browsing history.
Our findings illuminate the interplay between consumer learning, online reviews, and assortment planning, offering insights for navigating the e-commerce landscape.
",Unraveling Information Traps - Consumer Learning in Assortment Planning with Online Reviews,[50159],414,"[7, 124]",1987,Pricing and learning 2,11,4,59,Pricing and Revenue Management,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S08 [building - 101],"['Analytics and Data Science', 'Revenue Management and Pricing']",MC-59
"This study addresses the dynamic challenges of urban transportation in logistics caused by time-varying traffic congestion, introducing the Robust Time-Dependent Green Lo-cation-Routing Problem with Time Windows [R-TDGLRP-TW]. A key innovation is incorporating a time-dependent speed function, strategically capturing the nuanced dynamics of urban transportation. Additionally, the inherent uncertainty in travel speed [time], which significantly impacts the system’s performance, is considered in a robust optimization context.
The R-TDGLRP-TW optimizes hubs, vehicle routes, and delivery schedules, minimizing economic and environmental costs. To tackle this, we develop a mixed-integer linear programming model for R-TDGLRP-TW, incorporating the linearization of dynamic programming recursive equations proposed in the literature into a deterministic formulation. A matheuristic solution algorithm is then used to solve large-scale instances efficiently within a reasonable computation time. Our focus on the dynamic aspects of ur-ban transportation aims to efficiently provide practical insights, helping service providers navigate the complex trade-offs encountered in logistics planning.
",A Robust Time-dependent Green Location-Routing Problem With Time Windows,"[71296, 36097, 50253, 55094]",760,"[145, 127, 138]",1989,Logistics 1,5,13,58,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S07 [building - 101],"['Vehicle Routing', 'Robust Optimization', 'Supply Chain Management']",WB-58
"To inform the urgently needed energy transition, models are used to find the optimal design and operation strategy of future energy systems. Since the focus of studies in this field has largely been on the power sector, synergies between coupled sectors have not been sufficiently researched. The integration of heating is important because of three reasons - First, heating is mostly fossil fuelled and offers a large decarbonisation potential. Second, decarbonisation achieved through electrification is a mix of both a burden through increased demand and a benefit through greater flexibility to the power sector. Third, heating and cooling demands depend on the weather, which is subject to uncertainty in the future.
Therefore, we propose an integrated model of the European power system and heating and cooling demands that can be met flexibly while taking different climate projections into account. The methodology can be split into three steps. The first step is data clustering and merging of the European electricity transmission and generation capacities, heating and cooling demands and respective technologies and weather timeseries. Second, heat demands are used to derive aggregated buildings’ multi-node representations. Finally, the model will be optimised using Backbone and decarbonisation is achieved through GHG emissions limits. 
The results of this work allow for the electricity and heating sector to be analysed in an integrated fashion and will be publicly available.
",Integrated flexible heating and cooling under climate uncertainty in a European energy systems model,"[77466, 70125]",840,"[93, 37, 84]",1990,OR in Heating Systems,23,9,19,OR in Energy,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,44 [building - 116],"['OR in Energy', 'Energy Policy and Planning', 'Optimization Modeling']",TC-19
"In this research, we study the case mix planning problem [CMPP] accounting not only for the resources in the operating room department but also the most important resources in other departments such as for example, ward beds, ICU beds, and medical staff by considering the master surgery schedule [MSS] and its implications towards the demand for care in downstream ward units. The MSS allocates operating room time [OR time] among different departments, whereas the CMPP searches for the most profitable mix of patient types. Integrating the strategic and tactical patient planning decisions will reveal the impact of capacity decisions on bed utilization on the operational level and improves decision-making. Current literature treats these two as different, yet related problems, that are solved in a sequential manner. Most works on this topic first identify the optimal mix of patient types [case mix], and then construct an MSS based on this case mix. In this study, however, we try to solve the CMPP and the MSS in an integrated manner. The integrated problem is solved using branch-and-price and is applied to a real case study in a large Belgian hospital. Computational experiments show the effectivity of the presented approach.",Integrating case-mix planning and master surgery scheduling using branch-and-price,"[77550, 19342]",967,"[56, 111, 129]",1992,Integrated planning in healthcare,3,13,10,OR in Health Services [ORAHS],"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,11 [building - 116],"['Health Care', 'Programming, Mixed-Integer', 'Scheduling']",WB-10
"With the rise of battery electric vehicles and advancements in smart charging, there is potential to enhance flexibility and decrease charging expenses. However, degradation is often cited as a major concern for vehicle owners and must be considered. This paper focus on unidirectional smart charging, often overlooked despite its prevalence. To do so, the authors proposed three kinds of Mixed Integer Linear Problems [MILPs] - 1] implementing good usage practices, 2] two-steps direct evaluation of a semi-empirical battery degradation model, and 3] introducing economic weighting for charge rate and State of Charge degradation into the objective function. The objective function includes energy price arbitrage against retail tariffs and to increase realism, MILP models introduced minimum charging value and a maximum charging curve. Each of these models have different strengths and weaknesses which are explored in detail. The authors performed 384 scenarios, considering different charging time slots, initial State of Charge, State of Health, maximum delivering power, and energy price signal, all applied across 6 models. On average the adoption of smart strategies implied a total cost [energy bill plus battery degradation associated cost] reduction between 13.3% and 14.4%, compared with immediate charging. When comparing against a simple smart model without consideration for degradation, total cost improvements ranged between 0.9% and 1.4%.",Degradation-Conscious Charge Management - Comparison of Different Techniques to Include Battery Degradation in Electric Vehicle Charging Optimization,"[77238, 77558, 41827, 77561]",468,"[93, 129]",1993,Optimization for electric vehicles,21,8,22,Energy Management,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,81 [building - 116],"['OR in Energy', 'Scheduling']",TB-22
"This study centers on off-grid PtX plants powered by wind, whose optimal sizing is challenged by the variability and uncertainty of wind power profiles. This investigation spans various sizing methods, employing deterministic and 2 stage stochastic approaches, and also emphasizes the importance of accurate fuel production cost estimates to derive optimal plant sizing and investment decisions. Results indicate that using a deterministic least-cost investment and operation model with an average power profile year is most likely to lead to accurate fuel production cost estimates. Operational details’ influence on cost estimates is minimal, advocating for a linear plant operation model for efficient solving time. In analyzing the optimal sizing and profitability performances, this study shows that the most suitable approach depends on the expected ammonia selling price and the fuel production cost estimates. For scenarios with significantly higher selling prices than cost estimates, a deterministic sizing method with the worst weather year and modeling the flexible ammonia plant as fully flexible proves to lead to the most profitable operation. For scenarios with ammonia selling prices in the same range as cost estimates, using a 2-stage stochastic method with a set of average years leads to the highest profits during operation. While the study focuses on Bornholm, in Denmark, there is acknowledgment of the need for further research to generalize findings.",Method comparison for optimal power-to-X plant sizing and operation under wind profile uncertainties,"[76651, 71019, 77553, 62517]",261,"[33, 136, 38]",1995,Hydrogen Modeling and Regulation I,22,14,09,Energy Markets,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,10 [building - 116],"['Economic Modeling', 'Stochastic Optimization', 'Engineering Optimization']",WC-09
"We introduce a pointwise axiomatic approach to completion of incomplete preference relations through appropriate value functions. We consider an abstract scalarization framework, originally developed in the field of vector optimization, in order to tackle generic partial quasi orders. We apply this approach to study non cooperative games where players have incomplete preferences. In this context, our results allow us to obtain both necessary and sufficient Nash equilibrium conditions. We compare our representation results with other approaches to preference completion, in the special case where value functions are Aumann utilities. In detail, we identify minimal sets of Aumann utilities that allow us to find all Nash equilibria of a game with incomplete preferences, with no need of additional assumptions. As a special case, we reframe our pointwise approach in vector games, where players have vector-valued payoffs and partial orders on players' outcomes naturally arise. We characterize Nash equilibria in vector games by scalarization, extending to the non convex case the results originally developed by Shapley [1959].",Games with incomplete preferences - a pointwise completion approach,"[11677, 57366, 61281]",50,"[50, 27, 77]",1997,Vector and Set Optimization II,33,3,41,Vector and Set Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,97 [building - 306],"['Game Theory', 'Decision Theory', 'Multi-Objective Decision Making']",MB-41
"Spatial relations in data such as house prices are known to vary strongly depending on their location. Several modeling techniques have been proposed from the domain of spatial statistics such as geographically weighted regression. This technique addresses this problem by allowing to vary the parameters over space. Nevertheless, more recent machine learning methods such as gradient boosted trees perform better in terms of prediction and improve scalability towards large datasets significantly. Nevertheless, global tree-based models, trained over the complete spatial region of the dataset, present large local errors which indicate their flawed ability to model spatially varying relations in the data. A solution consisting in learning local models on spatial partitions of the dataset is not preferred due to the loss of information shared by the partitions. In this study, we confirm the presence of this phenomenon in a real-life house price dataset and explore pathways towards a unification of global and local models to address this problem.",Harnessing spatial granularity in boosted tree models for house price prediction,"[71661, 57466, 68588]",392,"[7, 66]",1998,Analytics for Decision Making,17,12,31,Analytics,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,54 [building - 208],"['Analytics and Data Science', 'Machine Learning']",WA-31
"Many problems in quantum information are formulated as convex optimization problems involving the quantum relative entropy function. These problems cannot be directly expressed as semidefinite programs. In this talk I will discuss various tools to deal with such optimization problems. In particular, I will present a self-concordant barrier with optimal parameter for the quantum relative entropy cone. This barrier function can be used with interior-point schemes to solve convex optimization problems with the quantum relative entropy function. Based on joint work with James Saunderson [arXiv:2205.04581].

",Quantum relative entropy optimization,[76629],270,"[19, 115, 60]",1999,"Semidefinite Programming and implementations, Quantum Information Theory and other applications",68,10,38,"Conic Optimization - Theory, Algorithms, and Applications","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,34 [building - 306],"['Continuous Optimization', 'Programming, Semidefinite', 'Interior Point Methods']",TD-38
"This study introduces an optimal operational model for a smart building, showcasing the integration of indoor Carbon Dioxide [CO2] forecasting. The study presents an hourly operational model that encompasses the Heating, Ventilation, and Air conditioning [HVAC] system, electrical storage [ES], and Light-Emitting Diode [LED] lights, along with the utilization of photovoltaics [PV]. The first objective is to employ a Long Short-Term Memory [LSTM] neural network to predict indoor CO2 concentrations and identify hours when these concentrations exceed the healthy threshold. Subsequently, the operational model is presented to ensure optimal energy usage, flexibility to selectively activate or deactivate the entire HVAC system during hours when forecasted CO2 levels surpass unhealthy thresholds. This proactive measure aims to prevent the escalation of indoor CO2 levels. By combining CO2 forecasting with operational optimization, this study contributes to enhancing the sustainability and performance of smart buildings while prioritizing the well-being of occupants. Additionally, the model ensures that optimal lighting levels are maintained at all times without compromising the visual comfort of building occupants. Theses algorithms and models have been developed in the context of the Netbuild project [CPP2021-009031], where the developments will be implemented in a real smart building located in Viladecans, Spain.",Enhancing User Comfort in Smart Buildings through CO2 Forecasting and Operational  Optimization,"[77551, 41827, 77558, 77559]",813,"[93, 129, 38]",2000,Emissions and Heating Sector,80,13,53,Sustainable and Resilient Systems,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,8007 [building - 202],"['OR in Energy', 'Scheduling', 'Engineering Optimization']",WB-53
"Hourly matching—the concept of matching energy demand with clean electricity supply on an hourly basis—has gained immense attention. The concept got into the spotlight when regulations on how to certify hydrogen as “green” [and thus qualify for support schemes] sparked active debates among both the European Union and the United States’ policymakers, industry stakeholders, and research communities. Hydrogen regulation is not the only application of the concept. Through hourly Carbon-Free Energy matching [usually called 24/7 CFE], some public and private energy buyers have pledged to eliminate all greenhouse gas emissions associated with their electricity use. The concept of hourly matching is founded on three pillars - temporal alignment, geographical scope, and additionality of clean energy supply. Several research groups have recently developed energy system models to explore the designs and implications of temporal hydrogen regulation and voluntary 24/7 CFE commitments. The resulting studies and models behind these studies consider the three pillars of hourly matching. Our work discusses these core principles of hourly matching and their implementation in models. Using our own research papers and reports as examples, we take a critical look at which modeling choices we make and what their real-world implications are. Our talk aims to bridge the gap between the land of energy models and the real world using the narrow yet crucial lens of hourly matching.",Three Pillars of Hourly Matching - A Tour From Model Land to Real World,"[70705, 77415]",643,"[37, 33, 93]",2005,Hydrogen Modeling and Regulation II,22,15,09,Energy Markets,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,10 [building - 116],"['Energy Policy and Planning', 'Economic Modeling', 'OR in Energy']",WD-09
"Maritime transport is vital to global trade, with ships carrying over 80% of the world's goods in volume. Efficient port planning is critical to the functioning of intermodal hubs like the Port of Hamburg, but long concessions of 20-30 years make port planning a complex and challenging task. A port is constantly evolving due to technological, economic, political, and environmental factors, requiring innovative approaches to port planning.
Our project, HafenplanZen, is funded by the IHATEC program and is a collaborative effort between the Hamburg Port Authority, Hamburg Port Consulting, and the University of Hamburg. The Port of Hamburg already has a high degree of digitalization and employs multiple digital twins to simulate and optimize various aspects of port operations. HafenplanZen leverages this digital infrastructure and serves as a support tool for port planners, integrating data from multiple digital twins to provide a comprehensive overview of the port's behavior and efficiency.
One of the project's key focuses is the development of a sophisticated 3D visualization tool that uses simulation to evaluate the port's performance. HafenplanZen's simulation-based approach enables port planners to test various improvement strategies and make informed decisions about optimizing port performance. The tool also provides improvement suggestions based on simulation results, allowing planners to make data-driven decisions about how to improve the efficiency of port operations.","HafenPlanZen - Port Master Planning through Simulation, Optimization, and Visualization",[75424],167,"[145, 131, 134]",2006,Sustainable freight transportation,52,10,62,OR in Port Operations,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S12 [building - 101],"['Vehicle Routing', 'Simulation', 'Software']",TD-62
"The field of human-machine interaction is increasingly focusing on the anthropomorphism of computerized agents, especially their design to mimic human behavioral patterns. This research investigates a pivotal question - Do humans respond to human-like agents in the same way as they do to human counterparts in interactive scenarios? We explore this question using lab experiments within a supply chain bargaining context, where the supplier has private information about production costs. The retailer proposed wholesale prices to the supplier, and decisions to accept or reject these offers were made by a computerized agent acting on behalf of the supplier. We find that individuals respond to the response time of agent partners similarly to how they respond to human partners. Compared to human-human bargaining, the retailer’s profit is lower and the supplier’s profit is higher in human-machine bargaining. These findings hold significant implications for the design of online bargaining tools in e-business, particularly in enhancing the use of human-like agents in commercial negotiations.",Humans respond to machines’ response times in human-machine interaction,"[77479, 77556, 77557]",183,"[10, 138, 8]",2009,Behavior in supply chain collaboration,13,14,07,Behavioural OR,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,1019 [building - 202],"['Behavioural OR', 'Supply Chain Management', 'Artificial Intelligence']",WC-07
"Machine learning algorithms, where the computing system learns from data in order to adjust its behavior, are the key enabler of modern AI-based technologies. This dependency on data, however, is also the Achille's heel of AI systems. The data, which can come from a wide variety of sources, is not always trustworthy. Some sources can provide erroneous or corrupted data. With current machine learning algorithms, a single bad source can lead the entire learning scheme to make critical mistakes. Moreover, to handle the huge amounts of data, machine learning algorithms are often deployed over a large network of machines. Consequently, as the network size increases, the likelihood of machine errors also increases. Indeed, software and hardware bugs are prevalent, and machines can sometimes be hacked by malicious players. Some of these players attempt to corrupt the entire learning procedure, merely for the pleasure of claiming to have destroyed an important system. Others attempt to influence the learning procedure for their own benefit. 

Building distributed machine learning schemes that are robust to these events is paramount to transitioning AI from being a mere spectacle capable of momentary feats to a dependable tool with guaranteed safety. In this talk, I will cover some effective techniques for achieving such robustness. Specifically, I will present distributed machine learning algorithms that do not trust any individual data source or computing unit. ",Machine Learning in Untrusted Distributed Environment,[77554],370,"[66, 127]",2010,Distributed and Federated Optimization,84,15,32,Advances in large scale nonlinear optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,41 [building - 303A],"['Machine Learning', 'Robust Optimization']",WD-32
"This paper studies an interesting logistical problem related to maintaining a set of selected turbines in offshore wind farms. A service operation vessel [SOV], used to support maintenance activities, also carries a safe transfer boat [STB], which is used to transport technicians, parts, and equipment to the turbines. The maintenance activities are optimised over a planning horizon by simultaneously coordinating the SOV and the STB. A mixed-integer linear programming model is designed, where the objective function is to minimise the total maintenance cost. As the proposed mathematical model is hard to solve with a commercial solver, a decomposition approach is developed. A two-stage stochastic programming model is also designed to tackle uncertain conditions, including the duration of maintenance and the STB's travel time. The performance of the solution method is evaluated using the Thanet Offshore Wind Farm, in the southeast of the UK. The results of our experiments show that joint use of an SOV and an STB can yield a lower total maintenance cost than only using an SOV or crew travel vessels [CTVs] that are currently the common practice in maintaining offshore turbines.",Routing in offshore wind farms - A multi-period location and maintenance problem with joint use of a service operation vessel and a safe transfer boat,"[29342, 37832, 62429, 58073]",858,"[14, 151]",2012,"Discrete, continuous or stochastic optimization and control in networks, transportation and design II",64,3,25,Combinatorial Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,011 [building - 208],"['Combinatorial Optimization', 'Practice of OR']",MB-25
"The economies of many geographic markets are dependent on fossil fuels. Due to geopolitical tensions, some geographic markets may decide to partially or totally halt their gas trades. We therefore propose a model which casts the optimal taxation problem as a networked game in two settings - a] perfect competition, where a global market operator is responsible for adjusting the export prices, leading to a partial equilibrium; b] a variational approach, where the export prices are determined endogenously as dual variables of the supply-demand balances.
Our model is built on an agent based representation of suppliers and generators interacting in a certain number of geographic markets. We aim to assess the impact of tariffs or sanctions on the geographic markets’ imports of gas and on their utility. To that purpose, the optimal taxation problem is framed as a Stackelberg game where a regulator at the upper level is responsible for the sanction definition considering different criteria, while the geographic market at the lower level reacts by adjusting their exports and trade prices. We derived closed-form expressions for the export prices, and proved that the Stackelberg equilibria can be explicitely mapped to the perfect competition equilibria. Finally, considering a degree of bounded rationality on the part of stakeholders, we rely on Prospect Theory to extend the optimal taxation games to in situations involving risks linked to the behavior of other market stakeholders.",Gas Trading with Tariff Versus Sanction as a Networked Game,"[77493, 71651, 79830]",326,"[50, 27, 10]",2014,Machine Learning for Electricity Market Applications,22,4,14,Energy Markets,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,16 [building - 116],"['Game Theory', 'Decision Theory', 'Behavioural OR']",MC-14
"In this paper, we bridge the psychological Job Demands & Resources [JD-R] literature with the operations literature on activity analysis. The  JD-R model provides a comprehensive framework, categorizing risk factors associated with different occupations. On one hand, resources positively affect performance, while on the other hand, demands relate to work conditions. Understanding the health impairment process [strain due to excessive demands] and the motivational process [resource-driven well-being enhancement] is essential in defining a sustainable level of workload. We apply a multidimensional nonparametric activity analysis on the JD-R model, to assess the relation between resources, demands, and mental well-being of employees. Using event-level data of railway control room operators, we construct a multidimensional JDR indicator for digital production, which we empirically validate by using proxies for operational risk and employee well-being. ",Measuring and managing [un]sustainable workload under digitization - A frontier-based job demands-resources model,"[76960, 62482]",391,"[24, 139, 122]",2015,DEA and its application,89,2,48,Data Envelopment Analysis and its Application,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,60 [building - 324],"['Data Envelopment Analysis', 'Sustainable Development', 'Railway Applications']",MA-48
"The fast development of e-commerce and higher customer expectations for quicker order delivery emphasise the importance of efficient management in urban warehouses, which operate in a setting with dynamic order patterns and stringent order lead time. Dynamic order-picking strategies became increasingly important as they allow for the dynamic assigning of incoming orders to pickers during each pick cycle. Collaborating with an industrial partner that develops cutting-edge Internet-of-Things technologies and platforms for smart warehousing, our proposed model extends the original dynamic order-picking problem by considering the due date and the order priority. Given the set of pickers and the sets of continuously coming new orders, our model focuses on assigning incoming orders to pickers and redesigning the routes of each picker on the sequence of item locations they will visit and the depot they will return to while meeting the time frame requirements for each order and considering the order priority in order to ensure a more efficient order-picking process in smart warehouses. Due to the time constraints and the need for quick response to the complex order fulfilment processes, the efficiency of the algorithm is as important as the solution quality. We propose a problem-tailored heuristic solution approach by extending the well-known heuristics, which can efficiently deal with the decisions of picklist updating, picker assignment, and rerouting.",Multi-Depot Dynamic Routing of Multiple Pickers With Order Priorities and Due Dates in Smart Warehouses,"[70418, 51101, 61009, 62405, 77626]",733,"[65, 146, 145]",2016,Routing in Warehouses,5,2,64,VeRoLog - Vehicle Routing and Logistics,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S16 [building - 101],"['Logistics', 'Warehouse Design, Planning, and Control', 'Vehicle Routing']",MA-64
"We present a methodology for establishing the existence of quadratic Lyapunov inequalities for a wide range of first-order methods used to solve convex optimization problems. In particular, we consider [i] classes of optimization problems of finite-sum form with [possibly strongly] convex and possibly smooth functional components, [ii] first-order methods that can be written as a linear system on state-space form in feedback interconnection with the subdifferentials of the functional components of the objective function, and [iii] quadratic Lyapunov inequalities that can be used to draw convergence conclusions. We present a necessary and sufficient condition for the existence of a quadratic Lyapunov inequality within a predefined class of Lyapunov inequalities, which amounts to solving a small-sized semidefinite program. We showcase our methodology on several first-order methods that fit the framework. Most notably, our methodology allows us to significantly extend the region of parameter choices that allow for duality gap convergence in the Chambolle-Pock method.",Automated tight Lyapunov analysis for first-order methods,"[76646, 47554, 66635, 50891]",366,"[115, 21, 81]",2020,Computer-Assisted Proofs in Optimization,84,14,32,Advances in large scale nonlinear optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,41 [building - 303A],"['Programming, Semidefinite', 'Convex Optimization', 'Non-smooth Optimization']",WC-32
"Hexaly Studio is a no-code SaaS platform for mathematical optimization. Hexaly No-Code Modeler allows users to create a mathematical optimization model based on a business description of its characteristics. The verticals currently handled by Hexaly No-Code Modeler are Vehicle Routing and Production Scheduling. Consequently, using Hexaly Studio, software developers and data scientists can build rich Vehicle Routing or Production Scheduling apps without writing a line of code. This no-code approach is particularly powerful for fast prototyping.

For each vertical, the user can describe the business problem – decisions, constraints, objectives – by selecting the appropriate business features along a straightforward workflow. Then, the Hexaly No-Code Modeler builds a Hexaly model, together with sample data inputs and a dashboard with appropriate widgets to visualize the solutions provided by Hexaly Optimizer. The widgets can be texts, tables, maps, [Gantt] charts, etc. Having explored the solution using sample data, the user can feed the Hexaly model with real-world data and adjust the model constraints and objectives in no-code, or directly by customizing the Hexaly model.",Hexaly Studio - no-code modeler for routing and scheduling problems,[74918],61,"[84, 145, 129]",2023,"Theory of Knowledge, Technology, and Innovation",54,12,08,"Knowledge, Technology, and Innovation","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1020 [building - 202],"['Optimization Modeling', 'Vehicle Routing', 'Scheduling']",WA-08
"A typical approach to tackle stochastic control problems with partial observation is to separate the control and estimation tasks. However, it is well known that this separation generally fails to deliver an actual optimal solution for risk-sensitive control problems. This paper investigates the separability of a general class of risk-sensitive investment management problems when a finite-dimensional filter exists. We show that the corresponding separated problem, where instead of the unobserved quantities, one considers their conditional filter distribution given the observations, is strictly equivalent to the original control problem. We widen the applicability of the so-called Modified Zakai Equation [MZE] for the study of the separated problem and prove that the MZE simplifies to a PDE in our approach. Furthermore, we derive criteria for separability. We do not solve the separated control problem but note that the existence of a finite-dimensional filter leads to a finite state space for the separated problem. Hence, the difficulty is equivalent to solving a complete observation risk-sensitive problem. Our results have implications for existing risk-sensitive investment management models with partial observations in that they establish their separability. Their implications for future research on new applications is mainly to provide conditions to ensure separability.",On the separation of estimation and control in risk-sensitive investment problems under incomplete observation,"[71595, 77608]",71,"[136, 45, 20]",2024,Robust decisions in finance and investments,74,7,57,Modern Decision Making in Finance and Insurance,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S06 [building - 101],"['Stochastic Optimization', 'Financial Modelling', 'Control Theory']",TA-57
"This talk examines the gradient projection method as a solution approach for constrained optimization problems in kappa-hyperbolic space forms, particularly for potentially non-convex objective functions. We consider both constant and backtracking step sizes in our analysis. Our studies are based on the hyperboloid model, commonly referred to as the Lorentz model. In our investigation, we present several innovative properties of the intrinsic kappa-projection into convex sets of kappa-hyperbolic space forms. These properties are crucial for analyzing the method and also hold independent significance. We discuss the relationship between the intrinsic kappa-projection and the Euclidean orthogonal projection as well as the Lorentz projection. Moreover, we provide formulas for the intrinsic kappa-projection into specific convex sets using the Euclidean orthogonal projection and the Lorentz projection. Regarding the convergence results of the gradient projection method, we establish two main findings. Firstly, we demonstrate that every accumulation point of the sequence generated by the method with backtracking step sizes is a stationary point for the given problem. Secondly, assuming the Lipschitz continuity of the gradient of the objective function, we show that each accumulation point of the sequence generated by the gradient projection method with a constant step size is also a stationary point. Additionally, we provide an iteration complexity bound.",On projection mappings and the gradient projection method on hyperbolic space forms,"[8875, 45761, 74957]",58,"[21, 113, 19]",2025,Optimization on Manifolds,69,8,41,Optimization on Geodesic Metric Spaces - Smooth and Nonsmooth,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,97 [building - 306],"['Convex Optimization', 'Programming, Nonlinear', 'Continuous Optimization']",TB-41
"In this presentation, we consider sensitivity analysis to combinatorial optimization problems [or binary programming problems]. In these problems, there is a ground set of elements, each of them with a given cost. The objective is to find a feasible combination of elements which, in our case, minimizes a total objective value. 
Sensitivity analysis concerns finding the range of changes in cost values such that current solutions remain optimal. The supremum decrease and increase in a single element’s cost, the lower and upper tolerances, respectively, can be computed by solving an additional instance of the problem. When k element costs increase or decrease simultaneously, there is a k-dimensional area in which solutions remain optimal. Jäger et al. [2018, 2023] introduce set tolerances as the infimum and supremum sum of cost changes that lie inside and outside that area. For problems with sum objective, it requires solving an exponential number [in k] of instances to optimality to describe the area [Andersen et al, 2023] and to compute set tolerances. 
We present new results which show that set tolerances of problems with a bottleneck objective [the objective value is the largest cost of the elements in a solution] require the solution of only one problem instance. We also show that tolerance computations can be independent of the choice of optimal solution.
",Computing the effect of multiple cost changes in combinatorial problems ,"[24079, 71226]",883,"[14, 0]",2026,Topics in Combinatorial Optimization II [Contributed],64,15,25,Combinatorial Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,011 [building - 208],['Combinatorial Optimization'],WD-25
"Preterm neonates with low birth weight often require frequent small-volume blood transfusions. Instead of addressing each transfusion event individually, reserving blood bags in advance may prove to be economical and minimize donor exposure. However, it might introduce storage challenges and could lead to the wastage of blood. An optimal strategy for reserving blood bags depends on neonatal clinical characteristics and available blood reserves in the transfusion service.

To model this decision problem under uncertainty, we assume that the known and unknown random variables are structured as a Bayesian network and will use a partially observed Markov decision process. At each day, the state of the process is described by the blood inventory and the neonatal clinical characteristics. Neonatal gestational age, weight, and medical conditions are observed variables. In contrast, the length of stay and the total amount of required blood are unknown at the time of decision-making.

The proposed model can be expressed as a dynamic Bayesian network model in which the decision variables can be optimized by dynamic programming. We explore the sizes of the problem instances that can be solved in this exact way and also propose an efficient approximate dynamic programming method for large instances. We illustrate the methods with one year of data from a neonatal ICU in South India where financial and blood-availability constraints play a crucial role in finding optimal strategies.",Optimal Usage of Packed Red Blood Cells in Preterm Neonates Requiring Frequent Small-Volume Transfusions,"[77215, 77205, 77569, 77574, 77568, 77579, 77583, 77581, 59078]",979,"[136, 108, 56]",2028,Medical decision making,3,5,17,OR in Health Services [ORAHS],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,40 [building - 116],"['Stochastic Optimization', 'Programming, Dynamic', 'Health Care']",MD-17
"We investigate the incentive for a retailer to share private demand information with two rebate-offering manufacturers who sell substitutable products through the retailer. We characterize how the retailer’s information sharing decision depends on key factors such as the proportion of rebate-sensitive consumers and competition intensity.  Without side payment, the retailer will not share information with a monopolistic manufacturer, but he may do so with none, one or both of the manufacturers when there is competition.  With side payment, the retailer always prefers to sell information concurrently instead of sequentially to the manufacturers.",Information Sharing and Manufacturer Rebate Competition ,[6398],633,"[138, 0]",2029,Retail Cooperation and Competition,30,13,61,Retail Operations,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S10 [building - 101],['Supply Chain Management'],WB-61
"To quantify the relative importance of nodes in a network, one possible approach uses the concept of centrality. However, classical measures of centrality do not consider how the network behaves if any subset of nodes is removed from the network and, when applied to transportation networks, issues such as arc capacity, route choice, and congestion are not taken into account. Alternative measures of centrality based on game theory are more effective, such as those based on the Shapley value. The Shapley value is a well-established concept in cooperative game theory, used as a metric for assessing the significance of each player in a transferable utility game. Recently, it has found application in gauging the importance of individual arcs or nodes within transportation networks. However, the exact computation of the Shapley value is often computationally expensive, particularly in the context of extensive networks. Here, we approximate the Shapley value in a transferable utility game defined on a network, wherein the network’s characteristics are parameterized by a variable of interest [e.g., the traffic demand]. Smoothness properties of the Shapley value as a function of the parameter are investigated and exploited to motivate using machine-learning techniques for its approximate computation.",Approximate computation of the Shapley value in cooperative games on transportation networks via machine-learning techniques,"[35427, 12557, 58069, 1632]",855,"[50, 143, 66]",2031,Recent advances on Variational Inequalities and Equilibrium Problems III	,51,15,43,Variational Inequalities and Equilibrium Problems - From Theoretical Advances to Real World Applications,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,99 [building - 306],"['Game Theory', 'Transportation', 'Machine Learning']",WD-43
"In this talk, we show how we tackle a production planning challenge arising in the real-world application of rubber flooring production. Here, the last of the four major production steps is hard to plan well in advance, due to a large product variety, hard-to-formalize context-dependent rules, sometimes insufficient material supply and unforeseeable events such as staff shortages or machine failures. We developed a tactical planning approach in which we compute a rough quantity-based production schedule for the four major production steps by solving a linear optimization problem. This schedule adapts to the changing production environment and state of production seamlessly and has two main functions - Firstly, it is used to monitor the flow of material through production to proactively take action if a discrepancy is observed. Secondly, the model is setup in a way such that satisfactory operational schedules for the respective production steps can be derived easily from a well-balanced rough schedule.

The approach was integrated into the company's production planning software. After a one-year test phase in which the model was further customized to this specific production setting, the resulting schedule is now used as the main decision-making tool and process control system by several departments.",An Optimization Approach for Controlling Material Flow in a Production Network on a Tactical Level - A Case Study,"[63558, 23161, 61771, 78583]",836,"[129, 26, 110]",2036,Lot-sizing with industrial applications II,32,5,49,"Lot Sizing, Lot Scheduling and Production Planning","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,M1 [building - 101],"['Scheduling', 'Decision Support Systems', 'Programming, Linear']",MD-49
"Railways, offering environmental efficient transport services, are a potential alternative to roadways. However, the increasing complexity of rail networks and interactions between infrastructure, resource planning, particularly locomotive scheduling, and personnel deployment, poses significant challenges. Railway companies use optimization techniques to generate circulation plans, promising efficient resource utilization. Nonetheless, these plans are often deterministic and ignore information on disruptions arising from delayed crew, crowded infrastructure and breakdowns. As a consequence, ad hoc short-term rescheduling and adjustments are necessary, resulting in economic penalties and a negative impact on perceived service quality. In this paper, we aim to determine reliable locomotive schedules that minimize the number of locomotives and empty-run distances using a time-space network formulation. To generate reliable plans, we design and include buffers considering stochastic travel times. We evaluate the effect of inserting these buffers on the number of locomotive and empty-run distances for real-world Austrian railway use cases. At the same time, we assess schedule reliability through feedback from simulations. The trade-off between reliability, the number of locomotives, and empty-run distances can offer valuable insights to planners seeking to address disruptions without stressing already constrained infrastructure.",A real-world locomotive scheduling problem with stochastic travel times and buffers,"[74876, 16919]",269,"[123, 129, 143]",2038,Disruption management and recovery,85,9,54,Public Transport Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S01 [building - 101],"['Reliability', 'Scheduling', 'Transportation']",TC-54
"Mixed-model assembly lines [MMALs] manufacture several variants of the same product base to meet the varied model-based customer demands. However, this model diversity usually causes fluctuations in the total processing time at some workstations. When these fluctuations exceed the cycle time, work overload is generated. This occurs, for example, when several high processing time models are processed consecutively. Therefore, a careful sequencing of these models is essential to reduce overload. The main objective of the Mixed-Model Sequencing Problem [MMSP-W] is to establish a sequence of the models to be manufactured so that the total overload is minimized.

Manual labor is common in MMALs, where their competence levels determine operator performance. Thus, competence deficiencies may lead to higher processing times and increased work overload. Therefore, competence-driven operator assignment should be considered alongside model sequencing. We achieved this by extending MMSP-W to include operator assignment in its formulation, and performing the assignments to minimize the gap between task competence requirements and the competence levels of operators. Initial findings show that this extension enables appreciation of the influence of operator competency in the overloads generated in a sequence. Furthermore, the extension results in a problem with additional complexity and incremental solving times, which we address by proposing a Benders composition-like heuristic.",Mixed-model assembly sequencing and operator assignment,[76990],809,"[69, 111, 57]",2039,Lot-sizing with energy aspects,32,8,49,"Lot Sizing, Lot Scheduling and Production Planning","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,M1 [building - 101],"['Manufacturing', 'Programming, Mixed-Integer', 'Human Resources Management']",TB-49
"Airline operations are prone to delays and disruptions, since the schedules are generally tight and depend on a lot of resources. Since the COVID pandemic, most airlines also have shortages of these resources, making the problem worse. In case of disruptions, such as aircraft breakdowns or crew sickness, the schedule needs to be adjusted. This means either changing resource assignments or delaying or cancelling flights. Such adjustments need to resolve a disruption while minimizing overall costs and impact on passengers. For practical use, this needs to be done as close to real-time as possible.

We focus on both the aircraft and crew schedules. Resolving disruptions for these can be done sequentially, but decisions in one problem may lead to more conflicts in the other. Thus, we take an integrated approach. Such an approach comes with computational challenges, especially because of the runtime required for practical use.

For this, we develop a local search algorithm based on simulated annealing. Where, in order to not spend time on making changes in unaffected parts of the schedule, we test different neighbour generation and selection approaches that steer the local search into resolving these disruptions more directly. The work is done in collaboration with and with data from KLM Royal Dutch Airlines.

We show that our approach is very fast, achieving runtimes well below a minute. Furthermore, we also show that the disruptions are resolved in a cost-efficient manner.",Simulated Annealing to Solve the Integrated Airline Fleet and Crew Recovery Problem,"[74987, 19331, 78152, 76846, 77584]",269,"[4, 74]",2040,Disruption management and recovery,85,9,54,Public Transport Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S01 [building - 101],"['Airline Applications', 'Metaheuristics']",TC-54
"Healthcare providers are continuously striving to organize care and cure more efficiently while maximizing outcomes for the patient. Benchmarking is seen as an important instrument to improve processes and the organization of care on the level of hospitals and processes within these hospitals. Data envelopment analysis [DEA] shows to be a successful method to facilitate such benchmarking, thanks to its ability to combine inputs and outputs into a single efficiency score. However, the method might not have been used to its full advantage given recent evolutions in the healthcare landscape. By means of a systematic literature review, including 498 articles for the period 1980 till now, we developed a classification system to position DEA benchmarks. Only a very small number of studies can be classified as DEA applications aimed at investigating process flows within and between hospitals, compared to a more traditional static or silo-oriented approach. This opens a future research agenda to investigate efficiency when care processes extend beyond multiple units within a hospital, or even across multiple hospitals which we now see emerging by formation of hospital networks and systems. In this presentation, we break down the DEA literature on different new dimensions and show a clear imbalance in problem settings studied by the academic community, highlighting new opportunities towards problems practice is currently challenged with.",A new classification of data envelopment analysis applications in hospitals.,"[77562, 66686, 77573]",970,"[24, 0]",2041,DEA in healthcare,3,3,17,OR in Health Services [ORAHS],"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,40 [building - 116],['Data Envelopment Analysis'],MB-17
"The European ambitions for a hydrogen economy require the rapid development and scaling of technology and business models. As frontrunners, the Nordics have the potential to contribute to and benefit from the establishment of hydrogen valleys with the development of energy hubs. Analyzing specific locations is essential to gain insight into the characteristics of optimal production, cost structures, and investments of hydrogen and its derivatives, like methanol. The application of an energy hub model for a case study of producing e-methanol in Southern Jutland helps to obtain specifics on the operation and structure of these business models. The model uses the Spine environment using Mixed Integer Linear Programming to investigate cost-optimal production and investments. Thus, the model helps to obtain specifics of the hub by a detailed representation with variable efficiency rates to improve the representation of flexibility by electrolysis and by-products such as the use of excess heat. An integrated sensitivity analysis enables the evaluation of the impact of regulatory measures and costs on the feasibility and viability of energy hubs. Results show the optimal costs alongside the energy flow of each technology. The evaluation of the energy hub focuses on the levelized cost of the depicted energy carriers. Further development will concentrate on refining investment implementation and integrating technological, and socio-economic aspects identified by the Nord_H2ub project.",Fueling the Future - Optimizing Power-to-X Production in Renewable Energy Hubs through Flexible Operating Units,"[77572, 77577, 71181]",261,"[93, 0]",2045,Hydrogen Modeling and Regulation I,22,14,09,Energy Markets,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,10 [building - 116],['OR in Energy'],WC-09
"This talk presents an approach to quantify the value of data sets available to the operator of an electric distribution system that seeks to compute optimal control setpoints for distributed energy resources. We leverage the ability of the Wasserstein metric to encode the quality of available data sets and present a modification of a data-driven AC optimal power flow problem with stochastic load and renewable energy injections that can internalizes this data quality information. The proposed approach offers an effective tool for the system operator to handle data sets of varying quality, e.g., due to the necessary privacy protection or aggregation of data collected at the grid edge. We show that this not only helps the system operator to improve operations with existing data, but also informs planning decisions for investments in advanced data collection and communication technology. ",Data value at the grid-edge,[76631],430,"[136, 37]",2047,Data Valuation from Data-driven Optimization,49,12,35,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,44 [building - 303A],"['Stochastic Optimization', 'Energy Policy and Planning']",WA-35
"Variable annuities [VAs] are insurance contracts designed to meet retirement and long-term investment goals. These products combine the participation in equity performance with insurance coverages. The 
subscriber of a VA [policyholder] enters the contract by paying a premium, and chooses to invest it from a selection of investment funds. Besides the life insurance protection, VAs provide a number of financial 
guarantees and options. A minimum rate is guaranteed by the insurer in order to protect the policyholder’s capital against market downturns. Moreover, the policyholder has the right to early terminate the contract 
[surrender option] and to receive the account value. In general, a penalty is applied by the insurer in case of early surrender, which decreases in time. These financial guarantees are financed by a fee paid by the 
policyholder, typically in the form of a fixed proportion of the account value. At maturity, the policyholder can choose to obtain the account value or convert it into an annuity stream. These features of the VAs contracts are liabilities to the issuer and constitute a potential hazard to company solvency. In this paper we provide an analytical study of the pricing formula for the VAs and a characterisation of the optimal exercise 
strategy for the surrender option.",An analytical study of variable annuities with surrender option,"[77571, 77601, 77602]",188,"[45, 136]",2048,Applications in Finance and Economics,4,2,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S14 [building - 101],"['Financial Modelling', 'Stochastic Optimization']",MA-63
"The One-commodity Pickup and Delivery Location Routing Problem [1-PDLRP] is a new problem that combines characteristics of the Location and Routing Problem and the One-commodity Pickup and Delivery Traveling Salesman Problem. We are given a set of customers that provide or demand given amounts of a product, and a set of potential facility locations that can be opened or not in order to give service to the customers. Each facility has an opening cost and is the depot of a vehicle. The problem consists in deciding which facilities to open, assigning customers to open facilities, and designing the routes that connect each facility with its customers. The objective is to minimize the sum of the cost of the routes and the facilities. This NP-hard problem has not been previously studied. We propose for it two mathematical formulations, compare them, and present a branch-and-cut algorithm able to solve instances with up to 100 nodes.",The one-commodity pickup and delivery location routing problem,"[1634, 71268, 31277, 77580]",859,"[143, 64, 11]",2050,"Discrete, continuous or stochastic optimization and control in networks, transportation and design III",64,4,25,Combinatorial Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,011 [building - 208],"['Transportation', 'Location', 'Branch and Cut']",MC-25
"We introduce a suite of multiple criteria methods that fuse two sources of information. On the one hand, we exploit a crisp outranking relation in the set of alternatives. For each option, we analyze the outranked and outranking alternatives to compute its strength, weakness, and comprehensive quality. We propose four variants that differ in weights assigned to outranking or being outranked by particular alternatives and how to quantify the difficulty or easiness of instantiating such relations. On the other hand, we consider the Decision Makers' holistic judgments, indicating subsets of alternatives deemed as comprehensively strong or weak. We apply the proposed methods to a case study concerning the performance of technological parks in Poland. We also compare the results obtained with the novel approaches with the state-of-the-art ELECTRE methods in an extensive experiment involving simulated decision problems. ",ScoreBin - Scoring Alternatives by Fusing a Crisp Outranking Relation and Indirect Decision Maker's Judgments,"[68153, 68165, 19484]",388,"[25, 135, 90]",2051,Robustness analysis in MCDA  1,44,5,44,Multiple Criteria Decision Analysis,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,20 [building - 324],"['Decision Analysis', 'Stochastic Models', 'OR in Development']",MD-44
"Railway infrastructure maintenance is essential for the continuous operation of train transportation; it is costly and can only be performed in limited time windows. Maintenance companies are contracted to fix segments of railway tracks that fall below a certain quality threshold. We will be concerned with tamping, which is the process of smoothing out track irregularities. 

Due to the short time windows, maintenance companies must choose which segments will be tamped each day. The tamping machines have fast travel times but are slow to tamp with significant starting and stopping times. Therefore, tamping as many contiguous segments as possible is more efficient. Before the end of a shift, the machine must return to the depot.

We use constraint programming to create multi-daily maintenance plans [and re-plans] that maximise the total length of tamped segments. Our test data is based on real-life planning requirements with instances of continuous tracks of approximately 120 KM divided into more than 800 segments of varying sizes from 30 to 400 meters. Our results are promising, and we can quickly find near-optimal solutions using Google OR-Tools. Our model has several non-obvious improvements, such as symmetry breaking and over-approximations to the upper bound, significantly improving solving time.",Using Constraint Programming to Plan Annual Maintenance of Railway Tracks,"[77575, 64121, 77848]",523,"[14, 122, 143]",2053,Optimization in transportation infrastructure design and management,6,5,56,Transportation,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S04 [building - 101],"['Combinatorial Optimization', 'Railway Applications', 'Transportation']",MD-56
"The problem-based workflow is now an established interface making it much easier to model and solve optimization problems in MATLAB. Two new improvements to the problem-based workflow will be presented. First, techniques for enhanced analysis of problems will be outlined, allowing for significant performance gains on certain problems. Next, a new graphical interface for problem-based modeling in MATLAB will be introduced. Usage of the problem-based workflow for industrial applications will be illustrated with an example. Finally, key improvements to the linear, mixed integer and least squares MATLAB optimization solvers will also be shown. ",Recent Modeling and Solver Improvements in MATLAB,[61684],432,"[134, 76]",2054,Modeling tools,76,9,30,Software for Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,53 [building - 208],"['Software', 'Modeling Systems and Languages']",TC-30
"There are several more or less established options for transporting parcels. Classic parcel delivery
is usually done by van, with only a few parcel services available. These parcel services operate in
large areas, e.g., across an entire country. Looking at recent developments, there are services that
only operate in small areas, e.g., a city, and deliver parcels, for example, by bike. In recent research,
delivery per drone or autonomous vehicle is investigated, usually restricted to smaller areas like cities
or city districts. In addition to these different types of delivery services, we want to consider commuters as a resource for parcel transportation. The inclusion of commuters could open up previously
untapped transport capacity potentials. Commuters with car could make their transport capacity
available on the way to work and transport parcels as crowdworkers. If we integrate all these types
of delivery services and crowdworkers into a general delivery network, we can split parcel deliveries among several parties. A single parcel can then be transported by several delivery services or
crowdworkers. We present an algorithm, based on a shortest path approach, that, given a customer's
delivery order, computes a transport from sender to recipient independently of other orders. With
this approach, we can immediately provide the customer with transportation proposals based on
different objectives such as delivery time, emissions, or price.",Parcel delivery planning with multiple parcel services and crowdworkers,"[74122, 5078]",588,"[143, 53, 65]",2055,Crowdsourcing Logistics,6,8,56,Transportation,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S04 [building - 101],"['Transportation', 'Graphs and Networks', 'Logistics']",TB-56
"This paper applies Institutional theory to benchmark sustainability [via Return-To-Scale] and identify key factors enhancing market capitalization and shareholder equity through Panel Data Analysis. This study uses benchmarking analysis- to study the Return to Scale of 16 global automobile firms, specifically utilising Data Envelopment Analysis [DEA-VRS] using MAXDEA software and Panel Data Regression Analysis using STATA software for Years 2019-2022. The findings suggested that Volkswagen [1464.09] has the highest and positive return to scale, followed by Kia [39.19], Ford [19.14], Volvo[13.73], Maruti Suzuki [7.97], Mercedes [4.60], Honda [2.06], Tata Motors [1.83] respectively. On the other hand, Nissan [0.60], Hyundai[0.48], and Toyota [0], respectively, have constant Return to Scale. Furthermore, General Motors [-0.06], Mahindra and Mahindra [-0.13], Isuzu[-0.69], BMW[-0.77], and Stellantis[-1.09]have negative Return To Scale. Panel Data Regression highlights that GHG emissions, Energy Consumption, Waste Generation, Total Assets, R&D, and Operating costs significantly affect Automobile firms' Market Capitalization. Gender diversity is entirely insignificant. GHG emissions, waste generation, social contribution, total assets, and operating costs significantly affect Shareholder equity in the firm. Energy consumption and Lost-Time accidents minimally affect shareholder equity. Water Consumption, Gender Diversity, and R&D investment are insignificant for Shareholder Equity.",Examining the Influence of Sustainable Operational Practices on Corporate Performance - A Study on Automotive Sector,"[77578, 77665]",689,"[139, 24, 94]",2056,Sustainability in Consumer Systems & Industry,80,9,53,Sustainable and Resilient Systems,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,8007 [building - 202],"['Sustainable Development', 'Data Envelopment Analysis', 'OR in Environment and Climate change']",TC-53
"This talk introduces a strategy for detecting representative patterns in a time series of financial data. The proposal can be framed in Functional Data Analysis, where the time series is considered as a sample observed on a discrete time grid of an underlying continuous function. We are interested in studying the dynamics of this time series from the perspectives of temporal evolution and frequency content. To this end, we adopt a wavelet transformation of the raw time series to recover the underlying function. Wavelets provide a multi-resolution representation where the signal is represented as the sum of a coarse signal approximation and a set of multiscale detail coefficients providing information about the temporal data at different frequency levels. This allows highlighting trends and cycles in the time domain, also assessing variance at different frequency levels. Our contribution is the development of a clustering method for Functional Data, in which the time series, represented as a matrix of wavelet coefficients, is navigated to discover interesting patterns. We propose splitting the time domain into non-overlapping frames and clustering the data by a functional k-means. In this approach, each centroid is a representative pattern of time frames. The distance used to compare data is an optimally weighted Euclidean distance giving weights to frequency components. In this sense, we propose a new clustering objective function and an algorithm that allows optimizing it.",Detecting patterns in financial data through time-frequency domain clustering ,[77582],441,"[5, 7]",2058,Models for Financial Data and Risk Management,4,7,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S14 [building - 101],"['Algorithms', 'Analytics and Data Science']",TA-63
"As the consumption of news articles continues to shift towards online platforms, personalized news recommendation engines have become an indispensable asset for news providers seeking to enhance users’ reading experience.
 
However, as the shelf life of news items is limited, traditional collaborative filtering approaches fall short to provide accurate recommendations because they heavily rely on user signals, which may not be readily available for newly introduced news articles. Contextual multi-armed bandits [CMAB] emerge as a promising alternative by leveraging intrinsic characteristics and attributes of both news items and users.
 
While CMAB systems predominantly optimize for clicks, this research argues for a more comprehensive evaluation framework that also promotes the diversity of recommended items, as diversity-aware recommenders are able to respond to news readers’ needs on information variety and to expose users to counter-attitudinal behavior.
 
By conducting a benchmark study on a large-scale proprietary dataset obtained from an international news provider, we demonstrate the feasibility of creating an effective and efficient personalized news article recommendation system based on CMAB models while simultaneously addressing the multifaceted nature of news article recommendation quality. In particular, this study highlights the necessity of considering diversity alongside clicks to enhance user satisfaction and engagement.",Towards effective and efficient personalized ranking in news article recommender systems - balancing clicks and diversity,"[71927, 35404]",356,"[7, 66, 26]",2059,Recommender systems,17,2,31,Analytics,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,54 [building - 208],"['Analytics and Data Science', 'Machine Learning', 'Decision Support Systems']",MA-31
"In this study, we empirically compare the performance of portfolios optimized to minimise selected risk measures under a cardinality constraint. The chosen risk measures include standard deviation, semi-deviation, minimax, mean absolute deviation, maximum drawdown, conditional value at risk, and entropic value at risk. Additionally, we introduce different values of the cardinality constraint and vary the period length used for parameter estimation in the portfolio optimisation problem. This approach allows for a comprehensive comparison of risk-minimization portfolios, taking into account the selected risk measure, cardinality constraint, and period length simultaneously. Furthermore, we evaluate portfolio performance under various market conditions, specifically in the United States, Germany, and Chinese stock markets. Evaluation metrics include the Sharpe ratio, the Sortino ratio, and the Calmar ratio. The results reveal that no single risk measure dominates the others, and significant differences exist among them. Although the durations of optimal estimation periods vary for different risk measures, certain patterns emerge from the findings.",Comparison of risk-minimization portfolios with cardinality constraints and period lengths,"[23683, 77588]",411,"[126, 45, 44]",2060,Market risk in a volatile world,9,3,51,Risk management in finance,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,M5 [building - 101],"['Risk Analysis and Management', 'Financial Modelling', 'Finance and Banking']",MB-51
"Shared mobility-on-demand [SMOD] systems are expected to make public transport more sustainable, especially in rural areas. From the social perspective, this means that an SMOD system should contribute to providing area-wide, reliable, and non-discriminatory access to mobility. In ecological terms, the main aim is to decrease emissions per passenger kilometer. Finally, economic efficiency must also be considered to limit the subsidy requirements. Since providers typically offer customers multiple fulfillment options, i.e., rides with alternative pick-up times, dynamic pricing can be applied to actively steer demand, and thereby, improve the system performance. Operationally, an integrated demand management and vehicle routing problem arises, which features sustainability-based objectives in addition to the usual profit maximization. In this talk, we discuss the challenges associated with dynamic pricing in this multi-objective setting and present algorithmic strategies to tackle them. We evaluate our approaches based on real-world data from a rural SMOD provider.",Sustainability-oriented Dynamic Pricing for Shared Mobility-on-Demand Systems,"[67944, 72928, 14031, 16305]",587,"[119, 124, 145]",2061,Demand-responsive public transport 2,85,14,54,Public Transport Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S01 [building - 101],"['Public Local Transportation Systems', 'Revenue Management and Pricing', 'Vehicle Routing']",WC-54
"Domain-independent dynamic programming [DIDP] is a recently proposed model-based paradigm for combinatorial optimization based on dynamic programming [DP]. In DIDP, similar to mathematical programming, a user formulates a combinatorial optimization problem as a declarative DP model and then solves the formulated model by calling a general-purpose solver. The currently developed DIDP solvers are based on heuristic search algorithms studied in the artificial intelligence community. In this work, we develop parallel DIDP solvers using parallel heuristic search algorithms. First, we empirically confirm that the multi-thread versions of our parallel DIDP solvers outperform commercial multi-thread mixed-integer programming and constraint programming solvers in multiple combinatorial optimization problems. Then, we develop distributed parallel DIDP solvers using the Message Passing Interface. Our experimental evaluation demonstrates the high scalability of the parallel DIDP solvers in a massively parallel distributed environment.",Parallel General-Purpose Dynamic Programming Solvers for Combinatorial Optimization,"[77416, 29543, 54367]",191,"[102, 108, 14]",2062,Parallel Solvers,76,7,30,Software for Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,53 [building - 208],"['Parallel Algorithms and Implementation', 'Programming, Dynamic', 'Combinatorial Optimization']",TA-30
Just-in-Time sailing is a recently introduced concept by the IMO to avoid ships waiting before terminals. In this talk we investigate to which extent this concept saves money as well as CO2 emissions. First we consider the case without arrival slots and investigate how much slowing down saves. Secondly we consider the case that ships also have information about the ships behind them. Finally we consider the case where terminals define arrival slots at a tactical level with penalties for late arriving. We consider several strategies for ships and investigate the case with two shipping lines with different arrival reliability.,Just-in-Time sailing for container ships,[1752],670,"[70, 123]",2063,Seaside Planning II,52,5,62,OR in Port Operations,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S12 [building - 101],"['Maritime applications', 'Reliability']",MD-62
"It is a common practice that an investor makes a decision based on not only in-sample efficiency, but also takes into account out-of-sample performance of the portfolios. To combine in-sample efficiency with out-of-sample optimality, one can employ a bilevel stochastic optimization problem. In our paper, the lower level of the problem consists of the mean-risk optimization model, where Conditional Value-at-Risk as a frequently used risk measure is applied. The upper level searches for in-sample efficient [lower level] portfolios with maximal out-of-sample performance [out-of-sample return].

The reformulation of the problem assuming a discrete probability distribution yields to a linear bilevel optimization problem. Additional assumption can be made about variables in the upper level. In a moving window analysis, these variables can vary or remain the same for all windows. Therefore, two linear bilevel optimization problems with multiple followers are considered. One of them can be split in time, the other one not. The results for these two cases will be presented. ",Bilevel models in portfolio selection problems,"[77587, 12024]",475,"[83, 117]",2064,Portfolio optimization ,49,9,34,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,43 [building - 303A],"['Optimization in Financial Mathematics', 'Programming, Stochastic']",TC-34
"Improving menstrual hygiene and reproductive health is one of the United Nations’ Sustainable Development Goals. Intervention programs target girls in developing countries and provide them with knowledge and material to manage menstruation. This paper considers the cost-effectiveness of such programs with a case study from a rural area in Madhya Pradesh, India. A sequence of maximal-covering location problems [MCLPs] was solved to select a set of villages as optimal locations to conduct multiple interventions in the study area. Pre- and Post-surveys with participants are used to estimate the proportion of girls who adopted safe and hygienic menstrual practices. Interviews with 528 adolescent girls from nearby villages are employed in a diffusion model to estimate the indirect reach of the intervention across the neighboring villages. Health benefits are measured in terms of Disability-Adjusted Life Years [DALYs]. The cost-effectiveness ratio of a menstruation intervention is estimated to be about $28.5 per DALY. These programs also provide additional benefits such as an increase in awareness, a reduction in school absenteeism, and an increase in the self-confidence of girls.",Cost-effectiveness of a menstrual hygiene program in rural India,"[77205, 77215, 77595, 77598, 77599, 77596, 59078]",54,"[56, 43, 28]",2066,OR for Medical Services in Developing Countries,67,12,18,OR for Development and Developing Countries,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,42 [building - 116],"['Health Care', 'Facilities Planning and Design', 'Developing Countries']",WA-18
"In recent years, the share of electric vehicles [EVs] for freight transport increased. Transportation companies must create daily routes for these vehicles by solving the electric vehicle routing problem [EVRP]. This is mostly done in the same manner as for traditional internal combustion vehicles but aspects regarding range and recharging needs are added to the problem. However, dealing with EVs brings up a new problem - The Charging Scheduling Problem [CSP]. EVs have not only to recharge along the route but also at their depot after operating hours. This can be done either by simple plug-and-charge, manual timing, or an optimized schedule. An important aspect of flexible recharging solutions is the connection time of the EV to the charging device. Therefore, the schedule of an EV over the day [timeline] is important for the CSP. However, most EVRP consider electricity prices as a constant fuel cost and do not consider the volatility of electricity prices. We address this problem and show how to combine the EVRP with the CSP.",Effects of time-of-use electricity prices on electric vehicle schedules,[75307],624,"[36, 72, 145]",2068,Advancing mobility towards sustainable solutions III,6,12,56,Transportation,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S04 [building - 101],"['Electricity Markets', 'Mathematical Programming', 'Vehicle Routing']",WA-56
"The transportation sector, encompassing road [the primary source], sea, and air transport, accounts for 14% of global emissions over the last decade and around 25% of total greenhouse gas emissions in the European Union. This underscores the imperative for companies to prioritize transitioning to cleaner and greener modes of transport in their medium- to long-term vehicle investment plans. In this work, we address an original vehicle replacement problem with environmental concerns. A keep-or-replace decision must be made for every asset of the fleet in every time period while meeting budget constraints to minimize total discounted costs and CO2 emissions. Multiple options for replacement are given for every family of assets in such a way that fuel-based vehicles may be replaced by, for instance, electric and hybrid vehicles. The problem is inspired by a real-world industrial application in Italy and, because of its generality, can be used to model a large variety of further applications. We provide a dynamic programming model and integer programming formulations for the problem. Then, we compare the formulations and propose different bi-objective approaches to solve the problem in practice.
",Bi-objective Algorithms for a Vehicle Replacement Problem including Budget Constraints and CO2 Emissions Minimization,"[69845, 79662, 7965, 68090, 62595]",871,"[14, 112, 143]",2070,Exact Algorithms and Formulations for Combinatorial Optimization Problems,64,10,29,Combinatorial Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,157 [building - 208],"['Combinatorial Optimization', 'Programming, Multi-Objective', 'Transportation']",TD-29
"In recent years, stochastic integral equations of Volterra type have been employed as modelling tools for financial, economic, and demographic problems. In such a perspective, it is well known that a nonlinear stochastic Ito–Volterra integral equation is well-known model widely used simulating stock and electricity prices. Unfortunately, these kinds of equations do not allow closed-form solutions, but require the use of numerical methods to obtain suitable approximations. In this paper, we present a feasible, fast, and accurate numerical algorithm to evaluate the unknown solution of such equations. We mainly exploit the Lipschitz condition, which needs to be assumed in order to ensure the existence and uniqueness of the solution of this kind of equations, and a particular version of the mean value theorem for stochastic integrals. We also provide an extensive convergence analysis for our proposal.
",Stochastic Ito-Volterra integral equations arising in finance - a streamlined numerical approach,"[77542, 71017]",798,"[5, 135]",2071,Financial Modelling,50,14,39,Stochastic Modelling,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,35 [building - 306],"['Algorithms', 'Stochastic Models']",WC-39
"An M-decomposable set is a closed convex set which is the sum of a compact convex set and a closed convex cone. We present properties of these sets in the context of integer programming. In particular, we present properties of their integer hull, some cutting plane closures and subadditive duality.",M-Decomposable Sets in Integer Programming,[77597],194,"[109, 113]",2072,Recent Advances in MINLP,86,9,04,MINLP,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,1001 [building - 202],"['Programming, Integer', 'Programming, Nonlinear']",TC-04
"This article explores Critical Success Factors [CSFs] in project management, focusing on key elements for project success. By analyzing common CSFs such as clear and realistic project objectives and strategies, top management support and commitment, effective project planning and scheduling, stakeholder engagement, effective communication, risk management, optimal resource allocation, a competent and motivated team with clear roles and responsibilities, adaptability to change, and leadership, alongside more specific factors, the research aims to deepen the understanding of the factors that drive project success. Through a comprehensive literature review and utilization of Multi-Attribute Decision Making [MADM] methods, the study provides valuable insights to enhance project outcomes and achieve organizational goals. By ranking and prioritizing these critical factors based on input from experienced project managers and practitioners in the field, using expert judgment and the Project Management Body of Knowledge [PMBOK] guide, the research offers practical recommendations to enhance project performance and ensure successful project delivery. This effort aims to bridge the gap between project management theory and practice, ultimately leading to more effective project results.",Exploring and Prioritizing Critical Success Factors [CSFs] in Project Management Utilizing MADM approach and PMBOK guide,"[58071, 58059]",961,"[118, 129, 22]",2073,Project Management,35,7,60,Project Management and Scheduling,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S09 [building - 101],"['Project Management and Scheduling', 'Scheduling', 'Critical Decision Making']",TA-60
"For several decades, electrical networks have experienced continuous expansion in size and scale, facilitating the transmission of electrical power from sources to end-users. The growing complexity of these networks necessitates the use of tools like Load Flow calculations, which are crucial for power systems engineering, playing a critical role in ensuring grid reliability and efficiency. This numerical analysis of electric power within interconnected systems determines steady-state voltages, currents, and power flows across network components. Given the significance of LF calculations, the power systems community has invested substantial efforts in enhancing the convergence of the methods used to solve them. Despite these efforts, real-world scenarios often involve imperfect or erroneous network parameters, leading to challenging convergence issues for existing methods. The physical problem studied may be mathematically infeasible due to complex inconsistencies in the parameters. This research project explores a novel approach centered around Mixed-Integer Non-Linear Programming to detect inconsistencies or errors in the parameters of a power network. The study extends with the implementation of an open-source tool using the AMPL modeling language, and conducts a solution to the problem using the commercial solver Artelys Knitro. Finally, the tool has been tested to ensure that it successfully identifies inconsistencies in a real-world use case within an acceptable time.",Mixed-integer non-linear programming approach for identifying  parameters inconsistencies in load flow calculations,"[77150, 77710]",842,"[93, 72, 14]",2074,OR in Energy,23,13,19,OR in Energy,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,44 [building - 116],"['OR in Energy', 'Mathematical Programming', 'Combinatorial Optimization']",WB-19
"We introduce a novel Java framework for Evolutionary Computation and multiple-criteria Decision Making named JECDM. It predominantly focuses on facilitating research on a joint area of preference-driven evolutionary multi-objective optimization. From the programmer's perspective, the main features of JECDM are as follows. Firstly, we were focused on efficiency during the development to guarantee a low computational burden imposed when executing the code. Second, the code is highly object-oriented, facilitating readability, reusability, modularity, and low code redundancy. Thirdly, we reduced the use of external libraries to a bare minimum, securing the framework's self-sufficiency. From the practitioner's viewpoint, our framework provides a vast spectrum of well-organized functionalities. They are categorized into several top-level modules dedicated to evolutionary computation, multiple-criteria decision analysis, visualization, and experimental validation. The framework is available online and free to use for academic purposes. We explain how to operate it in a series of extensive tutorials provided as PDF files accompanied by code examples that are published along with the framework. These tutorials, e.g., discuss the general structurization of the framework's components, show how to implement new algorithms by oneself or demonstrate how to design experiments even of an extensive nature without much effort spent. 
",Java Framework for Evolutionary Computation and Multiple-Criteria Decision Analysis,"[52756, 19484]",888,"[134, 26, 77]",2077,Preference Learning 2,44,3,44,Multiple Criteria Decision Analysis,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,20 [building - 324],"['Software', 'Decision Support Systems', 'Multi-Objective Decision Making']",MB-44
"The joint schedule and volume optimization aims at creating an optimal truck schedule with a corresponding volume plan that can satisfy the demand of the network in a speedy manner and with minimum cost. The speed of delivery is related to the path that a package follows from an origin up to a final destination node, while the cost is proportional to the number of trucks that we schedule in order to move all the required packages. The problem has many operational and practical constraints that make it challenging. For example, all sites have inbound and outbound restrictions and capacity constraints, the packages are consolidated in intermediate nodes [sort centers] in various ways, there are driving bans that the schedule needs to comply with, etc. The problem is formulated as an MILP, to which we apply a set of techniques to reduce complexity such as smart pruning of decision variables and solving sequentially the problem to incrementally finer granularities. Experimental results on large-scale networks illustrate the effectiveness and scalability of our method.",Joint schedule and volume optimization at scale,"[77307, 77603, 77605]",290,"[129, 111, 151]",2079,Scheduling and sustainability,92,13,57,Optimization at Amazon,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S06 [building - 101],"['Scheduling', 'Programming, Mixed-Integer', 'Practice of OR']",WB-57
"This study aims to investigate sustainable urban transportation in the Asprela Campus at Porto. This area was chosen because three universities and a hospital are located there, and also attracts a variety of travelers such as students, workers, and visitors. This allows the study to explore the different preferences and perceptions that shape urban transportation. The study analyzes the choices of urban transportation, such as public transit, micro-mobility, ride-hailing, and car-sharing, made by individuals aged 18 years or older who commute to the area for various purposes, such as work, study, hospital visits, and others. To gather the data, the survey was developed in three parts, the initial part asks how the importance of the factors affecting transportation mode choices, then asks about the specific modes used to access the area, and finally asks socio-demographic information. The survey’s novelty is asking about factors affecting urban travel choices in the first part and examining actual behaviors in the second part, it gives a comprehensive view of urban transportation choices, closing the gap between preferences and real-world actions. The study’s results can help mobility companies and policymakers improve multi-modal transportation and sustainable mobility. It also finds key factors for choosing urban transportation modes, which can assist in developing enhanced demand models for urban mobility design and strategic decisions.",How transport mode choices are influenced and what they mean for sustainable mobility - a data-driven study of the Asprela University Campus,"[77604, 35712, 78553, 78554, 1999, 663]",514,"[143, 139]",2080,"Advancements of OR-analytics in statistics, machine learning and data science 5",16,8,28,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,065 [building - 208],"['Transportation', 'Sustainable Development']",TB-28
"A complete retail supply chain consists of suppliers, retailers, and customers. Retailers generally bear greater risks, such as how to choose suppliers and how many suppliers to choose. Portfolio theory seeks to optimize portfolio structure and maximize returns. This project is based on the perspective of the retailer and applies the Portfolio Theory to the decision-making of the retailer. The aim of this project is to select an order structure and financing method for the retailer that match the expected revenue based on mean-variance theory. For this purpose, this project collected data on a liquor retailer, basically classifying suppliers into two categories, which include five products, and finding out how the retailer should build orders to achieve the maximum expected revenue. In addition, considering the situation that the retailer has inventories and has difficulty financing, this project uses the classical newsvendor model to optimize the management of the inventory of the retailer and gives financing suggestions based on the results. Conclusively, find out the supplier portfolio under the modified portfolio theory and give private suggestions.",Decision-making of Liquor Retailer based on Modified Portfolio Theory,[76683],413,"[25, 138, 61]",2085,Portfolio risk management,9,5,51,Risk management in finance,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,M5 [building - 101],"['Decision Analysis', 'Supply Chain Management', 'Inventory']",MD-51
"The project selected ten listed commercial banks as the research object, and based on the financial data of the sample banks in 2022, the KMV model was used to measure the credit risk of the banks. The aim of this project will be to test the applicability of the KMV model in the measurement of the credit risk of commercial banks in China. First, the volatility of stock prices, the value of bank assets, and their volatility are calculated according to the Black-Scholes option pricing model. Then the KMV model is applied to calculate the default distance of each commercial bank. Finally, the expected default frequency is obtained. The expected default frequency obtained is compared with the standard range of Standard & Poor's and Moody's ratings. The final results show that the expected default frequency of the banks calculated using the KMV model generally matches the credit ratings of the banks by the credit rating agencies. The higher the expected default frequency, the lower the credit rating. Lastly, relevant suggestions are made for commercial banks to improve credit quality and strengthen the level of supervision.",Based on the applicability of the KMV model in Chinese commercial banks,[76703],413,"[126, 44]",2086,Portfolio risk management,9,5,51,Risk management in finance,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,M5 [building - 101],"['Risk Analysis and Management', 'Finance and Banking']",MD-51
"In this work, we present a proof-of-concept investigation of Autonomous Modular Public Transit [AMPT] at a network scale and compare it with the traditional fixed-route, fixed-vehicle size transit service in terms of total cost, which consists of both agency’s capital and operational cost [including energy cost] and passenger time cost. We formulate and solve stylized design models for AMPT on a grid network in a range of demand density scenarios with both homogenous and heterogeneous distributions.  The AMPT models explicitly account for pod joining and disjoining [and therefore en-route transfers of passengers] and potential energy savings due to pod train formation [pod platooning], which represent major departures from the traditional transit models in the literature.  Numerical results find that AMPT, if designed properly, may save the total cost compared to traditional transit systems thanks to demand responsive pod train capacity, particularly in the low demand scenarios.  The cost savings of AMPT are largely attributed to passenger time saving by en-route transfer; the agency cost of AMPT has a mixed picture.  The load factor of AMPT generally improves over the traditional transit service.  We also show how key parameter values may affect the AMPT costs through sensitivity analysis.",A Fixed-route Autonomous Modular Public Transit Service,[42266],333,"[143, 119, 72]",2087,Public transportation ,6,13,55,Transportation,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S02 [building - 101],"['Transportation', 'Public Local Transportation Systems', 'Mathematical Programming']",WB-55
"Electric vehicles [EVs] emerge as a promising alternative to fossil fuel-powered vehicles, noted for their zero emissions.  As more EVs take to the roads, managing their interaction with road transportation and power distribution systems becomes crucial to meet electrical demand. This study presents a new multistage model that integrates EV delivery routing with energy charging management. The model considers the constraints of EVs and parcel delivery in the road transportation system, while the power distribution system operation is modeled via a linearized AC power flow model including generators and conventional demand. This work considers an EV demand aggregator, whose aim is to find the best strategy in terms of costs and benefits by adjusting EV routing via incentives for availability time and charging EVs at points with the lowest locational marginal prices [LMPs]. The model unfolds through three main iteratively interconnected stages. First, delivery allocation and detailed routing are tackled to minimize traveled distances and operational costs. Then, an iterative linkage between the road transportation and energy distribution systems is performed. Finally, a bi-level model is proposed to optimize the charging strategy by the EV demand aggregator in the power distribution system, minimizing costs based on LMPs. Numerical results validate the proposed model with a case study that includes 3,000 EVs within a 284-intersection map and a 119-bus power distribution system.",An Iterative Multistage Model for Electric Vehicle Routing and Charging Management in Road Transportation and Power Distribution Systems,"[71074, 27748, 71075, 77613, 77612]",802,"[93, 143]",2090,Electric Vehicles within Electric Power Systems,23,5,19,OR in Energy,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,44 [building - 116],"['OR in Energy', 'Transportation']",MD-19
"The problem of harvest scheduling consists in determining the optimal planning solution selecting from different stands in a forest in a multiperiodal setting. The objective is to maximize the total revenues under some economical and regulatory constraints. This problem involves both financial and environmental considerations. In the latter the main concern is the one of the conservation of wildlife and natural habitats and it results in a restriction on the maximum contiguous area of clearcut regions in a forest. One approach to address this problem is the area restriction model [ARM] an Integer Linear Programming model in which decisions variables are binary variables prescribing the cut of a specific stand in a specific period. In particular the Path formulation of the ARM is an efficient way to manage the maximum clearcut area constraint. In this work an extension of this formulation is presented in which the planner is provided with an additional option to schedule the harvesting. The consideration of some special areas between each pairs of stands in the forest, called buffers, allows for considerable improvements in the optimal value of the objective function and makes the resulting Mixed Integer Linear Programming model more flexible. In fact the introduction of buffers often allows for feasible solutions in scenarios in which the problem would be otherwise unfeasible. ",An extension of the Area Restriction Model ,"[77570, 66630]",5,"[48, 89, 14]",2092,OR in Forestry I,20,3,12,OR in Agriculture and Forestry ,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,13 [building - 116],"['Forestry Management', 'OR in Agriculture', 'Combinatorial Optimization']",MB-12
"Laser scanning technology has facilitated advanced analysis of complex outdoor environments by capturing 3D point clouds. Identifying points within a point cloud corresponding to individual objects of a particular class, semantic instance segmentation is of great relevance in numerous fields. In the context of forest point clouds, in particular, semantic instance segmentation proves valuable for forestry applications such as tree inventorization and infrastructure contexts such as transmission line corridor surveys, where the positioning of trees is used for safety considerations. While recent developments in deep learning algorithms for point cloud processing have yielded solutions for semantic instance segmentation in terrestrial laser scans of forests, applying such methods to lower spatial resolution aerial laser scans still needs to be explored. Compared to their terrestrial counterparts, aerial laser scans present an attractive prospect for large-scale forest analysis. This paper introduces a benchmark dataset for forest semantic instance segmentation created from publicly available aerial laser scans. The dataset is an evaluation platform for standard geometry-based classical tree instance segmentation algorithms. Additionally, it is used to evaluate a deep learning model for tree instance segmentation. A comparison between the different methods is made in the context of aerial laser scans of forests.",Geometry and deep learning-based methods comparison for segmentation of trees in aerial laser scan point clouds,"[70981, 77201]",516,"[8, 66, 18]",2094,"Advancements of OR-analytics in statistics, machine learning and data science 6",16,9,28,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,065 [building - 208],"['Artificial Intelligence', 'Machine Learning', 'Computer Science/Applications']",TC-28
"Mobility as a Service represents a topical theme in the current research and innovation ecosystem thanks to its potential disruptive impacts on transportation systems.
The current literature shows a lack of tools and examples which could provide public authorities [or other relevant stakeholders] with useful decision-making support to quantitatively evaluate the expected effects of the introduction of MaaS packages on the transportation system, thus conceiving and planning them most properly. Many works in the literature evaluate MaaS effects by means of agent-based simulation tools which miss the systemic and strategic perspective typical of the planning stage. Other contributions fully neglect the planning task, by evaluating the effects of MaaS services as empirical evidence from pilot programs and field trials.
Therefore, this contribution proposes a decision support system for estimating the effects of the introduction of MaaS services on different stakeholders [i.e., MaaS users, other transport systems customers, transport operators, etc.] based on the aggregated representation of demand [in a four-step approach] and supply systems in the presence of MaaS. As a first attempt, the proposed approach has been tested on a toy network to show an example of the quantitative evaluation of the adherence of different MaaS packages to the territorial contexts and governance goals pursued.
The proposed contribution appears to be unique in the current research literature.",Planning and managing MaaS services,"[62183, 77621, 77619, 77622, 77620]",332,"[143, 26, 131]",2095,MOST - MaaS & Innovative Services for Sustainable Mobility,6,12,55,Transportation,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S02 [building - 101],"['Transportation', 'Decision Support Systems', 'Simulation']",WA-55
"Multiple Criteria Decision Analysis [MCDA] methods are widely used in various disciplines as a logical solution to a complex decision-making problem. The majority of the research to date has been focused on the development of new MCDA methods for different decision-making challenges. However, given this large number of potential solutions, there is an accessibility challenge in identifying an appropriate method, especially for people who are unfamiliar with them. There has been work which proposed a framework to help select a MCDA method, but the challenge with this is that it requires an understanding of MCDA terminology.

We propose a decision support system to solve such a challenge, utilising artificial intelligence in the form of a large language model [LLM] as an interface between the decision-maker and a framework for the selection of MCDA methods from a pool of 56 potential options. The decision-maker would briefly describe their decision-making problem in a natural language format which would then undergo a LLM reasoning process to extract the necessary information and align them with the choice framework model. It will identify and interact with decision-makers to provide and justify a method that is most appropriate to their problem.
",Choice and the Machine - overcoming the challenge in the selection of multiple criteria decision analysis method through the use of large language model.,"[76083, 46127]",477,"[26, 8, 77]",2096,Multiple-Criteria Decision Support,45,7,45,Decision Support Systems,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,30 [building - 324],"['Decision Support Systems', 'Artificial Intelligence', 'Multi-Objective Decision Making']",TA-45
"This paper investigates the impact of customer withdrawal on retail food waste. Customer picking for the freshest items [called last-in-first withdrawal] leaves products with shorter expiration dates on the shelves. However, currently applied barcodes do not include the expiration date. We partnered with a retailer to collect missing expiration data. We implemented a process adaption in a pilot warehouse and develop a novel method to quantify waste caused by customer picking in panel data. Our findings show that, on average, 45% of the food waste in our sample is caused by customer picking. Building on the results of the customer picking waste quantification, an exploratory approach is applied to identify product and store-related drivers. ",The Impact of Customer Picking on Retail Food Waste,"[22691, 61830, 77618, 41788]",424,"[61, 100, 130]",2097,Food Waste,30,2,50,Retail Operations,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,M2 [building - 101],"['Inventory', 'OR in Sustainability', 'Service Operations']",MA-50
"The transition of energy infrastructure towards climate neutrality poses significant challenges for infrastructure planners and policymakers. Large-scale energy system models play a fundamental role in evaluating alternative scenarios for future development and enabling informed decision making. However, modeling renewable energy sources and flexibility options requires high spatial and temporal resolution, resulting in some of the largest linear optimization problems tackled.

Various approaches have been developed to deal with this complexity, including heuristics, mathematical decomposition, and complexity reduction. This study combines the REMix framework with the parallel solver PIPS-IPM++ to compute transformation paths for the European energy system. This combination can provide insights into the ideal timing for converting natural gas infrastructure to hydrogen and explore the trade-off between investment rates and the remaining carbon budget.

First, this talk addresses the computational challenges of solving large linear optimization problems with parallel solvers and discusses the tuning of the decomposition approach to exploit the block structure. Second, it presents insights and key findings on the cost-optimal transformation path of the European energy infrastructure towards climate neutrality. These findings include spatial allocation of capacity and storage, as well as robust network topologies for both power grids and gas pipelines.",Computing transformation pathways towards a climate neutral European energy system with high spatial and temporal resolution,"[54534, 43524, 72639]",947,"[12, 37, 18]",2101,Pathways to Climate Resilience,80,12,53,Sustainable and Resilient Systems,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,8007 [building - 202],"['Capacity Planning', 'Energy Policy and Planning', 'Computer Science/Applications']",WA-53
"In the rapidly evolving field of machine learning, the increasing complexity of models necessitates larger datasets and greater computational resources. This demand for scalability and enhanced computational capabilities has driven the study of distributed optimization to the forefront. Byzantine fault-tolerance [BFT] consensus is a fundamental building block of distributed systems. BFT is the property of a system that can resist the class of failures which can continue operating even if some of the nodes fail or act maliciously, thanks to the capacity to prevent the no more than one third adversaries from gaining a consensus and the mechanism of view changes for replacing faulty nodes.
Our research introduces a novel mathematical programming framework aimed at optimizing the performance of BFT algorithms. Our approach is twofold - firstly, we optimize node network distribution strategies to maximize throughput; secondly, we investigate mechanisms for selecting backup nodes to replace the leader during view change phases when failure occur. To validate our framework, we conducted experiments on a real-world testbed. The results demonstrate significant improvements in BFT algorithm performance, showcasing our method's potential in optimizing throughput and ensuring robust fault tolerance during view changes. This study contributes to the development of more resilient and efficient distributed systems, supporting the growing demands of machine learning applications.",A framework for modelling view change in Byzantine fault-tolerance network,"[77267, 63918, 78893, 55657, 78831]",516,"[66, 84, 18]",2110,"Advancements of OR-analytics in statistics, machine learning and data science 6",16,9,28,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,065 [building - 208],"['Machine Learning', 'Optimization Modeling', 'Computer Science/Applications']",TC-28
"This paper aims at providing insights on the ability of systems of commodities to absorb external microscopic shocks. Such ability is labelled as systemic resilience. Commodities are suitably evaluated along with their interconnections, on the basis of the similarities of their financial performances.
The resilience is measured through the comparison of the shocked networks with the original unshocked ones. The employed device is the clustering coefficient, which is a nodal centrality measure associated with the community structure of the network. The proposed methodology is empirically tested over a large set of commodities of different nature.",Systemic resilience of networked commodities,[7142],438,"[79, 0]",2111,Risk Management in Private and Public Finance,4,13,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S14 [building - 101],['Network Design'],WB-63
"Newly introduced price subsidy programs without physical procurement have resulted in an increased [vis-à-vis the subsidy programs with physical procurement] preponderance of deliberate quality degradation for certain crops by Indian farmers. The physical procurement of the crop from farmers provides an alternate sales channel to farmers enabling the government to subdue the competition between farmers in the open market. The farmers' deliberate quality degradation may moderate the benefits of the altered competitive structure. The viability of such programs has been questioned, and policymakers are looking for guidance. Using a multi-stage incomplete information-based [Bayesian] game-theoretic model, we comprehensively characterize the farmers’ strategic production and selling decisions. We demonstrate that the government price support with multiple sales channels created by physically procuring the crop improves the producer surplus when the farmers' landholdings are highly disparate and the quality-based competition is lower. A higher minimum support price [MSP], a higher landholding for a farmer producing a high-quality crop, and a lower high-quality crop price premium in the open market improve the consumer surplus, the social surplus, and the average quality of the crops supplied by farmers when the government does not procure the crop physically v/s when it does. We provide guidance to the government exercising caution in selecting MSP and farmer subsidy program.","Price Subsidies with or without Physical Procurement - Impact on Quality, Profits, and Welfare","[77630, 41742, 77632]",537,"[10, 28, 50]",2113,Agri-Food Supply Chains,19,9,24,Sustainable Supply Chains,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,83 [building - 116],"['Behavioural OR', 'Developing Countries', 'Game Theory']",TC-24
"This study introduces an innovative ensemble framework that leverages Explainable Artificial Intelligence [XAI] to enhance feature selection processes in machine learning [ML] models. The primary aim is to improve the interpretability and predictive performance of these models. Our approach integrates multiple ensemble learning algorithms, including Gradient Boosting Machines and Random Forests, with XAI techniques like SHAP to create a transparent, interpretable model. We evaluate our framework on various datasets, demonstrating its ability to maintain high predictive accuracy while providing insights into the feature selection process. The results indicate that our ensemble framework significantly enhances model transparency and interpretability without compromising on performance.",A Novel Ensemble Framework for XAI-Based Feature Selection in Machine Learning Models,"[11028, 77631]",801,"[66, 0]",2115,Recent Methodologies in Explainable AI [XAI] 2,71,3,04,Recent Advancements in AI ,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,1001 [building - 202],['Machine Learning'],MB-04
"Scenarios are stories about alternative possible futures which capture key uncertainties.  A key concept in the development and presentation of scenarios is that of plausibility.  This paper is organised into three sections.  First, we offer reflections on how plausibility has been defined and operationalised within the literature.  Second, we reflect on experiences of students working with scenarios and how they have explored the plausibility of scenarios developed as part of an assessed participative exercise.  Third, we reflect on how the concept was operationalised in a recent application of scenario planning that supported strategic conversations amongst a group of museums during the Covid19 pandemic.  We end by drawing together our reflections and comment on their relevance for the practice of scenario development.",Exploring the concept of plausibility in the development of future scenarios,"[9010, 53756, 77633]",569,"[137, 133]",2116,Scenarios and foresight practices - Behavioural issues II,13,13,11,Behavioural OR,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,12 [building - 116],"['Strategic Planning and Management', 'Soft OR']",WB-11
"Bilevel optimization deals with decision problems in which two decision-makers, the leader and the follower, control different sets of variables and have their own objective functions subject to interdependent constraints. A lower-level optimization problem is embedded in an upper-level problem. When the lower-level problem has multiple objective functions, the leader should cope with the uncertainty related to the follower’s reaction. The leader can adopt a more optimistic or more pessimistic stance regarding the follower’s choice within his efficient region, which is restricted by the leader’s choices. The leader may also have multiple objective functions. 
The concepts of optimism and pessimism are well established when the leader has a single objective function. However, this is not the case when there are multiple objective functions at both levels. Most approaches aim to calculate the optimistic Pareto frontier [efficient solutions for the leader, considering that the follower's choices are always the best for the leader]. However, this approach is seldom realistic, and the definition of the pessimistic Pareto frontier may not be consensual.
We propose a definition of pessimistic Pareto front for bilevel optimization problems with multiple objective functions at both levels, which is compared with the optimistic Pareto front. These new concepts are illustrated using examples, emphasizing the difficulties associated with the computation of those solutions.",What is the meaning of pessimistic Pareto front in multiobjective bilevel problems?,"[123, 12412]",602,"[77, 72]",2119,Theory of Multiobjective Optimization,34,14,37,Multiobjective Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,33 [building - 306],"['Multi-Objective Decision Making', 'Mathematical Programming']",WC-37
"Medium-term airline schedule planning is traditionally tackled in a sequence of steps, including route selection, frequency planning, timetable design, fleet assignment, aircraft routing, crew scheduling, and tail assignment. Optimising each step separately is globally sub-optimal, but is often the only viable strategy because of the computational difficulty of solving the joint problem. This paper focuses on simultaneously optimising medium-term strategic decisions related to frequency planning, timetable design, and fleet assignment, as well as dealing with limited changes to the route selection process. Two conditions make this joint optimisation possible. First, we focus on the special case of regional airlines that work as feeder for larger mainline carriers. Second, we exploit a tight exponential-size formulation; we solve its continuous relaxation via column generation and obtain integer solutions via a restricted master heuristics. In this way, we obtain gaps consistently under 0.2\%. An important characteristic of our approach is that it incorporates explicit passenger preferences via discrete choice modelling. Preliminary results on the network of Mesa Airlines and Republic Airways show a significant potential for increased profits and load factors. The main reason is a better synchronisation of feeder flights allowing for more connections and, thus, capturing a larger share of passengers whose itinerary involve two flights with a layover at the hub.",Integrated Schedule Planning for Regional Airlines Using Column Generation,"[49677, 29068]",874,"[4, 13]",2120,Applications of combinatorial optimization II,64,8,25,Combinatorial Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,011 [building - 208],"['Airline Applications', 'Column Generation']",TB-25
"We propose a provably convergent algorithm for approximating Wasserstein barycenter of continuous and non-parametric measures. Our algorithm is inspired by the fixed-point scheme of Álvarez-Esteban, del Barrio, Cuesta-Albertos, and Matrán [2016], which begins with an initial measure and iteratively updates it via its pushforward by the mean of optimal transport [OT] maps to the input measures. However, the convergence of their scheme relies on obtaining exact OT maps which is computationally intractable for general non-parametric measures. Hence, we develop consistent estimators of OT maps via shape-constrained least squares regression, and replace the OT maps in the fixed-point scheme with their estimated counterparts. This gives rise to a stochastic fixed-point algorithm which is provably convergent to the true barycenter. Specifically, in order to obtain consistent OT map estimators, we develop tailored approximation techniques for preserving the regularity of both the true OT maps and the estimated OT maps throughout the iterations. Moreover, our algorithm does not restrict the support of the barycenter and can be implemented in a distributed computing environment. These properties make it suitable for large-scale information aggregation problems, e.g., multi-expert consensus in recommender systems and distributed sensor networks. In our numerical experiment, we showcase the practicality and efficiency of our algorithm for expert consensus on probabilistic forecasting.",Provably convergent algorithm for free-support Wasserstein barycenter of continuous non-parametric measures,"[77637, 77646, 77645]",323,"[5, 51, 72]",2121,Data science meets strongly NP-Hard CO ,14,10,03,Data Science Meets Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,1005 [building - 202],"['Algorithms', 'Generalized Convex Optimization', 'Mathematical Programming']",TD-03
"This study is based on a decision scenario of an industrial gas company that applies the Vendor Managed Inventory [VMI] model and supplies customers with liquid oxygen using a self-operated heterogeneous vehicle fleet. We name it a Joint Replenishment and Heterogeneous Vehicle Routing Problem with Cyclical Schedule. To solve this problem, we formulated it as a non-linear mixed-integer programming model that simultaneously determines the length of the planning cycle [PC], the length of the replenishment cycle, the dates to replenish each customer, and the vehicle routes of each day within PC, such that the average total cost within PC is minimized where the total cost includes inventory holding cost, setup cost, transportation cost, and overtime labor cost. We proposed a genetic algorithm [GA], which is embedded with an innovative encoding and decoding mechanism, local search operators, and a hash function to avoid repetitive fitness evaluation for identical solutions. Our numerical experiments demonstrate that the proposed GA can effectively solve the problem under different lengths of PC and number of customers. Also, when the demand of a customer increases, it can be applied to determine whether the company should expand the capacity of storage installed at the customer. Sensitivity analysis of the vehicle fleet composition showed that deploying a mixed fleet can reduce the average total cost.",Joint Replenishment and Heterogeneous Vehicle Routing Problem with Cyclical Schedule,"[77448, 64768, 77640]",803,"[65, 145, 74]",2124,Lot-sizing with joint replenishment and routing decisions,32,2,49,"Lot Sizing, Lot Scheduling and Production Planning","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,M1 [building - 101],"['Logistics', 'Vehicle Routing', 'Metaheuristics']",MA-49
"Disasters are increasingly affecting human lives with greater frequency and severity over the years. Depending on the forecasting capabilities, it is possible to anticipate and mitigate the impacts of certain events. Anticipatory actions, also known as forecast-based early actions, play a crucial role in this regard by allowing humanitarian actors to take proactive measures before the disasters strike. This study focuses on a repositioning problem in anticipation of storms within a collaborative countrywide network. The network involves multiple humanitarian agencies that strategically preposition relief supplies across different regions. When a storm is anticipated to hit the country, the agencies begin receiving forecast updates providing predictions on storm conditions. They estimate disaster impacts by utilizing the available forecasts and relocate the prepositioned supplies to the potentially affected regions for a faster response. We address this problem by proposing an analytical approach that provides relocation recommendations based on the forecasts. We collaborate with the Emergency Supply Prepositioning Group to design and integrate such a decision support tool into their “STOCKHOLM” platform, which offers prepositioned stock mapping and analyses to the stockholders. We test our approach and analyze the benefits of collaboration with a case study on Madagascar. The analyses are conducted by processing data related to the logistics network and storm forecasts.",Forecast-driven collaborative repositioning optimization. An application to Madagascar,"[65598, 35594, 54717, 66434]",551,"[58, 138, 84]",2125,Demand Forecasting in Humanitarian Operations,38,8,21,OR in Humanitarian Operations [HOpe],"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,49 [building - 116],"['Humanitarian Applications', 'Supply Chain Management', 'Optimization Modeling']",TB-21
"Energy storage plays a key role in harvesting energy among heterogeneou energy sources. To transforming heterogeneous energy and planning storage capacity at regional strategic level, this paper simulates storage capacity settings for heterogeneous energy in a certain region [Jiangsu Province in China] from the perspective of investment portfolio. This paper then achieves the adjustments between generation side and storage side during operation under hedging ideology. At last, this paper takes thermal energy on storage side as an example to apply switching real option method in decision making for shifting from thermal energy into renewable energy during disposal. The conclusion shows that under a financial derivative perspective, planning of heterogeneous energy storage capacity is more efficient than existing regional plans and decision making of transformation contains investment triggers with higher option values. Thus, this paper can help the power system plan energy storage and manage heterogeneous energy more economically and completely at regional strategic level. ",Transformation of heterogeneous energy and planning storage capacity at regional strategic level - A financial derivative perspective,[77647],282,"[12, 22, 45]",2129,Advancements in energy system optimization and analysis tools,21,4,22,Energy Management,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,81 [building - 116],"['Capacity Planning', 'Critical Decision Making', 'Financial Modelling']",MC-22
"We study a triadic supply chain with a retailer and two complementary suppliers. The retailer is close to the market and can privately acquire the market segmentation information, i.e., the proportion of prosocial consumers in the market, while the suppliers only know its distribution. The retailer reports the demand information of prosocial customers to the suppliers publicly, and the suppliers independently decide their own CSR investments to expand prosocial demand in the market under the positive externality of CSR investments. We consider supply chain members' social preferences, including the retailer's lying aversion behavior for the possible manipulation of information shared with suppliers, the suppliers' trust preferences towards the retailer's information sharing, as well as the suppliers' peer-induced fairness concerns caused by the possible free-riding given CSR investments' positive externality characteristics. The results show that - [1] Under full rationality, suppliers' CSR investments are independent of the retailer's report and the equilibrium is uninformative. [2] However, the existence of social preferences can facilitate the effective sharing of prosocial demand information in the supply chain, and enhance suppliers' CSR investments, leading to a win-win-win condition for supply chain members as well as prosocial customers. [3] The critical factors in achieving this condition are the matching between the retailer's lying aversion degree and the suppliers'",Prosocial demand information sharing in a triadic supply chain with social preferences,[53530],576,"[138, 10]",2132,Behavioural operations and games ,13,13,07,Behavioural OR,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1019 [building - 202],"['Supply Chain Management', 'Behavioural OR']",WB-07
"We study single-machine scheduling problems involving two competing agents sharing a batching machine. This machine can handle at most one job from each agent simultaneously. Our focus is on two due date-related scheduling criteria - the number of late jobs and the total late work. We investigate two variations for each problem - minimizing the combined objectives of both agents, and minimizing the objective of one agent while imposing an upper bound on the other agent's objective. All four versions of the problem are demonstrated to be strongly NP-hard. To address this complexity, we devise efficient Mixed-Integer Linear Programming [MILP] formulations, and employ constructive algorithms and Tabu Search for approximate solutions. A comprehensive numerical analysis validates the effectiveness of the approximation algorithms.",Cooperation and competition in two-agent scheduling with parallel batching,"[77648, 2881]",932,"[129, 108, 16]",2133,Machine Learning in Machine Scheduling,35,14,60,Project Management and Scheduling,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S09 [building - 101],"['Scheduling', 'Programming, Dynamic', 'Complexity and Approximation']",WC-60
"Prepositioning of relief supplies is crucial for the timely response to disasters. In a country, central government leads the management of mass casualty incidents [MCIs], while local governments must be able to handle non-MCIs that may occur in their responsible areas. To preposition relief supplies, the central and local governments must determine the number, locations and sizes of facilities, and the amount of disaster relief supplies prepositioned in the facilities. Each local government individually optimizes its prepositioning decisions. However, once an MCI happens, the central government needs to coordinate local governments to offer sufficient suppliers to meet the demand from the affected areas. That is, the central government must take the local governments’ prepositioning decisions into account to optimize its nationwide prepositioning decisions. Therefore, this research intends to design incentive schemes that help the central government coordinate the local governments to make optimal prepositioning decisions. To this end, we mathematically model the central government’s prepositioning problem, and then propose a solution algorithm based on the model to solve the problem. An empirical study is conducted to verify the proposed mathematical model and solution approach.",Central-Local Governments Cooperate to Pre-Position Relief Supplies for Natural Disaster Response,"[77649, 11924]",552,"[30, 64, 65]",2134,Site selection and aid allocation in humanitarian operations,38,5,21,OR in Humanitarian Operations [HOpe],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,49 [building - 116],"['Disaster and Crisis Management', 'Location', 'Logistics']",MD-21
"Despite widespread discussion that near-expired food commercialization [NFC] may assist fresh-food suppliers to recover value from unsold inventory and enhance revenues, most suppliers remain skeptical about NFC, hesitant to abandon the potential of and concerned about eroding the higher-margins of fresh food sales. We develop in this paper a game-theoretic model to investigate the optimal strategy and optimal price-setting timing for the supplier that decides whether and how to commercialize its near-expired food. We find that increasing fresh food production shifts the supplier from non-commercialization to partial and ultimately full NFC. Under full NFC, the supplier with high bargaining power and high consumers’ willingness to pay may benefit from adopting clearance-priority pricing, which prioritizes setting of the retail price of the near-expired food. Conversely, the supplier with limited bargaining power and low consumers’ willingness to pay can gain an advantage by adopting fresh-priority pricing, which entails setting the retail price of the fresh food first. Under partial NFC, the supplier generally profits from clearance-priority pricing. However, given high disposal costs, even the seemingly strong supplier may choose fresh-priority pricing to avoid potentially significant losses from negotiation failure. We further analyze extended models to confirm the robustness of our findings. Our study not only contributes to the literature but also guides practice.",When should fresh-food suppliers embrace near-expired food commercialization?,[77456],633,"[25, 138, 84]",2135,Retail Cooperation and Competition,30,13,61,Retail Operations,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S10 [building - 101],"['Decision Analysis', 'Supply Chain Management', 'Optimization Modeling']",WB-61
"Customer churn is a common problem for companies. The problem is that traditional business analytics models, particularly when it comes to predicting customer churn. The core of the issue is that these traditional models don't necessarily align with business objectives. 
For instance, a typical customer churn model might tell us whether a customer is likely to discontinue service, but it stops short of considering the financial implications of churn. Additionally, algorithms' decision-making processes and outcomes should be understandable and explainable to humans. Rule-based models [e.g.rule ensembles] are among the methods used to provide this level of explainability.
In this study,Profit Driven Spline Rule Ensembles Customer Churn Prediction Model has been developed for this problem.  The unique aspect here is the profit-driven approach, which ensures that the predictions are aligned with the company's financial goals. Part of this approach involves adapting model performance metrics EMPC to be understandable to humans. By utilizing spline rule ensembles, the model captures complex relationships in customer behavior that standard models might miss. The key to the model's success lies in its interpretability. In today's complex decision-making landscape, clarity is very important. The model doesn't just churn out predictions; it provides a window into the 'why' and 'how' of customer behavior, empowering decision-makers with the knowledge to act decisively and effectively.",Interpretable Ensemble Learners for Explainable Business Analytics,"[77446, 31537, 11028]",678,"[7, 8]",2137,Machine Learning and Ensemble Learning with optimization methods,15,14,27,Mathematical Optimization for XAI,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,047 [building - 208],"['Analytics and Data Science', 'Artificial Intelligence']",WC-27
"The optimization of appointment scheduling within parallel server systems is important for enhancing operational efficiency and customer satisfaction in diverse service industries, ranging from healthcare and telecommunications to transportation and manufacturing. In this context, we tackle the challenge of appointment scheduling in systems where clients are served by two correlated servers operating in parallel, with given distribution. We devise a stochastic optimization framework to minimize a comprehensive loss function that balances clients' waiting times, server idle times, and the effects of individual client's sojourn times. To capture the diverse operational scenarios encountered in practice, we introduce novel metrics, including maximum and minimum sojourn times, reflecting real-world constraints and priorities and redefine the scheduling objective solely in terms of sojourn times, facilitating the application of standard optimization methodologies. To accurately model the distributions of sojourn times, we propose a bivariate extension of the conventional two-moments fit using Weibull distribution and leverage the Frechet bounds for bivariate distributions with fixed marginals to get bounds on correlation between the two queues. Finally we conduct a series of numerical experiments to illustrate the efficiency of our method. We compare our method with simulated results and examine the effect for both negative and positive correlations with different distributions. ",A bivariate two-moments fit and its application in appointment scheduling,"[72197, 68609, 68630]",533,"[121, 129, 136]",2138,Stochastic Optimization - Advanced Applications,49,13,34,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,43 [building - 303A],"['Queuing Systems', 'Scheduling', 'Stochastic Optimization']",WB-34
"Walmart’s mission is to be the number one food destination in America. A critical driver of this mission involves creating an efficient and flexible grocery supply chain. We present an optimization model for Walmart’s grocery import network, designed to determine the optimal locations of import facilities, inbound flows of products from overseas suppliers, and outbound flows to Walmart’s distribution centers. The model enables us to design the imports network for both fresh and freezer items. We describe how the solutions from this optimization model have shaped Walmart’s strategy for direct importation of groceries, saving billions of dollars, and exploring new opportunities for creating efficiency in Walmart’s overall grocery network. ",Optimal Design of Walmart's Grocery Import Network,"[76841, 79196, 74138]",101,"[79, 138, 151]",2139,Retail Distribution II,30,13,50,Retail Operations,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,M2 [building - 101],"['Network Design', 'Supply Chain Management', 'Practice of OR']",WB-50
"We examine the product line design and pricing problems faced by a firm when the standard and green products exhibit markedly different environmental performances. We assume that consumer valuations of the green product are influenced in part by their level of 'environmental awareness.' The market comprises two segments with varying levels of awareness. We introduce a utility-based consumer choice model and validate it through experimentation. Initially, we study the firm's profit maximization problem, and then integrate it into the regulatory agency's incentive design problem. The regulatory agency aims to achieve a predetermined reduction in the realized use-phase environmental impact of products. Through computational analyses, we highlight the significant impact of segment sizes and their levels of environmental awareness on both the product design decisions of the firm and the incentive design mechanism of the regulatory agency.",Product Line Design under Sustainability Incentives,"[77652, 77653, 77654, 77655]",11,"[124, 139, 100]",2140,Sustainable Supply Chain Management,19,12,24,Sustainable Supply Chains,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,83 [building - 116],"['Revenue Management and Pricing', 'Sustainable Development', 'OR in Sustainability']",WA-24
"This paper develops a capital structure model with a financial covenant that sets an upper limit on a firm’s debt-earnings ratio. Shareholders will reduce debt or default when the ratio exceeds the upper limit. In the model, firm value, debt repayment policy, and capital structure are derived explicitly. For low levels of the limit, shareholders prefer to reduce debt every time the ratio exceeds the limit. Then, the covenant removes cost of debt, while it decreases equity value by restricting shareholders. By this trade-off, the covenant can improve firm value. The covenant can also improve firm value by suppressing the leverage ratchet effect and removing the restriction on future debt issuance. With the covenant, the firm can begin with high leverage to take advantage of no cost of debt. The covenant tends to improve firm value for higher bankruptcy cost and volatility, and the additional debt channel can greatly improve firm value for higher growth and tax rates. These results are consistent with empirical evidence and support the optimal contracting hypothesis regarding debt covenants.",The effects of a financial covenant on optimal capital structure and firm value,"[8187, 2266, 75698]",696,"[45, 44, 135]",2146,Learning and pricing,11,7,59,Pricing and Revenue Management,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S08 [building - 101],"['Financial Modelling', 'Finance and Banking', 'Stochastic Models']",TA-59
"The classical minimum Steiner tree problem is an NP-hard problem that involves finding the shortest network spanning a set of terminals in the plane using additional points called Steiner points. In the minimum k-Steiner tree problem, the number of Steiner points is limited to some positive integer k. In a minimum Steiner tree, every Steiner point has degree 3, but the restriction on the number of Steiner points also allows degree-4 Steiner points to occur. This increases the number of topologies that are potentially optimal, which further increases the complexity of the problem. 

To address the large number of topologies in the classical minimum Steiner tree problem, algorithms such as the GeoSteiner Algorithm implement various pruning tests. These pruning tests are based on the structural and geometric properties of a minimum Steiner tree and are used to discard the majority of sub-optimal topologies early on to deal with the large number of topologies. We take this approach and create novel pruning tests that tackle topologies involving a degree-4 Steiner point. Hence, we create an algorithm that solves the minimum k-Steiner tree problem exactly, incorporating these new pruning tests in order to make it more efficient.",An Exact Algorithm for the Euclidean k-Steiner Tree Problem,"[77643, 27563, 10472, 19200]",866,"[53, 5]",2148,Topics in Integer Programming II,64,13,25,Combinatorial Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,011 [building - 208],"['Graphs and Networks', 'Algorithms']",WB-25
"Transportation systems, such as bus systems, are important areas in which to apply models for strategic customer behavior.
We then propose a dynamic pricing mechanism, in which the fees for each customer and the timing for the service commencement depend on their experienced time cost, for batch service [shuttle bus] systems, to achieve operational efficiency and passenger fairness under a strategic joining/balking scheme. 
In this dynamic pricing scheme, the monopolist decides the fee for each customer such that all customers on the same shuttle have rigorously equivalent utilities [including waiting costs], and,
if the total time cost for waiting customers exceeds certain criteria, the monopolist lets the shuttle depart even though the number of customers is less than the full capacity.
For this model, we show an equilibrium strategy within certain parameters, and propose an algorithmic procedure that rigorously calculates the transition probability between states under long-term observation and construct a Markov chain for the number of waiting customers.
Based on the results, we present a method for calculating performance measures such as the mean of social welfare, monopolist revenue, and experienced utility.
Throughout some numerical experiments,  we show that the dynamic pricing control has the potential to improve both fairness among customers and performance efficiency.",Fair and efficient sharing - dynamic pricing control in batch service system with strategic customers,"[77657, 67036]",510,"[121, 50, 135]",2151,Advances in Stochastic Modelling and Applied Probability II,47,4,40,Advances in Stochastic Modelling and Learning Methods,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,96 [building - 306],"['Queuing Systems', 'Game Theory', 'Stochastic Models']",MC-40
"We consider a problem faced by multiple searchers that have to explore an unknown simple polygon P. At the beginning of the exploration, all the searcher are located at some point inside P, called the origin, and the exploration finishes when any point on the boundary of P is seen by at least one searcher and  all the searchers return to the origin. The cost of the exploration is the time needed to finish the exploration, and the objective of the problem is to minimize it. We consider an online exploration, where the searchers create a map of P during the exploration and use it to decide which direction to move.

In this paper, we consider a restricted class of simple rectilinear polygons, which can be created by, starting from the root rectangle, recursively attaching a base of a new rectangle to a lateral side of some rectangle constituting the current object. We present an online exploration algorithm for this class of polygons, and show an upper bound on its competitive ratio that is sublinear in the number of searchers.",Online Exploration of Rectilinear Polygons by Multiple Searchers,"[77651, 45244, 77658]",883,"[5, 18, 14]",2152,Topics in Combinatorial Optimization II [Contributed],64,15,25,Combinatorial Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,011 [building - 208],"['Algorithms', 'Computer Science/Applications', 'Combinatorial Optimization']",WD-25
"Pairwise learning is a specialized form of supervised learning that focuses on predicting outcomes for pairs of objects. Data with pairwise observations naturally arise, for instance, in recommender systems, information retrieval, drug-target interaction [DTI] prediction, and link prediction in social networks. Here, we formulate the pairwise learning problem as a difference of convex [DC] optimization problem using the Kronecker product kernel, L1- and L0-regularizations, and various possible nonsmooth loss functions. Our aim is to create an efficient learning algorithm, SparsePKL, that produces accurate predictions with the desired sparsity level. In particular, we aim to solve the realistic form of the DTI problem called zero-shot learning, which corresponds to predicting labels for drug-target pairs where neither the drug nor the target is encountered during the training phase. In addition, we propose a novel limited memory bundle DC algorithm [LMB-DCA] for general large-scale nonsmooth DC optimization and apply it as an underlying solver in the SparsePKL. The performance of the SparsePKL algorithm is studied in seven real-world drug-target interaction data, and the results are compared with those of the state-of-the-art methods in pairwise learning. Further, we evaluate the LMB-DCA as a standalone optimization method by comparing it with the well-known DCA.",Limited memory bundle DC algorithm for sparse pairwise kernel learning,"[13346, 42131, 31701, 19538]",515,"[66, 81, 5]",2155,"Advancements of OR-analytics in statistics, machine learning and data science 7",16,10,28,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,065 [building - 208],"['Machine Learning', 'Non-smooth Optimization', 'Algorithms']",TD-28
"In recent years, the proliferation of automated trading strategies has underscored the significance of anchoring effects in financial markets. Despite efforts to tailor these strategies to user preferences, anchoring effects persist, influencing decision-making among traders. This phenomenon, observed by Tversky and Kahneman, affects buyers and sellers differently, leading to market distortions. Additionally, the presence of market manipulators further complicates market dynamics, exacerbating wealth distribution imbalances. To better understand these dynamics, we employ agent-based modeling, allowing for the exploration of interactions among different agents in the market. Through simulations, the study aims to shed light on the impact of anchoring effects and market manipulation on wealth distribution, offering valuable insights for financial regulators. This study aims to address these challenges by constructing an agent-based asset market incorporating anchored traders and market manipulators. Inspired by previous research, the study aims to understand how different scenarios and manipulative practices influence market efficiency and wealth distribution using agent-based modeling.",Investigating the Impact of Anchoring Effects and Market Manipulation on Wealth Distribution in Agent-Based Asset Markets,[77025],564,"[9, 45, 15]",2156,Simulation in economics I,77,4,43,"Agent-based Models in Management, Economic and Organisation Sciences","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,99 [building - 306],"['Auctions / Competitive Bidding', 'Financial Modelling', 'Complex Societal Problems']",MC-43
"Land use fragmentation is an important issue for management and rural development. For the last few decades, the relationship between mankind and land has become more dynamic though the traditional cadastral system has been slow in responding to the changing needs of the society. Generally defined, land-use suitability analysis aims at identifying the most appropriate spatial pattern for future land uses respecting specific requirements and preferences in what concerns the activities developed. The analysis of the land use suitability implies not only to consider the best aggregate solution, but also the balance among the different criteria, which calls for the necessity of multicriteria decision analysis techniques. This article proposes a compromise programming approach for determining the biophysical potential of land parcels. The approach has 4 steps - 1] Cluster analysis do identify homogeneous zones within the study area; 2; Compromise programming approach for the suitability index building and ranking construction; 3] Statistical analysis [statistical tests to assess the differences among clusters]; 4] Spatial analysis of the results. The approach was implemented in the Loulé municipality, Portugal, in more than 50000 parcels. Results were promising since the approach was able to create a ranking of parcels that have different biophysical conditions, proving that this approach is relevant and interesting for land management.",A Compromise Programming Approach for Assessing the Biophysical Suitability of the Territory - The Case of the Loulé Municipality,"[77376, 77663, 77664]",109,"[72, 89, 139]",2159,MCDA and urban planning 1,44,8,47,Multiple Criteria Decision Analysis,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,50 [building - 324],"['Mathematical Programming', 'OR in Agriculture', 'Sustainable Development']",TB-47
"Restless Multi-Armed Bandits [RMABs] are extensively used in scheduling, resource allocation,
marketing and clinical trials, just to name a few application areas. RMABs are Markov Decision Processes
with two actions [active and passive modes] for each arm and with a constraint on the number of active arms
per time slot. Since in general RMABs are PSPACE-complete, several heuristics such as Whittle index and LP 
index have been proposed. In this talk, I present reinforcement learning schemes for both Whittle and
LP indices with almost sure convergence guarantee in the tabular setting and an empirically efficient 
Deep Q-learning variants. Several examples, including scheduling in queueing systems, will be presented. 
This talk is based on joint works with V.S. Borkar and P. Shah from IIT Bombay.",Learning Whittle and LP indices in Average-Reward Restless Multi-Armed Bandits,[75612],14,"[66, 82, 135]",2160,Reinforcement Learning - Methods and Applications ,47,8,40,Advances in Stochastic Modelling and Learning Methods,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,96 [building - 306],"['Machine Learning', 'Optimal Control', 'Stochastic Models']",TB-40
"Topology and connectivity design with in-network processing [sortation, cross docking] is a critical component for Middle Mile transportation network design to achieve operational efficiency, cost-effectiveness, and delivery speed. In this paper, we consider a middle mile transportation network that consists of warehouses, sortation centers and delivery stations. Given a warehouse to delivery station volume forecast, our goal is to design cost and speed optimal transportation network. We formulate the topology and connectivity design problem as a Mixed Integer Linear Programming [MILP] problem, which is NP-hard.  We describe the key characteristics of the problem and analyze the arising modeling and optimization challenges. To solve large scale real-world problems [>100 million variables], we present our optimization-based matheuristic pipeline, which comprises preprocessing and path pruning strategies, local neighborhood search, and incremental model solving. We show the efficacy of our model using a range of operational datasets. We conclude with potential research threads that can further improve the solution quality and runtime. 
",Large Scale Topology and Connectivity Design with In-Network Processing,"[77660, 77687, 77688, 77689]",289,"[79, 143]",2161,Large-scale network optimization and inventory management,92,10,57,Optimization at Amazon,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S06 [building - 101],"['Network Design', 'Transportation']",TD-57
"Matrix Completion [MC] involves reconstructing the missing entries of a partially observed matrix M by leveraging the assumption that M has low rank [1]. Problems that can be approached with MC arise in various fields, such as recommendation systems, image inpainting, and compressed sensing. Our work starts from the idea of reformulating MC as the problem of finding a matrix in the intersection of two sets - the set of matrices with the same observed entries as M, and the nonconvex set of matrices of rank less than or equal to r. This is a feasibility problem [FP], which generally involves finding a point x in the intersection of two sets A and B. Since the 1950s [2], the Alternating Projection Method [APM] has been proposed to solve FP. Starting from a point in B, APM generates a sequence of points by alternately projecting onto A and B. We will discuss  two theoretically well founded regularized variants of the method, employing  exact projections [RAPM] and inexact projection onto the set B [iRAPM]. The obtained theoretical results guided us to the  definition of  a new stopping criterion for the Krylov iterative method employed  for computing the approximate  truncated SVD [nedeed to compute the inexact projection onto the rank set].  Some numerical experiments to validate the convergence results and the performance of the studied methods will be also shown.

[1] Candès E. J. et al., Found. Comput. Math., 9, 717--772, 2009
[2] Von Neumann J., Functional Operators, 1951",Alternating Projections Methods for Matrix Completion - Regularized and Inexact Projections,"[77662, 21177, 67214]",421,"[19, 72, 5]",2163,Optimization and learning for data science and imaging [Part I],84,2,34,Advances in large scale nonlinear optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,43 [building - 303A],"['Continuous Optimization', 'Mathematical Programming', 'Algorithms']",MA-34
"The moment-sum of squares hierarchy by Lasserre has become an established technique for solving polynomial optimization problems. It provides a monotonically increasing series of tight bounds, but has well-known scalability limitations. For structured optimization problems, the term-sparsity SOS [TSSOS] approach scales much better due to block-diagonal matrices, obtained by completing the connected components of adjacency graphs. This block structure can be exploited by semidefinite programming solvers, for which the overall runtime then depends heavily on the size of the largest block. However, already the first step of the TSSOS hierarchy may result in large diagonal blocks. We suggest a new approach that refines TSSOS iterations using combinatorial optimization and results in block-diagonal matrices with reduced maximum block sizes. Numerical results on a benchmark library show the large potential for computational speedup for unconstrained and constrained polynomial optimization problems, while obtaining almost identical bounds in comparison to established methods.",Refined TSSOS,"[77661, 52307, 14240]",164,"[52, 115]",2164,Special Classes of  Convex Conic Optimization problems,68,7,38,"Conic Optimization - Theory, Algorithms, and Applications","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,34 [building - 306],"['Global Optimization', 'Programming, Semidefinite']",TA-38
"Over the past decade, amidst the rise of e-commerce enterprises and the transformative impact of the Covid-19 pandemic, the Order Batching Problem [OBP] has garnered significant attention. This heightened focus is fueled by the substantial volume of order transactions and the subsequent exponential expansion of warehouse dimensions, comprising multiple zones. These complexities have underscored the potential for over-costing in picking operations, prompting the adoption of intelligent solutions. Despite this, scant attention has been devoted to addressing OBP at the scale of Trendyol Group, a prominent player in Turkey and an increasingly influential international e-commerce entity offering a diverse array of product categories. This study particularly focuses on an OBP problem that groups orders into worklists for warehouse pickers, with the objective of maximizing the average number of items per zone. This approach seeks to minimize the duration of item picking for pickers, as a higher concentration of items per zone enables greater efficiency in item collection. To address this problem, we introduce a MIP model and implement a tailored Adaptive Large Neighborhood Search [ALNS] algorithm. Through extensive computational experiments utilizing real-life instances, our findings consistently demonstrate that ALNS yields near-optimal solutions and substantially enhances operational metrics by approximately 30%.",An Adaptive Large Neighborhood Search Algorithm for Order Batching Problem,"[57737, 52407, 78734, 78759, 78733]",473,"[32, 26, 74]",2166,Methods and Algorithms of Decision Support,45,4,45,Decision Support Systems,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,30 [building - 324],"['E-Commerce', 'Decision Support Systems', 'Metaheuristics']",MC-45
"In recent years, the adoption of Electric Vehicles [EVs] has increased, driven by government initiatives to require them to reduce greenhouse gas emissions. This increased adoption, while environmentally beneficial, has also led to an increase in electricity consumption. This paper analyses the advantages of bidirectional vehicle-to-grid [V2G] chargers for Charging Station Operators [CSOs] - Electric Vehicles energy flexibility could be a valuable resource during grid congestion episodes. We analyze this flexibility by proposing a two-stage stochastic programming model under uncertainty, regarding the presence of EVs at charging stations. We utilize a mixed Long Short-Term Memory [LSTM] neural network to generate scenarios for each charger and then blend and reduce them into the most representative ones. We propose a large-scale optimization approach for solving the two-stage stochastic model. The use of a large-scale optimization approach helps to reduce the time for executing the optimal solution, which is beneficial to ensure management of grid variability. The results demonstrate that electric vehicles can provide demand flexibility, resulting in potential cost reductions and contributing to the prevention of power grid congestion during periods of low generation or high demand. Also, the developed solutions help to integrate renewable resources into the grid and reduce greenhouse gas emissions.",Management of electric vehicles to provide energy flexibility under uncertainty,"[77667, 71350, 77238, 41827]",468,"[136, 93, 63]",2168,Optimization for electric vehicles,21,8,22,Energy Management,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,81 [building - 116],"['Stochastic Optimization', 'OR in Energy', 'Large Scale Optimization']",TB-22
"In this paper, we introduce PathWyse 1.0, a versatile dual licensed library addressing the Resource Constrained Shortest Path Problem [RCSPP], a fundamental and hard combinatorial problem.

PathWyse is capable of modeling and solving a variety of standard RCSPPs implementing state-of-the-art algorithms. The library interfaces with both C++ and Python, easing the task of modeling customized problems with minimal programming required.

Algorithmically, the latest version presents parallel implementations of both exact and heuristic algorithms. Particularly, heuristic algorithms are integrated into a competitive ensemble framework, boosting their effectiveness. Moreover, this release highlights an enhanced bi-directional dynamic programming technique. The joining procedure, usually a time-intensive aspect of the algorithm, now utilizes binary search and Pareto frontier exploration, resulting in notable performance improvements.

We conducted computational experiments using instances from literature, encompassing scenarios featuring a singular resource constraint across cyclic networks and large acyclic networks. We present findings related to instances with multiple resource constraints within cyclic networks. 

The flexible parameterization showcases how users can easily model and efficiently resolve multi-constrained problems. To aid comprehension, we provide a tutorial on a column generation algorithm, utilizing PathWyse as the solver for the pricing problem.",PathWyse 1.0 - a software library for the Resource Constrained Shortest Path,"[19625, 77674]",872,"[134, 14, 108]",2169,Optimization problems on graphs,64,5,26,Combinatorial Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,012 [building - 208],"['Software', 'Combinatorial Optimization', 'Programming, Dynamic']",MD-26
"Scheduling of activities and resources occurs across various sectors, especially those susceptible by uncertainty, where budgets are significantly impacted by delays, and a need for sudden reprioritization. At Ørsted Wind Power operations, we face challenges due to weather conditions that affect accessibility of wind farms for multiple types of operation & maintenance. To address these challenges, we have created a Scheduling Engine designed to integrate weather uncertainties into our planning systems. We are aiming to deploy the engine to planning problems ranging from tactical asset planning over major component exchange campaign planning, to operational weekly planning. Embedding the engine into our planning systems enables the creation of multiple scenarios and assists in understanding the potential impacts of weather conditions durations. Building upon its generic foundation, with the capability of custom adaptation to the specific scheduling problem, we aim to extend the Scheduling Engine to become the go-to tool for all planning and scheduling needs within Ørsted Wind Power Operations. This presentation outlines some of the complexities in designing the generic scheduling engine and developing it for real-world application. ",Scheduling in Offshore Wind - Developing a Simulation Tool Representative of Stochastic Weather,"[77548, 76969]",347,"[129, 5, 131]",2171,Advanced heuristics for machine scheduling,35,9,60,Project Management and Scheduling,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S09 [building - 101],"['Scheduling', 'Algorithms', 'Simulation']",TC-60
"Various dispatchable power generation technologies can complement each other due to their diverse economic characteristics. Screening curves have been used to determine optimal mixes of power plant capacity from load duration curves since the 1970s. Starting with only fossil and nuclear generation technologies, recent applications have incorporated variable renewable energy sources by considering residual load duration curves. However, the applicability of screening curves to net-zero energy systems with growing shares of renewables remains untested. Here, we aim at filling this gap by creating net-zero screening curves and comparing the derived optimal capacity with the results of a linear model minimizing total power system costs. In a first step, we exploit the screening curve approach to identify cost-optimal decarbonized dispatchable technologies as a function of renewable energy shares and across a wide range of cost assumptions. In a second step, we process the linear model and investigate aspects that are neglected in the screening curve model, i.e. the deployment of short-term storage and time-variant prices of synthetic fuels. Preliminary results show consistent technology choice for both the screening curve and linear model. However, the optimal capacity differs with rising shares of renewables, particularly for the technologies with lowest and highest variable cost. This raises concerns about the applicability of screening curves in net-zero energy systems.",Screening Curves in Net-Zero Energy Systems - Insights and Limitations,"[77198, 77696, 69559]",407,"[36, 37, 84]",2172,Market-based analyses in long-term energy system models,22,9,09,Energy Markets,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,10 [building - 116],"['Electricity Markets', 'Energy Policy and Planning', 'Optimization Modeling']",TC-09
"In Influence Maximization [IM], the objective is to select the optimal set of entities in a network to target with a treatment so as to maximize the total effect. For instance, in marketing, the objective is to target the optimal set of customers that maximizes the total response rate, resulting from both direct treatment effects on targeted customers and indirect, spillover effects that follow from targeting these customers.

In selecting the optimal set of entities, current IM methods typically overlook the features of the entities. Nevertheless, these features might contain important information on the propagation of effects through the network. Moreover, current methods assume that all entities that receive treatment will be activated [e.g., buy the product], ignoring varying susceptibilities to treatment depending on an entity’s characteristics.

Current IM approaches mainly rely on assumptions about how effects propagate through the network, potentially leading to a suboptimal selection of entities. To improve upon current methods, we propose a causal machine learning approach which estimates both direct and spillover effects from data, instead of relying on assumptions. After training a causal estimator on observational data, in a second step, treatment allocation is optimized by integrating the treatment effect estimates into current IM algorithms. This two-step approach is shown to improve the expected influence spread compared to traditional methods.",Optimizing Treatment Allocation in the Presence of Interference - a Causal Machine Learning Approach to the Influence Maximization Problem,"[76064, 72028, 46180]",63,"[7, 66, 53]",2174,Causal Machine Learning,17,4,31,Analytics,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,54 [building - 208],"['Analytics and Data Science', 'Machine Learning', 'Graphs and Networks']",MC-31
"Price-based residential demand response [DR] schemes should be planned to enable win-win situations where both the flexible buildings and energy system enjoy economic benefits. Our study modelled the electrically heated Finnish residential building stock, assuming flexible space heating and cooling as well as water heating. We integrated different penetrations of such buildings to a North European energy system model. Two DR schemes were considered - a grid-interactive scheme, in which flexible buildings participated in the minimization of total system costs, and a self-optimizing scheme in which the buildings minimized their own heating/cooling costs based on pre-determined and fixed electricity prices. Annual operational costs of the combined systems were solved as linear optimization problems to assess the cost savings potential from DR.

The results suggest that while flexible consumers often benefited directly from timing their consumption on more inexpensive hours, wide-scale DR penetration can also impact non-flexible consumers by influencing electricity spot prices. In addition, basing wide-spread DR on pre-determined and fixed pricing, such as day-ahead spot prices, may carry risks for the energy system and electricity market if many buildings shift their demand synchronously, causing new demand peaks.

H2020 TradeRES [grant agreement no. 864276]
",Economic benefits of residential heating and cooling flexibility - consumer and system perspective,"[77666, 78151, 58607]",403,"[36, 33, 93]",2177,New Market Designs & Models for 100% Renewable Power Systems,22,4,09,Energy Markets,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,10 [building - 116],"['Electricity Markets', 'Economic Modeling', 'OR in Energy']",MC-09
"This paper studies a routing and wavelength assignment problem for an optical-fiber telecommunication network, known as the RWAP with partial path protection [RWAP-PPP]. The existing literature lacks any studies on its relaxations. As a result, the quality of solutions to the RWAP-PPP cannot be properly measured, which is critical to telecommunication service providers in their bidding to customers, and in their determination of service improvements. We derive and compare various linear programming [LP] relaxations of the RWAP-PPP, from which we propose a novel LP relaxation that can be efficiently solved by a Benders decomposition algorithm to produce good lower bounds for the RWAP-PPP. Computational results show that the newly proposed LP relaxation can be solved efficiently, even for some large-scale practical networks of hundreds of nodes and thousands of requests.",Solving LP Relaxations for Routing and Wavelength Assignment Problem with Partial Path Protection,"[64381, 77675, 77461, 70665, 77684, 77678]",297,"[110, 141, 79]",2178,Advances in Optimization for Industrial Applications,64,12,29,Combinatorial Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,157 [building - 208],"['Programming, Linear', 'Telecommunications', 'Network Design']",WA-29
"Negative effects on human health and the environment call for ambitious policies to reduce the risks of agricultural pesticide use. Yet, strategic uncertainty exists, which is often neglected. New policies must be of utility to a range of stakeholders to be implemented. We studied how preferences for achieving multiple conflicting objectives and domain-specific risk attitudes towards uncertain consequences influence the support of new policy alternatives. To this end, we combined concepts from policy analysis and Multi-criteria Decision Analysis. We collected data on preferences, risk attitudes, and policy support from 24 key stakeholders and their collaboration partners in Swiss pesticide governance. Analyzing the data with Bayesian ordinal logistic regression models, we found specific relations between the support of certain policies and preferences, risk attitudes, or both. In wicked socio-environmental decision problems with great uncertainty, we thus suggest considering risk attitudes additionally to preferences to develop ambitious policies with potential for a sustainable transition. Our contribution is threefold - Theoretically, we present an innovative combination of the Advocacy Coalition Framework and Multi-attribute Value/Utility Theory. Methodologically, we describe a standardized approach to elicit preferences and risk attitudes, novel to policy analysis. Practically, we discuss the potential of pesticide risk reduction policies in the case of Swiss agriculture.",Preferences and risk attitudes in Swiss pesticide governance,"[70770, 31727, 71013]",112,"[77, 126, 10]",2179,BOR in public policy and environmental decisions,13,10,11,Behavioural OR,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,12 [building - 116],"['Multi-Objective Decision Making', 'Risk Analysis and Management', 'Behavioural OR']",TD-11
"Motivated by the growing literature on Pareto-optimal matchings, we examine two fundamental graph problems considering that vertices of a graph have preferences. The first is the circulation problem, in which every vertex sends an amount of flow equal to the one it receives; here each vertex has strict and transitive preferences over its out-going arcs, i.e., over the vertices it sends its flow. The second is the list coloring problem where each vertex has preferences over its available colors. Both problems admit an elegant characterization of the Pareto-optimal solutions, i.e., solutions in which no vertex can receive a better stake [i.e., flow or color] without some other vertex receiving a worse one. Well-known mechanisms, namely Top Trading Cycles or Serial Dictatorship, can find a Pareto-optimal circulation or coloring, respectively. Interestingly, although recognizing Pareto-optimality is easy for circulations, it becomes coNP-complete for colorings. We discuss further hard optimization problems for both settings, along with special cases that defy hardness by restricting the structure of either the graph or the preferences of its vertices. We also present several motivating applications, review relevant literature, and propose open issues for future work. ",Pareto optimality in graphs - circulations and colorings,"[23864, 23929, 68803]",876,"[14, 50]",2180,Optimization issues on graphs II [Contributed],64,15,29,Combinatorial Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,157 [building - 208],"['Combinatorial Optimization', 'Game Theory']",WD-29
"Data envelopment analysis [DEA] models appear in the envelopment and the multiplier form, which are in a primal-dual relationship. Both forms are essential for the proper interpretation and application of the model. New models are typically presented in the envelopment form, yet if they are not linear, it can be more challenging to acquire their dual counterparts. In this contribution, we focus on path-based DEA models, which search for the benchmarks by specifying various parametric paths running from the assessed unit to the boundary of the technology set. The class of path-based models includes well-known models such as the BCC input and output models, the hyperbolic distance function model and the general directional distance function model. For a general scheme of path-based models in the envelopment form we derive the general multiplier form of path-based models. Since path-based models in general are not linear, we study the duality relations between the primal-dual pair of envelopment and multiplier path-based models. Finally, we focus on the multiplier form of the BCC input and output models, the general directional distance function model and the general hyperbolic measure model using the derived general multiplier form of path-based models.",Path-based DEA models,"[71997, 71963, 77059]",946,"[24, 72, 21]",2181,DEA methodological developments II,89,15,48,Data Envelopment Analysis and its Application,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,60 [building - 324],"['Data Envelopment Analysis', 'Mathematical Programming', 'Convex Optimization']",WD-48
"Hydrogen, particularly 'green hydrogen' generated from renewable feedstock, is a promising alternative to fossil fuels for decarbonizing the transport and energy sectors. However, achieving a sustainable energy economy based on green hydrogen poses significant challenges due to limited resources and production capacities in many countries. The discrepancy between high demand and low self-supply has to be compensated through imports. Since overreliance on a single exporting nation can lead to geopolitical, financial, and moral complications, the import of green hydrogen should be diversified. In contrast to only regarding parallel bilateral imports, a multinational transportation network offers potential cost reduction through consolidations in production, conversion, and transportation. We propose an optimization model for such a multinational import network for green hydrogen, that considers import diversification as well as factors like transportation losses and conversion efficiencies. Using Germany as a case study, we demonstrate how enforced diversification impacts sourcing decisions and overall costs to further emphasize the trade-off between resilience and cost-effectiveness. Furthermore, we highlight how reducing losses of liquid hydrogen during transportation [through advancing storage technologies] could further diminish the hydrogen price.",A Model to Diversify the Import of Green Hydrogen,"[74991, 13086]",513,"[100, 79, 138]",2183,Raw Material Supply Chains,19,3,24,Sustainable Supply Chains,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,83 [building - 116],"['OR in Sustainability', 'Network Design', 'Supply Chain Management']",MB-24
"In carsharing pricing problems, stochastic demand is typically viewed as an exogenous random variable, where the uncertainty is governed by known probability distributions. This assumption neglects the inherent correlation between up-to-date user’s adoption decisions and the released carsharing prices, which makes the stochastic demand decision-dependent. To this end, we come up with an optimal carsharing service pricing problem that incorporates endogenous uncertainty of carsharing demand, specifically by accounting for the decision-dependent distributions of stochastic demand. The problem is formulated as a two-stage mixed-integer stochastic program with decision-dependent uncertainty. We show that the problem can be written as an equivalent mixed-integer bilinear program whose size is exponential in the input parameters. To tackle the computational complexity we adopt a version of the L-shaped method which accounts for decision-dependent distributions. The computational experiments demonstrate empirically that the algorithm converges finitely. In addition, the method outperforms by far a commercial solver used to solve the monolithic formulation, hereby extending the domain of practically tractable problems.",Optimal carsharing service pricing under decision-dependent demand uncertainty,"[77669, 45480]",533,"[117, 72, 143]",2184,Stochastic Optimization - Advanced Applications,49,13,34,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,43 [building - 303A],"['Programming, Stochastic', 'Mathematical Programming', 'Transportation']",WB-34
"Transmission system operators employ reserves to deal with unexpected variations of demand and generation to guarantee the security of supply. The French transmission system operator dynamically sizes the required margins using a probabilistic approach relying on continuous forecasts of the main drivers of the uncertainties of the system imbalance and a 1 % risk threshold. However, it does not consider the cost of units and the lost load. In addition, it does not provide insight into which means to activate upward/downward and when to face a deficit of margins. 

This work presents a unit commitment strategy using stochastic optimization, including i] the fixed and variable costs of power plants and the costs of lost load and spillage and ii] uncertainties of the renewable generation, consumption, and technical failures of units. 

A multi-stage stochastic program formulates this problem and is approximated with a rolling sequence of two-stage stochastic programs. 
A simplified version that solves a single two-stage stochastic program is implemented on a case study comprising a few nuclear and fossil-based units due to tractability issues related to the technical constraints of units. Extensions are required to:
Better model the uncertainties.
Consider other risk-aversion approaches, such as using a CVaR or chance-constrained.
Deal with the tractability issues.
Model the lost load cost according to the volume of not-served energy and outage duration.",Stochastic optimization for unit commitment applied to the security of supply,[77252],405,"[136, 127, 129]",2186,Stochastic Optimization for Energy Transition,49,3,35,"Stochastic, Robust and Distributionally Robust Optimization","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,44 [building - 303A],"['Stochastic Optimization', 'Robust Optimization', 'Scheduling']",MB-35
"The transition towards Electric Vehicles [EVs] is an important step to reduce greenhouse gas emissions and local air pollution. An important obstacle for the large-scale adoption of EVs is the availability of sufficient public charging infrastructure. To determine how much infrastructure is needed, scholars often rely on EV adoption data or transactional charging data at existing stations. However, a major drawback of these studies is that they extrapolate the charging behavior of the early EV adopters, which may not be representative for the entire population.

In this study, we present a two-step approach to analyze the impact of different behavioral charging scenarios on the required number of chargers. Based on two large-scale datasets of charging transactions in Brussels and San Francisco, we first employ K-means clustering to find segments of EV drivers that exhibit similar charging behavior in terms of where, when, and how much energy they charge. Second, we develop a Discrete Event Simulation model to determine how much additional charging infrastructure is needed to accommodate an increase in charging activity in each of the behavioral segments. Our results indicate that, depending on the behavioral type, up to 8 times more infrastructure is needed to accommodate the same increase in demand. This highlights the potential for behavior-steering policies such as dynamic- and penalty pricing to make more efficient use of the available charging infrastructure.",Aligning the Public Charging Infrastructure to the EV Driver - Lessons from Brussels and San Francisco,"[76854, 77760, 9491, 77700]",547,"[143, 139, 131]",2189,Advancing mobility towards sustainable solutions II,6,10,56,Transportation,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S04 [building - 101],"['Transportation', 'Sustainable Development', 'Simulation']",TD-56
"The high rate of population growth in the UK leads to an increase in freight transportation activities and shortage of urban space. This in turn raises negative impacts in urban areas such as congestion, pollution, etc. In addition, the largest contributor of emissions in urban areas is transportation, which produced about 24% of carbon emissions in the UK and 22% in Portsmouth in 2020. Freight transportations have negative environmental, economic and social impacts. To tackle such problems, this paper investigates the potential of the establishment of a macro consolidation centre to consolidate freight flows between suppliers and customers. This research is part of the Solent Future Transport Zone project [FTZ] funded by the UK’s Department for Transport and led by Solent Transport.
This paper has developed a Multi-Criteria Decision-Making [MCDM] framework using  Analytic Hierarchy Process and Best Worst Method to determine a suitable location for a macro consolidation centre for the city of Portsmouth. The three main sustainability criteria proposed for the selection of the location include Environmental, Economic and Social. Each of these criteria has sub-criteria that are designed to meet the needs of the study area. The results show the efficiency of the MCDM framework and the sustainability criteria in determining the most suitable site for the macro consolidation centre in Portsmouth.",MCDM Analysis for the Selection of a Location for the Freight Macro Consolidation Centre - Portsmouth Case,"[73206, 22418, 77695, 51324]",506,"[6, 64, 143]",2192,Freight transportation and logistic I,6,8,55,Transportation,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S02 [building - 101],"['Analytic Hierarchy Process', 'Location', 'Transportation']",TB-55
"The Jointly Replenishment Problem [JRP] is concerned with determining lot sizes and scheduling replenishment times for $n$ products to minimize total costs per unit time. 
In practice, consolidating replenishments to share common setup costs is crucial. Additionally, researchers often introduce constraints to enhance the applicability of inventory systems. 
Examples include the adoption of a power-of-two [PoT] policy for streamlined scheduling, imposing minimum order quantities for commitment, and enforcing maximum shipment sizes based on capacity constraints.

In this paper, we explore JRP models incorporating the PoT policy and bounds for replenishment time. We provide a theoretical analysis of these models, 
propose a search algorithm to identify global optimal solutions, analyze the complexity of the algorithm, and conduct randomized experiments to validate our approach.

By addressing these issues, our work contributes to the advancement of JRP models, offering practical insights and solutions that accommodate real-world constraints and enhance operational efficiency.",The Joint Replenishment Problem with Permissible Delay in Payments and Resource Constraints under Power-of-Two Policy,[77685],803,"[138, 16]",2193,Lot-sizing with joint replenishment and routing decisions,32,2,49,"Lot Sizing, Lot Scheduling and Production Planning","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,M1 [building - 101],"['Supply Chain Management', 'Complexity and Approximation']",MA-49
"Allied health professionals possess expertise to improve primary care for frail older adults but structural interprofessional collaboration remains difficult. We aim to characterize person and group-level behavioral factors influencing collaboration and decision-making between care professionals. 
We conducted 24 semi-structured interviews with allied health- and other primary care professionals and asked about collaborative experiences between disciplines. We deductively coded the interviews with elements [actors, actions and outcomes] of three game theoretical models [Principal-Agent [PA] game, Volunteer’s Dilemma [VD] and Battle of the Sexes [BS]] to characterize collaboration patterns.
We found empirical situations of behavioral patterns representing characteristics of each model. Based on these patterns, we clustered situations on two axes, resulting in four categories. PA games differentiate between the principal using power or the agent’s information to decide, and direct versus indirect collaboration between professionals. VD-games show waiting or volunteering behavior, central to patient care or professionals themselves. BS-games differ between collaborative- and conflict-inducing behavior, and resulting collective or individual actions.
Our findings may help raise awareness of [un]desirable dynamics in primary care collaboration and decision-making. Furthermore, it provides starting points to change behavioral patterns and improve collaboration.",Collaboration and decision-making patterns between allied health- and primary care professionals using game theoretical models,"[77342, 75744, 77705, 77704]",578,"[56, 50]",2194,Behavioural Studies in Health Care ,13,8,07,Behavioural OR,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,1019 [building - 202],"['Health Care', 'Game Theory']",TB-07
"Flow shops are widely studied machine environments in which all jobs must visit all machines in the same order. While conventional flow shops assume that each job traverses the shop only once, many industrial environments require jobs to pass through the shop multiple times before completion, i.e., after traversing the shop and completing its processing on the last machine, a job must return to the first machine and traverse the shop again until it has completed all its required loops. There are numerous applications, e.g., in semiconductor manufacturing, of such a setting, which is called a flow shop with reentry. The planning problem is to schedule all loops of all jobs while minimizing the total [weighted] completion time. We consider a reentrant flow shop with unit processing times and show that the Least Remaining Loops First [LRL] priority rule minimizes the total completion time.  Furthermore, we show that the problem of minimizing the total weighted completion time is NP-hard. For this objective, we show that the Weighted Least Remaining Loops First [WLRL] priority rule has a worst-case performance ratio of about 1.2. ",Flow shops with reentry - Total completion time optimal schedules,"[69668, 78910]",933,"[129, 16, 69]",2195,Flow shop and single machine scheduling ,35,15,60,Project Management and Scheduling,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S09 [building - 101],"['Scheduling', 'Complexity and Approximation', 'Manufacturing']",WD-60
"Logistics service providers [LSPs] employ transportation assets like car wagons and containers to meet customer needs. To adequately service these needs, it is essential to use assets with the right capacity for each transportation task. The time needed for these operations can vary due to disruptions in the network, which introduce the need to have additional resources. This situation leads to a complex version of the newsvendor problem, where the LSP must decide the number of assets to acquire and plan their deployment to cater to demands that specify a certain capacity but have an uncertain duration. Any demand that exceeds the planned capacity can be met by using spot assets, although at a higher expense. This concept is applied to an LSP that leases train wagons yearly and manages demand fluctuations through occasional spot train rentals. We formulate a two-stage stochastic program to represent this problem. The formulation's structural features, combined with column generation, lead to an efficient solution method. The effectiveness of our method is demonstrated through computational studies.",Strategic capacity planning for logistics service providers,[67084],197,"[14, 143, 12]",2196,Combinatorial Optimization models and applications in Logistics and Transportation I,64,2,29,Combinatorial Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,157 [building - 208],"['Combinatorial Optimization', 'Transportation', 'Capacity Planning']",MA-29
"Being competitive in last-mile delivery is a key factor for retailers, considering both cost and customer satisfaction. Crowdshipping addresses this by using ordinary people making deliveries on their habitual routes, being presented as a sustainable option. Yet, industrial initiatives have shown that it does not imply to be sustainable as compensation can attract more people, executing deliveries out of their normal routes. Our work introduces a crowdshipping problem optimizing both cost and CO2 emissions in a deterministic approach, referred to as the Bi-Objective Vehicle Routing Problem with Multi types of Occasional Drivers and Time Windows [BO-VRPMODTW]. It generalizes the participation of Occasional Drivers [ODs] using different types of vehicles [fueled and non-fueled]. The compensation policy defined assigns higher compensation to the non-fueled type. An epsilon constraint approach and a Column Generation-based epsilon approach were developed by considering short instances of clustered and random clients. The first experiments exhibit a competitive CPU time obtained by the latter approach. Also, it solves small and medium instances with an optimality gap smaller than 5%. Managerial insights on the impact of OD usage cost and CO2 are provided.",Bi-objective sustainable crowdshipping with multi types of occasional drivers,"[66370, 77143, 75846, 6645]",488,"[112, 145, 32]",2197,Sustainability in Vehicle Routing,19,7,24,Sustainable Supply Chains,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,83 [building - 116],"['Programming, Multi-Objective', 'Vehicle Routing', 'E-Commerce']",TA-24
"We consider a setting of corporate social responsibility [CSR] that suppliers choose responsibility effort and hiding effort, given buyers’ audit levels. Theoretical research indicates a potential backfiring condition where increased audits drive suppliers to prioritize hiding evidence of CSR violations over investing in social responsibility. To validate this prediction, we conduct a behavioural experiment with two treatments to examine suppliers’ decisions in responsibility and hiding efforts under low and high audit levels. Different from the standard model's prediction of significantly higher responsibility efforts in the low-audit treatment compared to the high-audit treatment, experimental results show no significant difference. Additionally, actual hiding efforts by suppliers significantly exceed [fall below] the theoretical value in the low-audit [high-audit] treatment. We build a behavioural model consisting of probability weighting, mental accounting, and bounded rationality. Structural estimation of behavioural parameters shows that probability weighting leads suppliers to increase hiding efforts and reduce responsibility efforts. Conversely, mental accounting causes suppliers to overvalue the additional loss from CSR violations exposure, prompting higher responsibility efforts and lower hiding efforts. These findings highlight the necessity of comprehensively considering the decision biases in supply chain social responsibility management.",Does audit backfire on corporate social responsibility? An experimental study on supplier's behaviours and motivations,"[77140, 23627]",183,"[10, 138]",2198,Behavior in supply chain collaboration,13,14,07,Behavioural OR,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,1019 [building - 202],"['Behavioural OR', 'Supply Chain Management']",WC-07
"Discounting perishable products before they expire, known as markdown pricing, is today's most common and popular reactive food waste mitigation practice among retailers. Product age and price influence customer behavior - While some customers do not buy products that are about to expire, discounted prices may attract new customers who initially did not plan to buy the product. The time of discounting and the applied discount level are crucial decisions. Early discounts can lead to margin losses, while late discounts may generate too little demand. If the price discount is set too high, the remaining stock may be sold quickly and completely, but the margin may be considerably lower. Current literature lacks consideration of different product ages on the shelf and the corresponding behavioral dynamics of customers. We contribute with a simulation study that considers price elastic demand, customers' freshness preferences, and changes in customer behavior when a discount is applied. We conducted several experiments considering different customer preference patterns. Results show that discounts are particularly worthwhile for products where customers prefer fresh products and withdraw them according to last-expired-first-out. Furthermore, customers switching from fresh to old, discounted products contribute to food waste reduction, while the influence on profit depends on the product's initial waste level.",Analysis of Markdown Strategies for Perishable Products,"[71571, 22691]",853,"[100, 131, 130]",2199,Reducing Food Waste,78,15,13,Secure & Sustainable Food Supply,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,15 [building - 116],"['OR in Sustainability', 'Simulation', 'Service Operations']",WD-13
"In semiconductor manufacturing, product reliability and quality are crucial, especially for critical applications such as automotive or robotics. Hence, the outputs of production machines are measured through control operations performed by metrology [measurement] tools. As these tools become increasingly expensive, optimizing their use to minimize the risk is essential .
Risk indicators can be used to prioritize lots in metrology. This work relies on the Wafers at Risk [W@R], which is the number of lots processed on a production machine between two lots that are measured. Each lot selected to be measured induces a gain [risk reduction] on the W@R of each of the production machines on which the lot was processed, and the earlier the lot is measured, the earlier the gains are obtained. In this presentation, we introduce a scheduling problem on parallel machines where the total risk reduction over time is maximized. Compared to classical parallel machine scheduling problems, computing the criterion requires to consider how the W@R of all production machines evolve when lots are measured on all metrology tools. To ensure the synchronization between metrology tools, time-indexed binary variables are required in the mathematical model of the problem. Various solution methods have been proposed, including a column generation approach, a greedy heuristic, and a Greedy Randomized Adaptive Search Procedure [GRASP]. Numerical results will be presented in the conference.

",Scheduling on parallel metrology tools for risk minimization,"[76875, 16790, 16259]",409,"[13, 109, 14]",2203,Manufacturing scheduling with sustainability considerations,35,12,60,Project Management and Scheduling,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S09 [building - 101],"['Column Generation', 'Programming, Integer', 'Combinatorial Optimization']",WA-60
"Kidney exchange programs allow patients with end-stage renal disease to receive a kidney transplant from a living donor. The maximization of the number of transplantations within a pool of potential donors and recipients is a combinatorial optimization problem that has generated abundant literature over the past 20 years. Several authors have pointed out the relevance of the classical literature on stable matchings in this context. However, the optimization of stable kidney exchanges has not been widely investigated. We extend recent work on stable exchanges by introducing and underlining the relevance of a new concept of locally stable exchanges. 
We show that locally stable exchanges in a compatibility digraph are exactly the so-called local kernels of an associated blocking digraph [whereas the stable exchanges are the kernels of the blocking digraph]. We prove that it is NP-hard to determine whether a graph has a nonempty local kernel. Next, we propose several integer programming formulations for computing a locally stable exchange of maximum size. These formulations can be viewed as modeling the maximum local kernel problem in the blocking digraph. We conduct numerical experiments to assess the quality of our formulations and to compare the size of maximum locally stable exchanges with the size of maximum stable exchanges. It turns out that nonempty locally stable exchanges frequently exist in digraphs which do not have any stable exchange.
",Local stability in kidney exchange programs,"[66779, 1344, 24368, 68357]",592,"[56, 14, 53]",2204,Kidney Exchange I,3,9,10,OR in Health Services [ORAHS],"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,11 [building - 116],"['Health Care', 'Combinatorial Optimization', 'Graphs and Networks']",TC-10
"The European and global energy systems need a drastic shift toward low-carbon technologies to achieve deep decarbonization across all sectors. Technological innovation is crucial for facilitating an economically and ecologically viable transition. However, a widespread concern exists that overly optimistic anticipation of emerging low-carbon technologies, i.e., “unicorn technologies”, may delay effective emission reductions today. In this study, we quantify the impact of technology optimism emerging from various forecasting errors, e.g., infinitely fast capacity expansion, on the feasibility and cost of the European energy transition. To this end, we describe the decision-making process as a myopic planner who plans their investment and operation based on a foresight horizon with an overly optimistic technology forecast. Using the linear cost-minimization framework ZEN-garden, we optimize the transition of the sector-coupled European energy system, including the electricity, residential heating, transport, and industry sectors. We investigate the dependency of the energy transition on such “unicorn technologies” and quantify the change in the investment strategy when the decision-maker plans with erroneous forecasts. We find that the overly optimistic forecast leads to a delayed investment in mature low-carbon technologies and, therefore, increased emissions in early years because the decision-maker wrongfully assumes to be able to remove or abate them in the future. ","Bet on horses, not unicorns – The impact of technology optimism on the decarbonization of the European energy system","[77234, 69248]",397,"[37, 93]",2206,Decarbonized energy systems & markets,22,12,09,Energy Markets,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,10 [building - 116],"['Energy Policy and Planning', 'OR in Energy']",WA-09
"Reducing energy consumption has become an important goal in all sectors of the economy and especially in transportation. Our goal is to examine potentials for energy reduction in railway operations.
We propose the Green Locomotive Assignment Problem [GLAP] which aims at minimizing overall energy consumption through efficiently allocating locomotives to scheduled trains. The objective covers all key aspects of locomotive scheduling, with energy consumption during light-traveling trips standing out as the primary cost driver. Light-traveling occurs when a locomotive needs to relocate to a different station and therefore moves without any train attached. The Davis equation, a physical model that factors in the resistance encountered by a moving train, is augmented with track gradients to accurately compute energy consumption during light-traveling trips.
We directly minimize energy consumption and compare this to the conventional approach of minimizing light-traveling distance. Additionally, we propose a model variation of the GLAP, which incorporates the practice of deadheading on trains. Deadheading on trains refers to the attachment of locomotives to trains for repositioning purposes without utilizing their engines and has the potential to reduce energy consumption.
We propose mixed integer programs and test them on small and medium sized real-world instances using historical planning data and detailed gradient information from the Austrian Railway network.",The Green Locomotive Assignment Problem with Deadheading on Trains,"[75050, 75173, 49181, 16919]",630,"[122, 143, 84]",2207,Railway Applications,6,14,56,Transportation,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S04 [building - 101],"['Railway Applications', 'Transportation', 'Optimization Modeling']",WC-56
"We investigate the limitations of existing quantum algorithms to solve discrete optimization problems. First, we discuss the quantum counting algorithm of Brassard et al. [1998], and show that it has performance that is equivalent to that of a brute-force approach when approximating the number of valid solutions. In addition, we show that a straightforward application of Grover's algorithm [referred to as GUM by Creemers and Pérez in an earlier paper] dominates any quantum counting algorithm when verifying whether a valid solution exists. Next, we discuss the nested quantum search algorithm of Cerf et al. [2000], and show that it is dominated by a classical nested search that uses an approach such as GUM to find [partial] solutions to [nested] problems. Last but not least, we also discuss amplitude amplification [a procedure that generalizes Grover's algorithm], and show [once more] that it may not be possible to outperform GUM.",Discrete optimization - Limitations of existing quantum algorithms,"[77673, 19453]",382,"[14, 5, 111]",2208,Quantum Computing for Discrete and Combinatorial Optimization,83,4,42,Quantum Computing Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,98 [building - 306],"['Combinatorial Optimization', 'Algorithms', 'Programming, Mixed-Integer']",MC-42
"Sufficient midwifery staff is crucial to the quality of maternity care. However, the shortage of staff that France has been facing for several decades creates situations of understaffing. In order to determine the optimum number of midwives in a given department, several methods have been presented in the literature, e.g., a 1998 decree that sets out minimum staffing levels, recommendations from scientific societies, or calculation tools like the British BirthRate Plus tool. Yet, we lack evidence on how these models affect quality of care.
We observed the operation of a French public hospital maternity and built a discrete-event simulation model using Simul8 software. The simulation model reproduces the structure, staffing schedules and patient characteristics of the maternity. Because it has been shown that one-to-one care [one midwife for one patient] has a positive impact on service indicators such as readmission rates, stillbirth rates and average length of labour, we used the rate of one-to-one care in labor as our main output. We used design of experiment methods to determine the most influent factors in our model.
Using this model, we analysed the impact of different staffing recommendations on quality of care measured by the rate of one-to-one care. In the future, the model will be enriched with time-and-motion analysis data to provide more accurate estimates of how much time midwives can really devote to patient care in delivery units.
",Evaluating the impact of staffing recommendations in maternity units,"[77399, 30174, 77713, 77712, 77714]",971,"[56, 131]",2210,Simulation models in healthcare,3,4,17,OR in Health Services [ORAHS],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,40 [building - 116],"['Health Care', 'Simulation']",MC-17
"In this work, we introduce a novel interpretation of the variance associated with batch gradient estimates in machine learning model training. By reevaluating this variance, we introduce a novel strategy for the dynamic adjustment of batch sizes throughout the training process. This approach diverges from traditional approaches by prioritizing a more detailed representation of variance behavior, which can lead to more efficient and effective training regimes. We test our methodology against a series of problems, benchmarking it against current state-of-the-art methods to demonstrate its advantages and applicability. ",A Novel Criterion for Batch Size Adaptation in Stochastic Gradient Methods,"[77690, 65800, 10662]",323,"[7, 66]",2211,Data science meets strongly NP-Hard CO ,14,10,03,Data Science Meets Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,1005 [building - 202],"['Analytics and Data Science', 'Machine Learning']",TD-03
"This study introduces an adaptive large neighbourhood search approach for solving path-oriented public transport scheduling with time-dependent travel time and demand data. The decision variables are the departure times of the vehicles from each stop, while the objective function is a weighted sum of passengers’ travel time components costs and a penalty cost for violating the desired arrival time window. The mathematical model was developed by Lee et al. [2022]. Due to the complexity of this model, solving real-world instances on the fly is challenging for exact solution methods. To address this problem, this study proposes an adaptive large neighbourhood search algorithm featuring five destroy-repair operators. Its main novelty is that two operators employ a heuristic to repair the solution, whereas the remaining three utilize a MIP solver. The operators also differ on which part on the solution they destroy; three of them randomly destroy parts of the incumbent solution, and the other two are more likely to target parts of the solution with a higher contribution to the objective function. The algorithm is implemented and tested on real data from the public transport system of the metropolitan area of Copenhagen, Denmark. Benchmarked against a MIP solver, our solution method outperforms it in all instances. On average, it achieves 7.5% to 17.5% better results in the three largest instances. Our algorithm is also much more stable.",Optimizing Public Transport Timetables - Adaptive Large Neighbourhood Search with Heuristic and Exact Repair Operators,"[77269, 77709, 77053, 74502]",823,"[119, 142, 74]",2212,Timetabling 1,85,14,51,Public Transport Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,M5 [building - 101],"['Public Local Transportation Systems', 'Timetabling', 'Metaheuristics']",WC-51
"This talk will introduce a novel hub location problem with a mixed fleet of diesel-based conventional vehicles and alternative fuel vehicles including electric vehicles and unmanned aerial vehicles. Compared to diesel-based vehicles, the driving range of alternative fuel vehicles in terms of travel distance is more limited. In addition to the driving range, the proposed problem also captures the capacity limitations of these vehicles. The objective function of the problem minimizes the total cost including the fixed cost of locating hubs and the transportation cost of different vehicle types. The latter cost component consists of the recharging cost associated with energy [fuel] consumption depending on the traveled distance, the recharging fee costs depending on the number of visits to recharging stations, and also the waiting cost for drivers and vehicles due to the recharging time of vehicles. We develop a mixed-integer linear programming formulation with preprocessing rules. Different extensions of the problem including a multiple allocation version and solution methods will be discussed. The talk will present computational results over the CAB, TR, and the newly introduced German datasets to analyze the impact of a mixed fleet of range and capacity-limited vehicles on optimal hub location and allocation decisions.",Hub location problem with a mixed green fleet,"[57071, 9272, 7178]",582,"[64, 79, 111]",2213,Hub Location,29,4,61,Locational Analysis,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S10 [building - 101],"['Location', 'Network Design', 'Programming, Mixed-Integer']",MC-61
"Operating rooms are one of the most expensive resources of a hospital, which makes it important to use them as efficiently as possible. This involves maximizing utilization, but also minimizing the probability of overtime, which are two conflicting objectives. The problem of scheduling surgeries is usually modelled as an integer linear program [ILP], where maximizing utilization is considered as objective, while restricting the probability of overtime. The constraint used to restrict the probability of overtime includes the lognormally distributed surgery duration, which typically leads to a nonlinear constraint where the total surgery duration in one operating room is a stochastic variable. 
In the literature, various ways to deal with this nonlinearity are considered - 1] only use the expected surgery duration and limit the total surgery duration, 2] model the uncertain surgery duration using a normal distribution and linearize the resulting constraint using a piecewise linear function, 3] model the uncertain surgery duration using a discrete probability distribution resulting in a linear constraint, and 4] model the uncertain surgery duration using the lognormal distribution and use column generation and simulation to solve the resulting model.
We implemented and compared all mentioned methods on both solution quality and computation time. To determine the solution quality, we tested the resulting schedules using simulation. 
",Dealing with uncertainty in surgery duration,[31214],599,"[56, 109, 131]",2215,Surgery Scheduling and Operating Room Planning,3,8,10,OR in Health Services [ORAHS],"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,11 [building - 116],"['Health Care', 'Programming, Integer', 'Simulation']",TB-10
"In this work, we study a simple example of a two-stage
linear-quadratic dynamic game in which the presence of simple
linear state dependent constraints results in nonexistence of continuous symmetric feedback Nash equilibria and the existence of
continuum of discontinuous symmetric feedback Nash equilibria.
The example is not an abstract model—it has obvious applications
in economics of resource extraction.",Discontinuous Nash Equilibria in a Two-Stage Linear-Quadratic Dynamic Game With Linear Constraints,"[77698, 52070]",527,"[114, 81, 95]",2219,Nonsmooth optimization algorithms I,70,14,41,Nonsmooth Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,97 [building - 306],"['Programming, Quadratic', 'Non-smooth Optimization', 'OR in Fisheries']",WC-41
"This study examines the estimate of surrender probabilities with explanatory variables using a Beta Binomial Generalized Linear Model for Location, Scale, and Shape [BBGAMLSS] and introduces an algorithm for the update of surrender probabilities by future observations. The Binomial Generalized linear Model is widely used in actuarial practice and literature to predict surrender probabilities per policy count depending on the policy, policyholder, and economic variables. We propose an alternative regressive model with a response variable assumed to be Beta Binomial. In contrast to a binomial, a discrete random variable called a Beta Binomial has a beta distributed probability of success at each of a certain number of trials rather than a fixed probability. The surrender is a binomial phenomenon where the probability of success is not fixed, because it depends on many variables that cannot be considered simultaneously in a single regressive model, without introducing overparametrization. Lastly, the Beta Binomial random variable has a Bayesian interpretation, because the beta is a conjugate prior to the Beta Binomial. Then, it is possible to define an algorithm that allows updating the estimates of conditional surrender probabilities by the future observations of surrender rates avoiding the estimate of new parameters every year.",A beta binomial approach for the estimate of surrender rates by GAMLSS,"[65897, 67461]",189,"[59, 5, 83]",2220,Insurance Risk Management,4,3,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S14 [building - 101],"['Industrial Optimization', 'Algorithms', 'Optimization in Financial Mathematics']",MB-63
"Consumers often see buy-now-pay-later [BNPL] as a convenient payment mechanism allowing them to spread the purchase cost over an interest-free period. Many consumers do not realise it is a form of credit and may result in late charges being incurred. This lack of understanding is one of the contributing factors that has led many BNPL users to overspend and accumulate debt. This project aims to transform our understanding of how BNPL impacts consumers’ spending behaviour and inform information provision practices so that consumers can make better informed decisions. A series of experiments will be conducted to evaluate how BNPL should be presented so that consumers are given appropriate and adequate information prior to instigating a contract. The results will inform financial regulators as they introduce regulation on BNPL offers, whilst helping protect consumers and ensure that BNPL is sustainable. ",Unveiling the credit nature of Buy-Now-Pay-Later,[28653],567,"[10, 25]",2222,Behaviour and decision support,13,10,07,Behavioural OR,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,1019 [building - 202],"['Behavioural OR', 'Decision Analysis']",TD-07
"The expansion of renewable energy generation increases the need for short-term reserve facilities to compensate for their short-term variations. This makes reserve markets increasingly profitable and attractive for renewable energy producers [REPs], who are facing diminishing subsidies and returns. We study this problem by formulating the operation of a profit-maximizing REP, with and without storage, providing reserve services as a multi-stage stochastic integer program, separately with the support of discrete [D-ID] or continuous [C-ID] markets. We combine the Benders decomposition and stochastic dual dynamic programming algorithm [SDDP] to solve the problem efficiently.  Our analysis of real data from the German market provides interesting insights into REPs' participation in short-term reserve markets.",Delivering Reliable Reserve Commitments from Intermittent Electricity Resources,"[70846, 72924, 56298, 40136, 64212, 10530]",448,"[36, 136, 93]",2223,Decentralized multi-energy markets,22,7,09,Energy Markets,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,10 [building - 116],"['Electricity Markets', 'Stochastic Optimization', 'OR in Energy']",TA-09
"In this presentation, we are dealing with the Cumulative Scheduling Problems[CuSP]. In the CuSP-decision, we are given a set I of n non-preemptive tasks, meaning that once a task begins execution, it cannot be interrupted until completion. A constant amount m of a given resource is available over the time horizon to process the tasks. Each task i has a  release date r_i, a processing time p_i, and a deadline d_i. The task i must be executed within its time window [r_i, d_i], and throughout its execution it requires a constant known amount c_i of resources. The objective of this decision problem is to detect infeasibility. Until now, for this problem, the most efficient algorithm is based on energy reasoning. We have already proposed a more powerful energetic reasoning by solving a tripartition problem and built a checker by using a Dynamic Programming method. 
In the CuSP-optimization, the deadline d_i is replaced by the tail q_i which is equal to C_max-d_i and the objective is to find the minimal makespan C_max. The energetic lower bound corresponds to the minimal value for which infeasibility may not be detected. We can solve this problem by using the checker already proposed for the CuSP-decision. We also improved the efficiency of the method by introducing redundant resources. Computation results confirm the efficiency of the proposed methods.
",More powerful energetic reasoning using redundant resources for the Cumulative Scheduling Problems,"[77635, 66786, 36336, 74050]",347,"[129, 108, 16]",2224,Advanced heuristics for machine scheduling,35,9,60,Project Management and Scheduling,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S09 [building - 101],"['Scheduling', 'Programming, Dynamic', 'Complexity and Approximation']",TC-60
"Modern societies are developing an ever-increasing concern toward issues related to equity, often synonymously called fairness. In location problems, scholars soon realized the importance of incorporating equity in siting decision models. Often a decision-maker has to locate a set of facilities, which provide a service [or good] and are reached by customers at their own costs. In such situations, improving system efficiency requires minimizing the average cost paid by customers to reach a facility, while improving fairness requires minimizing the variability in the cost distribution. The latter is especially relevant in the public sector, where providing customers with a fair access is a crucial concern. We measure accessibility fairness by using the conditional beta-mean, which can be intuitively defined as the average distance traveled by beta% of the customers that travel the longest distances to reach the assigned facility. We show that incorporating such measure into a classic multi-source CFLP leads to a non-linear MIP. Borrowing some ideas from bilevel programming, we propose two linear reformulations of the latter MIP, and develop two solution methods that work along the general lines of a cutting-plane algorithm. Computational experiments show the effectiveness of the solution methods proposed. Further, a trade-off analysis between efficiency and fairness show that the proposed approaches may be used as managerial tools to balance these two objectives.",Solution approaches for multi-source capacitated facility location problems with fairness objectives,"[12473, 27108, 1090]",222,"[64, 111, 5]",2225,Applications to Logistics and Transportation,64,7,26,Combinatorial Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,012 [building - 208],"['Location', 'Programming, Mixed-Integer', 'Algorithms']",TA-26
"In this talk we focus mainly on linear programming problems, and particularly on Lipschitz-type properties of the feasible set mapping, the optimal value function, and the optimal set [argmin] mapping. Roughly speaking, we aim to compute or estimate the rate of variation of feasible/optimal solutions with respect to the problem's data perturbations. Some of these properties are local [as Aubin property and calmness], as far as they concentrate around a certain solution nearby a given parameter. Some other properties [such as Hoffman stability] are of a global nature, since they tackle global variations of the whole solution set. We emphasize the fact that the quantitative stability measures provided in this talk are mainly point-based; thus they are conceptually implementable in practice.",Measuring the stability. A paradigmatic problen in optimization,[3641],295,"[21, 110, 72]",2232,Variational Methods in Vector Optimization,82,8,42,Variational Analysis and Continuous Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,98 [building - 306],"['Convex Optimization', 'Programming, Linear', 'Mathematical Programming']",TB-42
"In attended home delivery, retailers usually let customers select a delivery time slot for receiving their orders. The delivery capacity, i.e., the number of vehicles and drivers, is often fixed and inflexible in the short term. To effectively use their delivery capacity, e-grocers may dynamically close time slots for certain new customers given the already accepted customer orders. One complicating factor is that for example online grocers allow customers to change their order basket at any time before the cut-off. Consequently, the e-grocer is uncertain about how much vehicle capacity should be reserved for each of the customers during the booking process. We study the challenges that arise with this order size uncertainty and propose strategies to deal with it. For each of the time slots to be evaluated, we use machine learning to quickly predict the probability that a feasible route plan exists that visits all accepted customers in their selected time slot.",Feasibility Prediction in Attended Home Delivery with Uncertainty - A Machine Learning Approach,"[67957, 9140, 27939]",868,"[66, 143, 32]",2233,Combinatorial Optimization models and applications in Logistics and Transportation II,64,3,29,Combinatorial Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,157 [building - 208],"['Machine Learning', 'Transportation', 'E-Commerce']",MB-29
"Modern technologies make it possible to collect and analyze large amounts of educational data. Learning Analytics [LA] uses this data to improve learning. To deliver LA insights to the educational stakeholders, Learning Analytics Dashboards [LADs] aggregate learning metrics into a single screen using a variety of visualization techniques.
A learner must be provided with a frame of reference for proper data interpretation, with peers being the most popular choice in modern LADs. However, it carries the risk of negatively impacting learners by encouraging competition rather than mastery of knowledge, thereby widening the gap between high-performing and struggling learners. This problem is exacerbated in conceptual modeling education, where mastery orientation is crucial to learn to solve complex task of modeling.
This research presents a student-facing LAD, grounded in achievement goal orientation theory, with the primary goal of supporting mastery orientation [task completion/understanding]. The LAD also supports performance goal orientation [social comparison] through an optional peer comparison feature. By monitoring dashboard usage, student activity, and academic performance, our study investigates [RQ1] the added value of the LAD to student activity and performance in conceptual modeling education, [RQ2] the relationship between dashboard usage and learning orientation, and [RQ3] the added value of the performance-oriented component to student activity and performance.",Where am I in the course? Student-facing dashboard for Conceptual Modeling Education,"[71140, 77716, 77717, 10234, 47565]",149,"[7, 34, 92]",2234,Learning Analytics using Mathematical Optimization and XAI,15,12,27,Mathematical Optimization for XAI,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,047 [building - 208],"['Analytics and Data Science', 'Education and Distance Learning', 'OR in Education']",WA-27
"Transporting valuable or dangerous goods may require constant surveillance to prevent potential thefts or incidents. Such supervision can be carried out by installing cameras or hiring security guards, which entails a monetary investment. In this work, we introduce the Covering Arc Routing Problem [CovARP], that combines arc routing and location problems. Let us consider an undirected graph with a cost associated with the traversal of each edge and a subset of edges [required edges] that must be traversed to perform some kind of service. Additionally, there is a set of potential monitors that can be activated or not. Each monitor has an activation cost and covers a certain set of edges if it is activated. The CovARP consists of both finding a closed path [tour] that traverses all the required edges at least once and determining a set of monitors that should be activated, so that all the edges traversed by the tour are covered by at least one active monitor, while minimizing the sum of the tour cost and the activation cost of the monitors. We present an integer linear formulation for this problem and a branch-and-cut algorithm. This algorithm has been tested on a randomly generated set of instances. We have analyzed three different situations - when the activation cost of the monitors is clearly higher than that of the tour, when the cost of the tour outweighs the activation cost of the monitors, and the balanced case in which both costs are comparable.",The Covering Arc Routing Problem,"[9794, 1174, 52056, 1758]",743,"[145, 64, 11]",2236,Arc Routing,5,8,64,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S16 [building - 101],"['Vehicle Routing', 'Location', 'Branch and Cut']",TB-64
"This paper addresses a realistic home health care and home care problem which has become increasingly complex in the face of demographic aging and post-COVID-19 disruptions. The HHC&HC sector nowadays faces unique challenges in efficiently scheduling and routing caregivers to meet the rising demand. Traditional approaches often fall short in addressing the dynamic nature of care requests, especially in accommodating new, same-day service requests without compromising scheduled visits. To tackle these issues, We define the problem as an HHC&HC routing and rescheduling problem with rejection of new customers, focusing on rescheduling for a single HHC&HC caregiver in response to new customer requests within a single period. A mixed integer linear programming model is developed to cater to two groups of customers - pre-scheduled existing customers and same-day service new customers. The model emphasized maintaining minimal disruptions to the original schedule for existing customers as a constraint, highlighting the balance between adhering to scheduled visits and accommodating new customers. A hybrid memetic-Adaptive Neighborhood Search optimization algorithm is proposed to  tackle the model. This approach aims to minimize operational costs and opportunity costs while enhancing service quality and patient satisfaction. Through computational experiments, our proposed algorithm demonstrates notable performance, offering significant improvements in both efficiency and robustness.",A hybrid memetic-ANS optimization algorithm for the home health care and home care routing and rescheduling problem,[77697],953,"[56, 129, 145]",2238,Healthcare logistics and routing,3,5,10,OR in Health Services [ORAHS],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,11 [building - 116],"['Health Care', 'Scheduling', 'Vehicle Routing']",MD-10
"Rapid global warming and declining ice are making Arctic routes more navigable. These routes can reduce shipping time between Europe and Asia, offering economic and environmental benefits by lowering fuel consumption and associated emissions. The main uncertainty in the planning of the routes stems from dynamically changing ice conditions. In this paper, we formulate a risk-averse stochastic shortest path problem for a vessel sailing in the Arctic Ocean. It estimates an optimal vessel path before the departure, without knowing the exact realization of ice conditions. The objective is to minimize total travel time under conditional value at risk, considering the performance in the worst scenarios. The sailing speed [and hence the travel time] depends on the ice conditions, where a ship can sail at normal service speed in ice-free water, while the speed must be decreased in unfavorable ice conditions. The model is tested on historical and projected ice data to estimate transit routes between Europe and Asia via the Arctic Ocean. We are interested in obtaining estimates of travel times between Europe and Asia and evaluating possible delays to gain insights into the future of Arctic Shipping.",Navigating Arctic Waters - A Risk-Averse Stochastic Shortest Path Problem,"[76944, 62559, 6946]",445,"[65, 136, 143]",2242,Supply Chain Network Optimization,6,15,55,Transportation,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S02 [building - 101],"['Logistics', 'Stochastic Optimization', 'Transportation']",WD-55
"The COVID-19 pandemic dramatically affected global tourism, especially South African tour operating Small, Medium, and Micro Enterprises [SMMEs]. These enterprises are now leveraging machine learning [ML] to make informed decisions using predictive models. However, choosing the right ML technique is challenging for these operators due to their limited technical expertise. This study compares shallow and deep learning methods for predicting tour operator performance, using a dataset from 650 South African SMMEs, spanning 2019-2021. The enhanced radial basis function [K4-RANN], a shallow learning model, outperformed deep learning models, showing better prediction accuracy with lower error rates and higher R² values. Therefore, the study recommend that tourism SMMEs adopt the K4-RANN model for more accurate booking and revenue projections, thereby improving decision-making and efficiency post the Covid-19 pandemic era. ",Optimizing post-pandemic recovery - a comparative analysis of machine learning techniques for tourism SMMEs revenue predictive modelling,"[70777, 11548, 25951, 77672]",57,"[66, 28, 7]",2243,AI and ESG for the small economy SDG agenda [EWG-ORD Workshop 2],67,10,18,OR for Development and Developing Countries,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,42 [building - 116],"['Machine Learning', 'Developing Countries', 'Analytics and Data Science']",TD-18
"In automated container terminals, automated guided vehicles [AGVs] are used to transport containers from the quayside to their location in the storage area. AGVs navigate between key terminal locations, following predefined guide paths. They have bidirectional movement capabilities that enable forward and backward travel, and their movements remain predominantly linear, aligning with the terminal’s layout. Limited buffer space at pickup and drop-off points causes AGVs to wait until ongoing operations conclude, creating delays. Delays may also occur when a quay crane must wait for an AGV. Therefore, minimizing both the delay times of QCs and the travel time of AGVs is vital for reducing vessel dwell time and enhancing operational efficiency. AGV conflicts, like deadlocks and collisions, worsen delays, causing congestion and decreased throughput. Implementing advanced routing algorithms and control systems is thereby essential to optimize AGV movements and enhance operational efficiency. This work presents a time-space network formulation for the problem scheduling and routing of AGVs in container terminals where conflict avoidance constraints are explicitly modeled. The problem’s objective is to minimize the vessel’s dwell time while ensuring the smooth flow of container handling operations. A solution method that combines a mathematical formulation and an insertion heuristic is implemented. Testing and validation are conducted using real data from Norwegian port cases.",Scheduling and conflict-free routing of automated guided vehicles in container terminals,"[61225, 57352, 6946]",669,"[65, 72, 70]",2244,Seaside Planning I,52,2,62,OR in Port Operations,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S12 [building - 101],"['Logistics', 'Mathematical Programming', 'Maritime applications']",MA-62
"This work investigates the flow shop scheduling problem with setup requirements and additional resources for setups from a multi-objective perspective. The setups required on machines after processing a job depend on the sequence, as well as the resources needed to perform them. The two objectives studied are minimizing the makespan and minimizing the additional resources used. Exact algorithms and metaheuristics are proposed to solve the problem. To evaluate the resolution methods, sets of non-dominated solutions obtained by each method are compared.",Multiobjective flowshop with additional resources during setups,"[59017, 13353, 5876]",834,"[129, 74, 77]",2245,Flow shop scheduling problems,32,13,49,"Lot Sizing, Lot Scheduling and Production Planning","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,M1 [building - 101],"['Scheduling', 'Metaheuristics', 'Multi-Objective Decision Making']",WB-49
"The constant growth of e-commerce challenges last mile delivery service providers to find cost-efficient route plans that fulfill the demand of thousands of customers daily. To facilitate problem abstraction, a variety of distribution or collection problems as postal delivery or garbage collection are modelled as arc routing problems. Compared to dense urban areas, where an aggregated view on arcs instead of nodes is straight forward for reducing the complexity of the problem, the situation in rural or suburban areas is different. Aggregation of nodes along a long street segment might cause a depletion of the route quality. We are investigating the possibility of various node aggregation heuristics for such situations, which transforms a large-scale vehicle routing problem [VRP] into a smaller capacitated general routing problem [CGRP], where demand might be located on nodes, edges, or arcs. Several mathematical problem formulations for the latter are analyzed and evaluated regarding their solution performance compared to standard VRP models. The goal is to specify the trade-off between the level of aggregation, the loss of solution quality and the decrease in computational time.",Analyzing the Benefits of Node Aggregations in Vehicle Routing Problems,"[71666, 13889]",785,"[145, 65, 111]",2247,Heuristics for Vehicle Routing 2,5,15,64,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S16 [building - 101],"['Vehicle Routing', 'Logistics', 'Programming, Mixed-Integer']",WD-64
"The performance of a Branch-and-Cut algorithm lies, among other things, in the ability of selecting a given number of available cuts to be added to the current LP relaxation, in order get to tight bounds. Clearly, adding many cuts the best bound improvement can be achieved, but, as a counterpart, also the time to solve the LP sensibly increases.  Moreover, it is possible that only a fraction of the added cuts really contributes to the bound improvement, the remaining being not tight. This is to say the cut selection is a very challenging task. Modern ILP solvers, like CPLEX, GUROBI, XPRESS, MOSEK adopt greedy rules. Cuts are first ranked with a score function; then, cuts with the highest scores are added to the LP relaxation. In general, a cut score is a combination of well-established measures of cut quality, e.g. violation, parallelism, etc.  
Adopting as cut score the bound improvement, in this talk we show how a regression model can be learned to predict the cut scores for unseen ILP instances. Preliminary results and future research directions are discussed.
",A regression model for selecting effective cuts in a Branch-and-Cut algorithm,"[23987, 12727]",731,"[109, 11]",2251,	[Deep] Reinforcement Learning for Combinatorial Optimization 2,14,5,03,Data Science Meets Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,1005 [building - 202],"['Programming, Integer', 'Branch and Cut']",MD-03
"In this paper, we propose optimization models to support the design of an application deployed on the Cloud. The models aim to minimize the total deployment cost subject to constraints on the application quality of service [QoS]. We illustrate how this problem, initially modeled as a non-linear mixed integer problem, can be reformulated with an outer linear approximation, which allows us to exploit efficient solvers. Extensive experimentation demonstrates the effectiveness of the proposed approach in instances based on real data from cloud service providers",Optimization model for the provisioning for geo-distributed cloud applications,"[50651, 77721]",163,"[84, 86, 109]",2252,Applications of conic optimization,68,5,38,"Conic Optimization - Theory, Algorithms, and Applications","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,34 [building - 306],"['Optimization Modeling', 'OR and the Internet', 'Programming, Integer']",MD-38
"This article presents a generalized method for multi-dimensional optimal stopping, tailored to problems that arise in electricity markets, when addressing decisions under uncertainty. Electricity markets are highly transparent, and considerable research is available that is dealing with the impact of the ongoing energy transition. However, this class of problems does not fit well to established methods for optimal stopping, and therefore, we propose an alternative, generalized approach.
We derive a general form of the Hamilton-Jacobi-Bellman equation and set up an example which represents the retrofit of an electrolyzer to an offshore wind farm under the conditions of a transitioning day-ahead market for electricity. We demonstrate the functionality of this approach by solving the optimal stopping problem numerically and show that the method supports decision making well on such an irreversible investment under uncertainty.",Generalized Optimal Stopping for Decision Making in Energy Markets,"[77715, 21108]",92,"[108, 27, 36]",2253,Real Option Analysis,8,15,57,Real Option Analysis,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S06 [building - 101],"['Programming, Dynamic', 'Decision Theory', 'Electricity Markets']",WD-57
"Regulatory requirements and market demand necessitate that companies accurately report on sustainability, monitor risks, and undertake actions to reduce emissions and improve ethical performance [SEC, 2022; CSR Germany, 2023; European Commission, 2023; Khan et al., 2021]. This can be a daunting task in a complex global economy. High-level strategies and highly detailed emission reduction initiatives have been suggested [Bressanelli et al., 2022; Preut et al., 2021; Izmirli et al., 2021; Andersen and Jaeger, 2021; Bogers, 2011; Puertas et al., 2008; Frias et al., 2015; Karell and Niinimaki, 2020; Akadiri and Olomolaiye, 2012]. However, addressing this challenge requires scalable solutions. This paper shows how the Bill-of-Materials [BoM], as the skeleton of a company’s operations and supply chain, can be an instrument for such scalability. By enhancing the BoM with the emissions and activity data needed to guide these important initiatives, companies can improve accurate and continuous reporting. The enhanced BoM graphs are used for emission optimization and product carbon footprint calculation, and guide business decisions about the focus of emission reduction efforts. Utilizing network analysis techniques, the study demonstrates the effectiveness of an enhanced BoM in scaling sustainable initiatives and complying with regulations, thereby bridging the gap between high-level sustainability strategies and detailed emission reduction initiatives.
",Capitalizing on the Bill-of-Materials to efficiently respond to sustainability challenges,"[76808, 11802, 1024]",546,"[100, 53, 39]",2254,Sustainable Operations,19,5,24,Sustainable Supply Chains,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,83 [building - 116],"['OR in Sustainability', 'Graphs and Networks', 'Enterprise Resource Planning Systems']",MD-24
"Prospect stochastic dominance conditions can be used to compare pairs of uncertain decision alternatives when the decision makers' choice behavior is characterized by cumulative prospect theory, but their preferences are not precisely specified. This paper extends the use of prospect stochastic dominance conditions to decision settings in which the use of pairwise comparisons is not possible due to the high or possibly infinite number of decision alternatives. In particular, we first establish equivalence results between these conditions and the existence of solutions to a specific system of linear inequalities. We then utilize these results to develop stochastic optimization models whose feasible solutions are guaranteed to dominate a specified benchmark distribution. These models can be used to identify if there exists a decision alternative within a set that is preferred to a given benchmark by all decision makers with an S-shaped value function and a pair of inverse S-shaped probability weighting functions. As such the models offer a flexible tool to conduct behavioral analyses in decision settings such as portfolio selection, procurement optimization or inventory management. We demonstrate the application of these models using real return data on industry portfolios.

",Optimization Models for Cumulative Prospect Theory under Incomplete Preference Information,"[7118, 57704]",570,"[10, 25, 111]",2255,Behavioral Decision Analysis III,13,5,11,Behavioural OR,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,12 [building - 116],"['Behavioural OR', 'Decision Analysis', 'Programming, Mixed-Integer']",MD-11
"The Chinese Postman Problem [CPP] is a classical arc routing problem whose objective is to find a minimum cost tour on a connected undirected graph G that traverses each edge of G at least once. An extension of this problem introduced in the literature is the Chinese Postman Problem with load-dependent costs [CPP-LC], in which the cost of traversing an edge depends on its length and also on the weight of the vehicle at the moment the edge is traversed. This is motivated by the intention of reducing pollution in transportation, since the level of emissions from a vehicle is influenced by factors beyond the distance traveled, such as its load. Two mathematical programming formulations and two metaheuristic algorithms have been proposed in the literature for the CPP-LC solution. We present in this work a new mixed-integer linear programming formulation for the CPP-LC, and propose some valid inequalities to reinforce it. We also develop a study of the incompatibilities of a subset of variables from the formulation and describe all the cliques of the underlying intersection graph, categorizing them into five families of inequalities that are valid for the problem. We design a branch-and-cut algorithm for the CPP-LC solution based on this formulation that incorporates the separation of all the valid inequalities proposed. Some computational results obtained with our new exact procedure are presented and compared with those provided in the literature.",On the Chinese Postman Problem With Load-Dependent Costs,"[77402, 9794, 1758]",743,"[143, 111, 11]",2256,Arc Routing,5,8,64,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S16 [building - 101],"['Transportation', 'Programming, Mixed-Integer', 'Branch and Cut']",TB-64
"Decisions that involve multiple attributes are commonly supported by decision analysis methods deploying a multi-attribute value/utility function, which captures decision-makers’ preferences on multiple decision attributes and their risk attitude. Often, there are uncertainties related to the evaluation of the decision alternatives' attribute-specific performances. In the context of single-attribute problems, these uncertainties cause the ex post realized performances of the selected alternatives to be systematically lower than the ex ante estimates, causing the decision-maker to experience post-decision disappointment. We study the impact of attribute-specific estimation uncertainties and nonlinear utility functions on post-decision surprises and the quality of decision making in a multiattribute setting. In particular, we present both analytical and simulation results to analyze the performance of several selection approaches by examining whether these approaches lead to the optimal choice that maximizes the expected multi-attribute utility and whether these choices correspond to the truly optimal choices that maximize the ex post true multi-attribute utility. Furthermore, we demonstrate the impacts of these selection approaches in a realistic example on choosing a contractor for a construction project.",Poor choices and post-decision surprises - the impact of evaluation uncertainties in multiattribute decision-making,"[23949, 65552, 7118, 31731]",557,"[10, 25, 77]",2259,Behavioral Decision Analysis II,13,4,11,Behavioural OR,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,12 [building - 116],"['Behavioural OR', 'Decision Analysis', 'Multi-Objective Decision Making']",MC-11
"Conventional last-mile delivery logistical systems remain overburdened despite large-scale optimization. This paper explores the usage of drones to complement road-based last-mile delivery systems to ensure more time-cost efficiency. The work also extends the truck shipment routes to the case of real-time multiple routes. A bi-objective mixed integer linear program is introduced, accompanied by an exact solution methodology utilizing multi-choice conic goal programming. This approach efficiently solves smaller-sized problems. For larger instances, a three-step iterative heuristics is proposed. Exhaustive computational analysis and an illustrative case suggest that substantial savings in both time and cost can be achieved compared to traditional frameworks.",Drone-Assisted Delivery Optimization - Balancing Time and Cost With Multiple Truck Routes for Efficient Service,"[76767, 38681]",781,"[65, 77, 84]",2260,Routing Unmanned Aerial Vehicles 2,5,4,64,VeRoLog - Vehicle Routing and Logistics,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S16 [building - 101],"['Logistics', 'Multi-Objective Decision Making', 'Optimization Modeling']",MC-64
"Overcrowding within emergency departments [EDs] remains a persistent challenge facing healthcare systems globally. A key contributor to this issue is the subgroup of patients known as frequent attenders [FAs] – individuals who repeatedly visit the ED over short periods of time. While the negative impacts of FAs are well-documented, including increased healthcare costs and strain on limited resources, early identification of this vulnerable population enables getting them access to the right care and support sooner. However, defining and identifying this patient population lacks standardisation. Our research introduces an innovative, data-driven mathematical methodology to objectively define and detect FAs based on time intervals between ED visits rather than arbitrary frequency counts. We introduce a two-stage k-means clustering method to determine distinct patterns in the time intervals between ED visits for each patient. This study introduces a standardised framework for defining ED FAs, providing a consistent and effective means of identification across different EDs. As patient groups differ across health systems, the methodology allows for tailoring the FA definition to the unique characteristics of each ED population. Furthermore, the methodology can be used to identify patients who are at risk of becoming a FA, allowing targeted interventions aimed at reducing the number of future attendances. We apply this methodology to a case study of ED attendances in Wales, UK.",Mathematical methodology for defining a frequent attender within Emergency Departments,"[66572, 65156]",611,"[56, 0]",2261,Machine learning and game theory in healthcare,3,2,15,OR in Health Services [ORAHS],"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,18 [building - 116],['Health Care'],MA-15
"We will provide an overview about current modeling and analysis relevant to the financial, biological and ESG related risks in the optimal management of aquaculture businesses with partiuclar focus on the Norwegian salmon farming industry. These rsults are based on the following research papers:

Ewald, C.O. and Kamm, K. [2024] On the Impact of Biological Risk in Aquaculture Valuation and Decision Making. https://doi.org/10.48550/arXiv.2402.08686

Ewald, C.O. and Kamm, K. [2024] On the impact of feeding cost risk in aquaculturevaluation and decision making. Accepted and Published Online Quantitative Finance

Ewald, C.O. and Zou, Y. [2021] Analytic formulas for futures and options for a linearquadratic jump diffusion model with seasonal stochastic volatility and convenience yield - do fish jump? European Journal of Operational Research, 294[12]

Ewald, C.O. and Ouyang, R. [2017] An analysis of the fish pool market in the contextof seasonality and stochastic convenience yield. Marine Resource Economics, 32[4], pp.431-449.

Ewald, C.O. , Ouyang, R. and Siu, T. K. [2017] On the market consistent valuation of fish farms - using the real option approach and salmon futures. American Journal ofAgricultural Economics, 99[1], pp. 207-224.as well as research that is currently in development.",Risk Management for Aquaculture Businesses in the Presence of Multiple Risk Sources,[77724],190,"[45, 78, 33]",2263,Natural Resource Management and Commodity Markets,4,4,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S14 [building - 101],"['Financial Modelling', 'Natural Resources', 'Economic Modeling']",MC-63
"To study the impacts of additive manufacturing [AM] in the circular economy [CE], we develop a causal loop diagram [CLD] that consists of contingent factors which are detected in a structured literature review by systematic analyses of content and contingencies. The CLD is developed further to a simulation model to analyze the system behavior and to investigate core factors of AM and their roles and interrelationships in CE contexts. The findings inform about the [in-]stability of the system and how it can be influenced.",Additive manufacturing in the circular economy - a simulation approach,"[47085, 68151, 43371]",927,"[131, 69, 138]",2268,New technology for sustainable supply chains,18,14,23,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,82 [building - 116],"['Simulation', 'Manufacturing', 'Supply Chain Management']",WC-23
"For a given time minimizing transportation problem comprising of m sources and n destinations, the set of m sources is to be optimally partitioned into two mutually disjoint subsets L1 and L2 where, L1 contains m1 sources called Level-I sources and L2 contains the remaining [m − m1] sources termed as Level-II sources. First, the Level-I decision maker sends the shipment from Level-I sources to partially meet the demand of destinations. Later, Level-II decision maker sends the material from the Level-II sources to meet the left over demand of the destinations. A finite number of mixed 0-1 programming problems are solved to judiciously generate a subset of partitions of the set of m sources. The aim of this study is to find an optimal partition of the set of m sources such that the sum of times of transportation in the Level-I and Level-II shipments is the minimum. The proposed algorithm to find the global minimizer has been successfully run on a variety of randomly generated test problems differing in input data.",Two Level Time Minimizing Transportation Problem,"[38681, 58450]",329,"[143, 52, 72]",2269,ML & OR Applications in Transport Modelling,6,7,55,Transportation,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S02 [building - 101],"['Transportation', 'Global Optimization', 'Mathematical Programming']",TA-55
"As household plastic products have become essential parts of our lives, increasing the plastic recycling rate is imperative. However, the global average recycling rate for plastic is remarkably low at 9%, and the low profitability of uncertain waste management is one of the major reasons. In this study, we developed a data-driven robust optimization model of closed-loop supply chain design to address the uncertain volume of plastic waste. Subsequently, we proposed a theorem capable of reformulating the worst-case of the uncertain public sectors model as a tractable model. Based on this proposed theorem, we have constructed a data driven robust bilevel optimization program to express decentralized decision-making between a manufacturer and public sectors. Conducting sensitivity analysis using real annual operational data, we confirmed that if public sectors adopt a conservative solution regarding uncertain plastic waste volume, it can lead to a decrease in manufacturer profit and a reduction in the plastic recycling rate. Furthermore, we found that if the manufacturer is concerned about fluctuations in the purchasing price for plastic waste resulting from the conservative solution of public sectors, the manufacturer may attempt to purchase plastic waste from various regions.",Data-driven robust optimization of closed-loop supply chain design for household plastic waste ,"[64881, 50235]",922,"[138, 127, 50]",2270,Supply chain design in the circular economy,18,8,23,"Circular Economy, Remanufacturing and Recycling ","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,82 [building - 116],"['Supply Chain Management', 'Robust Optimization', 'Game Theory']",TB-23
"Conic optimization has recently emerged as a powerful tool for designing tractable and guaranteed algorithms for non-convex polynomial optimization problems. On the one hand, tractability is crucial for efficiently solving large-scale problems and, on the other hand, strong bounds are needed to ensure high-quality solutions. In this research, we consider constraints based on linear, second-order cone, and semidefinite programming and study their impact when used to tighten the baseline relaxations of a given global optimization algorithm. We describe how to design these conic constraints and their performance with respect to each other and with respect to the standard RLT relaxations for polynomial optimization problems. Our finding is that the different variants of nonlinear constraints [second-order cone and semidefinite] are the best performing ones in around $50\%$ of the instances in widely used test sets. Additionally, we present a machine learning approach to decide on the most suitable constraints to add to a given instance. The computational results show that the machine learning approach significantly outperforms each of the individual approaches.",Polynomial Optimization - Tightening branch-and-bound schemes with conic constraints,"[66640, 77731, 77732, 24214, 43831]",478,"[19, 0]",2274,Algorithms for Mixed-Integer Nonlinear Programming and Nonconvex Optimization,86,12,04,MINLP,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1001 [building - 202],['Continuous Optimization'],WA-04
"We face a single-machine NP-hard scheduling problem, whose objective is to balance the average weighted completion times of two different classes of jobs. Since both the job sets contribute to the same objective function, this problem can be interpreted as a cooperative two-agent scheduling problem, whereas the standard multiagent problems are of the competitive type since each class of job is involved only in optimizing its agent’s criterion. Balancing the average completion times of different sets of tasks finds application in many fields, such as in in services, for balancing the waiting times of groups of people, and in logistics for balancing the delivery times.
To solve this problem, we propose a Lagrangian relaxation approach based on a network flow formulation. A possible bi-criterion extension is also investigated, taking into account the additional objective aimed at minimizing the maximum between the two average weighted completion times. Preliminary numerical results are presented. 
",Balancing the average weighted completion times by network design,"[18064, 69681]",933,"[129, 53]",2275,Flow shop and single machine scheduling ,35,15,60,Project Management and Scheduling,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S09 [building - 101],"['Scheduling', 'Graphs and Networks']",WD-60
"A phylogeny is an edge-weighted binary tree with leaves labeled by a collection of species that represents the evolutionary relationships between the species. Given a phylogeny, a popular measure for the biodiversity of a subset of the species is the phylogenetic diversity, defined by the sum of the edge weights in a minimal spanning subtree of the phylogeny leaf-labelled by the subset. However, if we want to make conservation efforts for particular species, then we consider a phylogenetic diversity index, a function that apportions the biodiversity of a subset across all of its species. In general, for a fixed subset of species, the difference between the phylogenetic diversity and the sum of phylogenetic diversity indices is a non-negative function, called the diversity difference. We study the combinatorics of phylogenetic diversity indices and show that the maximum robust diversity difference for a fixed number of species can be calculated in polynomial time. Furthermore, we provide computational evidence to compare the quality of the optimal robust diversity difference index with other popular phylogenetic indices from the literature in practice.",The Robust Diversity Difference Problem,"[77725, 77740]",883,"[127, 110, 17]",2276,Topics in Combinatorial Optimization II [Contributed],64,15,25,Combinatorial Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,011 [building - 208],"['Robust Optimization', 'Programming, Linear', 'Computational Biology, Bioinformatics and Medicine']",WD-25
"The aim of this paper is to illustrate the connection between the energy sector and the stock market, specifically how changes in energy prices, such as oil or gas, influence the returns of the main European stock indices. The analysis was carried out at the level of European Union countries, both by taking into account the specific situation of each state, as well as the case of EU countries groups based on different criteria such as geographical region, industrial production level, or economic development status. Even though the EU countries have several common economic characteristics, the results of the analysis show that the impact of energy prices on the stock market can differ significantly from one state to another. Another important aspect observed in the results is that energy price dynamics have a different type of impact for countries with the highest level of industrial production in comparison with the rest of the EU countries. From a quantitative perspective, the paper studies, on the one hand, the co-movement between variables, and, on the other, the impact of several types of shocks in a VAR framework. Moreover, we focused the analysis on the dynamic links between energy and stock markets; in this sense, models such as Dynamic Conditional Correlation [GARCH] and Time-Varying Parameter [VAR] were applied.",The impact of energy prices on European stock markets,"[77334, 55475]",512,"[33, 45, 37]",2277,AI for Energy Finance,17,15,31,Analytics,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,54 [building - 208],"['Economic Modeling', 'Financial Modelling', 'Energy Policy and Planning']",WD-31
"This paper introduces a novel approach that integrates Robust Optimization with Machine Learning [ML] to dynamically estimate optimal risk parameter' values for uncertain environments. By leveraging historical data and engineering informative features, we train an ML model to predict risk protection values tailored to each instance's characteristics. Unlike static approaches, our method offers adaptability and enhances decision-making in uncertain domains. Through experimentation, we demonstrate the efficacy of our approach in improving robustness and performance.",ML-based Adaptive Risk Parameters Tuning for Robust Optimization,"[70589, 77690, 71727]",535,"[127, 66, 126]",2281,Risk Averse and Contextual Stochastic Optimization,49,8,35,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,44 [building - 303A],"['Robust Optimization', 'Machine Learning', 'Risk Analysis and Management']",TB-35
"The precision of planning and decision making in Amazon is heavily reliant on sophisticated forecasting methods. These forecasts, often made independently, enable different aspects of logistic chain, such as truck scheduling or labor planning. However, in reality, they forecast values that often interact or depend on each other, or in the least contain strong correlations. We explore various methods to take these relationships into account, such as hierarchical reconciliation, combinations of forecasting and simulation, modelling input-output system constraints, as well as joint time series forecasting approaches based on diffusion models.",Coherent forecasting for Supply Chain Optimisation,"[77719, 78665, 78666, 77895]",434,"[47, 66, 138]",2282,Forecasting for the middle mile,92,14,57,Optimization at Amazon,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S06 [building - 101],"['Forecasting', 'Machine Learning', 'Supply Chain Management']",WC-57
"In this talk, a Block Cubic Newton method is proposed for the unconstrained minimization of a non-convex function with Lipschitz continuous Hessian matrix.
At each iteration, the proposed method first selects a block of variables by means of a greedy rule, that is, on the basis of the optimality violation.
Then, the next point is obtained by computing an approximate minimizer of the cubic Newton model.
Global convergence to stationary points is proved and complexity results are given.
Finally, some numerical results on optimization problems from machine learning are presented.",A Block Cubic Newton method with Greedy Rule,[52230],305,"[19, 113, 66]",2283,Algorithmic Advances in Large Scale Nonconvex Optimization,84,3,32,Advances in large scale nonlinear optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,41 [building - 303A],"['Continuous Optimization', 'Programming, Nonlinear', 'Machine Learning']",MB-32
"
In the realm of pulmonary health, diagnosing Nontuberculous mycobacteria [NTM] infections presents a formidable challenge, particularly in patients with non-cystic fibrosis bronchiectasis [NCFBr]. These infections are notoriously elusive, with their clinical presentations and radiological findings mirroring a plethora of other pulmonary conditions, thereby complicating accurate diagnosis. Our study embarked on an ambitious journey to harness the potential of machine learning in predicting NTM infections within this patient demographic. Through a comprehensive retrospective analysis of 771 NCFBr patients, our investigation illuminated the presence of NTM in 12.2% of cases, as confirmed by bronchoscopy cultures. Our findings advocate a paradigm shift towards incorporating machine learning as an integral component of the diagnostic toolkit for NTM infections in NCFBr patients
Delving into the intricacies of machine learning, we meticulously evaluated seven distinct models, with the Naive Bayes and Random Forest models emerging as frontrunners, boasting AUCs of 0.81 and 0.71, respectively. This comparative analysis not only underscored the viability of machine learning in the clinical prediction landscape but also highlighted the critical role of radiological and clinical parameters—most notably, patient age and the presence of GERD—in refining predictive accuracy.
",Diagnostic advancements - machine learning in identifying nontuberculous mycobacteria [NTM] in non-cystic fibrosis bronchiectasis [NCFBr] population,[71593],539,"[7, 56, 26]",2284,"Advancements of OR-Analytics in Statistics, Machine Learning and Data Science 13",16,8,06,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,1013 [building - 202],"['Analytics and Data Science', 'Health Care', 'Decision Support Systems']",TB-06
"In the context of procuring cricket players during an IPL auction, a comprehensive assessment of various elements becomes imperative, encompassing metrics such as batting average, strike rate, and total runs amassed for batsmen, alongside economy rates and wicket tallies for bowlers. The process of player selection inherently mirrors a Multi-Criteria Decision Making [MCDM] problem, wherein the challenge lies in reconciling conflicting objectives to identify the most suitable candidates. This paper undertakes an empirical investigation utilizing the multi-MOORA method, leveraging real-life data from the recent IPL 2023 season. By meticulously considering all pertinent attributes of players, we establish a prioritized ranking scheme. Moreover, we conduct a comparative analysis with established methodologies such as MOORA and WASPAS to discern their efficacy in this context, thereby shedding light on their respective strengths and weaknesses.",Optimizing IPL cricket 2023 - selecting top batsmen and bowlers with the multi-MOORA method  ,"[41207, 79759, 79758, 79375, 79765]",949,"[25, 99, 55]",2285,Sports analytics,37,13,16,OR in Sports,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,19 [building - 116],"['Decision Analysis', 'OR in Sports', 'Group Decision Making and Negotiation']",WB-16
"The talk is devoted to an extension of the property of strong metric 
sub-regularity of mappings. We focus on the so-called optimality mapping associated with the system of first order necessary optimality conditions for
ODE optimal control problems. Problems that satisfy the Legendre-Clebsch condition and affine optimal control problems will be considered separately.

Two applications will be presented:
[i] existence of Lipschitz continuous optimal feedback control;
[ii] convergence with error estimates of discretization and Newton-like
methods for optimal control. ",Regularity in optimal control and applications,[22710],898,"[82, 0]",2286,Optimal control theory,90,10,33,Optimal Control Theory and Applications,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,42 [building - 303A],['Optimal Control'],TD-33
"Network design is a fundamental problem in public transport planning, fostering sustainability and efficiency. Here, we model the problem of reducing a given street network to a public transport network as a Generalized Optimum Requirement Graph [GORG] Problem. Our aim is to uncover a sub-graph within a fixed edge budget that minimizes routing costs associated with demand, where routing costs encapsulate the summation of all shortest path costs weighted by demand. When considering orb-webs, as an abstraction of circular cities, we can exploit their symmetry and a reduced solution space. We show that finding optimal symmetric generalized optimum requirement graphs is nevertheless NP-hard in general and identify polynomially solvable cases. By comparing integer-programming based solution approaches for both the general and the symmetric GORG problem, we demonstrate the computational efficiencies of the reduced problem space. Allowing only symmetric solutions, however, also influences the solution quality and introduces the so-called price of symmetry. We analyze the influence of the budget and the underlying cost structure on the price of symmetry and show that especially for a Euclidean cost structure, generic and symmetric solutions have almost identical routing costs. Moreover, beyond a global perspective on routing costs, our evaluation extends to assessing the fairness of solutions across passenger groups.",Designing public transport networks via symmetric generalized optimum requirement graphs,"[77679, 78545, 52489]",574,"[79, 14, 119]",2287,Network Design and Line Planning for Public Transportation 2,85,12,54,Public Transport Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S01 [building - 101],"['Network Design', 'Combinatorial Optimization', 'Public Local Transportation Systems']",WA-54
"The chromatic number of a set avoiding graph has a lower spectral bound, which is often sharp. 
We assume that the vertices and the avoided set are invariant under a crystallographic reflection group. 
Then the spectral bound can be expressed as the minimum of a polynomial on a basic semi-algebraic set. 
We compute several bounds for interesting graphs analytically and numerically. 

Based on joint work with Evelyne Hubert, Philippe Moustrou and Cordian Riener. ",Spectral bounds for set avoiding graphs via polynomial optimization,[77702],155,"[72, 115, 53]",2288,Applications of polynomial optimization,68,4,38,"Conic Optimization - Theory, Algorithms, and Applications","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,34 [building - 306],"['Mathematical Programming', 'Programming, Semidefinite', 'Graphs and Networks']",MC-38
"Energy is a critical input in modern economic systems. Much literature has looked into energy forecasting to support critical decisions at various levels and entities; from operational purchasing decisions at individual business organizations to policy-making. In pursuit of maximum accuracy, the field has considered numerous forecasting methods. In this paper, we examine the evolution of methodology across different generations of forecasting methods, starting from classical econometric approaches, over machine learning methods, and earlier sequence learners like LSTMs, to deep transformer networks, which may be regarded as the latest trend in forecasting. We also consider the latest ideas on adopting pretraining and transfer learning, concepts that have revolutionized the analysis of unstructured data and enable modern AI, for time series forecasting.  We offer a systematic review of the related literature and categorize different forecasting methodologies. More importantly, using data from EU energy markets,  we conduct a large-scale benchmarking experiment, which contrasts the forecasting accuracy of different approaches, focusing especially on alternative propositions for time series transformers. ",Energy Price Modelling - A Comparative Evaluation of Four Generations of Forecasting Methods,"[9422, 77750, 55475]",512,"[47, 8, 37]",2289,AI for Energy Finance,17,15,31,Analytics,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,54 [building - 208],"['Forecasting', 'Artificial Intelligence', 'Energy Policy and Planning']",WD-31
"Since the last three years, a multidisciplinary research group of CCR and Politecnico di Torino has been scientifically supporting the activation of a new strategic digital tool aiming at the preservation of Cultural Heritage [CH] across Piedmont and Liguria and pursuing the transition from existing cultural policies. The presented activities are Institutions’ support initiatives promoted by Compagnia di San Paolo through two editions of PRIMA, a call for proposals oriented towards the cultural, economic and social sustainability of CH Conservation Plans. Quite experimentally and in addition to the funding activities, capacity building, thematic coaching and networking have been activated, indirectly educating to the importance of collecting, creating, managing and sharing knowledge, particularly on the issues of CH programmed conservation. The sharing of interdisciplinary expertise and local experiences represents an innovation element which, through peer-to-peer comparison and knowledge sharing, promote the activation of collaborative networks and open up to the use of new financial tools for a more aware and sustainable CH management. In order to measure PRIMA social impact, it has built a monitoring system whose first outcomes [growth of skills, interventions sustainability, involvement and participation of all stakeholders and community], allow to identify planned conservation as one of the most solid and concrete keys to the achievement of UN SDGs in the field of CH.",Supporting stakeholders' and citizens’ participation and inclusion in planning maintenance processes - multidisciplinary testing practices in Piedmont and Liguria,"[77734, 77744, 77751, 77745, 77739, 77747, 77738, 77743]",274,"[62, 137, 139]",2290," Enhancement of circularity, inclusivity, and smartness in cities I",79,4,18,Sustainable Cities,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,42 [building - 116],"['Knowledge Engineering and Management', 'Strategic Planning and Management', 'Sustainable Development']",MC-18
"The ongoing evolution of climate change presents unprecedented challenges to electricity system performance and reliability, which are inherently sensitive to weather conditions affecting both energy demand and generation capacities. To aid decision-making in energy systems under climate change influence, energy system optimization models can be combined with climate model outcomes. As future climate development is highly uncertain, there are many different climate models and human greenhouse gas emission scenarios projecting the future. Thus, we propose a new time series clustering algorithm. This algorithm allows to incorporate various climate models and emission scenarios into energy system optimization while focusing on providing a robust solution, meaning that the amount of unmet demand is low no matter which climate scenario is realized. The algorithm operates by first splitting the time series into single days which are optimized individually. The investment results of the single days are then used to calculate the amount of unmet demand which occurs when the single-day investment decsisions are used with the complete time series. This data informs the clustering process, prioritizing days with high unmet demand. Our results demonstrate that this new clustering algorithm can reduce unmet demand compared to traditional cluster algorithms like k-means. In further research we plan to compare our approach to traditional robust optimization approaches.",Enhancing electricity system robustness through the application of a new time series cluster algorithm,"[70125, 33470]",247,"[93, 37, 127]",2291,Planning problems in electrical energy systems,23,2,21,OR in Energy,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,49 [building - 116],"['OR in Energy', 'Energy Policy and Planning', 'Robust Optimization']",MA-21
"Supply chain resilience is a growing priority in the biopharmaceutical sector. In recent years, the industry has seen the market boom of next generation therapies against life-threatening diseases and the urgent demand for vaccines during the pandemic. Manufacturers catering for these markets reported shortages and delays due to unforeseen demand trends combined with uncertainty in manufacturing capabilities for platforms still under development. We present a decision-support tool that can help assess supply chain resilience a priori for networks that require rapid scale-up to commercialisation and operate under supply-demand uncertainty. Firstly, the propagation of underlying manufacturing uncertainty to scale dependent throughput-related and cost-related KPIs of manufacturing at the investment level is assessed via uncertainty analysis and global sensitivity analysis. Secondly, a stochastic mixed-integer linear problem is formulated for the optimisation of investment planning under process and demand uncertainty. Finally, we introduce a scenario-based supply-demand chance constraint, which quantifies manufacturing supply robustness in meeting target demands. Solution feasibility is tested via Monte Carlo based simulation. Candidate investment plans for single-product and multi-product systems are assessed and cost-resilience Pareto frontiers are constructed via the ɛ-constraint method to support decision-makers allocate resources to reliable manufacturing systems. ",Decision-support tool for resilient biopharmaceutical supply chains,"[77733, 68238, 69161]",705,"[138, 111, 126]",2292,Resilient Networks,80,10,53,Sustainable and Resilient Systems,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,8007 [building - 202],"['Supply Chain Management', 'Programming, Mixed-Integer', 'Risk Analysis and Management']",TD-53
"When developing software for solving various classes of optimization problems, e.g., MILP, and MINLP, it is crucial to have tools for comparing the impact of new functionality. The performance can be measured using many different metrics including solution time and quality of the solution, so it is not always trivial to assess the added value. It is also important for end users to be able to compare the performance of available optimization solvers when selecting which one to use. So far, available tools for performing such analysis have been mainly static, i.e., there have been limited options for the users to interact, filter, and sort the results.

In this presentation, we introduce a tool called VIVA [Visualize, Interact, Verify, and Analyze Benchmarking Data for Optimization Solvers] for comparing the performance of optimization solvers in an efficient and user-friendly way. VIVA is an open-source project implemented in TypeScript and generally based on standardized web technologies. This means that VIVA can be used directly in a web browser without installing and configuring additional dependencies. Currently, VIVA implements most of the functionality of the Python-based PAVER tool, including its visualization capabilities, but the open framework of VIVA means it can be extended further in the future.

To illustrate the capabilities of VIVA, the tool is also demonstrated on recent benchmark results from convex and nonconvex MINLP solvers during the presentation.",VIVA - A web-based tool for analyzing and visualizing optimization solver benchmarking results,"[22807, 77749, 77365]",704,"[72, 111, 134]",2294,Optimization Tools,76,12,30,Software for Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,53 [building - 208],"['Mathematical Programming', 'Programming, Mixed-Integer', 'Software']",WA-30
"The Sponsored-Products [SP] advertisement is a popular way for promoting products on Amazon. Etailers can choose and pay for specific keywords to secure ad placements for their products. These keywords are the ones that shoppers are likely to search for when looking for products on Amazon. To improve the efficiency of SP ads, etailers who have a large catalog of products often create ad groups for products with similar attributes. An ad group consists of a set of products and a set of keywords, and all the products in the ad group share the same keywords set. Etailers may also choose to link to external websites for advertising their products and attracting consumers, which is called off-Amazon [OA] ads. This study focuses on the sequential SP and OA ads optimization problem for etailers. Practically, many etailers set sales targets for products as manufacturing and logistics are planned ahead of time. Hence, we consider that the objective of the etailer is to minimize the expected long-run average cost associated with advertising and the cumulative deviation between the sales target and actual sales. When the mean of the sales number per unit time [i.e., sales rate] for each product is known, we characterize the optimal sequential Sponsored-Products and off-Amazon advertising policy [abbreviated as SSPOA policy] for products in an ad group, which is of a threshold type. However, in reality, etailers may not know the exact mean of sales rates, and therefore the SSPOA problem ",Sequential sponsored-products and off-amazon advertising optimization for etailers,"[77735, 41854, 73319]",187,"[25, 5, 32]",2296,Experimental economics and game theory 2,73,14,40,Experimental economics and game theory,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,96 [building - 306],"['Decision Analysis', 'Algorithms', 'E-Commerce']",WC-40
"We discuss algorithmic complexity of general discrete nonlinear optimization models with augmentation, verification, and evaluation oracles. We show that in many situations where at least a verification oracle is available, which verifies whether a given solution is optimal, an optimal solution for the respective discrete optimization problem can be found in strongly polynomial oracle time. Also, we discuss algorithms based on machine-learning methods with performance guarantees for practical black-box optimization scenarios of large-scale scheduling problems and optimization problems with partial differential equations where only evaluation oracles are available.",On the complexity of optimizing black-box functions in oracle models,[6281],225,"[14, 52, 72]",2298,Integer Programming and Combinatorial Optimization - Complexity Questions and Algorithms,64,10,52,Combinatorial Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,8003 [building - 202],"['Combinatorial Optimization', 'Global Optimization', 'Mathematical Programming']",TD-52
"Logistics service providers [LSPs] have to respond to the growing demand from their customers, who offer increasingly widespread e-commerce services. This work focuses on pickup operations, meaning that, real-time changes, such as the arrival of new customers, can be accommodated as the vehicle is initially empty and progressively filled up.  

The problem under study is a vehicle routing problem with split pickup, time windows and three-dimensional loading constraints, namely, geometric, vertical stability, orientation, and multi-load constraints.  

Throughout the day, the LSP is likely to face real-time changes, called disruptions, and needs to quickly adapt his ongoing routes to accommodate as many disruptions as possible while minimizing costs. We will present a greedy randomized adaptative search procedure [GRASP] to deal with two types of disruptions - the arrival of a new customer and the addition of boxes to a planned customer request.
",A GRASP for the three-dimensional loading vehicle routing problem with split pickup and time windows under real-time disruptions,"[67242, 663, 36600, 63211]",544,"[145, 23, 14]",2300,Cutting and Packing 3 - 3D loading,81,4,07,Cutting and Packing [ESICUP],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,1019 [building - 202],"['Vehicle Routing', 'Cutting and Packing', 'Combinatorial Optimization']",MC-07
"Training a classifier is a challenging task as it typically requires a significant amount of labelled data, while most available samples are unlabeled.  For instance, in the field of diagnostic imaging, labelling medical images requires the expertise of a specialist.
In this case, Active Learning [AL], a machine learning approach that aims at minimizing the number of samples used in the learning phase, can assist the decision maker. A performing AL algorithm selects the most informative data points based on a defined metric and passes them to a human labeler, who progressively adds the labeled data to the training set. The motivation for adopting such a strategy is related to the significant effort required in terms of costs and man-hours to label the data.

The concept of active learning is based on the consideration that not all data points are equally important for training a classifier model. Therefore, AL algorithms select only the most valuable data instances. The key point is how this selection is made. This work focuses on possible sampling based on large-margin strategies of the Support Vector Machine [SVM] type. The distance to the separating hyperplane can be considered a reliable metric for measuring the model confidence or certainty on unlabeled data samples. In this study, the role of samples close to the classifier boundary is investigated by comparing supervised and semi-supervised approaches. Preliminary numerical experiments are reported.",A comparative study of large-margin strategies for Active Learning,"[23784, 18064]",561,"[66, 81, 84]",2301,"Advancements of OR-analytics in statistics, machine learning and data science 14",16,9,06,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,1013 [building - 202],"['Machine Learning', 'Non-smooth Optimization', 'Optimization Modeling']",TC-06
"Dendrograms are graphical representations of hierarchical clustering that illustrate how individual elements can be grouped into clusters. Depending on the chosen distance measure, different dendrograms can be obtained. The purity measure is used to evaluate the quality of the dendrograms. Purity is defined in terms of the homogeneity in the distribution of observations in the dendrogram with respect to their classes. The first part of this work focuses on single-linkage dendrograms and on purity measure. We introduce the first mathematical optimization model to calculate the purity of single-linkage dendrograms in hierarchical clustering with labeled data. This mixed integer linear model is based on the ordered median problem. The second part addresses complete-linkage dendrograms. We propose a system of linear inequalities whose solutions are complete-linkage dendrograms.  We analyze the impact of ties in the cost matrix in the system and prove that when there are no ties the solution of the system is unique and is the complete-linkage dendrogram while when there are ties the system has as many solutions as alternatives complete-linkage dendrograms  the matrix has.  This study contributes to understanding the construction and evaluation of dendrograms in hierarchical clustering.",Hierarchical clustering with Mixed Integer Linear Optimization Models,"[77508, 1174, 62186, 46717, 5426]",141,"[14, 111]",2302,Machine Learning for and with Mathematical Optimization,15,15,27,Mathematical Optimization for XAI,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,047 [building - 208],"['Combinatorial Optimization', 'Programming, Mixed-Integer']",WD-27
"In this work we present different advances in exact techniques for solving hub location problems. Firstly, we carry out the analysis of the classical model with path variables for multiple-allocation and we propose a new family of clique constraint.   Some properties of the new inequlities and corresponding lifting are discussed, and a separation heuristic algorithm is proposed. A set of computational experiments are reported to evaluate the usefulness of the proposals when embedded in a commercial solver.The new family of inequalities is notably effective in r-allocation problems. Secondly, we propose a model for the single-allocation hub location  problem with upgraded connections. In this case, the goal is  to invest the budget both in reducing the cost of the network and in improving some of its connections. For this new model we propose an exact resolution algorithm based on the  ordered median problem and on an existing approximation for large class of binary quadratic programs. In the computational analysis  we compare the results for complete and incomplete hub networks. Finally, an analysis of the added-value of upgrading is conducted.
",New insights in hub location models,"[1174, 66290, 52056, 22045, 9684]",220,"[64, 111]",2303,Advanced Topics in Combinatorial Optimization,64,8,26,Combinatorial Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,012 [building - 208],"['Location', 'Programming, Mixed-Integer']",TB-26
"We propose two alternative ways of reducing the number of objective functions  of a given multiobjective discrete optimization [MODO] problem and investigate the impact of the reduction on the nondominated set of the original problem empirically.  The number of objective functions  is a major factor that determines the computational effort of obtaining the nondominated set of a MODO problem.  Transforming the problem in a way to reduce the number of objective functions is expected to result in computational efficiencies; however, some information loss should also be exptected since the  nondominated set will no longer be computed exactly.  With the goal of devising methods that lead to minimal information loss, we investigate two alternative approaches - one suggests eliminating objectives, the other combining them.  The elimination method  uses the similarity between an objective function’s gradient and its projection onto objects defined by the remaining objectives.  The combination approach relies on clustering the objective function gradients.  We assess the effectiveness of the approaches on library problem instances.  We report quality measures such as the coverage error, additive epsilon indicator and hypervolume to evaluate the representations obtained by solving the reduced problems.  Preliminary findings indicate that proposed reductions may be  effective  ways to obtain quick representations of the original problem.",An Empirical Investigation of Two Reformulation Approaches in Multiobjective Discrete Optimization,"[1193, 75567, 77773]",51,"[112, 77]",2304,Multiobjective Combinatorial Optimization,34,12,37,Multiobjective Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,33 [building - 306],"['Programming, Multi-Objective', 'Multi-Objective Decision Making']",WA-37
"Crowdsourced delivery utilizes the services of independent actors. As opposed to traditional modes of delivery, availability and acceptance decisions of crowdshippers are uncertain and cannot be fully controlled by an operator. We consider a setting in which an operator groups tasks into bundles and in which the resulting bundles are offered to crowdshippers in exchange for some compensation. Uncertainty in the crowdshippers' behavior who may accept or reject offers is considered via [individual] acceptance probabilities. We consider generic probability functions whose main parameters are the compensation offered, the number of tasks in a bundle, and the total detour to deliver a bundle. The objective of the resulting optimization problem is to minimize the expected total cost of delivery. We propose a mixed-integer non-linear programming [MINLP] formulation that simultaneously decides how to group tasks into bundles, which bundles are offered to which crowdshipper, and the compensation offered for each bundle. We show that this MINLP can be reformulated as a mixed-integer linear program with an exponential number of variables. We present a column generation algorithm for solving instances of the latter whose pricing subproblem corresponds to an elementary shortest path problem with resource constraints [ESPPRC] and a nonlinear objective function. Our experiments show that the algorithm is capable of solving large instances.",Pricing and bundling decisions considering driver behavior in crowdsourced delivery,"[69650, 23193, 67397, 12046, 71207]",861,"[13, 65, 143]",2305,Combinatorial optimization approaches for freight deliveries,64,3,52,Combinatorial Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,8003 [building - 202],"['Column Generation', 'Logistics', 'Transportation']",MB-52
"In recent years, the assessment of structural performances and functionality of roadway bridges and viaducts has become a key issue in Italy and in most European Countries. Large part of the existing stock exhibits deficiencies due to the aging of the structures and the ever-growing demanding traffic conditions and safety requirements, compared to the original design phase. In 2020, in Italy were issued to assess bridges and viaducts risk exposure and proposed a multilevel approach based on census, visual inspection, and a preliminary and qualitative risk indicator. To define this risk indicator, four types of risks are considered - structural and foundation risk, seismic risk, landslide risk, and hydraulic risk. After this preliminary evaluation, in-depth tests and more accurate analyses should be implemented on any bridge or viaduct exposed to high risk. During the first application of these guidelines, several difficulties were detected in finalizing a ranking of the infrastructures included in the same class of risk. This paper proposes an AHP absolute model to rank roadway bridges and viaducts by priority on intervention, based on structural and foundation risks. In detail, to identify criteria, sub-criteria, and rating, and define the hierarchy, an extensive literature review was conducted and a pool of experts was interviewed via a Delphi survey process. Focus groups were organized to validate the hierarchy by dynamic discussion and identify local and global priorities.",A Multicriteria Approach for the Definition of Priorities of Intervention for Roadway Bridges and Viaducts,"[10666, 62129, 77767, 77768, 77771]",109,"[6, 26, 137]",2306,MCDA and urban planning 1,44,8,47,Multiple Criteria Decision Analysis,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,50 [building - 324],"['Analytic Hierarchy Process', 'Decision Support Systems', 'Strategic Planning and Management']",TB-47
"With the rising need for chemotherapy treatment exceeding the limited capacities of oncology clinics and hospitals efficient planning becomes crucial. For that, first stage is allocating patients to treatment days within the constraints of their treatment plans and hospital schedules. Second stage is preparing the daily appointment schedules of patients to maximize resource utilization and patient satisfaction considering nurse, chair, and other constraints. 
Based on the observations at Hacettepe Oncology Hospital in Ankara, we present a compact deterministic Mixed-Integer Linear Programming [MILP] model to address the first stage, known as the Outpatient Chemotherapy Planning [OCP] problem.  The model accommodates multi objectives to comply the hospital policies and decision maker’s goals. We find that for any selected objective, the solutions retuned by our model outweigh the solutions currently implemented.
",Outpatient Chemotherapy Planning at Hacettepe Oncology Hospital - A Mixed-Integer Linear Programming Model ,"[77149, 44538]",609,"[56, 111, 129]",2309,Radiotherapy and chemotherapy planning,3,7,10,OR in Health Services [ORAHS],"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,11 [building - 116],"['Health Care', 'Programming, Mixed-Integer', 'Scheduling']",TA-10
"The digital revolution has brought about profound changes in the food service industry, particularly with the emergence of online food delivery. Our study aims to delve into the intricacies of order bundling to not only enhance courier satisfaction but also to maximize operational efficiency. Leveraging a combination of a Clarke and Wright heuristic and MIP model with a careful consideration of factors such as route length and operational constraints, we developed robust strategies geared towards improving bundled order ratios. A significant hurdle we encountered was the challenge posed by courier complaints regarding route structure between buyer locations. To effectively address this issue, we introduced a variety of angle constraint structures into our model. The outcome was striking – a substantial 7% increase in bundled orders was achieved without any compromise on route lengths, underscoring the pivotal role of prioritizing courier satisfaction in operational decision-making processes. In summary, our research provides invaluable insights into the optimization of online food delivery operations. By placing a premium on courier satisfaction and proactively tackling operational challenges, we present actionable recommendations for enhancing overall efficiency and service quality within the industry.",Optimizing Bundling Choices in Online Food Delivery,"[77756, 57737, 75012, 78601, 78599, 79754]",760,"[32, 65, 74]",2310,Logistics 1,5,13,58,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S07 [building - 101],"['E-Commerce', 'Logistics', 'Metaheuristics']",WB-58
"In the rapidly evolving landscape of cryptocurrency markets, the predictive power of social media sentiment has emerged as a pivotal area of exploration. This study presents a comprehensive analysis aimed at uncovering the intricate relationship between public sentiment expressed on Facebook and the volatility of Bitcoin trading volumes. Leveraging an extensive dataset of 120,000 Facebook posts tagged with Bitcoin-related keywords, combined with granular financial data from CoinGecko, we employ a methodologically advanced approach integrating Long Short-Term Memory [LSTM] networks, sentiment analysis via FinBERT, and topic modeling techniques.
The LSTM model's ability to capture temporal dependencies in data makes it particularly suited for the volatility inherent in cryptocurrency markets, providing a more accurate forecasting tool compared to traditional time-series models. The findings from our analysis reveal significant correlations between the sentiment derived from social media and the fluctuations in Bitcoin market trends. These correlations not only validate the hypothesis that social media sentiment can serve as an effective predictor of market movements but also underscore the efficacy of LSTM networks in enhancing the precision of volume predictions.", BitMood - Analyzing Bitcoin Trends through Facebook Emotions with AI,"[55475, 76296, 77913, 78713, 78714, 78715]",511,"[33, 7, 8]",2311,Innovations in Digital Assets - IDA,17,14,31,Analytics,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,54 [building - 208],"['Economic Modeling', 'Analytics and Data Science', 'Artificial Intelligence']",WC-31
"One of the key properties of convex problems is that every stationary point is a global optimum, and nonlinear programming algorithms that converge to local optima are thus guaranteed to find the global optimum. However, some nonconvex problems possess the same property. This observation together with empirical evidence that local search solvers often find global optima of nonconvex problems such as optimal power flow and pooling has motivated research into generalizations of convexity. This presentation proposes a new generalization which we refer to as optima-invexity - the property that only one connected set of optimal solutions exists. We discuss conditions for optima-invexity and its applications within spatial branch-and-bound algorithms for mixed-integer nonlinear programs.",Generalized convexity applied to branch-and-bound algorithms for MINLPs,[63612],194,"[51, 11, 52]",2313,Recent Advances in MINLP,86,9,04,MINLP,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,1001 [building - 202],"['Generalized Convex Optimization', 'Branch and Cut', 'Global Optimization']",TC-04
"The line planning problem consists of finding a set of lines to serve as well as which frequencies to operate in a public transport network. Including frequency-setting gives an indication of a timetable and allows for a better approximation of the operator costs and passenger service, however, it makes the problem difficult to solve for real-life instances. In this talk, we present an exact, more scalable solution method for solving line planning problems. The problem is modelled using a mixed integer programming formulation, which minimizes both operating costs and passenger costs subject to budget and capacity restrictions. The passenger cost is the perceived travel time considering frequency-dependent transfer and waiting time costs. The proposed algorithm starts with a reduced representation of the problem. It iteratively adds frequencies to the model by utilizing the internal trade-off between the two conflicting objectives in the problem. We present a computational study based on both well-known artificial and real-world problems. Our results show that the algorithm is able to find solutions faster and without explicitly creating the complete formulation. ",An exact solution method for solving the line planning and frequency setting problem,"[77283, 27296, 70927]",283,"[119, 5, 79]",2314,Network Design and Line Planning for Public Transportation 1,85,9,51,Public Transport Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,M5 [building - 101],"['Public Local Transportation Systems', 'Algorithms', 'Network Design']",TC-51
"Efforts to mitigate emissions have led to the introduction of more distributed and volatile generation, such as photovoltaics, along with flexible and smart loads. This shift strains the reliability of power distribution systems [PDSs], which are expected to adapt to these challenges. Volatile energy sources further affect the interaction between medium-voltage [MV] and low-voltage [LV] levels in PDSs. Hence, distribution system operators need novel strategies for efficient capacity enhancements.

The distribution expansion planning [DEP] problem seeks optimal PDS design, minimizing investment and operational costs while meeting service requirements. Existing literature on DEP often overlooks the interplay between LV and MV systems. However, isolated investment plans may not remain optimal [or even feasible] for integrated MV/LV systems, necessitating coordinated models. This work addresses the gap by exploring coordinated DEP for MV and LV systems with volatile energy sources.

We leverage spatial aggregation techniques for large-scale planning. To evaluate the proposed approach, we utilize representative PDSs in Switzerland. This research enhances the adaptability and efficiency of modern PDSs amidst the increasing deployment of distributed and volatile energy sources.",Coordinated Medium- and Low-voltage Distribution Systems Planning Under Volatile Energy Sources Deployment,"[73599, 73675, 69248]",857,"[93, 12, 79]",2315,OR in Energy III,23,15,19,OR in Energy,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,44 [building - 116],"['OR in Energy', 'Capacity Planning', 'Network Design']",WD-19
"Efficient and accurate production planning is crucial in the automotive industry. Artelys has developed a scheduler service for Toyota Motor Europe that creates plannings for post-production workshops operations across Europe. The problem consists in scheduling operations on vehicles on different production lines with available resources. This case study details how this industrial problem differs from the classical Resource Constrained Project Scheduling Problem [RCPSP] with its additional operational constraints [multiple shifts, preemptive breaks, sequence constraints, etc.] and its multi-objectives [minimizing late tasks, late vehicles, maximizing efficiency and workers ergonomics, etc.].
Artelys has worked closely with Toyota to deploy this scheduler service as micro-service in order to solve this complex problem in a real-time context and as a flexible decision-aid software. The associated constraint programming model is implemented with Fico Mosel modelling language and the model is solved using Artelys Kalis solver. This micro-service has replaced a manual tedious task that was taking place differently in all workshops by delivering faster, higher quality solutions. 

",Production planning in the automotive industry with Artelys integrated scheduler service,"[77008, 77279]",837,"[118, 65, 107]",2316,Production planning problems,32,10,49,"Lot Sizing, Lot Scheduling and Production Planning","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,M1 [building - 101],"['Project Management and Scheduling', 'Logistics', 'Programming, Constraint']",TD-49
"Liner shipping networks are a central feature of modern supply chains that consist of cyclical, periodic services operated by container vessels. This specialized, cyclical structure eases planning for both shipper and carrier, but the combination of cyclical planning with the available time windows at ports can lead to inefficient operations. We propose to relax the cyclical assumption and allow vessels to move between services to avoid inefficient connections without interruption to container flows. From the view of a shipper, the cyclical and periodic properties of the services still hold, and the liner carrier can offer a more efficient overall network. The ensuing optimization problem consists of a combined vessel routing problem and cargo allocation problem, resulting in large and challenging instances. We model the problem using  mixed-integer linear programming and use an expanding horizon heuristic to find starting solutions for our model. We use real-world data to show that giving flexibility to a liner network can result in a significant cost reduction over standard cyclical schedules.",Rethinking Cyclic Structures in Liner Shipping Networks,"[61911, 27003]",788,"[145, 65, 70]",2317,Logistics 2,5,14,58,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S07 [building - 101],"['Vehicle Routing', 'Logistics', 'Maritime applications']",WC-58
"The aim of the study is to demonstrate a new non-parametric statistical procedure, the Sum of Ranking Differences [SRD], through an example taken from sports. The method was introduced by Héberger [2010] and is becoming increasingly popular in various fields of applied sciences. With this procedure, solutions or methods can be compared through a reference. In our study, we analyze the similarities and differences in the style of play of four major European football leagues [La Liga, Bundesliga, Premier League, and Serie A]. First, we characterize the leagues based on the teams' statistical attributes featured in the games. We assess the importance of game elements such as goals scored and conceded, ball possession, successful passes, etc., by examining how closely teams are ranked by each element in comparison to their final standings on the scoreboard. Essentially, this is an SRD calculation where the teams' final scores in the league serve as the reference. Aggregating the results for the leagues, we obtain a data table that can be subjected to yet another SRD analysis. By setting any league as a reference, we can measure how much the playing style of a particular league differs from the others.",Comparison of Football Leagues Using the Sum of Ranking Differences,[63046],440,"[99, 134, 77]",2319,Ranking in sports,37,8,16,OR in Sports,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,19 [building - 116],"['OR in Sports', 'Software', 'Multi-Objective Decision Making']",TB-16
"Uplift modeling facilitates decision optimization by predicting the instance-dependent impact of treatments on specific outcomes. In a setting with binary treatments, cost-sensitive causal classification enables the classification of instances into treated or untreated groups, aiming to maximize the expected causal profit which is the core objective. We extend upon the binary treatment case and allow for continuous individual treatment effects, represented by a dose-response curve. From an application perspective, the motivation for integrating continuous treatments stems from the potential convexity of individual dose-response curves. Deciding which entities to assign what dosage is a function of individual dose-response curves, which reflect the change in positive outcome probability per assigned dosage, and the cost and benefit parameters of treatment and positive outcomes of the specific problem setting at hand. Our predict-then-optimize approach involves two steps. First, individual dose-response curves are estimated on observational data. Since historically treatments are not assigned at random but the allocation of doses depends on pre-treatment covariates, observational data may be subject to selection bias. To address this bias, we employ causal machine learning methods. Second, we formulate the individual dosage allocation as an optimization problem to maximize the expected causal profit while adhering to constraints, such as budget limitations or fairness criteria.",A predict-then-optimize approach for uplift modeling with continuous individual treatment effects,"[71159, 46180]",63,"[26, 42, 7]",2321,Causal Machine Learning,17,4,31,Analytics,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,54 [building - 208],"['Decision Support Systems', 'Expert Systems and Neural Networks', 'Analytics and Data Science']",MC-31
"The built environment has a large share in energy consumption and CO2 emissions in Europe, mainly caused by fossil fuel-based heat supply. In the energy transition context, a drastic decarbonisation of the existing building stock is required. Additionally, there is a significant potential for heat demand and emission reduction via envelope retrofits, which can also reduce required heat supply temperatures. Strong interactions occur between heat supply and demand, requiring a holistic assessment of investments on both sides. However, energy system optimisation models usually treat heat demand profiles as static inputs without considering envelope retrofits or treat retrofits in a simplified way using pre-defined sets. In both cases, the heat demand is exogenous and cannot be altered. Only a few studies endogenously consider envelope retrofits and their impact on heat demand.

This work presents a mixed-integer optimisation model to overcome the abovementioned limitations. Our model includes a simplified representation of the building’s thermal behaviour, allowing us to consider investments in envelope retrofits and supply technologies endogenously. Comparing our methodology with existing approaches, we can show that considering envelope retrofits endogenously can yield more optimal results. Thus, our methodology enables exploiting synergies between investments in envelope retrofits and heat supply technologies.
",Exploiting synergies between heat supply and demand through endogenous modelling of building envelope retrofits,"[77759, 78563, 78258, 68618]",465,"[93, 72]",2322,Multi-energy systems,23,4,19,OR in Energy,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,44 [building - 116],"['OR in Energy', 'Mathematical Programming']",MC-19
"Our study investigates the use of causal machine learning for estimating the effects of continuous treatments from observational data. This is a challenging task, as in such data treatments are often not randomly assigned, leading to confounding, a systematic difference between units that were assigned different treatments. Additionally, many modern decision-making problems are high-dimensional and complex, hindering the application of traditional statistical causal inference techniques, and calling for specialized data-driven methodologies. We review established causal machine learning methods for continuous treatment effect estimation, as well as previously presented benchmarking datasets. The goal of our study is to derive sensitivities of model performance to different data-generating processes and accompanying challenges, such as the strength of confounding, the amount of training data, and the complexity of relationships between variables of interest. To further aid practitioners and researchers, we also test several state-of-the-art methodologies from supervised learning in the capability to estimate treatment effects, aiming to further aid the decision when to adopt targeted methodologies for continuous treatment effect estimation.","Estimating continuous treatment effects from observational data - An empirical evaluation of methods, scenarios, and challenges","[70949, 67533, 46180]",63,"[66, 7]",2325,Causal Machine Learning,17,4,31,Analytics,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,54 [building - 208],"['Machine Learning', 'Analytics and Data Science']",MC-31
"Staff planning for airport ground operations usually starts well ahead of the day of operation. Aircraft delays, caused by factors such as weather and technical issues, subsequent cascading delays on the ground, as well as workforce uncertainties, are all unknown at the moment that staff plans are made and are typically disregarded. In practice, airlines have traditionally prioritized equipment over staff as the primary constraint, thus devoting less attention to the latter. Moreover, the computational complexity associated with staff shift planning problems has led the prevailing approach to be deterministic, assuming complete knowledge of the specific tasks and their respective timings. However, recently, the labour market has tightened significantly, causing a focal shift toward staffing constraints.
In this study, we explore the impact of uncertainty on schedule performance, drawing inspiration from the practical operations of a major European hub-and spoke carrier, with a particular focus on the planning of baggage [un]loading staff. We simulate disruptions and the consequent recovery strategies using historical data to demonstrate the quality of priori decisions regarding shift and break timing, staff allocation, and long-term workforce planning. Based on this, we offer insights into the significance of diverse uncertainty types, their magnitudes, and correlations, demonstrating how planning decisions can be adjusted without sacrificing computational tractability.",Analyzing the impact of operational disruptions on airline ground staff planning,"[77471, 45941, 78309, 46236]",300,"[4, 129, 131]",2327,Automated Timetabling,36,2,58,Automated Timetabling,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S07 [building - 101],"['Airline Applications', 'Scheduling', 'Simulation']",MA-58
"The legislation of the European Commission for regulating the European Internal Market of Electricity [EIME] establishes measures for the electricity market harmonization. These measures are designed to encourage the active participation of renewable generation in balancing markets [BMs]. RESTrade [Multi-agent Trading of Renewable Production] tool models and enables the simulation of existing secondary and tertiary markets but also simulates improvements to current market designs. These changes, developed within the scope of H2020 TradeRES project [grant agreement no. 864276] are - i] 15-minute trades of reserves and penalty prices, ii] participation of smaller [0.1 MW] aggregated or single players, and iii] separate procurement and bidding of energy and capacity reserves. They are aligned with the EIME legislation. Furthermore, the tool can also compute the imbalance settlement [IS] based on the Portuguese or Spanish mechanisms such as a new approach for computing the dynamic procurement of secondary capacity. 
This work uses RESTrade to simulate the BMs of Portugal and Spain during 2019, where the weight penalties are around 18.5% and 21.3% of the wholesale market prices. The dynamic procurement increased the usage of its up and down capacities by 12% and 6% in Spain during the period 2019-2022, respectively. The simulation of a new IS mechanism during 2019 in Portugal and Spain, resulted in a reduced penalty of 12.2% and 4.4% of the wholesale prices paid, respectively. ",The reform of Balancing Markets - Insights from Portugal and Spain,"[76787, 76325, 77764, 77763]",403,"[37, 93, 9]",2330,New Market Designs & Models for 100% Renewable Power Systems,22,4,09,Energy Markets,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,10 [building - 116],"['Energy Policy and Planning', 'OR in Energy', 'Auctions / Competitive Bidding']",MC-09
"This work presents a complete model for the optimal planning of helicopter operations during the extinguishing of a large wildfire. In large wildfires, many resources work together and must be coordinated in order to extinguish the wildfire in the shortest possible time while minimizing the damage caused. It is the responsibility of the fire coordinator to plan the work of these resources. To assist in this task, a model has been developed to find the optimal planning of helicopters in a firefighting situation. The model determines which aircraft should work on the fire and at what time. It also selects where the helicopters should load water and where they should rest. Using the level of efficiency associated with each area of the wildfire at each time, provided by the coordinator, the model also indicates at which points on the wildfire each helicopter will drop water. It also considers the fact that the aircraft work in groups, forming elliptical circuits that share water load and drop areas. It also takes into account Spanish aviation regulation. The model is presented in a time-extended graph, which allows the evolution of the wildfire to be reflected. The complexity of the model makes it impossible to solve it in a short time with the tested commercial solvers. Therefore, a heuristic based on the Simulated Annealing technique was implemented. The heuristic was tested with realistic data instances, achieving good results in a short time. ",On a heuristic algorithm for optimal coordination of firefighting helicopters on large wildfires,"[76348, 43840, 77769, 46344, 77772]",473,"[30, 26, 5]",2331,Methods and Algorithms of Decision Support,45,4,45,Decision Support Systems,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,30 [building - 324],"['Disaster and Crisis Management', 'Decision Support Systems', 'Algorithms']",MC-45
"Training neural networks using combinatorial optimization solvers has gained attention, especially in low-data scenarios. Utilizing advanced solvers like mixed integer linear programming can precisely train networks without relying on intensive GPU-based methods. We focus on few-bit discrete-valued neural networks, including Binarized Neural Networks [BNNs] and Integer Neural Networks [INNs]. These lightweight architectures are notable for their ability to operate on low-power devices, for example, by being implemented using boolean operations. Our proposed method involves training a network for each class pair of the classification problem and using a majority voting scheme for the output prediction. The optimization process adheres to the principles of robustness and sparsity by employing a multi-objective function. We compare this approach, named BeMi, to existing solver-based and gradient-based methods, particularly in few-shot learning contexts with BNNs. We also assess the trade-offs between INNs and BNNs. Empirical results on the MNIST dataset show that BeMi achieves higher accuracy [up to 81.8%] with a significant reduction in active weights, outperforming previous methods.",Simultaneous Training and Optimization of Few-Bit Neural Networks through a Multi-Objective MILP,"[77762, 73057, 72032, 3357, 71261]",207,"[14, 112]",2333,Combinatorial Optimization for Machine Learning,64,4,26,Combinatorial Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,012 [building - 208],"['Combinatorial Optimization', 'Programming, Multi-Objective']",MC-26
"Bike sharing systems [BSSs] are an important component of sustainable urban transportation, providing flexible and eco-friendly alternatives for city logistics. However, these systems often face challenges with unbalanced bike distribution among stations, requiring rebalancing operations to ensure efficient operations. Adding to this complexity is the uncertain demand at stations, which can further complicate rebalancing efforts even during off-peak hours. This work introduces the Robust Bike Sharing Rebalancing Problem [RBRP], which employs robust optimization to improve rebalancing operations in BSSs. Despite the significant impact of uncertainty on system performance, few studies have addressed this aspect in the literature. We propose two new formulations and a tailored branch-and-cut algorithm for the RBRP. The first formulation is compact, leveraging the linearization of recursive equations, while the second relies on robust rounded capacity inequalities and feasibility cuts. Computational results based on benchmark instances demonstrate the effectiveness of our approaches, highlighting the benefits of robust solutions in supporting decision-making for BSSs.",The Robust Bike Sharing Rebalancing Problem - Formulations and a Branch-and-Cut Algorithm,"[74392, 55063, 30176]",281,"[145, 127, 14]",2334,Robust and Stochastic Routing Problems,49,2,35,"Stochastic, Robust and Distributionally Robust Optimization","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,44 [building - 303A],"['Vehicle Routing', 'Robust Optimization', 'Combinatorial Optimization']",MA-35
"In the response to the COVID-19 pandemic, vaccine distribution was initially characterised by scarcity with limited production capacity and high demand. While policy decisions to prioritise certain patient groups over others have been widely discussed, this paper focuses on providing a user-friendly and adaptable approach to align vaccine allocation decisions and distribution plans with the government-imposed prioritisation policies. We introduce the Vaccine Distribution Problem, and we propose four mathematical formulations relating to the Vehicle Routing Problem with profits, a concept that has not been adopted within humanitarian or healthcare supply chain applications. This problem aims to allocate the available vaccine supplies to a set of predefined vaccination sites fairly, following a given prioritization policy, while designing the corresponding set of vehicle routes that maximize the total collected profit. We incorporate split deliveries and incomplete service as well as a profit adjustment mechanism to ensure fairness in the vaccine allocation decisions and maximise resources utilisation. To validate and evaluate our approach, we created a real-world case study based on Liverpool, an English city that was affected severely by COVID-19, using only publicly available data.",Operationalising Policy - Distributing Vaccines in a Pandemic/Epidemic,"[61255, 79397, 79398]",554,"[58, 0]",2335,Infectious diseases and pandemics,38,13,21,OR in Humanitarian Operations [HOpe],"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,49 [building - 116],['Humanitarian Applications'],WB-21
"We develop an Adaptive Robust Optimization [ARO] model for managing spare parts inventory in the early stages of a product's life cycle. This model takes into account demand uncertainty and emergency shipments. To obtain the exact solutions of the ARO model, we establish its equivalence to a deterministic counterpart. We prove that the deterministic counterpart can be decomposed into two subproblems. Based on this decomposition, we develop an efficient algorithm to obtain near-optimal solutions for thousands of stock-keeping units.

We conduct a case study at ASML, highlighting the practical implications of our ARO model for spare parts inventory in the semiconductor industry. Our results show that the ARO solution is more cost-effective than that of the conventional stochastic optimization model. Additionally, the adaptability of the ARO model to parameter variations, such as the emergency shipment time and cost, demonstrates its superiority in providing flexible and economically viable solutions.",Robust Spare Parts Inventory Management with Emergency Shipment,"[77766, 58404, 36644, 21084, 77774]",379,"[127, 61]",2336,Trends and Open Problems in Robust Optimization,49,10,34,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,43 [building - 303A],"['Robust Optimization', 'Inventory']",TD-34
"We consider a newsvendor model with a retailer facing random demand and a primary supplier characterized by random capacity. The probability distributions of both the demand and the capacity are assumed to be known. The retailer places an order, which may not be satisfied in full because the quantity that can be delivered is capped by the capacity of the supplier. In order to mitigate this supply risk, the retailer places an order to a second supplier, which is reliable but more expensive, under a contract according to which he is allowed to return a portion of the order after the delivery by the primary supplier. We also assume that this option must be exercised before the realization of the demand.  We characterize the optimal orders to the two suppliers and present numerical results that provide interesting insights into the effect of various model parameters on these optimal quantities.",Analysis of a flexible order quantity contract in a newsvendor setting,"[24304, 20356]",162,"[138, 135]",2338,Stochastic Models in Logistics,50,8,39,Stochastic Modelling,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,35 [building - 306],"['Supply Chain Management', 'Stochastic Models']",TB-39
"Advertising is one of the crucial tools for retailers on the e-commerce platform for promotion, such as Amazon. This study focuses on the joint bidding and pricing policy for retailers subject to budget limitation under multiple periods. The bidding is for the impression on the sponsored advertising list. The click-through rate and conversion rate functions with respect to bid and price are learned and estimated based on historical data. We formulate a stochastic model and construct a dynamic programming formulation. We prove the existence of the unique optimal solution and characterize the structure of an optimized bidding and pricing policy. Through theoretical analysis and a case study, we investigate the relationships among the optimal policy,  value function, and key factors, and demonstrate the effectiveness of the optimal policy and the proposed dynamic model.  ",Data-driven advertising policy on e-commerce platform with budget constraint,"[73490, 73319, 78051]",695,"[32, 117, 124]",2339,Customer behaviour,11,5,59,Pricing and Revenue Management,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S08 [building - 101],"['E-Commerce', 'Programming, Stochastic', 'Revenue Management and Pricing']",MD-59
"The capacitated facility location problem [CFLP] is a core problem in location science. While the influence of the spatial patterns underlying a CFLP problem instance is widely acknowledged, their relationship with the optimal decisions is insufficiently explored. I will give an overview of my research on how spatial relationships between candidates and customers can be accessed through pattern recognition. With the help of an illustrative example, I demonstrate that these relationships affect the stability of solutions, the performance of solution algorithms, and even indicate the potential added value of moving to a more comprehensive problem formulation including further aspects like time or uncertainty. I outline how explicitly acknowledging these relationships bears significant potential for algorithm selection, algorithm development, and data-driven modeling.",On the interplay between data and decisions in discrete location problems,[59703],915,"[64, 5, 25]",2340,YW4OR_2,39,13,12,WISDOM - Women in OR,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,13 [building - 116],"['Location', 'Algorithms', 'Decision Analysis']",WB-12
"In the airline revenue management, dynamic pricing for substitutable itineraries presents significant challenges due to the stochastic nature of customer choices. The dynamic pricing problem can be conceptualized as a Markov decision process with the objective of maximizing the total expected revenue over a finite selling horizon. However, the complexity of this model, characterized by its multi-dimensional state and action spaces, makes it computationally prohibitive to solve exactly, even for for small-to-medium-sized instances. As a result, classic methods such as dynamic programming prove inefficient to address the dynamic pricing problem for larger instances. In this work, we introduce learning-based policies tailored to dynamic pricing problem of substitute itineraries with the same origin and destination. These policies are encoded by hybrid machine learning pipelines, while taking into account the heterogeneous customer choices. We proposed two different configurations of pipelines, where we have designed tailored training process for each of them. To demonstrate the performance of our learning-based policies, we first benchmark it against the optimal policy obtained from dynamic programming on small-sized instances. Then, we benchmark it against the state-of-the-art approximated policy heuristics on larger-sized instances. In addition, our method can be easily adapted to airline network scenarios, where itineraries of different origins and destinations are included.",Machine-learning-based policy for the dynamic pricing of substitutable itineraries,"[77357, 41118, 77775]",678,"[4, 66, 108]",2341,Machine Learning and Ensemble Learning with optimization methods,15,14,27,Mathematical Optimization for XAI,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,047 [building - 208],"['Airline Applications', 'Machine Learning', 'Programming, Dynamic']",WC-27
"Flight times are influenced by many factors. Examples are weather conditions or congestion at the airport, leading to ATC delays or limited gate availability. These factors influence the arrival time at the gate, which can thus vary considerably.

Since COVID, ground personnel for loading and unloading luggage to and from an aircraft are scarcely available, so we need to deploy them efficiently. Because of this scarcity, the process is highly sensitive to the variation of arrival times. To deal with this, a fast algorithm to [re]schedule the allocation of the luggage handlers is required.

We consider the assignment of luggage handlers to tasks arising from arriving and departing aircraft on the day of operations. We aim to make real-time updates that solve disruptions and are cost-efficient. This problem can be considered as a technician routing and scheduling problem [TRSP], where we consider teaming, skill and time window constraints in a dynamic setting. We have to synchronise multiple tasks corresponding to the same flight, which is a new aspect in regard to TRSP literature. Our algorithm proposes delays or cancellations of flights; these levers are often unexplored from a ground operations perspective.

We solve these disruptions using a simulated annealing algorithm, with the original solution as a starting state. This solves the problem in real-time. We will present computational results from experiments with real-life data provided by KLM Royal Dutch Airlines.",Disruption Management in Airline Luggage Handling using Local Search,"[76846, 19331, 78487, 74987, 78152]",269,"[4, 0]",2342,Disruption management and recovery,85,9,54,Public Transport Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S01 [building - 101],['Airline Applications'],TC-54
"The rapid increase in waste from used products, coupled with growing environmental awareness, has encouraged original equipment manufacturers [OEMs] to collect and properly treat these used products through recycling, remanufacturing, or hybrid recovery strategies. Concurrently, independent remanufacturers [IRs] have begun entering the market to remanufacture used products. However, the effect of such industry competition on OEM’s recovery strategies, consumer surplus and overall product returns remains unclear. In this study, we address these research questions by employing a stylised model in which an OEM faces competition from an IR, aiming to characterise optimal solutions for both parties. Our findings indicate that the entry of an IR positively influences the overall recovery of used products. Additionally, after the IR enters in the market, the OEM will be more reluctant to recover used products and less frequently opt for hybrid recovery strategies. Finally, through numerical analysis, we explore the variations in OEM’s and IR’s profits, and consumer surplus, across different scenarios.",Product recovery strategies considering competition - an analytical model,"[77770, 41201, 77795, 19453]",925,"[100, 50, 25]",2343,Game theory for the circular economy,18,12,23,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,82 [building - 116],"['OR in Sustainability', 'Game Theory', 'Decision Analysis']",WA-23
"In this talk, we present novel algorithms for online learning of linear programming heuristics for weakly coupled Markov Decision Processes [MDPs]. Leveraging two-timescale stochastic approximation theory, we design simple and efficient algorithms with theoretical guarantees. Furthermore, we show that we can extend our approach to learn policies for continuous state space weakly coupled MDPs, while also demonstrating that certain theoretical guarantees are preserved. Finally, we demonstrate the efficiency of our algorithms on real-world applications, including load-balancing coupled with autoscaling in parallel queues, and monitoring strategy of slowly varying signals in networks.",Online Learning Linear Programming Heuristic for Weakly Coupled MDP,[77506],14,"[82, 66, 121]",2344,Reinforcement Learning - Methods and Applications ,47,8,40,Advances in Stochastic Modelling and Learning Methods,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,96 [building - 306],"['Optimal Control', 'Machine Learning', 'Queuing Systems']",TB-40
"Modeling to generate alternatives [MGA] is an increasingly popular method in energy system optimization. MGA explores the near-optimal space [NOS], namely, system alternatives whose costs are within a certain fraction of the globally optimal cost. These alternatives may be preferred by real-world stakeholders due to intangible factors. Widespread MGA adoption is hampered by its additional computational burden. Current MGA methods identify boundary points of the NOS through repeated, independent optimization problems. Hundreds of model runs are usually required, and such individual runs are often inefficient because they repeat calculations or retrace previous trajectories.  In this presentation, we introduce a novel algorithm, called Funplex, which uses elements from multi-objective Simplex to optimize many MGA objectives with minimal computational redundancy . On a simple linear program energy hub case study, Funplex is ten times faster than existing methods and yields higher-quality NOSs. Sensitivity analyses suggest that Funplex scales well with the number of investment variables, making it promising for capacity planning models. The current implementation uses a full multi-objective tableau and therefore requires high memory and has poor stability on large models. Nonetheless, Funplex is a proof-of-concept that demonstrates the untapped potential of new solver methods. Further research on the topic may make MGA more standard and accessible among modeling teams.",Modified Simplex Algorithm to Efficiently Explore Near-Optimal Spaces,"[77039, 77779, 69598, 69248]",902,"[5, 93, 112]",2345,Advances in algorithms and applications for linear and convex optimization,68,14,38,"Conic Optimization - Theory, Algorithms, and Applications","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,34 [building - 306],"['Algorithms', 'OR in Energy', 'Programming, Multi-Objective']",WC-38
"Applications for optimization with uncertain data in practice often feature a possibility to reduce the uncertainty at a given query cost, e.g., by conducting measurements, surveys, or paying a third party in advance to limit the deviations. To model this type of applications, we introduce the concept of optimization problems under controllable uncertainty [OCU]. For an OCU we assume the uncertain cost parameters to lie in bounded, closed intervals. The optimizer can shrink each of these intervals around a certain value called hedging point, possibly reducing it to a single point. Depending on whether the hedging points are known in advance or not, different types of OCU arise. Moreover, the models may differ with respect to when the narrowing down, the underlying optimization, and/or the revelation of true data take place.
In the talk, we discuss two example problem settings - one with known and one with unknown hedging points - in more detail, where we handle the remaining uncertainty by the paradigm of robust optimization. For both cases, we give conditions under which a single-level reformulation is possible. Throughout the talk, we use shortest-path problems as underlying optimization problem for illustrating specifics of and phenomena arising in OCU.
",Robust Optimization Under Controllable Uncertainty,"[73248, 73299, 36110, 72662, 67066]",466,"[127, 111, 14]",2353,Robust and Multi-Level Optimization,86,10,04,MINLP,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,1001 [building - 202],"['Robust Optimization', 'Programming, Mixed-Integer', 'Combinatorial Optimization']",TD-04
"Innovative strategies are emerging to handle the growing challenges of last-mile delivery, such as high operational costs and increasing customer expectations. Crowdshipping is a novel solution for last-mile delivery that offers a quick and cheaper way to provide same-day delivery and the flexibility to scale up or down the delivery capacity as needed - retailers, in addition to professionals, employ ordinary citizens [a.k.a. occasional couriers - OCs] to perform deliveries [this is the case of companies such as Amazon Flex and Roadie]. The OCs can be categorized into two classes depending on their commitment to the company - dedicated OCs, who actively register, provide a schedule with their availability, and can accept some proposed deliveries for compensation; and en-route OCs, who are in-store customers who make deliveries on their commute, also for a compensation.
This work proposes a decision-support tool to empower decision-makers with insights into the trade-off between customer service levels and operational costs across different workforce compositions and compensation values. To achieve this, we conduct simulations where different compositions handle deliveries - exclusively the retailer’s fleet, or the en-route OCs, or the dedicated OCs, or a hybrid combination of those workforces. As a collateral result, the traveled distance of different workforce compositions and compensation strategies is measured, offering insights into their environmental impact.",Last-mile Delivery With Heterogeneous Crowdshipping and Service Levels,"[77048, 24368, 68357]",747,"[138, 145, 26]",2354,Last-Mile Delivery,5,12,64,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S16 [building - 101],"['Supply Chain Management', 'Vehicle Routing', 'Decision Support Systems']",WA-64
"This work introduces a production problem inspired by a real-world company, where the manufacturing process reflects a re-entrant flexible flow shop scheduling problem. Notably, in addition to the re-entrant feature, the challenge involves allocating additional resources, namely, workers, to specific production steps. This scenario can be characterized as a scheduling problem with dual-resource constraints.
A key aspect of the problem is the consideration of a cross-trained workforce, allowing workers to transition between stages during production. Furthermore, we consider a heterogeneous workforce, where workers can be differentiated through their skill level, which is defined by the number of stages they can operate on. 
The objectives of the problem are to minimize the maximum completion time and production costs influenced by employee salaries, reflecting their skill level. The payment structure assumes that each worker is paid for the period starting from their earliest operation starting time until the latest completion time including idle intervals in between. 
The bi-objective problem is solved using constrained programming and lexicographic optimization. To provide a comprehensive overview of solutions, prioritizing the makespan objective, the epsilon-constraint technique is employed to identify the set of non-dominated solutions. Preliminary findings suggest a substantial potential for cost reduction while maintaining the same throughput.
",A bi-objective production scheduling problem with re-entrant flows and labour resource constraints ,"[71660, 10538]",349,"[129, 107, 69]",2355,Resource constrained scheduling,35,10,60,Project Management and Scheduling,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S09 [building - 101],"['Scheduling', 'Programming, Constraint', 'Manufacturing']",TD-60
"This work introduces a methodology for optimising complex functions common in high-fidelity simulations across engineering fields, emphasising the strategic bridging of simulators and optimisation processes. Initially, we employ low-discrepancy sequence sampling to select simulation points, followed by training a surrogate model using a piecewise linear neural network with Rectified Linear Unit [ReLU] activation. Using Mixed Integer Programming [MIP], we reformulate the ReLU neural network as a MIP optimisation problem and solve it to find an optimal parameterisation of the simulator’s input. Based on the best solutions obtained, we then enter an iterative domain-refined phase of resampling the simulator, retraining the surrogate model, and rebuilding and resolving the MIP problem. For resampling, we use an infill strategy that incorporates error assessment to balance exploration and exploitation. This process continues until the accuracy of the objective function reaches the desired tolerance, ensuring high surrogate accuracy near the optimum, with techniques like memory structure reuse and warm starts for efficiency. Validation against standard test functions indicates that refining the surrogate model via an error-informed resampling strategy significantly enhances optimisation efficiency. This framework advanced in surrogate-based optimisation by synergistically combining adaptive sampling, neural networks, and MIP for enhanced performance in complex simulations.",Simulator-based surrogate optimisation - an enhanced framework for surrogate modelling and optimisation,"[76933, 57798]",520,"[66, 111, 131]",2356,"Advancements of OR-analytics in statistics, machine learning and data science 10",16,14,28,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,065 [building - 208],"['Machine Learning', 'Programming, Mixed-Integer', 'Simulation']",WC-28
"Assessing the impact of various hypotheses is a common task when working on models. In this paper, we focus on the linear parametric problem where we assess the impact of unknown constraints’ coefficients on the final objective. These multiple coefficients can vary linearly within a known range. Traditionally, this task is tackled using heavy computation, i.e., recomputing the optimum several times for every value of the coefficients, using approximations or using non-generic approaches. For large linear problems, we argue that computing every value of the coefficients in the interval is too computationally heavy and does not provide any information about the overall behavior between the points.

As a new approach to the problem, we derive several upper and lower bounds that allow us to avoid recomputing the problem for numerous coefficient values, provide guarantees between the computed points, as it does not allow for outliers, and require a smaller number of optimizations. We discuss these bounds and provide an iterative algorithm that uses them to compute an approximation to the original function within a given error threshold. We analyze the performance of the bounds on toy and real-world problems and demonstrate the effectiveness of the approach.",Parametric upper and lower bounds of linear variations of a linear problem’s LHS,"[74052, 74438, 54116, 74437]",902,"[110, 72, 127]",2357,Advances in algorithms and applications for linear and convex optimization,68,14,38,"Conic Optimization - Theory, Algorithms, and Applications","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,34 [building - 306],"['Programming, Linear', 'Mathematical Programming', 'Robust Optimization']",WC-38
"This work presents some calculus rules and new properties of the weak subgradients and the radial epiderivatives. The weak subgradient and the radial epiderivative concepts are based on the conical supporting methodology and therefore are able to characterize global behavior of a function under consideration and hence are used as a conical underestimation for nonconvex functions satisfying the Lipschitz condition from below. These generalized derivatives are used to establish global optimality conditions and to develop solution methods in nonconvex and nonsmooth optimization. Although the radial epiderivative concept can behave as the classical directional derivative in particular cases, it can be used to characterize and generate optimal solutions for many optimization problems that are impossible to handle with classical ones. In this work we present illustrative examples and demonstrate how the weak subgradients and the radial epiderivatives can be calculated for special classes of functions. The work presents applications for some classes of nonconvex and nonsmooth optimization problems.",Calculating of weak subgradients and radial epiderivatives and applications in nonconvex and nonsmooth optimization,[77617],357,"[52, 81, 19]",2358,Subgradient-based methods,70,9,41,Nonsmooth Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,97 [building - 306],"['Global Optimization', 'Non-smooth Optimization', 'Continuous Optimization']",TC-41
"In the last decades, there has been an increasing generation of solid waste in urban areas, imposing an environmental, economic, and social burden worldwide. Thus, there is a need to estimate solid waste generation rates for obtaining efficient collection routes and itineraries, particularly when two or more communes decide to collaborate their resources [e.g., vehicles and staff], in order to reduce costs and CO2 emissions, and increase equitable workload. This study performs spatial analysis using census data, and historical data based on daily GPS measurements and tare weights [tonnage] of collection vehicles in two communes in Santiago, Chile to identify collection zones and estimate initial solid waste generation rates at the street level. Machine learning techniques are applied to predict daily solid waste generation rates using demographic characteristics, land-use, socioeconomic status, and weather information, which are subsequently tested with the initial solid waste generation rates estimations. Prediction results are promising when compared with these initial estimations. Our results provide important insights for supporting the optimization of collaborative collection truck routes among different communes in urban areas.",Estimating Urban Solid Waste Generation for Collaborative Collection Routes using Spatial Analysis and Machine Learning,"[46356, 74356]",911,"[66, 145, 139]",2359,"Assessment Methods for Shaping the Green, Inclusive, and Digital Cities II",79,3,18,Sustainable Cities,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,42 [building - 116],"['Machine Learning', 'Vehicle Routing', 'Sustainable Development']",MB-18
"Medical transport is increasingly important in ensuring access to care for patients unable to travel independently. The patient transport business comprises many aspects in which optimization needs can arise. For example, it could benefit from optimized vehicle routes, improved and more reliable access to patient care, and improved working conditions for ambulance drivers thanks to more predictable schedules. 

This presentation will detail an industrial application developed by Hexaly for a French leading healthcare company.  This tool optimizes the operational planning of non-emergency patient transport carried out the following day.

The problem consists in assigning vehicles to employees and creating mission rounds for each employee. The underlying optimization problem is related to the Dial-A-Ride Problem. Indeed, one of the key features of non-emergency patient transport is the possibility of transporting several people in the same vehicle at the same time. Employees’ schedules are also subject to numerous contractual and regulatory constraints such as breaks.

A typical dataset of the problem corresponds to a daily volume of 500 missions to be planned, with around 50 employees and vehicles available.

The optimization tool based on Hexaly Optimizer brought significant gains when it moved into production. The quality of the employee schedules and vehicle tours has improved, with gains of around 10% in terms of revenue per hour and distance traveled.",Operational planning of medical vehicle routes,[63575],593,"[143, 56, 134]",2360,EMS logistics,3,2,10,OR in Health Services [ORAHS],"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,11 [building - 116],"['Transportation', 'Health Care', 'Software']",MA-10
"There is still much room for exploiting high-performance computing hardware to solve hard combinatorial optimization problems, such as those in logistics and scheduling. However, the difficulty of partitioning and organizing the work of typical MIP tree-search algorithms limits the impact of approaches aimed at massive parallelization, even though the use of large computing clusters or GPUs has considerable potential in theory. As a result, MIP solvers are frequently used on single CPUs with limited shared-memory parallelization via multi-threading; however, even in these situations it is often not worthwhile to use more than a few cores. This usually involves procedures that take advantage of performance variability by concurrently running different configurations in parallel. To gain a deeper understanding of the performance of available MIP solver software in these multi-threading scenarios, we conduct strong scaling experiments to analyze the influence of hardware characteristics [such as processor type, number of cores, clock speed, cache, RAM] and performance variability. We provide insights into performance optimization in terms of runtime, speedup, cost, and power consumption.",Performance Analysis of MIP Solver Software on Various Hardware Architectures,"[77555, 7569, 78581]",401,"[102, 134, 63]",2361,Parallel Optimization and Scalability,64,13,52,Combinatorial Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,8003 [building - 202],"['Parallel Algorithms and Implementation', 'Software', 'Large Scale Optimization']",WB-52
"We study a simple economic-ecological framework of resource extraction that explicilty incorporates [i] a regimechanging shock and [ii] the possibility a resistence threshold/Skiba-point to lead to divergent behaviours towards either systemic recovery or system collaps following the shock. This structure allows us to model resilience [in the sense of regime-changing shocks either being avoided or systemic recovery being possible and optimal] and its behavioural implications in a meaningful way. Specifically, we propose a model-based measure of resilience that upon proper calibration of the model can be employed in numerical assessments of the implications for resilience of different extraction policies. We also study a simple example and show that anticipation of shocks is leading to precautionary behaviour that boosts systemic resilience.",A model-based measure for the resilience of resource use under the risk of disruption,"[19666, 19669]",897,"[31, 82, 40]",2362,Optimal control and resilience,90,9,33,Optimal Control Theory and Applications,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,42 [building - 303A],"['Dynamical Systems', 'Optimal Control', 'Environmental Management']",TC-33
"Electric vehicles [EV] play a crucial role in achieving decarbonization targets and with increasing adoption, their charging patterns will have increasing impact on energy systems. In this presentation, we present a transferable methodology to assess the influence of EV charging strategies on the design and operation of district-level energy systems in urban, suburban, and rural municipalities in Northern Germany. We use the energy systems optimization model RE3ASON to assess the cost-optimal trajectory until 2050 subject to environmental constraints. We extend the detailed case studies by incorporating EV behaviour to evaluate the impact of passive charging, smart charging, and vehicle-to-grid [V2G] strategies on the energy system. In a two-stage optimization, the travel times and distances are sampled from a comprehensive German data set, aggregated in a way that ensures that energy and power limits of each EV are respected, and then implemented in the main optimization stage. Battery degradation is linearly approximated, which is particularly relevant for V2G modelling and a novel aspect of this study. Hence, our methodology can assess the trade-offs between the additional flexibility that V2G provides and the resulting costs in terms of battery degradation. Our findings provide quantitative estimates for the additional benefits that various charging methods may have in terms of reducing cost, facilitating photovoltaic integration and reducing battery storage requirements.",Integrating EV charging behaviour into a municipal energy system model to explore trade-offs between flexibility and battery degradation ,"[77779, 75113, 69378, 69248, 75114]",465,"[93, 143, 84]",2363,Multi-energy systems,23,4,19,OR in Energy,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,44 [building - 116],"['OR in Energy', 'Transportation', 'Optimization Modeling']",MC-19
"E-commerce retailers have substantially reinvented warehousing. To pick small, heterogeneous orders in a short period of time, novel warehousing approaches have been developed. The equivalent of classical manual warehousing is, hereby, mixed-shelf storage; where pieces of the same SKU can be found at many storage positions. This, however, makes the picker routing problem, which aims to find a shortest picking tour for collecting all items on the current pick-list, a combined selection and sequencing problem. The storage positions to be visited must be selected, and a tour through the selected positions must be planned. The resulting problem, often referred to as the Single Picker Routing Problem with Scattered Storage [SPRP-SS], has gained much attention recently. Most of the proposed procedures, however, assume a rectangular layout of warehouses, such that the subproblem, once the positions to be visited are selected, is solvable in polynomial time. However, for warehouses in existing dense inner-city buildings or multi-level mezzanine systems, the property of rectangular layouts is not met. In the talk, an exact and heuristic solution approach for solving the SPRP-SS for arbitrary distance matrices is presented and discussed.",A Flexible Procedure for the Single Picker Routing Problem With Scattered Storage,"[50403, 77784]",733,"[146, 32, 129]",2364,Routing in Warehouses,5,2,64,VeRoLog - Vehicle Routing and Logistics,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S16 [building - 101],"['Warehouse Design, Planning, and Control', 'E-Commerce', 'Scheduling']",MA-64
"Over the past decade, the use of learning management systems [LMS] for online and blended courses has become widespread in higher education institutions, resulting in a high increase in the available data. The unprecedented volume of data has allowed researchers to develop new and improved algorithms for monitoring the learning process, becoming a useful tool for teachers, given the absence of face-to-face interaction with the students. In our work, we developed a new iteration of a classification method that uses sequence mining from event log data to predict lower-performing students during the realization of a course. Previous versions of this algorithm were based on non-continuous scenarios, using data available for various courses during a single semester. In the present study, we made optimizations to the algorithm, considering a greater diversity in the available training data, fine-tuned using performance metrics that minimize false negatives, and delimited a new, more general methodology, for present and future use cases. We tested our enhanced method in a second-year undergraduate blended course from the Engineering School at Universidad de Chile, with online content, activities, and assessments supported in the institutional LMS. Results show that the enhanced algorithm improves both accuracy and recall, allowing professors to identify the segment of lower-performance students before the course concludes, allowing for proactive measures to be taken.

",Sequence mining algorithm for students failure classification in multiple blended courses scenarios,"[72057, 74609, 74714]",149,"[7, 18, 34]",2366,Learning Analytics using Mathematical Optimization and XAI,15,12,27,Mathematical Optimization for XAI,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,047 [building - 208],"['Analytics and Data Science', 'Computer Science/Applications', 'Education and Distance Learning']",WA-27
"Low carbon and greener Europe priority has also been a great challenge for heating of households. In Slovakia, the recent results of the 2021 Housing Census showed e.g., that up to 66.22% of households are heated with gas, up to 21.32% with solid fuel and 4.48% with electricity. Various subsidy mechanisms were introduced by government to support the transition towards low-emission energy sources used for heating. Among others, a web application “Heating in households a little differently” was developed in order to make the people in Slovakia more aware of air pollution from domestic heating as well as to show the ways how to improve the quality of air. This paper presents the regional data [for 79 Slovak districts] to show the huge regional differences concerning the heating [solid fuel, gas], but also analyses other regionally unevenly distributed variables with potential impact on the source used for heating. Instruments of exploratory spatial data analysis and spatial econometrics were used for analysis. The results showed that both the spatial autocorrelation and spatial instability were strongly confirmed in case of Slovak districts.",Low carbon and greener Europe priority as a challenge for heating in Slovakia,[12545],813,"[94, 40, 139]",2368,Emissions and Heating Sector,80,13,53,Sustainable and Resilient Systems,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,8007 [building - 202],"['OR in Environment and Climate change', 'Environmental Management', 'Sustainable Development']",WB-53
"The presented work is inspired by a real-world application in autonomous open pit mining. A drill rig is tasked with digging a large number of blast holes, organized in a tight semi-regular pattern. Digging each hole generates a pile of excess material, that represents a new obstacle for the drill rig.  The goal is to determine the piles placement and drill all holes, while minimizing total distance traveled, avoiding all already placed obstacles and getting stuck between them.
We propose the concept of self-deleting graphs, where visiting a vertex causes deletion of a predefined set of edges. This formalism can capture the dynamic addition of obstacles in the motivating application, which is dependent on the previous drill rig path. On these graphs, we formulate variants of the classical Hamiltonian Cycle Problem and Traveling Salesman Problem and design novel solvers for these problems, both exact and heuristic. 
In the next step, we address the problem of pile placement in a close vicinity of each hole, given a fixed tour. We formulate the problem as Path-Conforming Circle Placement Problem, propose a heuristic solver and derive solution bounds.
Finally, we combine both problems together and fully address the original application as the Traveling Salesman Problem with Circle Placement, which we solve by combining the already developed approaches. The resulting solver is evaluated on several newly created datasets mimicking the real-world applications.",Travel and Modify - Travelling Salesmen Problem in Environments Altered by the Salesman,"[74241, 74319]",758,"[97, 145, 31]",2371,Real-Life Applications in Routing,5,5,58,VeRoLog - Vehicle Routing and Logistics,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S07 [building - 101],"['OR in Mining', 'Vehicle Routing', 'Dynamical Systems']",MD-58
"As part of the research project “Bidding strategies and their influence on electricity market prices in the EU single market [BETS]” a methodology for characterization of strategic bidding strategies was developed. One of the main targets of the project is the development of a strategic bidding agent for agent-based electricity market models. A special focus will be on the cross-border strategic bidding. An important step for the implementation of the agent is the definition of different strategies that the bidding agent should use, and that could be found in today’s EU spot market. In this context, based on a profound literature research, a methodology for characterization strategic bidding was developed. A focus was set on aspects of the core bidding strategy, the field of application, such as the markets or the used solution and price methodology. Also, different boundary conditions considered in the literature, focus technologies or uncertainty about information for the bidders were considered in the characterization. With this methodology the relevance of different strategies could be analysed and relevant cross-border strategies could be specified, even if they have been analyzed only for individual market areas.",Methodology and results of characterization of strategic bidding strategies,[77781],244,"[3, 0]",2373,Modelling European market coupling ,22,5,14,Energy Markets,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,16 [building - 116],['Agent Systems'],MD-14
"The way to represent the position of an activity on the time axis is at the heart of planning and scheduling models. A first distinction is done between continuous and discrete time formulations. This work focuses on discrete-time models that consist in choosing the start time of an activity among a discrete set of successive values. For this purpose, several time-indexed formulations have been proposed. They use as many binary variables as time steps on the horizon for each activity. The number of variables
could be significantly reduced using binary encoding to represent the set of time values, but the attempts to determine activity start times this way have generally leaded to models with poor relaxations. Vielma and Nemhauser have proposed formulations to model disjunctions with a logarithmic number of variables and constraints. They study the association with SOS2 variables in order to model piecewise linear functions, but their models are also suitable for time-indexed formulations in scheduling problems.

This work presents the adaptation of Vielma and Nemhauser’s formulation to time-indexed scheduling models. The comparison is done with classical time-indexed formulations through the Resource-Constrained Project Scheduling Problem [RCPSP]. Experiments give the relaxation values for both formulations. An alternate formulation is proposed, with more constraints but experimentally competitive. Then, binary encoding is explored to improve the linear relaxation.
",MILP formulations with a logarithmic number of binary variables for planning and scheduling,[36176],835,"[129, 111, 14]",2374,Mathematical programming for machine scheduling,32,15,49,"Lot Sizing, Lot Scheduling and Production Planning","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,M1 [building - 101],"['Scheduling', 'Programming, Mixed-Integer', 'Combinatorial Optimization']",WD-49
"The industrial sector is one of the major contributors to greenhouse gas emissions worldwide; it is therefore important to promote the use of alternative and emerging technologies that employ non-fossil-based feedstocks and renewable energy sources to meet the net-zero greenhouse gas emissions target. The electrolysis of CO2 [CO2E] is a carbon capture and utilization technology with the potential to produce chemicals and fuels by using water [H2O] and electricity. We consider the generation of CO2-based syngas [CO\H2] by high-temperature CO2 electrolysis as an alternative route to steam methane reforming that employs natural gas. Syngas is used in the chemical industry as a chemical building block and in the transportation sector as an intermediate for fuel production. Currently, fossil-based syngas is only consumed at its production sites. In this paper, we propose syngas as a merchant product, but since its market transition is new, there is a lack of comprehensive data on its demand. This uncertainty in syngas market demand can significantly affect the adoption and successful implementation of CO2E technology. To deal with the uncertainty, we propose a robust supply chain design for CO2-based syngas where strategic decisions regarding investments in the electrolyser locations and pipeline infrastructure and operational decisions regarding the amounts of syngas produced and CO2 and syngas transported between echelons are made over a multi-period planning horizon.",Sustainable Supply Chain Design for CO2 Electrolysis under CO2-based Syngas Demand Uncertainty,"[76782, 56553, 26901, 77899]",545,"[127, 64, 138]",2375,Clean Energy Supply Chains,19,4,24,Sustainable Supply Chains,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,83 [building - 116],"['Robust Optimization', 'Location', 'Supply Chain Management']",MC-24
"This talk presents a parallel algorithm for the bi-objective minimum spanning tree [BMST] problem which takes advantage of a two-phase procedure for the generation of a complete set of efficient solutions.
The first phase involves computing the extreme supported efficient solutions by resorting to an algorithmic approach [Prim embedded in a dichotomic search], while the second phase focuses on determining the non-extreme supported and non-supported efficient solutions. 
In this latter phase, all spanning trees of a connected graph are generated through edge interchanges based on increasing evaluation of reduced costs of associated weighted linear programs.
The results of preliminary computational experiments will be discussed.",Parallel computation of the Pareto frontier for the bi-objective minimum spanning tree problem,"[77590, 60903, 2042]",51,"[112, 14, 102]",2376,Multiobjective Combinatorial Optimization,34,12,37,Multiobjective Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,33 [building - 306],"['Programming, Multi-Objective', 'Combinatorial Optimization', 'Parallel Algorithms and Implementation']",WA-37
"Humanitarian logistics literature commonly uses Equity, Efficiency and Effectiveness [3E] objectives. The equity objective aims to minimize differences in individual treatments by assuming equal impact for everyone in a disaster. Efficiency measures aim to reduce the costs of aid programs, while effectiveness focuses on the quality of humanitarian aid, measured by factors such as response time, reduced risk, or human suffering. 3E objectives presume beneficiary homogeneity, yet it is crucial to recognize that disasters disproportionately impact socioeconomically disadvantaged individuals. Vulnerable groups—low-income, elderly, disabled, or marginalized—face distinct challenges and heightened risks in disasters. Any measure assuming homogeneous demand neglects the intersectionality of vulnerable communities. This study presents an alternative approach to prioritizing vulnerable populations in disaster response, striving for a more inclusive and compassionate disaster management strategy. To assess the performance of this approach against traditional 3E measures, we analyze the emergency assembly point allocation problem using Istanbul's neighborhood-level vulnerability and population characteristics in our computational study. Our results show that the vulnerability-based prioritization approach can single-handedly achieve more inclusive results for vulnerable populations without significantly deteriorating 3E objectives and non-vulnerable population outcomes.",Vulnerability Based Prioritization in Disaster Response Efforts,"[77527, 66302, 66107]",775,"[30, 58]",2377,"Efficiency, equity and fairness in humanitarian operations",38,9,21,OR in Humanitarian Operations [HOpe],"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,49 [building - 116],"['Disaster and Crisis Management', 'Humanitarian Applications']",TC-21
"Landscape, as an environmental asset, can be evaluated using various techniques. One of the most common approaches is to use a system of indicators, to take account of the different characteristics that make up the landscape. Indicators alone cannot provide a comprehensive overview of territories. In addition, territories are characterised by dynamics and characteristics that change over time, requiring a diachronic analysis. Within this context, the present work proposes a Multi-Criteria Analysis [MCA] to provide a synthetic index capable of describing the economic value of the landscape. For the elaboration of that index, many data referring to various aspects and different periods are considered. Specifically, the current state of the landscape is compared with past recognized conditions, taking into account the dynamics of agriculture, tourism, real estate market and forest dimensions over time. The aim is to provide systematic information useful to support decision-makers and policymakers in the protection, conservation, and planning of the landscape at a regional scale. To test the selected set of criteria and the construction of the index, the present study applies the proposed method to the Piedmont region in north-west Italy, because of its diversity of landscapes, ranging from
mountains and hills to plains.",A Diachronic Index for Assessing Landscape Economic Value,"[77757, 7119, 68019, 77726]",158,"[26, 137, 139]",2378,"How to support complex decisions. Negotiating the trade-off between Social, Environmental and Economic values 1",44,4,47,Multiple Criteria Decision Analysis,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,50 [building - 324],"['Decision Support Systems', 'Strategic Planning and Management', 'Sustainable Development']",MC-47
"Many hospitals face the challenge of an increasing demand for admissions to their inpatient nursing wards. The possible solutions to this problem include relocating patients, merging nursing wards, and optimizing the balance of bed capacity between them. In any case, hospital planners need a model that describes the influence of the admission rate, length of stay, allocated capacity, and relocation policies on the occupancy of beds. This study proposes a model based on a continuous-time Markov chain. We reduce the model's computational requirements by evaluating each ward separately and approximating the process governing patients arriving from the other wards. We base our approximation on interrupted Poisson processes, where the interruption times follow hyper-exponential distributions. By employing this approach, we are able to evaluate the occupancy of all inpatient nursing wards in a Danish hospital. Our numerical experiments indicate that this approach is insensitive to the length of stay distribution type and accurately reflects the occupancy of many wards in the hospital.",A Markov chain for the evaluation of inpatient flow in hospitals,[51319],103,"[56, 135, 121]",2379,Healthcare services,3,5,15,OR in Health Services [ORAHS],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,18 [building - 116],"['Health Care', 'Stochastic Models', 'Queuing Systems']",MD-15
"In this presentation we consider a class of multi-objective optimization problems known as Minkowski sum problems or Pareto sum problems. Pareto sum problems are multi-objective optimization problems with a decomposable structure where the [global] Pareto set [the nondominated points of the feasible objective space] can be described as the Pareto set of the Minkowski sum of several [local] Pareto sets obtained from subproblems. Much research has been done on efficiently filtering for the set of [global] Pareto sum given several known [local] Pareto sets. In some cases, only subsets of the [local] Pareto sets are required for generating the global Pareto sum. As such, if a subproblem solution does not contribute in generating the [global] Pareto sum, then the computational effort spent on finding the solution is effectively wasted. For this reason, we investigate how small a subset of the Pareto optimal solutions from subproblems is necessary to describe the Pareto sum. Furthermore, we investigate when unnecessary Pareto optimal solutions exists in the subproblems and how to identify them. For this purpose we propose so-called generator upper bound sets, which can be used when solving the subproblems to exclude some unnecessary solutions. If and when these non-generating or unnecessary Pareto optimal solutions can be identified and excluded, the computational burden of identifying a generating subset of Pareto optimal solutions of a subset may be substantially reduced.",Generator sets for Minkowski Sums - Theory and Insights,"[77783, 36026, 55216]",200,"[77, 112, 14]",2380,Multi-objective Combinatorial Optimization,64,4,52,Combinatorial Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,8003 [building - 202],"['Multi-Objective Decision Making', 'Programming, Multi-Objective', 'Combinatorial Optimization']",MC-52
"Deep Reinforcement Learning [DRL] methodologies have garnered increasing attention in addressing combinatorial optimization challenges, particularly in domains such as routing and scheduling. While recent approaches have demonstrated notable efficacy, especially in classic problems like the Traveling Salesman Problem [TSP] and Capacitated Vehicle Routing Problem [CVRP], they often operate within simplified problem settings, lacking real-world side constraints. Consequently, DRL methods encounter difficulties in generating feasible solutions for more complex scenarios. In this study, we address these limitations by introducing additional real-world side constraints and exploring diverse mechanisms to accommodate them while steering the search towards feasible solutions. Our experimentation extends to a variety of combinatorial optimization problems, including the Capacitated Vehicle Routing Problem with Time Windows [CVRPTW] and Skill-VRP, showcasing our approach's effectiveness in handling practical constraints.",Addressing Real-World Side Constraints in Combinatorial Optimization with Deep Reinforcement Learning,"[77782, 27003, 57910]",312,"[14, 145, 66]",2381,[Deep] Reinforcement Learning for Combinatorial Optimization 1,14,4,03,Data Science Meets Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,1005 [building - 202],"['Combinatorial Optimization', 'Vehicle Routing', 'Machine Learning']",MC-03
"Hub-and-spoke problems have been widely studied in the literature and as they are more of a strategic problem, little effort has been devoted to incorporating service-related restrictions into them. However, current trends in transportation seek more flexibility and tighter service schedules. In this talk we will discuss the nuances of integrating time constraints into hub-and-spoke systems. Two alternative and efficient formulations for hub network design with strict time restrictions and service levels will be presented. To illustrate our contribution, a case-study inspired from the healthcare network of Québec, Canada, will allow us to show the interest and importance of considering this restriction explicitly in the model. ",Time-constrained hub-based networks - Designing beyond location-allocation,"[63299, 77800, 59323]",975,"[79, 65, 143]",2383,Transportation Network Modelling and Optimization II,6,3,55,Transportation,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S02 [building - 101],"['Network Design', 'Logistics', 'Transportation']",MB-55
"Emergency medical service [EMS] has to respond quickly and efficiently to all emergencies within a considered area. However, especially in areas with heterogeneous demand distribution, like urban, mixed and rural areas, the level of coverage can vary widely. To reduce inequalities in coverage, many approaches take into account fairness as model objective by explicitly addressing the coverage of the worst-covered area. Thereby, the coverage levels of the second, third etc. worst-covered areas are not directly addressed. 
Therefore, we propose to maximize the average expected coverage of the set p of worst-covered areas. Our model explicitly considers the second, third etc. worst-covered area and aims to improve not only the coverage level of the worst-covered area, but the average coverage level of the set p of worst-covered areas. 
Our fairness objective’s applicability is analyzed at hand of a real-world case study for the city of Duisburg [Germany]. We examine different sets p of worst-covered areas to analyze the influence on the individual coverage levels of the different areas. First results show that the proposed fairness objective can improve the average coverage of the set p of worst-covered areas in EMS location planning context. At the same time, computational complexity of our model is low enough to maintain solution quality. 
",Modeling Fairness in Covering Problems of Emergency Medical Service Location Planning,"[72701, 24622]",593,"[56, 64]",2384,EMS logistics,3,2,10,OR in Health Services [ORAHS],"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,11 [building - 116],"['Health Care', 'Location']",MA-10
"In the context of influence diagrams, we formalise the notion of information decisions which are associated with decision nodes that determine what information will be acquired to support decisions at a given decision node later; or, more specifically, what is the set of nodes whose states are known when making this later decision. Examples of such information decisions include choices among diagnostic tests because testing decisions determine what test results will be available when making treatment decisions, for instance. For the development of optimal strategies, we build on Decision Programming [Salo et al., EJOR 299/2, 2022] in which the influence diagram is solved by converting it into an equivalent mixed-integer linear programming [MILP] problem; this makes it possible to solve influence diagrams which do not fulfil the usual ‘no-forgetting’ assumption, or which involve logical, resource, or risk constraints that preclude the use of common solution approaches such as arc reversals and node removals. We present optimization formulations that employ binary variables to represent information decisions. We also provide numerical examples to illustrate the development of optimal testing strategies for information acquisition. 
",Optimal Testing Strategies for Information Acquisition in Decision Programming,"[2268, 77261, 57798]",483,"[25, 136, 126]",2385,Decision problems represented as influence diagrams,49,12,34,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,43 [building - 303A],"['Decision Analysis', 'Stochastic Optimization', 'Risk Analysis and Management']",WA-34
"Many-to-one assignment games [Sotomayor, 1992] are extensions of one-to-one assignment games [Shapley and Shubik, 1972]. It is know that the core of many-to-one assignment games is non-empty and has the two side-optimal special vertices. We consider many-to-one assignment games as special linear production games [Owen, 1975] and investigate the Owen core [the set of core allocations obtained from the dual optimal solutions of the LP for the grand coalition]. Unlike in assignment games where the two sets coincide, in many-to-one assignment games the dimension of the Owen core is at most the number of players allowed to be assigned to several players of the other type, while the dimension of the core is at most the number of the 1-capacity players.
We show that the Owen core of a many-to-one assignment game can be described as the core of an associated assignment game. This facilitates the application of various results known for the core of assignment games to the Owen core of many-to-one assignment games. For example, based on the graph-theoretic characterization of the two side-optimal core vertices [Solymosi, 2023], we show that the same allocation maximizes the payoffs for all 1-capacity players both in the Owen core and in the core, but the allocation which minimizes all these payoffs in the Owen core need not be an extreme point of the core. We also provide conditions for the coincidence of these minimizing allocations.
",On the Owen core of many-to-one assignment games,"[18008, 77801]",385,"[50, 0]",2386,"Game Theory, Solutions and Structures I",88,2,36,"Game Theory, Solutions and Structures","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,32 [building - 306],['Game Theory'],MA-36
"Within the defence and security domain, the need for foresight [in order to be able to anticipate trends that will impact the future security environment] is increasing due to the complexity of the changing world around us. Although there are forecasts supported by quantitative analyses, foresight is a discipline in which expert judgement remains central. Therefore we advocate a symbiosed approach of human creativity combined with analytical support.
TNO’s VISTA [View on STrategic Anticipation] toolkit allows to combine more analytical rigour with human creativity in foresight. The toolkit provides tools to policymakers and analysts with regards to foresight in the defence domain, based on seven steps. These steps do not need to be taken sequentially, nor are they always needed depending on the scope of the foresight case. 
The presentation of this paper will address the individual steps of our VISTA toolkit, including the way we combined input from analytical methods, scientists and military experts. Special attention will be given to elements that distinguish our toolkit from other, frequently used approaches - [1] a technology focus as starting point, [2] an integral approach, by not only looking at trends in isolation, but to explicitly relate trends with one another throughout the entire DESTEP spectrum and [3] a more detailed insight in possible future security challenges by adding a regional perspective to global future world scenarios. 
",Strategic foresight - Combining human creativity with analytical rigour,[77785],677,"[10, 15, 47]",2387,Scenarios and foresight practices - Behavioural issues III,13,14,11,Behavioural OR,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,12 [building - 116],"['Behavioural OR', 'Complex Societal Problems', 'Forecasting']",WC-11
"In supervised machine learning, especially with high dimensional data, feature selection is an important approach that aims to ensure efficient computation, to achieve a high generalization ability of the learning algorithm, and to support the interpretability of the results. However, the feature subsets selected during feature selection may vary depending on the partition of the training dataset used or the feature selection method deployed. This instability can be a result of including redundant and irrelevant features in some of these subsets or not retaining all relevant features in each of them. The goal of this study is to develop a methodology to find a subset of the relevant features using network science and community detection from among the individual feature subsets. The individual feature subsets will be used to construct a network, where each node represents a feature, and edges between nodes reflect whether two features were selected into the same subset. By considering several runs of a feature selection method, features that appear together often will have a stronger connection, represented by a higher weight on the edge between them. This network will then be analyzed to discover communities, which ideally represent a set of features that is highly connected and, thus, is jointly highly important for the classification task. Optimally, this also allows to visually and/or numerically distinguish the sets of relevant, redundant, and irrelevant features. ",Feature set aggregation using community detection in networks ,"[77706, 47369]",65,"[7, 53, 66]",2388,Network Analytics,17,5,31,Analytics,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,54 [building - 208],"['Analytics and Data Science', 'Graphs and Networks', 'Machine Learning']",MD-31
"SCIP is an open-source optimization solver for mixed-integer linear and nonlinear optimization problems and a framework for constraint integer programming and branch-cut-and-price implementations. This talk will highlight the latest developments in SCIP 9.0, released in February 2024, including symmetry handling, constraint handlers, primal heuristics, cut generation and selection techniques, branching rules, and LP interfaces. SCIP 9.0 has an improved overall performance in solving time, the number of nodes in the branch-and-bound tree, and the solver's reliability.",Recent Advances in the SCIP Optimization Solver,[67423],237,"[111, 11, 13]",2389,MIP Solvers,76,2,30,Software for Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,53 [building - 208],"['Programming, Mixed-Integer', 'Branch and Cut', 'Column Generation']",MA-30
"Cancer is one of the most prevalent causes of death worldwide, second only to heart disease. Chemotherapy is the most widely used approach to treat cancer patients. There are many challenges for planning of chemotherapy appointments in oncology clinics, including the limited capacity of clinics and the uncertainty of the duration of treatment. For this end, an effective planning of chemotherapy schedules is vitally important.

In this study, we consider the problem of chemotherapy planning, where patients are assigned to days in the planning horizon according to their treatment frequency. Due to the uncertainty of durations, we model the problem as a two-stage stochastic program, with an objective that minimizes a combination of earliness and tardiness of appointment days, as well as idle time and overtime of chemotherapy chairs. As realistically sized instances of the problem are impossible to be solved by commercial solvers, we propose a scenario reduction-based heuristic approach that relies on k-medoids clustering to find the representative scenarios. We test the performance of our approach on a dataset from a chemotherapy clinic and drive managerial insights.
",A stochastic programming approach for chemotherapy planning,"[61009, 77149, 68064]",609,"[56, 117]",2390,Radiotherapy and chemotherapy planning,3,7,10,OR in Health Services [ORAHS],"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,11 [building - 116],"['Health Care', 'Programming, Stochastic']",TA-10
"The growing importance of e-commerce retail, accompanied with the need of offering a high customer service level, requires companies to optimise their operations. One possible area of improvement is the integration of order picking and delivery decisions. If a new customer order arrives, the requested items first need to be picked in the warehouse. Afterwards, the items are delivered to the customer’s location. Traditionally, the picking and delivery operations are considered individually and in a sequential manner. By using integrated decision making, however, more efficient operations can be obtained.
 
Since customers demand very fast deliveries, the use of an online optimisation strategy is preferred, as it allows to consider new customer orders very quickly in the current operating schedule. While previous research considered the integration of picking and delivery in a static problem setting, its impact in a dynamic setting is yet unstudied.

We propose new metaheuristic algorithms to solve the integrated picking and delivery problem in a dynamic setting. The performance of several algorithms, i.e., a simple sequential algorithm, an iterative algorithm and two integrated algorithms, is compared to show the benefits of using a more involved optimisation strategy. 
",Integrating Order Picking and Delivery Decisions in a Dynamic Setting,"[67590, 36613, 23193, 23971]",733,"[65, 145, 146]",2391,Routing in Warehouses,5,2,64,VeRoLog - Vehicle Routing and Logistics,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S16 [building - 101],"['Logistics', 'Vehicle Routing', 'Warehouse Design, Planning, and Control']",MA-64
"In the Parallel Machine Scheduling Problem [PMS] with makespan minimization, we are given a set of jobs with processing times and a set of identical parallel machines. Each machine processes at most one job at a time, and preemption is not allowed. The problem consists of scheduling all jobs into the machines, minimizing the maximum completion time of the jobs. Considering the fact that the processing times of the jobs are not exactly known in advance but typically belong to a given interval, in this work, we address a variant of the PMS designed to represent data uncertainties affecting the processing times. The resulting Robust Parallel Machine Scheduling Problem [RPMS] uses a budgeted uncertainty set, where a limit is imposed on the number of jobs whose processing time can change from their nominal value on each machine. Starting from the Dynamic Programming [DP] algorithm for the 0-1 Robust Knapsack Problem, we introduce an arc flow formulation based on the DP algorithm. To assess the efficiency of the model, we have tested it on a set of instances from the literature, identifying the methods that yield the best results.",Arc Flow Formulation for the Robust Parallel Machine Scheduling Problem,"[77298, 68134, 9828, 7965]",835,"[129, 127, 108]",2392,Mathematical programming for machine scheduling,32,15,49,"Lot Sizing, Lot Scheduling and Production Planning","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,M1 [building - 101],"['Scheduling', 'Robust Optimization', 'Programming, Dynamic']",WD-49
"To benefit from digitization, decision support systems and recommender systems find wide business-to-business applications with the increasing digitization. To ensure that organizations benefit from implementing such systems, how employees interact with them as users is crucial. Certainly, modifications to system elements, such as design elements, can improve usability by mitigating the gap between what the user can work with and what the system offers. However, cognitive biases such as the hindsight bias and the overconfidence bias affect the usability of the system and cannot be mitigated through a system element modification. The present study focuses on the effect of overconfidence as a consequence of hindsight bias on the use of personalized recommender systems and a more general decision support system. This study aims to contribute to the literature on recommendation systems by showing that users benefit from recommender systems depending on their cognitive biases differently. Using an experimental approach, the study looks into the use of recommender systems and general decision support in repeated decisions with higher order structures. Because of the higher-order structure, the designed decision situation for this study entails uncertainty. To disentangle decision behavior from outcome, which is affected by the uncertainty posed by decision situation, the study uses a reinforcement learning model to describe decision-making behavior of users.",The Effect of Overconfidence on the Use of Recommender and Decision Support Systems,"[53040, 454]",127,"[10, 26, 67]",2393,Behavioural OR meets Information systems,13,9,07,Behavioural OR,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,1019 [building - 202],"['Behavioural OR', 'Decision Support Systems', 'Management Information Systems']",TC-07
"Addressing transportation problems with customer-centric objective functions has become essential for last-mile service providers seeking to enhance service quality, cultivate customer loyalty, and strategically position themselves in the market. These problems have been getting more and more attention from service providers and have shown substantial benefits in markets with large volumes of deliveries per day, such as the last mile. In this paper, we study two truck-and-drone delivery systems that minimize the total waiting time of customers. To study realistic scenarios, we incorporate the interdependencies of asymmetric transportation times, variable drone speeds, weather conditions, energy consumption, and time windows. We formulate these transportation problems as Mixed Integer Linear Programs [MILPs]. Furthermore, we propose a Simulated Annealing metaheuristic framework with Local Search procedures [SA-LS] with a cutoff mechanism to avoid large computational times in high-temperature stages. In the experiments, we provide insights into the benefits of addressing customer satisfaction for truck-and-drone last-mile delivery systems. Accordingly, the results show that even though the latency and makespan are time-related objective functions, minimizing the latency does not directly minimize the makespan, and vice versa. Moreover, enabling the drone to fly at slower speed levels generates significant reductions in the total customer waiting times.",Optimizing Last-Mile Logistics - Analyzing Customer-Centric Truck-and-Drone Delivery Systems,"[71569, 62511, 36160, 46236, 710]",977,"[145, 74, 65]",2394,Last mile delivery with drones,6,15,56,Transportation,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S04 [building - 101],"['Vehicle Routing', 'Metaheuristics', 'Logistics']",WD-56
"We present a simple approach for proving Hardy inequalities on Riemannian manifolds, with sectional curvature bounded from above. By using simple convexity arguments and divergence/comparison theorems, we show that the validity of these inequalities reduces to the solvability of corresponding Riccati-type differential inequalities. Since the proofs omit symmetrizations, the method allows for broad applicability - inequalities formulated on model space forms naturally extend to manifolds with smaller sectional curvature.",Hardy inequalities via Riccati pairs ,[74928],86,"[81, 0]",2395,Optimization on Geodesic Metric Spaces II - Nonsmooth case,69,7,41,Optimization on Geodesic Metric Spaces - Smooth and Nonsmooth,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,97 [building - 306],['Non-smooth Optimization'],TA-41
"While track possession times [due to railway construction and maintenance works] for the most part render portions of the railway infrastructure temporarily unavailable, they are necessary to upkeep the its long-term serviceability. Therefore, possession times and the construction processes themselves must be planned and executed in such a way as to minimize the impact on railway operations. This contribution proposes an approach to determine the optimal point in time and duration of track possessions with a focus on ETCS installation. The approach utilizes mathematical modeling techniques and a hybrid between an exact and heuristic method while considering effects on the railway operations, construction management requirements, and production resource constraints like available labor force. The proposed approach enables a better capacity consumption in the railway network by allowing an optimized planning of possession times and a faster realization of railway construction works. To validate the approach, the results are compared with real-world track possession time planning.",Determining the optimal Point in Time and Duration of Track Possession Times,"[77247, 69958, 75353]",504,"[122, 129, 143]",2396,Freight railway transportation ,6,3,56,Transportation,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S04 [building - 101],"['Railway Applications', 'Scheduling', 'Transportation']",MB-56
"This paper analyzes the impact of different electricity purchase conditions on the carbon emissions of hydrogen production. These conditions are also discussed in the context of the revision of the Greenhouse Gas Protocol's carbon accounting rules for the purchase of voluntary renewable energy certificates to report companies' scope 2 carbon emissions. The techno-economic analysis presents a case study of hydrogen production in Denmark, a country with a relatively high share of renewable energy sources [RES], comparing the cost-optimal energy system for 2030 with a counterfactual scenario with no electricity purchase conditions. The study analyzes the impact of annual and hourly matching through power purchase agreements [PPAs] with local RES versus purchasing renewable energy certificates [RECs] through virtual PPAs on carbon emissions, system costs, and carbon reduction costs. We compare several European countries with different characteristics, such as varying initial RES shares, grid carbon intensity, and RES potentials. Our initial results suggest that the purchase conditions significantly affect the distribution of RES between countries, the additionality of RES, and the total system emissions.",Conditions on electricity purchasing - More [emission reduction] bang for your buck?,"[77788, 77796, 39154, 77794, 58629]",643,"[37, 93, 36]",2397,Hydrogen Modeling and Regulation II,22,15,09,Energy Markets,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,10 [building - 116],"['Energy Policy and Planning', 'OR in Energy', 'Electricity Markets']",WD-09
"This work proposes a methodology for the combined assessment of energy and material supply risks [SRs], using a multi-objective optimization [MOO] approach in energy system models [ESMs]. Indeed, the transition from fossil fuels to low-carbon sources is decreasing the energy import dependency of many countries, while increasing the risk of potential bottlenecks along the entire supply chain of  low-carbon technologies, leading to an energy security trade off. As policymakers are promoting new devoted policies, ESMs represent suitable tools to test their effectiveness. However, despite the studies evaluating future material requirements are increasing in number, they are usually done ex-post starting from already available energy transition scenarios. In that way, such energy scenarios are not affected by potential materials and technology supply chain bottlenecks. That makes the analysis presented here among the first-of-a-kind assessments of energy and material SRs in a multi-objective energy system optimization framework. The proposed methodology involves the consistent definition of the energy and material SRs for a reference energy system as two separate objective functions to be used in a MOO. The trade-offs between such SRs, costs and CO2 emissions are studied through MOO optimization for a case study developed within the open-source Temoa framework, providing insights about technology competitiveness in terms of energy security.",A multi-objective optimization approach to assess the trade-off of potential energy and material supply risks in the energy transition,"[77337, 67896, 77804, 69526, 77802]",177,"[37, 77]",2398,Long-term energy system planning,22,10,09,Energy Markets,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,10 [building - 116],"['Energy Policy and Planning', 'Multi-Objective Decision Making']",TD-09
"Implementation of direct channels allows manufacturers to distribute new and remanufactured products through separate channels. This prompts careful consideration of which products to channel directly. In the production of remanufactured products, manufacturers often delegate the collection of used products to retailers. However, the efficiency of this collection process is typically private information, known only to manufacturers through its probability distribution. To explore the impact of information asymmetry on manufacturers' decisions, we constructed a dual-channel closed-loop supply chain model involving a manufacturer and a retailer where the manufacturer can design non-linear contracts, incentivizing the retailer to choose contracts that align with its capabilities, ultimately maximizing the manufacturer's profit. The study findings indicate that when the retailer exhibits high collection efficiency and remanufacturing costs are low, information asymmetry can distort the production quantities of new and remanufactured products. Manufacturer encroachment consistently proves advantageous, mitigating the negative effects of information asymmetry on product output and consumer surplus. It also raises the entry barrier for manufacturers contemplating remanufacturing. Additionally, a cost threshold for remanufacturing is identified, creating a divergence in how information asymmetry influences manufacturers' profits under various contract types.",Managing Manufacturer Encroachment and Product Conflicts in a Closed-Loop Supply Chain - The Case of Information Asymmetry,[77004],915,"[138, 125, 139]",2399,YW4OR_2,39,13,12,WISDOM - Women in OR,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,13 [building - 116],"['Supply Chain Management', 'Reverse Logistics / Remanufacturing', 'Sustainable Development']",WB-12
"In the run-up to 2030, the path towards the EU’s ambitious 55% CO2 reduction goal is shaped by existing energy infrastructure and already adopted policies. Each nation's energy system characteristics, reduction targets, and local challenges affect the EU's collective decarbonization. This work utilizes the open PyPSA-Eur model for the sector-coupled European energy system, comprising 33 countries and Denmark with a higher resolution of nine nodes. The study examines Denmark's green transition and how it could be disrupted when different sectors fail to meet set targets for decarbonizing agriculture and expanding capacity for green hydrogen electrolysis, renewable generators, and power-to-heat. We constrain CO2 emissions on a national level, aligned with each country’s pledged CO2 reduction targets, and, in Denmark, implement frozen-policy projections for energy production and sectoral emissions. Focused on 2030, we analyze eight scenarios to assess the impact of each sector on system investments and CO2 abatement costs. In addition, we extend the time horizon to 2040 to account for the increasing decarbonization efforts of neighboring countries. We show - [1] increased CO2 abatement costs if the agricultural sector fails to decarbonize; [2] the crucial role of heat electrification for low-cost flexibility; [3] the dependency of green hydrogen investments on renewable capacity limits and emission constraints in surrounding countries. Our model and data are openly available.",What Could Go Wrong? Disrupting Denmark’s Energy Transition Using Policy-Driven Scenarios,[77790],643,"[37, 21]",2400,Hydrogen Modeling and Regulation II,22,15,09,Energy Markets,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,10 [building - 116],"['Energy Policy and Planning', 'Convex Optimization']",WD-09
"Several optimization algorithms, including the ADMM and Douglas-Rachford algorithm [DR], can be cast as the fixed-point iteration [FPI] of an averaged operator. However, the convergence of these algorithms is slow in general [within sublinear regime]. We show linear convergence of the FPI of averaged operators by leveraging an error-bound condition on the operator. Our work captures several existing results on the linear convergence of the ADMM and DR under stronger assumptions. As a byproduct of our method, we bound the rate of convergence of the DR applied to linear and quadratic optimization problems.",Linear Convergence of Fixed-Point Iteration of Averaged Operators via a Generic Error-Bound Condition,"[77671, 77805, 53047, 57963]",393,"[5, 19, 72]",2401,Convex and conic optimization,68,13,38,"Conic Optimization - Theory, Algorithms, and Applications","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,34 [building - 306],"['Algorithms', 'Continuous Optimization', 'Mathematical Programming']",WB-38
"The research introduces the Bi-linear consensus Alternating Direction Method of Multipliers [Bi-cADMM] to address large-scale regularized Sparse Machine Learning [SML] problems over a network of computational nodes. These problems involve minimizing local convex loss functions over a global decision vector with an explicit $\ell_0$ norm constraint to ensure sparsity. This approach generalizes various sparse regression and classification models, including sparse linear and logistic regression, sparse softmax regression, and sparse support vector machines. Bi-cADMM reformulates the original non-convex SML problem using bi-linear consensus and employs a hierarchical decomposition strategy. This strategy splits the problem into smaller, parallel-computable sub-problems through a two-phase approach - initial sample decomposition and distribution of local datasets across nodes, followed by a delayed feature decomposition on GPUs available to each node. GPUs handle data-intensive computations, while CPUs manage less demanding tasks. The algorithm is implemented in an open-source Python package called Parallel Sparse Fitting Toolbox [PsFiT]. Computational experiments validate the efficiency and scalability of Bi-cADMM through numerical benchmarks on various SML problems with distributed datasets.",Distributed Interpretable Machine Learning on GPUs,"[77365, 22807, 70639]",721,"[66, 63, 102]",2402,Applications of Mixed-Integer and Nonconvex Optimization 1,86,14,04,MINLP,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,1001 [building - 202],"['Machine Learning', 'Large Scale Optimization', 'Parallel Algorithms and Implementation']",WC-04
"Urban areas are undergoing a digital transition of spatial planning systems. This transition implements advanced infrastructure technologies to define future scenarios efficiently. Spatial support system tools supporting sustainable development and help local governance processes to make better decisions. However, spatial planning systems at the local level often fail to adapt to small-scale and local social innovations, particularly in marginalized areas, highlighting the need for improved integration of digital technologies.
The study aims to create an interactive web-GIS dashboard, serving as a digital Multicriteria Spatial Decision Support System [MC-SDSS], to assist local authorities in fostering a responsive environment for future local policy actions aimed at recovering sustainable local development. 
The methodological framework of this study is structured in two parts - [I] Define an evaluation framework based on the set of Key Performance Indicators [KPIs] and perform the spatial impact assessment in four pilot case studies [II] Develop an interactive web dashboard consisting in KPIs through the use of GIS for setting targets and strategies through the implementation of Multicriteria Analysis [MCA] and a Social Design System Thinking [SDST] approach. 
The results demonstrate the operability of the web dashboard and the key findings of its implementation in the case studies, providing a set of suitable strategies and actions to recover sustainable marginal areas.
",Enhancing Local Sustainable Development - A Digital Spatial Decision Support System to Recovery Marginal Areas in Italy,[71730],890,"[26, 22, 67]",2404,MCDA and urban planning 2,44,9,47,Multiple Criteria Decision Analysis,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,50 [building - 324],"['Decision Support Systems', 'Critical Decision Making', 'Management Information Systems']",TC-47
"Renewable energy communities play a crucial role in the energy transition. Members own distributed energy resources and are stakeholders of their own energy supply. We propose two market designs for the optimal day-ahead scheduling of internal energy exchanges. The first one implements a collaborative demand-side management scheme inside a community where members objectives are coupled through grid tariffs, the second allows the valuation of excess generation in the community and on the retail market. Two grid tariff structures are tested, one academic and one which reflects the Belgian regulations in terms of grid tariffs. Individuals' bills are obtained through 4 cost allocation methods. Both designs are formulated as optimization problems and as noncooperative games. In the latter case, the existence and efficiency of the corresponding [generalized] Nash Equilibria are studied and algorithms for finding these are proposed. The models are tested on a use-case of 55 members and compared with a benchmark situation where members act individually. We compute the global and individual costs, inefficiencies of decentralized models compared to social optima, as well as technical indices. First, we show that there always exists an equilibrium that is a social optimum. Secondly, we show, analytically when possible, and empirically if not, that the community and the individuals' bills obtained with the centralized and decentralized approaches are equivalent.",Valuing the Electricity Produced Locally in Renewable Energy Communities through Noncooperative Resources Scheduling Games,"[73007, 72894, 73045]",631,"[21, 50, 37]",2406,Game Theoretic Market Equilibrium Modelling,22,8,09,Energy Markets,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,10 [building - 116],"['Convex Optimization', 'Game Theory', 'Energy Policy and Planning']",TB-09
"Currently, most organic waste in the Netherlands is used to produce biogas, but organic waste could also be used to produce higher value bio-based materials. This however adds additional considerations to the waste collection system, as it requires organic waste fractions to be collected separately, and to be processed within strict time limits [e.g., before mould can start to grow]. These shelf life and separation requirements limit the possibilities to achieve economies of scale in collection. As a result, the environmental and cost benefits of waste valorisation are compromised.

This research contributes to the existing literature by considering the shelf life for low-volume waste collection. In order to increase the efficiency of the collection of concentrated organic waste, we develop a modelling approach to schedule the collection of multiple waste types, also considering the possibility of using different, possibly compartmentalised, vehicles. By allocating the vehicles, the aim is to keep waste separated while reducing the travelled distance. The results show trade-offs between for instance the value of biomass and the cost of transport. In addition, the scheduling model provides insight in the usefulness of different vehicles types and sizes in waste collection activities in a more circular economy.
",Scheduling organic waste collection operations in the circular economy,"[77797, 37785, 71700]",919,"[100, 0]",2408,Sustainable food supply chains,18,4,23,"Circular Economy, Remanufacturing and Recycling ","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,82 [building - 116],['OR in Sustainability'],MC-23
"Mobility-as-a-Service [MaaS] transforms mobility systems into more flexible and efficient ones by integrating different transportation modes. In this research, the focus lies on the integration of timetabled public transport [PT], dial-a-ride [DAR] services, and walking, allowing passengers to travel by a combination of these three modes. From the perspective of the provider, the exploitation of such a hybrid mobility system leads, among others, to challenging routing and scheduling problems on the operational level. The ultimate aim is to generate efficient real-time solutions in response to all user requests by optimally combining and aligning the available transport modes with each other, while minimizing the operational costs and the total trip times of the users. 

Therefore, in this talk, an optimization problem is introduced which involves decisions on the combination of modes for each request and the routing and scheduling of the DAR vehicles. The focus lies specifically on a network in which users commute between a rural and an urban area. A heuristic algorithm based on Large Neighborhood Search [LNS] is proposed. To incorporate the trade-off between the operational costs and service level, a tailored scheduling sub-procedure is presented which minimizes the sum of the users’ trip durations for a given route. Lastly, a sensitivity analysis is carried out on small-scale artificial instances gaining more insight into the problem-specific parameters.
",An LNS-based heuristic for optimizing on-demand transportation services in a hybrid mobility system,"[77730, 23971, 46186]",376,"[74, 129, 145]",2409,Demand-responsive public transport 1,85,13,54,Public Transport Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S01 [building - 101],"['Metaheuristics', 'Scheduling', 'Vehicle Routing']",WB-54
"Climate policy makers aim to incentivize risk-averse firms to deploy large amounts of capital for clean electricity investments. This task is made harder by the limited availability of long-term contracts in electricity markets, which is known as the missing market problem. We study how this market failure affects optimal policy choices in power systems. To analyze optimal policy, we propose a bi-level programming approach that endogenizes policy decisions. Our model combines an “upper level” policy optimization problem and a “lower-level” generation expansion problem. The lower-level problem features risk-averse investors making decisions under demand and fuel price uncertainty as well as optimal power system dispatch. While bi-level programming imposes a computational burden, our formulation partly alleviates this problem. We are able to solve our model for a power systems featuring both variable renewables and energy storage with time-linking constraints. We use this model to study the optimal design of several common climate policies including renewable portfolio standards, investment tax credits, and carbon pricing. Preliminary results show that optimal climate policy instruments are more stringent in power systems subject to the missing market problem than in systems with complete markets. We also find that missing long-term markets can make renewable portfolio standards more cost-effective than carbon pricing in some cases. ",Climate policy design in liberalized electricity markets with incomplete risk trading,"[71030, 6412, 66867, 9923, 58544]",448,"[37, 36, 126]",2410,Decentralized multi-energy markets,22,7,09,Energy Markets,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,10 [building - 116],"['Energy Policy and Planning', 'Electricity Markets', 'Risk Analysis and Management']",TA-09
"The timely rollout of ETCS L2 as a standardized train control system in Europe is depending on capacities of human planning experts, which are especially limited in the field of plan review.
Central requirements for ETCS L2 data point [DP] plan review are, to assess the completeness and correctness of the ETCS L2 DP plan, following a different logic than the plan creation.
This proposal presents an approach to review the completeness of the ETCS L2 DP plan. The rule-based nature of the underlying planning rules and regulations leads to the choice of a supervised learning [SL] algorithm. To assess completeness, in this approach the ETCS L2 DP plan under review is segmented into functional and spatial areas, with specific DP types assigned to particular reference points within those areas. The SL algorithm learns in which cases, which type[s] of DP are required within each area. Training data are existing ETCS L2 DP plans for different scenarios by human experts. An important benefit of the SL algorithm is that it learns from pattern in the training data instead of applying the planning rules and regulations 1:1, hence allows a plan review following a different logic than plan creation. Additionally, the plan review supported by the proposed SL algorithm provides decision support for plan reviews carried out by human experts. Thereby it facilitates a faster rollout of ETCS L2, contributing to interoperability, resilience and thus increasing rail capacity in Europe.",Proposal for an ML-supported ETCS L2 Data Point Plan Review Process  ,"[76522, 69958, 69957]",659,"[26, 66, 122]",2411,Planning Techniques for Decision Support,45,15,45,Decision Support Systems,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,30 [building - 324],"['Decision Support Systems', 'Machine Learning', 'Railway Applications']",WD-45
"Carsharing operators commonly offer several pricing plans tailored for different customer profiles, constructed from various attributes such as registration, travel distance, and travel time fees. Each plan may have different rates across high/low demand periods and areas to increase fleet utilization and maintain a balanced supply-demand system. However, in most existing works, the heterogeneity of users is not considered, and plans consist of only a single attribute [i.e., travel time fee]. In this study, we formulate a mixed-integer linear programming model with the objective of profit maximization to optimize multi-attribute plans with time- and location-dependent rates, considering customer preferences and travel behavior. A discrete choice model is incorporated to estimate customer response to the value of plan attributes. We utilize real-world data from the Brooklyn taxi trip dataset to validate results and provide managerial insights. Results demonstrate the effectiveness of this approach in improving system profitability, particularly through time- and location-dependent rates. The model can find the optimal or near-optimal solutions for real-sized problems within a reasonable time limit. Moreover, we developed an approximate algorithm to efficiently produce high-quality results for large-scale instances within a shorter timeframe. Finally, we delivered valuable managerial insights regarding customer segmentation, pricing plan complexity, and parameter sensitivity.",Enhancing the performance of carsharing systems by optimizing multi-attribute pricing plans,"[71598, 35712, 77064, 663, 1999]",108,"[124, 143, 111]",2412,Revenue Management in Sharing/Platform Economy,11,3,59,Pricing and Revenue Management,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S08 [building - 101],"['Revenue Management and Pricing', 'Transportation', 'Programming, Mixed-Integer']",MB-59
"This study investigates the potential for a shared freight and passenger system to effectively utilize the remaining capacity of high-speed railway networks after serving passenger demand. Two sharing modes, which determine the mixing of passenger and freight flows, are considered and jointly optimized. A space–time network is constructed to capture the passenger/freight trajectories on an existing schedule. A penalty cost based on train load factors is introduced for each trip to represent the influence of shared transportation upon passenger satisfaction, which allows us to analyze the interplay between passenger and freight flows. The model is first formulated as a mixed-integer program that minimizes service and routing costs and then reformulated into a path-based model. A Benders decomposition approach is proposed to decompose the problem into two subproblems. A column-pool-based approximation approach is proposed to efficiently generate space–time paths and obtain good upper bounds. Two tailored acceleration techniques are presented to overcome the problems of slow iteration and wild oscillation. The developed algorithm is tested on 2 small-scale cases and 12 large-scale cases. The computational study provides insight into how the optimal network is affected by the train load factor, penalty cost, sharing mode, and commodity volume.",Scheduling shared passenger and freight transport for high-speed railway networks,"[77089, 77907]",504,"[72, 122, 129]",2413,Freight railway transportation ,6,3,56,Transportation,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S04 [building - 101],"['Mathematical Programming', 'Railway Applications', 'Scheduling']",MB-56
"In recent years, a growing body of literature has focused on training sparse deep neural networks or refining already trained ones towards making them sparser. The strategic reduction of complexity of the network has shown to effectively mitigate overﬁtting, improve overall generalisation performance and enhance explainability. This work introduces a feature selection approach tailored for regression tasks in trained neural networks, utilising Rectified Linear Unit [ReLU] activation functions. The methodology simplifies the network by identifying and retaining only the most important features for the input layer. This problem is mathematically formulated as a Mixed-Integer Linear Programming [MILP] model. This model formulates the ReLU operator with binary variables, enabling the application of big-M constraints, while limiting the number of active features. The objective of the model is to identify and eliminate the features that do not significantly contribute to the prediction quality of the neural network, so that it can be re-trained using only the rest of them. A binning strategy is employed on the output variable in order to assess the importance of features across the entire spectrum of the output. Through a number of real-world datasets, it is demonstrated that the complexity of the network is reduced, while maintaining good predictive performance.",Feature Selection for Regression Neural Network using Mathematical Programming,"[77504, 77811, 10664]",648,"[66, 72, 42]",2414,Feature attribution and selection for XAI,15,10,27,Mathematical Optimization for XAI,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,047 [building - 208],"['Machine Learning', 'Mathematical Programming', 'Expert Systems and Neural Networks']",TD-27
"The Balanced Minimum Evolution Problem [BMEP] is a hard combinatorial optimization network design problem arising in the field of phylogeny estimation. In this framework, the phylogeny is represented by an unrooted binary tree where the leaves are the individuals [the taxa], and the internal nodes represent speciation events. Given a matrix of estimated distances between each pair of taxon, the aim is to reconstruct the most accurate phylogeny according to the minimum evolution principle. We propose a Large Neighorhood Search [LNS] procedure to improve locally optimal solutions initially obtained by the state-of-the-art method [FastME]. This LNS procedure works by iteratively destroying a subset of the incumbent unrooted binary tree, and then reconstructs it with a matheuristic-based repair operator. Due to its local search nature, this LNS procedure provides a scalable way of obtaining more accurate phylogenies for large datasets by directly improving the solutions found by the state-of-the-art method.",A Large Neighborhood Search Matheuristic for the Balanced Minimum Evolution Problem,"[59648, 16035, 74761]",309,"[14, 17, 74]",2416,Integrative Approaches in Health and Disease - From Molecular Structures to Clinical Outcomes,2,9,20,"Computational Biology, Bioinformatics and Medicine","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,45 [building - 116],"['Combinatorial Optimization', 'Computational Biology, Bioinformatics and Medicine', 'Metaheuristics']",TC-20
"Industrial maintenance projects, known as shutdown projects, are highly impacted by the recent growth of digital transformation [DT] efforts. Technological and systemic advancements, sensors integration, data analytics tools, information systems, etc., are some of the factors constantly affecting and changing the operation of most industrial companies. This work drills down to efficient approaches in DT aspects of major maintenance projects and condenses the on-field experience from a dozen such projects over the past decade. Specifically, it highlights best practices on technological, systemic and managerial actions, such as - [i] the gradual development of knowledge databases to improve project planning efficiency by up to 38%; [ii]  digital collaboration techniques to elicit requirements and reduce scope creep by almost 16%; [iii] data analysis from on-board sensors to diagnose the operational condition and proactively identify maintenance needs; [iv] the use of digital maps of industrial plants for optimization of works sequences to respect safety constraints; and [v] the development of digital tools for near-real time collection of progress data from remote sites and diverse stakeholders to quickly update plans, forecast reliably, identify issues and cultivate employee engagement. The presentation of the temporal evolution and development of implemented solutions for each best practice highlights the combinatorial benefits of DT and establishes guidelines for the future.",Best Practices of Digital Transformation for Managing Large Industrial Maintenance Projects,[20366],961,"[118, 59, 7]",2417,Project Management,35,7,60,Project Management and Scheduling,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S09 [building - 101],"['Project Management and Scheduling', 'Industrial Optimization', 'Analytics and Data Science']",TA-60
"One way to achieve energy efficiency in manufacturing is to equip plants with on-site renewable energy generation system to partially power industrial processes.However, renewable energy sources are intermittent and the availability is difficult to predict accurately.Therefore,we study an integrated industrial production planning and energy supply problem under uncertain renewable energy availability.We propose a multi-stage stochastic programming model for this problem. The planning horizon is divided into multiple stages according to the time at which the PV availability information is updated.The intermittency of PV generation is represented by a scenario tree.The resulting production and energy supply planning can be seen as a multi-stage decision process where some decisions are made at the beginning of the planning horizon whereas the others are postponed to later decision stages when more information on the uncertain parameters are revealed.At the beginning of the planning horizon, we build a production plan for a proportional lot-sizing and scheduling problem in a single-machine multi-item setting. Then based on the available information on PV generation, an energy supply plan is constructed for the upcoming stage, which has to satisfy the energy demand of the system which has been previously determined by the production plan at the beginning of the planning horizon. Computational experiments will be presented to show the practical efficiency of the proposed approach.",A multi-stage stochastic programming model for lot-sizing with onsite generation of renewable energy,"[73931, 71381, 29574, 13276]",809,"[117, 69, 129]",2418,Lot-sizing with energy aspects,32,8,49,"Lot Sizing, Lot Scheduling and Production Planning","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,M1 [building - 101],"['Programming, Stochastic', 'Manufacturing', 'Scheduling']",TB-49
"This study focuses on designing logistics concepts for an electrical battery pack production setup, inspired by our industry partner. The production line is composed of automated workstations using industrial fixed robot arms and manual workstations with human operators. Alongside these, autonomous mobile robots [AMRs] and automated guided vehicles [AGVs] are employed to provide material supply and product transportation, respectively. The complexity rises due to the interdependencies among production, material supply, and product transportation. We propose several logistics concepts for this new production setup in order to achieve the best throughput rate while efficiently making use of AGVs and AMRs. Using real-world data from the industry partner, the efficacy of the proposed concepts is illustrated under varying numbers of AGVs, AMRs, and product types through simulation. Numerical results highlight significant changes in the logistics concept choice based on fleet sizes and product types, with extendable implications for high-mix-low-volume production and autonomous manufacturing systems.",Logistics Concept Design for a Battery Pack Production Initiative,"[76900, 56013, 77318, 63457]",789,"[69, 131, 143]",2419,Logistics 3,5,15,58,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S07 [building - 101],"['Manufacturing', 'Simulation', 'Transportation']",WD-58
"Imagine a world in which production companies do not have to worry about their energy supply. Not just because they have their own renewable energy sources [e.g., solar panels, wind turbines] and energy storage [e.g., industrial batteries], but more importantly because their entire production system has been designed and is managed to optimally use the available electrical and thermal energy. In essence, focus has shifted from optimizing production goals and subsequently determining where and how to get energy from, to optimizing production and energy goals together. As a result, production is more sustainable and cheaper in terms of energy costs, without sacrificing capacity. Unfortunately, the energy crisis has proven that we are presently far removed from such an ideal world. 

We will provide a holistic roadmap on energy-aware daily production scheduling, in which the technological aspects of energy supply and storage are taken into account, and are furthermore matched with energy demand from production. We will subsequently zoom in on a specific problem, namely the order acceptance and scheduling [OAS] problem, with the inclusion of electricity grid costs based on day-ahead prices. Aside from a mathematical model, we will discuss a metaheuristic algorithm, and compare the performance of both on a small dataset.
",Towards energy-aware daily production optimization,"[35904, 59857]",920,"[100, 14, 129]",2420,Scheduling for sustainability,18,5,23,"Circular Economy, Remanufacturing and Recycling ","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,82 [building - 116],"['OR in Sustainability', 'Combinatorial Optimization', 'Scheduling']",MD-23
"With  the ongoing energy transition in Europe, the  share of variable renewable energies [vRE] in the power system increases steadily and different ﬂexibility options gain importance. This work investigates the deployment of supply side ﬂexibility options in the future European power system. We use a cost-minimising energy system model that endogenously optimises the energy-to-power [E2P] ratio and particularly focus on lithium-ion batteries, vanadium redox flow batteries, adiabatic compressed air energy storage and hydrogen storage.  We explore the competition of these ﬂexibility options across a broad defossilisation spectrum. Starting at vRE shares of around 50%, we find non-linearly increasing storage requirements and observe a leap when approaching a zero-emission system, which is mainly driven by hydrogen storage investments. We also find that lithium-ion batteries with E2P ratios between 2 and 6 hours are particularly deployed in countries with high levels of solar power, whereas hydrogen storage with E2P ratios between 300 and 1000 hours is mainly deployed in countries with high levels of wind power. Adiabatic compressed air energy storage with E2P ratios between 15 and 20 hours are only deployed in a completely defossilised system. In addition, we analyse storage operating requirements including operating hours and ramp rates for different defossilisation levels.",Electrical energy storage investment requirements and operating characteristics in a gradually defossilised European power system,"[33470, 70125, 69526, 76848]",341,"[36, 37, 93]",2421,Modelling and Economics of Storage Technologies in Energy Markets,22,2,09,Energy Markets,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,10 [building - 116],"['Electricity Markets', 'Energy Policy and Planning', 'OR in Energy']",MA-09
"Demand response for water networks is an optimisation model that determines which water pumps will be turned on or off at each time period according to a dynamic electrical tariff. This problem is important in mining due to the high power consumption of water pumps, as well as the complicated dynamics of water flows and the power market. The problem faces difficulties from - i] the nonlinearities of the equations for the frictional losses along the pipes and pumps, which make the problem a nonlinear mixed-integer model, ii] the many possible combinations of pressure head and flow rates, which quickly lead to high computational costs, and iii] the uncertainty from energy prices and water demand. These limitations prevent the problem from being solved to optimality in a reasonable computational time in water systems with more than two pumps and reservoirs. Therefore, we develop new optimisation models for the demand response in large and high-altitude water transportation systems that use a binary expansion approach to efficiently account for the existing nonlinearities by reducing the computational difficulties while maintaining an excellent representation of the physical phenomena involved. We also tested this approach in different network topologies and case studies, and we conclude that the method significantly reduces the computational time for solving the problem with high precision, which can be relevant for the daily operation of real-world water transportation systems.",A Binary Expansion Approach for the Optimal Demand Response in Large and High Altitude Water Transportation System,"[74588, 3287, 60876]",856,"[93, 113, 147]",2422,OR in Energy II,23,14,19,OR in Energy,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,44 [building - 116],"['OR in Energy', 'Programming, Nonlinear', 'Water Management']",WC-19
"In India and other low and middle-income countries, delays in the approval of medical procedures from insurance companies often lead to extended hospital stays. Patients can not be discharged until the insurance company approves all procedures. 

In this talk, we study the prevalence of the problem and the financial implications by analyzing all discharge summaries from 2021 to 2023 in a tertiary hospital in Kerala, South India. Unnecessary hospital stays will cause substantial economic loss to the hospitals. They will also lead to patient dissatisfaction and reduced bed availability for patients with medical needs.

If the hospital allocates additional resources, it can improve discharge planning through system-level approaches. In this talk, we investigate the implications of different hospital policies. We use counting processes to model patients’ arrival on each day, survival models for the length of hospital stays, and a Markov decision process to model the effects of different policies. The decision variables can be optimized by approximate dynamic programming. We find that policies that prioritize a timely discharge should be recommended. Beyond costs, these policies also increase patients’ satisfaction and the overall efficiency of the health care system.",Prevalence and costs of delayed discharge in a tertiary Indian hospital,"[59078, 77205, 77215, 77813, 77816, 77817, 77818, 77819, 77821]",54,"[28, 56, 108]",2423,OR for Medical Services in Developing Countries,67,12,18,OR for Development and Developing Countries,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,42 [building - 116],"['Developing Countries', 'Health Care', 'Programming, Dynamic']",WA-18
"While the objectives and particulars of irregular C&P problems can differ, they all share a common feasibility check - whether or not an item can be placed at a certain position. 
This check is particularly complex due to the geometric irregularity of the items and containers.
For C&P problems, there is currently no sufficiently general approach to tackle this task in an easy and efficient manner. 
This not only heightens the barrier to entry, but also results in researchers having to continuously reinvent the wheel whenever they are addressing new problems or developing new approaches for existing problems.
This work aims to decouple geometry from optimization and develop a high-performing adaptable engine, capable of efficiently handing the geometric component of irregular C&P problems.
We envisage two target audiences. First, there are those who simply want to focus on their optimization problem at hand and who would therefore greatly benefit from having an engine they can incorporate into their own methodology. Such an engine would essentially outsource the geometric challenge and enable them to focus their efforts on developing smart solution methods.
The second target audience are those who, rather than solving problems themselves, might have good ideas concerning how to further improve and refine this open- source engine.
The project is called jagua-rs and is publicly available at - https://github.com/JeroenGar/jagua-rs",Decoupling Geometry from Optimization in 2D irregular Cutting and Packing problems - an Open-Source Collision Detection Engine,"[74365, 23268, 25830]",501,"[134, 23]",2424,Cutting and Packing 2 - 2D irregular,81,3,07,Cutting and Packing [ESICUP],"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,1019 [building - 202],"['Software', 'Cutting and Packing']",MB-07
"In this talk we aim to illustrate the efficient reduction of development time and enhancement of the developer experience for fully fledged analytic Python applications, utilizing FICO Xpress Workbench and Xpress Insight. Our discussion spans across all stages of the development cycle, encompassing model implementation, scenario analysis, and the crucial feedback loop closure with business users.  

Our initial focus involves showcasing the seamless deployment of an analytic Python model, transforming it into a fully operational application within Xpress Insight. Then we demonstrate the latest enhancements for Python language support in our IDE Xpress Workbench. We will showcase how advanced debugging features lead to an improved and integrated development experience. Beyond local debugging, we will spotlight remote debugging capabilities tailored for scenarios executed in Xpress Insight. This becomes particularly significant as production data might be inaccessible on a developer’s laptop due to security considerations. 

 ",Convenient and Efficient Implementation of Analytic Python Apps with Xpress Insight and Workbench ,[75219],703,"[72, 134]",2425,Python Modeling Tools,76,10,30,Software for Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,53 [building - 208],"['Mathematical Programming', 'Software']",TD-30
"We introduce measures of stochastic non-dominance to provide insights in situations when stochastic dominance between two random variables does not hold. They quantify how far a given random variable is from dominating a benchmark random variable by a specified order stochastic dominance. They are found by solving an optimization problem that searches for an optimal random variable, which is as close as possible to the given one but dominates the benchmark. The measure of stochastic non-dominance is then the Wasserstein distance between the optimal and the given random variable. Depending on the assumptions imposed on the optimal variable, we distinguish the general and the specific measures of stochastic non-dominance. 

We derive closed-form expressions of the measures of stochastic non-dominance under various parametric assumptions. Moreover, we present relations to selected types of almost stochastic dominance.  Finally, a formula is derived for the computation of the measures of stochastic non-dominance for discrete distributions with equiprobable atoms. It is applied to portfolio optimization problems in the empirical part.",Measures of Stochastic Non-Dominance,"[77806, 12024]",475,"[136, 127]",2426,Portfolio optimization ,49,9,34,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,43 [building - 303A],"['Stochastic Optimization', 'Robust Optimization']",TC-34
"In response to global environmental concerns and policy initiatives, the automotive industry is currently undergoing a significant transformation towards sustainable transportation. As EVs become a major component of the transition to a greener, more electrified future, governments are investing heavily in charging infrastructure. The rapid rise of EV adoption poses significant challenges to conventional power grids. Intelligent solutions are necessary in order to ensure reliable and efficient charging infrastructure.

Current studies concerning EV charging infrastructure often overlook the underlying power distribution network and the accessibility of charging stations for EV users. The majority of these studies are small in scale, employing metaheuristics without validating their results using exact solutions. Moreover, researchers have predominantly focused on fast chargers, while slow chargers play an equally crucial role in urban settings.

In an effort to address these significant gaps, our research focuses on the placement of slow charging stations in urban areas while taking into account [i] the distribution network, [ii] customer demand, and [iii] accessibility. We propose a novel Mixed-Integer Linear Programming [MILP] model to determine the optimal placement, quantity, and type of charging stations. To enhance scalability and computational speed, we also introduce a multi-stage matheuristic for solving large-scale instances by decomposing the grid network.",A Matheuristic for the Optimal Placement of EV Charging Stations in an Urban Environment,"[74516, 36732, 78548, 23268]",864,"[37, 5, 64]",2428,Sustainability in Distribution and Transportation,64,9,26,Combinatorial Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,012 [building - 208],"['Energy Policy and Planning', 'Algorithms', 'Location']",TC-26
"To meet carbon reduction targets, there will need to be investment in wind energy, solar PV, and battery storage in electricity markets across the world. In this work, we consider what the optimal investment mix for these technologies will be from the perspective of both generating firms and consumers. We present a stochastic Mixed Complementarity Problem where several generating firms maximise their profits while various consumer groups minimise their costs. All players modelled make hourly operational decisions in addition to long-term investment decisions. The generating firms may exert market power. The uncertainty of wind and solar PV are the sources of the model’s stochasticity.  We apply the model to a case study of the Irish electricity system in 2030.  We consider the optimal investment mix when market power is both present and absent from the market. We observe that the presence of market power increases electricity prices which leads to increased generating firms’ profits and consumer costs. It also leads to increased investment in renewable technologies and battery storage, which leads to reduced carbon emissions. Furthermore, we consider the effect a Feed-in-Premium [FiP] has on renewable investment and observe a counter-intuitive result whereby the absence of a FiP leads to less investment in renewables from generation companies but, consequently, increased investment in renewables from consumers.",The interactions between demand- and supply-side investment decisions in an oligopolistic electricity market. ,"[41933, 67896]",631,"[93, 36, 50]",2429,Game Theoretic Market Equilibrium Modelling,22,8,09,Energy Markets,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,10 [building - 116],"['OR in Energy', 'Electricity Markets', 'Game Theory']",TB-09
"Integrating increasing capacities of renewable energy sources necessitates sector coupling. The coupling between the electricity and gas sectors is critical since they can be coupled bi-directionally via electrolyzers and gas turbines. Furthermore, the natural gas system must either be defossilized or the energy carrier must be switched to prevent greenhouse gas emissions. Hydrogen is expected to partially replace the currently used natural gas. Different infrastructure elements can be repurposed to transport hydrogen, which could reduce infrastructure costs significantly. Currently, the different systems are planned independently. However, planning the system using an integrated model might lead to significant cost reductions due to exploiting synergies between the systems. The integrated optimization could also enhance the system's reliability and efficiency.
We propose utilizing stabilized Bender's decomposition to optimize integrated energy systems, including the electricity, natural gas, and hydrogen sectors. This approach improves performance by effectively decomposing the problem into smaller, more manageable subproblems. Integrating bundle methods improves the convergence of Bender's decomposition.
We demonstrate the performance of stabilized Bender's decomposition within a case study of an IEEE system. The complex integrated energy system model can be solved close to optimality by adopting methodological advancements such as stabilized Bender's decomposition.",Integrated Planning of Electricity and Gas Networks for the Energy Transition Using Stabilized Benders´s Decomposition,[70688],838,"[63, 79, 93]",2430,OR in Gas Networks ,23,7,19,OR in Energy,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,44 [building - 116],"['Large Scale Optimization', 'Network Design', 'OR in Energy']",TA-19
"Portuguese health spending has been rising, with the global health sector contributing 4.6% to worldwide greenhouse gas emissions. Single-Use Medical Devices [SUMD] are increasingly used in healthcare and their procurement is mostly price-based, with literature recognising the need to implement sustainable procurement practices but studies in the area being scarce. This study aims at identifying, with stakeholder engagement, which aspects related to the evaluation of SUMD should be considered to leverage a more circular healthcare system. A novel multimethodology, integrating Problem Structuring Methods within a Delphi to promote structured collaboration and address aspect interconnectedness, is proposed and tested in a case study with twenty Portuguese stakeholders. In Round 0, participants ideated new aspects to consider in evaluating medical devices to promote circular economy concerns. These aspects were organised into a cognitive map made available to participants in rounds 1 and 2, where participants stated their agreement level with each one and revised their views in light of the group's answers; and provided feedback about the Delphi. Twelve experts completed all rounds and results were validated through individual interviews with a core strategic group. Results suggest many new value aspects for evaluating SUMD. The adopted multimethodology was effective in depicting interrelationships between value aspects and promoting stakeholder agreement.",Promoting circular economy concerns in medical devices’ procurement through structured collaborative processes,"[76032, 25340, 63390, 24796]",578,"[100, 101, 133]",2431,Behavioural Studies in Health Care ,13,8,07,Behavioural OR,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,1019 [building - 202],"['OR in Sustainability', 'OR/MS and the Public Sector', 'Soft OR']",TB-07
"In principle, quantum computers can solve semi-definite programs using resources that scale extremely well in the matrix dimension, though very unfavorably in the precision. In 2019, Brandao, Franca, and Kueng proposed to use quantum SDP solvers in order to speed up the Goeman-Williams-type relaxations of combinatorial optimization problems. One motivation for their ansatz was that the final rounding step that maps SDP solutions to binary variables should be comparatively insensitive to limits in the precision. They did indeed succeed to identify an algorithm that achieves an asymptotic quadratic speedup over state-of-the-art classical methods. Here, we present an analysis of the non-asymptotic resource requirements of this ansatz. 
The work consists of two parts. First, we optimize the original algorithm with a view on performance for finite problem instances. In particular, we formulate a version with adaptive step-sizes, an improved detection criterion for infeasible instances, and a more efficient rounding procedure. In a second step, we benchmark both the classical and the quantum version of the algorithm against synthetic and real-world data. Unfortunately, we find that even the optimized quantum algorithm will not beat more standard classical approaches for input sizes that can be realistically solved at all.",Solving quadratic binary optimization problems using quantum SDP methods - Non-asymptotic run-time analysis,"[77058, 78454, 78592, 78591, 62043, 78594]",667,"[115, 16, 21]",2432,Quantum Computing for Continuous Optimization,83,7,42,Quantum Computing Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,98 [building - 306],"['Programming, Semidefinite', 'Complexity and Approximation', 'Convex Optimization']",TA-42
"The FICO Decision Optimizer [DO] application is an optimization software package to perform optimal assignment of treatments to a portfolio of customers. DO leverages optimization algorithms with the goal to empower non-OR professionals with a tool that creates and solves Generalized Assignment Problems, without requiring them to formulate the models. DO considers different kinds of constraints [e.g., budget, ratio] and allows users to generate highly interpretable decision trees.
The DO models depend on structural inputs [e.g., portfolio or customer attributes] and uncertain inputs [predictable target values, such as the impact of treatments on customers]. While the DO interface enables configuring and editing of the predictive models required to produce the uncertain data, developing Predictive Causal Models usually requires additional tools and knowledge for a Business Analyst. Furthermore, there is inherent historical data bias, because the historical actions are likely to be targeted on certain segments. Consequently, there may be significant data gaps for some actions.
To simplify the user experience, we developed an Action Effect [AE] approach to predict the impact of the treatment assignment on every customer of the portfolio. We present the logic behind the quadratic programming models that create the predictive models. We conclude our presentation with a discussion about the advantages of this methodology over standard unconstrained linear regression.",FICO Decision Optimizer – Generating predictive models with Action Effect,"[77351, 54143, 42895, 77841, 77838, 77839]",694,"[45, 151, 7]",2433,Complexity and Financial Patterns,4,15,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S14 [building - 101],"['Financial Modelling', 'Practice of OR', 'Analytics and Data Science']",WD-63
"This presentation lays out a foundation and taxonomy for Behavioral Decision Analysis, which can unify this work across various domains. Traditional research in the domain of Decision Analysis has focused on the design and application of logically consistent tools to support decision-makers during the process of structuring problem complexity, modeling uncertainty, generating predictions, eliciting preferences, and, ultimately, making better decisions. Two commonly held assumptions are that the decision maker’s cognitive belief system is fully accessible and that this system can be understood and formalized by trained analysts. However, in past years, an active line of research has emerged studying instances in which such assumptions may not hold. We aim to unite this community under the common theme of Behavioral Decision Analysis. The taxonomy we suggest categorizes research based on task focus [prediction or decision] and behavioral level [individual or group]. Two theoretical lenses that lie at the interface between [1] normative and descriptive research, and [2] normative and prescriptive research are introduced. We then proceed to highlight representative works across the two lenses focused on individual and group-level decision-making. ",Behavioral Decision Analysis,"[5167, 77936, 26223]",102,"[25, 10, 55]",2434,Behavioral Decision Analysis IV,13,7,11,Behavioural OR,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,12 [building - 116],"['Decision Analysis', 'Behavioural OR', 'Group Decision Making and Negotiation']",TA-11
"Across the clothing industry accounting for 60% of the world's textile waste, less than 1% of the used products undergo closed-loop recycling enabling the use for equal-value applications. This highlights the problem of a highly linear supply chain in the textile industry, where textiles are produced, used and mainly disposed of. 

This paper contributes to the current debate on transforming the textile industry into a circular economy. Taking into account the challenges of a highly seasonal industry with high demand uncertainties, we combine inventory management practices to reduce waste with a systematic analysis of the economic and environmental impacts of different sustainable sourcing labeling regulations. In particular, we focus on three potential policies set by governmental regulators and their impact on the manufacturer’s sourcing decision regarding traditional and chemically recycled feedstock.  In the analysis, we incorporate the effect of the manufacturer’s purchasing decisions and governmental regulations about sustainable sourcing labeling on consumer demand driven by consumer environmental awareness. We derive the manufacturer’s optimal sourcing and production quantities and analyze their impact on economic and environmental key performance indicators. 
",From a linear supply chain to a circular economy - The impact of sustainable sourcing labeling regulations,"[46997, 77815, 43640]",921,"[105, 139]",2435,Policy and legislation for a circular economy,18,7,23,"Circular Economy, Remanufacturing and Recycling ","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,82 [building - 116],"['Production and Inventory Systems', 'Sustainable Development']",TA-23
"Reinforcement learning-based methods for constructing solutions to combinatorial optimization problems are rapidly approaching the performance of human-designed algorithms. To further narrow the gap, learning-based approaches must efficiently explore the solution space during the search process. Recent approaches artificially increase exploration by enforcing diverse solution generation through handcrafted rules, however, these rules can impair solution quality and are difficult to design for more complex problems. In this paper, we introduce PolyNet, an approach for improving exploration of the solution space by learning complementary solution strategies. In contrast to other works, PolyNet uses only a single-decoder and a training schema that does not enforce diverse solution generation through handcrafted rules. We evaluate PolyNet on four combinatorial optimization problems and observe that the implicit diversity mechanism allows PolyNet to find better solutions than approaches the explicitly enforce diverse solution generation.",PolyNet - Learning Diverse Solution Strategies for Neural Combinatorial Optimization,"[57910, 27003]",731,"[8, 14, 66]",2436,	[Deep] Reinforcement Learning for Combinatorial Optimization 2,14,5,03,Data Science Meets Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,1005 [building - 202],"['Artificial Intelligence', 'Combinatorial Optimization', 'Machine Learning']",MD-03
"Multi-actor decision-making for transport innovations is complex, among others because of the uncertainty experienced by decision-makers about other actors’ behavior. Reducing this uncertainty by gathering more knowledge is insufficient and often impossible. Therefore, interventions are needed that assist actors in stimulating adaptive and collaborative thinking. The research gap this study aims to address is how to assess actor behavior under uncertainty for participatory interventions. Previous studies show that actors in the transport domain experience more uncertainty about governance than system problems regarding technology and sustainability. Also, actors treat long-term uncertainties as short-term risks, and are focused on reducing the latter by project management strategies. In this study, we intend to validate these findings, and how this is conditioned by individual and organizational variables. A stated preference survey has been performed in which mobility professionals had to indicate their strategy in alternative transport innovation scenarios. Scenarios include governance, system, long- and short-term uncertainties. Respondents are asked to state their preference for a strategy, given the scenario - Stop the project, do a risk-assessment for their own organization, or explore the possibilities collaboratively. The results of this study provide a theoretical basis for actor behavior under uncertainty and are illustrated by an intervention in a serious game.",Evaluating actor behavior under uncertainty for participatory interventions,[77250],95,"[10, 55, 143]",2441,Choice behavior,13,2,11,Behavioural OR,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,12 [building - 116],"['Behavioural OR', 'Group Decision Making and Negotiation', 'Transportation']",MA-11
"In this paper, a new methodology for the modelling and optimization of a two-echelon last mile freight transport system for urban areas is proposed.  Last mile deliveries are managed by means of an UDC [Urban Distribution Centre] located on the border of the urban area. At the UDC, freight is consolidated on small load units, for example pallets or small swap bodies, that are loaded on train wagons. Freight is then transported by train to some places on the border of the city centre, namely cross-docking points, where it is transhipped to small electric vans that carry out the last mile distribution. This problem has been addressed as a two-echelon urban freight distribution system with fixed satellites, where the UDC plays the role of the depot and cross-docking points play the role of satellites. A micro simulator has been developed to represent the second by second activity of each vehicle, of each train, and of each load unit. The micro simulator represents - the last part of the delivery trips, from cross docking points to receivers, but also the part of delivery trip carried out by train and the transhipment operations at cross-docking points. The simulator embeds a genetic algorithm that optimizes the vehicle routes from cross docking points [satellites] to final receivers. The proposed transport system has been applied in simulation to the city of Livorno, Tuscany, Italy.",Modelling and optimization of a two-echelon last mile urban freight distribution system.,"[77507, 77824, 77826, 77823]",503,"[65, 143, 131]",2443,Last mile delivery modeling,6,2,56,Transportation,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S04 [building - 101],"['Logistics', 'Transportation', 'Simulation']",MA-56
"This research addresses the profit maximization and pricing in a capacitated single assignment hub location problem. We assume that the demand between pairs of nodes in the network is price-sensitive and it is subject to uncertainty. The problem is formulated as a two-stage stochastic programming model, and a hybrid algorithm combining Lagrangian relaxation and Benders decomposition is developed to efficiently obtain optimal solutions. We also discuss some acceleration techniques to improve the performance of the proposed algorithm. Some numerical results and managerial insights are presented.",A hybrid Lagrangian relaxation and Benders decomposition algorithm to solve stochastic hub location problems with profit maximization,"[61096, 63492, 356]",582,"[64, 135, 5]",2444,Hub Location,29,4,61,Locational Analysis,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S10 [building - 101],"['Location', 'Stochastic Models', 'Algorithms']",MC-61
"The primary objective of this talk is to study exact and inexact versions of the high-order proximal-point methods in the nonconvex setting. The subsequential convergence is investigated by showing the monotonicity of the sequence Lyapunov function values,  where our Lyapunov function is the high-order Moreau envelope. We establish the global convergence of the method under Kurdyka-Lojasiewicz inequality. As a specific case study, we will discuss the effectiveness of these algorithms in addressing weakly convex optimization problems through theoretical analyses and numerical results.",High-order proximal point method in the nonconvex setting,"[45155, 66489]",509,"[81, 113, 5]",2445,Structured nonconvex optimization ,70,13,41,Nonsmooth Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,97 [building - 306],"['Non-smooth Optimization', 'Programming, Nonlinear', 'Algorithms']",WB-41
We will give an overview about recent developments and performance improvements within Gurobi 11.,Whats new in Gurobi 11,[48817],237,"[72, 111, 134]",2446,MIP Solvers,76,2,30,Software for Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,53 [building - 208],"['Mathematical Programming', 'Programming, Mixed-Integer', 'Software']",MA-30
"Hunger and famine pose great risks to global health and are the second in the UN's seventeen sustainable development goals to be achieved by 2030; however, they prove to be quite challenging to eradicate or alleviate. To mitigate their devastating impact, each year, aid agencies deliver tons of food commodities to populations in need. However, the delivery of food commodities is often expensive, and because of the complex intertwining factors shaping food security, it is very difficult to definitively predict future outcomes and demand for food aid. Without a timely identification of vulnerable populations, food aid often fails to arrive in the right place at the right time. We develop a stochastic optimization framework to assess the value of information - our analytical results quantify the advantages of incorporating food insecurity predictions in decision-making. Such predictions facilitate informed prepositioning decisions. Since acquiring relevant data might require heavy investments, we also analyze the delicate balance between allocating the limited available budget to the prepositioning of food commodities and investing in the acquisition of accurate data.",The Value of Demand Prediction for Improved Food Security,"[77609, 54717, 66434, 66302]",551,"[58, 138, 136]",2447,Demand Forecasting in Humanitarian Operations,38,8,21,OR in Humanitarian Operations [HOpe],"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,49 [building - 116],"['Humanitarian Applications', 'Supply Chain Management', 'Stochastic Optimization']",TB-21
"Motivation
Future energy systems will be dominated by variable Renewable Energy Sources [vRES]. Yet, it is unclear if all vRES investments will recover their costs on an energy only market [EoM] and which market designs could promote necessary investments. Thus. the German case study of the TradeRES project [1] aims to assess vRES remuneration needs and policy options.

Methods
We use AMIRIS [2, 3] to assess [among others] a scenario with ~85% vRES share [S0]. We assess a pure EoM and five different support instruments for vRES. These are - a fixed / variable market premium, two-way contracts for difference, capacity premia and a recent proposal from [4] [Financial CfD]. All premia are iteratively adjusted so each vRES agent is refinanced. We evaluate cost recovery rates, power market prices, curtailment and other measures of market performance.

Results
The data for Scenario S0 shows that EoM income alone is insufficient, as PV and wind operators cannot fully recover their costs. Market-based cost recovery rates vary depending on the support instrument due to the bidding and dispatch behaviour for vRES, resulting in different prices and market incomes. Our findings also indicate a significant sensitivity to the scenario considered, the degree of model-endogenous flexibility sources as well as backup fuel [hydrogen] prices.

Sources
[1] https://traderes.eu/
[2] doi - 10.21105/joss.05041
[3] https://dlr-ve.gitlab.io/esy/amiris/home/
[4] http://hdl.handle.net/10419",Assessing market effects of support instruments for renewables - Are they needed and how to design them?,"[77272, 78473, 67893, 80285]",403,"[36, 37, 131]",2448,New Market Designs & Models for 100% Renewable Power Systems,22,4,09,Energy Markets,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,10 [building - 116],"['Electricity Markets', 'Energy Policy and Planning', 'Simulation']",MC-09
"A fast, scalable, and easy-to-use API is an essential component when working with commercial high-end optimization solvers. FICO Xpress has been offering Solver APIs for more than two decades for an ever-growing set of programming languages [C, C++, Java, C#, Python, R]. 

The new FICO Xpress Solver API for Java and C# is designed as an object-based layer  ensuring a memory-efficient and reliable experience to the user. With the new API, the use of Solver features such as callbacks becomes easier, and it gives access to the full set of problem types available with Xpress Solver. 

Among the key features of the new Xpress Solver API are the ability to use modern programming concepts such as Collections, Streams, Lambdas, and operator overloading to build expressions and constraints. 

The new Solver API offers seamless integration with the Solver and access to cutting-edge features, and we observed model building times speedup by a factor of 8 on models with several millions of variables and constraints compared to previous APIs. 

In this talk, we focus  on design and architecture challenges when implementing  such a new API layer. Furthermore, we will cover new features available in the augmented API, as well as examples and guidelines how to implement optimization models in object-oriented languages. ",Introducing the new Object-Oriented Xpress Solver API,[76701],432,"[84, 134, 107]",2449,Modeling tools,76,9,30,Software for Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,53 [building - 208],"['Optimization Modeling', 'Software', 'Programming, Constraint']",TC-30
"Effective wildlife conservation relies on our ability to document changes in species abundance. Drones are commonly used to monitor wildlife numbers, such as the number of seals born during annual breeding aggregations. Within this context, we address harbour seal population monitoring as the Drone Routing Problem [DRP]. Given a set of islands [as locations] and an equal technology drone fleet, we aim to minimize the travel energy cost associated with routing while complying with drone energy limitations [i.e., battery consumption]. We model the DRP as a Mixed Integer Linear Program [MILP] resulting in a variant of the well-known Capacitated Vehicle Routing Problem [CVRP] model accounting for the relevant energy requirements. We also consider different drone cruising velocities and potential wind uncertainty. We solve the proposed MILP and present preliminary experimental results for a real-world application - monitoring seal pup production in Kosterhavet National Park, Sweden.",Optimizing Seal Pup Counting Operations - A Novel Drone Routing Application,"[47655, 60903, 77834, 11678]",217,"[94, 139, 145]",2450,Routing for hybrid fleets of vehicles,64,10,26,Combinatorial Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,012 [building - 208],"['OR in Environment and Climate change', 'Sustainable Development', 'Vehicle Routing']",TD-26
"In container shipping industry, container rollover [i.e. the container cannot be loaded onto the vessel that has been booked] is a common phenomenon, which is largely caused by uncertainties in demand, supply and operations. For example, in 2021, the global container rollover ratio was in the range of 30%~50% due to the COVID-19 disruption and the capacity crunch. The rollover phenomenon has a significant negative impact on shippers because their shipments will be delayed, and their operations will be disrupted. Recently, some shipping lines such as Maersk and Hapag-Lloyd have introduced an additional service called loading guarantee to shipping to differentiate them from other shipping lines and gain competitive advantage. In this study, we investigate the load guarantee service pricing problem in the container shipping industry. Specifically, we formulate the optimization problem of the pricing decisions of both loading guarantee service and freight rate for an ocean carrier. To examine the effects of capacity and demand uncertainties on the pricing decisions, we model the uncertainties in three scenarios - [i] pricing decisions considering capacity uncertainty, [ii] pricing decisions with demand uncertainty, and [iii] pricing decisions considering both capacity and demand uncertainties. Analytical analysis and numerical experiments are conducted to demonstrate the effectiveness of the models and results.",Pricing Decisions with Loading Guarantee Service in the Container Shipping Industry under Capacity and Demand Uncertainties,"[77296, 35540, 61417]",699,"[124, 84]",2451,Pricing and applications,11,10,59,Pricing and Revenue Management,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S08 [building - 101],"['Revenue Management and Pricing', 'Optimization Modeling']",TD-59
"Consider a situation in which an initial decision is made to prepare a set J of locations for potential future use supporting a set of customers under uncertain demand and uncertain future facility construction costs. Once the demand is revealed, the decision maker then makes a recourse decision selecting the set K of actual facility locations to be opened, where K is a subset of J. We present robust MILP formulations of this problem, assuming an objective function that minimizes the maximum regret. A budgeted uncertainty set is assumed, in which the realized demand is assumed to be equal to a most-likely value plus a potential positive deviation minus a potential negative deviation. Use of a regret-based objective function has a significant impact on the solution time when directly solving with a commercial MILP solver. In part, this is due to the fact that we assume demand could potentially be less than the expected value at any customer, which is relevant for a regret-based objective [but not for a total cost-based objective]. We then develop a computationally-efficient solution algorithm, based on a column-and-constraint generation [CCG] approach, and examine numerical test instances to identify the relationship between problem conditions and solution structure.",Minimizing maximum regret via robust optimization in a two-stage facility location problem under demand uncertainty and facility cost uncertainty,"[73711, 50330]",614,"[64, 127, 138]",2454,Location under uncertainty,29,5,61,Locational Analysis,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S10 [building - 101],"['Location', 'Robust Optimization', 'Supply Chain Management']",MD-61
"The importance of accurate planning and operation of energy ressources in today’s power system has made energy forecasting a popular topic of research. Transformer-based networks have shown promising results on various tasks including energy time series prediction. In this regard, neural architecture search [NAS], which facilitates the automated design of complex neural-based architectures tailored to a specific task, has been reported in the literature to outperform hand-crafted architectures. Therefore, in this research we apply NAS using Reinforcement Learning for the prediction of energy time series. We focus on the search for novel hybrid self-attention modules, which incorporate different functionalities presented in the encoder of Transformer-based frameworks for time series forecasting. Furthermore, we explore the automated design of self-attention components with memory states, in order to examine if the recurrence on a sequence-level could improve the expressive power of Transformer-based models. We report the results obtained from NAS on both real-world datasets and synthetic time series. We benchmark the performance of NAS-related models with Long Short-Term Memory neural cells, and with the Transformer-based frameworks of Informer, Autoformer and Pyraformer. ",Energy Time Series Forecasting with Neural Architecture Search,"[77750, 9422]",512,"[8, 47, 37]",2455,AI for Energy Finance,17,15,31,Analytics,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,54 [building - 208],"['Artificial Intelligence', 'Forecasting', 'Energy Policy and Planning']",WD-31
"The problem of frequency optimization consists in defining the number of vehicles per time unit, to operate each line of the system. The inverse of the line frequency is the time interval between consecutive vehicles in the line, which determines the waiting time. Given an available fleet and origin-destination demand data, we seek to minimize the total travel time of passengers, including in-vehicle and waiting components. To evaluate a frequency setting we apply the optimal strategies assignment model, which represents passenger behavior and outputs demand flows over the lines and passenger travel time from origin to destination. The capacity of a line is determined by its frequency and the size of the vehicles which operate the service. In systems operating over capacity, the flows produced by the assignment model exceed line capacity, causing failure to board the vehicles that passengers choose. To avoid this endogenous effect which invalidates the whole model, we propose a frequency optimization model that guarantees sufficient capacity in all lines. This is achieved by including a strict line capacity constraint in a bilevel programming model. After applying a reformulation, we obtain a mixed integer linear programming model which can be solved to optimality for small to medium size cases. We apply the model to a simple toy instance and to a well-known one, taken from literature. Numerical results allow for investigating the usefulness and the limitations of the model.",Frequency optimization in public transportation with strict capacity constraints,"[45413, 77835, 77836]",333,"[143, 150, 119]",2456,Public transportation ,6,13,55,Transportation,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S02 [building - 101],"['Transportation', 'Network Flows', 'Public Local Transportation Systems']",WB-55
"One-third of the global food production is lost or wasted. Of the total food waste in Europe, about 50% is caused by households. Food waste is the undesired result of multiple household food management behaviors. Food waste prevention behaviors, such as planning, aim to avoid a surplus of food reaching the household. However, the unpredictability of everyday life causes households to deviate from plans. Food recovery behaviors focus on using all the food that has entered the household. Although food prevention interventions have been thoroughly studied, interventions targeting food recovery have not yet received much attention in literature. Food recovery is, among other factors, influenced by household demographics, flexible cooking skills, shopping frequency, and shelf-stable food items on stock. As many of these factors are interrelated, it is difficult to quantify the effect of individual factors on food waste. Simulations of complex problems have proven helpful to shape interventions. However, they have not yet been used to study food waste. The aim of this study is to develop a meal planning model with a rolling horizon to simulate disruptions that might occur in everyday life with different shopping strategies. This gives insights into best practices regarding food recovery behaviors. The results of this study can be used to shape new interventions to reduce food waste.",Optimizing household food planning - a rolling horizon MILP model for reducing food waste,"[77377, 10561, 71700, 32080]",853,"[72, 100, 111]",2457,Reducing Food Waste,78,15,13,Secure & Sustainable Food Supply,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,15 [building - 116],"['Mathematical Programming', 'OR in Sustainability', 'Programming, Mixed-Integer']",WD-13
"Most sports associations regularly face the problem of determining and scheduling games for a huge number of non-professional [youth] teams. During planning, it is key to respect venue capacities and minimize travel distance.
A classic approach is to split up teams over leagues, and then having each league play a round robin tournament. In a round robin tournament, each team competes against every other team in the tournament an equal number of times.
In this talk, we propose an alternative approach, organizing a single yet incomplete round robin tournament. In this format, each team plays the same number of games, but teams are not required to face the same opponents. We exploit this flexibility to reduce the total travel distance and venue capacity conflicts.
Besides integer programming models, we provide theoretical results and an iterative two-phase decomposition heuristic. This heuristic first determines the home-away status of teams based on the venue capacity of clubs, and next selects suitable opponents while minimizing travel distances.
Extensive experiments using real-life benchmark instances from the literature confirm the advantage of an incomplete round robin tournament compared to the classic multi-league approach and validate the effectiveness of the proposed heuristic.",Beyond leagues - a single incomplete round-robin tournament for multi- league sports timetabling,"[9583, 67663, 55863]",664,"[99, 142]",2458,Sports scheduling and optimization,37,7,16,OR in Sports,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,19 [building - 116],"['OR in Sports', 'Timetabling']",TA-16
"The field of generative artificial intelligence [GenAI] has seen exponential growth over the past few years in research papers and publications. The techniques have gained traction across industries. A hype train is rolling around how to best leverage GenAI in the field of Operations Research - code generation to increase productivity, chatbots for documentation, generation of optimization models - just to name a few. Thinking further, an LLM may generate models for decision support tools and execute the code to guide complex business decisions - a CO2-optimal delivery network, an optimal production schedule, an optimized pricing table. 
Large language models [LLMs] occasionally produce incorrect or misleading results, a phenomenon commonly called hallucinations. We demonstrate basic examples where a slight adaptation of a question [prompt] turns the LLM response into a false statement. Besides basic inaccuracies, there are also considerations around data exposure, data leakage, privacy violations, copyright violations, biased answers, dangerous or unethical usage, inappropriate language and malicious code. Best practices need to be followed to responsibly apply GenAI in decision support tools!  
Our examples demonstrate the importance of balancing innovation with risk mitigation in leveraging GenAI for critical decision-making tasks advocating for a holistic approach that combines technological advancements with ethical considerations and human oversight.",Generative AI in Decision Support Tools - Managing Hallucinations,"[10087, 60230]",704,"[26, 8, 7]",2459,Optimization Tools,76,12,30,Software for Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,53 [building - 208],"['Decision Support Systems', 'Artificial Intelligence', 'Analytics and Data Science']",WA-30
"Many real-life problems frequently encompass multiple, sometimes conflicting, objectives that require simultaneous consideration. For this purpose, there is a need for a comprehensive assessment methodology. Multi-criteria decision analysis [MCDA] and multi-objective optimization problems [MOP] are methodologies attempting to overcome such problems. MCDA focuses on deriving a satisfactory solution while accounting for a stakeholder preference structure. MOP methodologies focus on the optimization tasks of identifying Pareto optimal solutions, where optimal solutions must be taken in the presence of a trade-off between two or more conflicting objectives. Over the past decade, extensive research has been conducted, resulting in substantial bodies of literature on both subjects, revealing potential opportunities and synergies between them. This presentation investigates the possible combination of MOP and MCDA methods in a novel framework to address large, complex decision problems involving conflicting objectives and several stakeholders. The framework will be tested on the combinatorial problem of distributing high school students within the capital region of Denmark. 
",A Combined Framework of MCDA and MOP Methods for Planning High School Student Distribution in the Capital Region of Denmark.,"[77830, 77812, 12015, 50371]",117,"[77, 72, 14]",2460,Discrete Multiobjective Optimization,34,13,37,Multiobjective Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,33 [building - 306],"['Multi-Objective Decision Making', 'Mathematical Programming', 'Combinatorial Optimization']",WB-37
"When a football team is experiencing a sporting downturn, there are limited short-term options available from the club's management perspective. Building a better-performing squad in the short run is not feasible. Another potential adjustment is to change the team’s head coach. This analogy can be transferred to leaders managing any organization. We focus on the question of which characteristics are attributed to a successful long-term leader. The paper investigates the impact of team performance and individual characteristics of the coach on the coach’s tenure. For this purpose, we use various data from the top five European leagues [first and second division] from the 2012/13 to the 2022/23 season. In order to adequately analyze this impact, we use elaborate competing risk models to identify the effect of cultural differences on various reasons of coach dismissals. The data is gathered at the matchday level and thus obtained a dataset covering 43,985 matchdays. To our knowledge, we are the first to simultaneously examine these effects across ten different European leagues over a decade. Just like the modern cooperation, most football teams consist of employees [players and head coaches] of different cultures producing language barriers which can particularly lead to communication and transaction costs. The unique dataset allows us to include these cultural and linguistic effects that could impact team performance, coach's performance, and ultimately, the coach's tenure.",Should he stay or should he go? Head Coach turnover and cultural disparities in the market for managers - Empirical Evidence from European Professional Football,[77828],668,"[99, 0]",2461,Performance and scouting in football,37,10,16,OR in Sports,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,19 [building - 116],['OR in Sports'],TD-16
"The decision-making process in urban planning and design projects is inherently complex as it embraces so many different actors, interests, dimensions and uncertainties. Evaluation plays a key role in supporting such processes, as a guide to prioritising decisions based on the observation of data derived from analyses, measurements and estimations. In this context, the paper investigates the potential benefits of argumentation theory aimed at managing the complexity of information and knowledge about the situation under analysis through specific schemes. Indeed, argumentation-based approaches make it possible not only to identify a 'good' choice, but also to formulate, through a logic-based formalism, a final recommendation in a graphic and mutually comprehensible form. In this sense, the paper starts from the exploration of an emblematic case study of the city of Turin, which saw numerous actors, interests and critical points in its decision-making process, proposing an a posteriori argumentation scheme. The latter highlights the points of agreement and disagreement that led to a design strategy that was justified and satisfactory to the decision-making actors. Through application, the potential and criticalities of this approach in decision support in the urban context are observed.",Argumentation Schemes for Urban Decision Support,"[15067, 7119, 61638, 77840, 13]",109,"[149, 139]",2463,MCDA and urban planning 1,44,8,47,Multiple Criteria Decision Analysis,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,50 [building - 324],"['Problem Structuring', 'Sustainable Development']",TB-47
"This paper presents an investigation into multi-echelon inventory optimization, emphasizing the integration of real-life constraints and uncertainties. By considering a case study for spare parts in the automotive industry, derived from practical inventory management scenarios, we address the complexities inherent in supply chain dynamics.
Through our analysis, we demonstrate the efficacy of our proposed approach in enhancing inventory control across multiple echelons while underscoring the importance of considering real-world factors such as lead times, demand variability, and capacity constraints.
Another aspect that is considered, in a distribution system with mixed ownership of the retailers, is the implementation feasibility of transitioning from a traditional single-echelon inventory control system, requiring extensive stakeholder management and buy-in.
The findings bridge the gap between practitioners and researchers in this area, offering valuable insights into future research of applying multi-echelon theory and techniques in dynamic supply chain environments.
",Multi-Echelon Inventory Optimization - A Case Study considering real-life constraints and uncertainties,"[77845, 77786, 77847, 13470]",480,"[61, 130, 138]",2464,Retail Inventory Management II,30,4,50,Retail Operations,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,M2 [building - 101],"['Inventory', 'Service Operations', 'Supply Chain Management']",MC-50
"Orienteering is a cross-country running sport where competitors use a map and compass to visit control points as quickly as possible. In a special variant called rogaining, each control point is assigned with a score, competitors are allowed to visit the given controls in the order of their own preference, and there is a time limit that prevents competitors from visiting all control points. Hence, it is important for the competitors to carefully plan which controls they are going to visit [like in the knapsack problem] and in which order [like in the traveling salesman problem]. Thus, winning a rogaining competition not only requires physical but also planning and navigational skills.
 
Unlike most operations research literature, we approach the rogaining problem from an organizers’ perspective - how to assign scores fairly, balancing physical and strategic skills? In practice, this typically translates to designing rogaining competitions such that brute physical skills are not overly favored. We follow a bi-level optimization approach where the outer-layer assigns scores to control points and the inner-layer simulates a portfolio of pre-defined competitors. The result is a Pareto front of possible score assignments with distinct trade-offs between physical and strategic skills. Our method's efficacy is shown in its application to the 2023 rogaining world championships. ",How to organize a fair rogaining competition?,"[55863, 73964, 73997, 9583]",948,"[99, 5, 131]",2465,Fairness in sports,37,12,16,OR in Sports,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,19 [building - 116],"['OR in Sports', 'Algorithms', 'Simulation']",WA-16
"Operational concepts i. e. service networks with macroscopic timetables are an essential railway planning tool and are used for various tasks such as capacity assessment or demand modelling in strategic and tactical planning stages. However, it is crucial to ensure the feasibility of such concepts as they form the high-level base for the timetables of the following planning stages, which need to ensure conflict-free railway operations. Providing feasible concepts is particularly difficult in large railway networks, where numerous and complex interdependencies exist.  We therefore propose a petri net based binary search algorithm to determine the feasibility of operational concepts in railway networks. There, the petri net based formulation is used to assess the networks operational period by determining the cycle mean. The minimal cycle mean i. e. the lowest operational period is determined by applying the petri net formulation within a binary search algorithm. The obtained approach determines the lowest operational period in a timely manner. It can thus be used as a lower bound method for constructive capacity assessment algorithms or for the evaluation of operational concepts in the strategic and tactical planning stages. The approach further allows to derive additional information on the critical processes and thus provides a starting point to further concept improvements. ",A petri net based binary search algorithm to assess feasibility of operational concepts for railway networks.,"[68239, 41723, 14909]",394,"[122, 130]",2466,Railway Capacity Management,85,15,54,Public Transport Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S01 [building - 101],"['Railway Applications', 'Service Operations']",WD-54
"Given an undirected graph, the bi-objective p-median and p-dispersion problem is to select p vertices of the graph such that the weighted average distance of the graph's vertices to their nearest of the selected p vertices is minimized and the distance between any two of the p selected vertices is maximized. For computing the bi-objective problem's non-dominated set the ε-constraint method as well as the hybrid weighted sum and ε-constraint method seem to be suitable approaches. ε-constraints put on the dispersion objective may, however, be enforced in different ways. Additionally, also the remaining constraint system can be formulated quite differently. In this talk, we compare the performance of different implementations of the ε-constraint and hybrid method based on different model formulations and means to enforce the ε-constraint. It thereby seems that in particular the application of Benders' decomposition for solving the scalarized problems gives promising results.",Exact solution methods for the bi-objective p-median and p-dispersion problem,[77833],602,"[64, 14, 111]",2471,Theory of Multiobjective Optimization,34,14,37,Multiobjective Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,33 [building - 306],"['Location', 'Combinatorial Optimization', 'Programming, Mixed-Integer']",WC-37
"Empty container repositioning [ECR] is crucial in handling global trade imbalances by managing the flow and storage of empty containers to effectively accommodate customer demands and returns. We formulate a Markov decision process that accounts for practical characteristics of ECR, including multiple transportation modes and lead times, and uncertainty and serial correlation in net container inflows. To determine a cost-minimising repositioning policy for real-life scenarios, we employ a stochastic approximate dynamic programming approach, integrated with statistical techniques, such as convex regression and Latin hyper cube sampling. We highlight the advantages of incorporating exogenous variables into the approximation model. A case study with historical daily data on empty container in- and out-flows demonstrates the effective control of on-hand inventory levels while optimising holding and leasing costs. Moreover, we quantify the benefits of leveraging all transportation modes, with cost reduction potentials of up to 26.05%. Finally, we evaluate the robustness of our algorithm under variations in key parameters.",Approximate dynamic programming for inland empty container inventory management,"[71484, 36601]",310,"[108, 82, 66]",2472,Industrial Optimization,14,2,03,Data Science Meets Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,1005 [building - 202],"['Programming, Dynamic', 'Optimal Control', 'Machine Learning']",MA-03
"In line with the 2030 Agenda, measurement through indicators is recognised as a mean to overcome economic, social and environmental challenges in favour of sustainable development. In this context, the SDG11 with its indicators constitutes the starting point for monitoring and implementing urban sustainable development. However, there is a need for measurements going beyond conventional statistics, including data generated, collected and stored by governments as well as those provided by external data sources. In this context, the present research shows a literature review of indicators useful for measuring and evaluating urban sustainability. More specifically, this research aims to - i] build a framework of the official SDG11 indicators at a global, European and Italian national level; ii] identify innovative and operative indicators related to SDG11 proposed in the scientific literature, in the Sustainable Development Strategies produced at the Italian level and in the Sustainable Assessment Tools; iii] provide a set of indicators relevant and useful both for monitoring urban areas and for constructing and evaluating projects. The research result constitutes the first theoretical preliminary step of the research Project of Relevant Interest [PRIN] called GLOSSA [A GLOcal Knowledge system for the sustainability assessment of urban projects]. ",Developing a knowledge base for the Sustainability Assessment of urban projects - the GLOSSA project,"[32846, 61638]",911,"[139, 43]",2473,"Assessment Methods for Shaping the Green, Inclusive, and Digital Cities II",79,3,18,Sustainable Cities,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,42 [building - 116],"['Sustainable Development', 'Facilities Planning and Design']",MB-18
"In the face of climate change threats to agriculture, understanding farmers’ decisions on crop selection and irrigation practices becomes crucial, as they tend to be significantly shaped by risk preferences. Farmers, who all recognize shifts like increased drought severity, exhibit diverse reactions influenced by individual risk aversion, satisfaction, uncertainty, and peer comparisons. Agent-based modeling [ABM] stands out as an essential tool for capturing system dynamics and simulating farmers-environment-climate interactions. Despite calls for realistic human behavior integration, the prevailing paradigm still favors rational agents.
Here we present an ABM application in Italy's Adda River basin, modeling farmers' decisions on crops and irrigation methods. To understand the system's response and resilience to changing climates, we implemented diverse behavioral sub-models. The first maximizes profit with classic perfect foresight. The second introduces climate uncertainty and risk aversion variations based on farmers' memory. The third, a comprehensive behavioral model, considers reference points and loss aversion, acknowledging decisions beyond profit. It offers a holistic view of farmers' choices amid uncertainty and risk, calibrated with collected survey data. Preliminary findings uncover significant differences in system dynamics and resilience across the behavioral setups, providing insights into their efficacy and suitability. 
",Unveiling Behavioral Heterogeneity - An Agent-Based Model Exploration of Farmer Decision-Making in the Face of Climate Change,"[77741, 77844, 58481]",601,"[3, 10, 33]",2474,Simulation in sustainability,77,8,43,"Agent-based Models in Management, Economic and Organisation Sciences","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,99 [building - 306],"['Agent Systems', 'Behavioural OR', 'Economic Modeling']",TB-43
"NOMAD is a free software package for blackbox optimization. It implements the mesh adaptive direct search [MADS], a derivative-free optimization algorithm. NOMAD is designed to solve industrial problems, and therefore it includes several practical features such as constraints handling, the possibility of processing several objectives, noisy evaluations, etc. Several parallel variants of the algorithm are also available. This talk introduces derivative-free and blackbox optimization, the MADS algorithm and the practical use of NOMAD. Realistic test cases will also be discussed.",Blackbox optimization with the MADS algorithm and the NOMAD software,[44641],701,"[19, 134, 102]",2475,Optimization Solvers,76,5,30,Software for Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,53 [building - 208],"['Continuous Optimization', 'Software', 'Parallel Algorithms and Implementation']",MD-30
"Given a set of jobs [or items], each of which is characterized by its resource demand and its lifespan, and a sufficiently large number of identical servers [or bins], the busy time minimization problem [BTMP] requires to find a feasible schedule [i.e., a jobs-to-servers assignment] having minimum overall power-on time. Although being linked to the field of temporal bin packing, BTMP represents an independent branch of research. Typically, such considerations [and generalizations of it] are very important in data center workload management to keep operational costs low. Hence, finding efficient and powerful solution techniques for BTMP is a relevant topic in cutting and packing, both from a theoretical and practical point of view. In this talk, we give an overview of heuristic and exact approaches for the problem under consideration and analyze their theoretical properties and computational behavior. As a first main contribution, we suggest a new best-cost based heuristic showing convincing results in a wide variety of numerical test. In terms of exact approaches, we propose some improvements for the ILP models from the literature and establish a new combinatorial flow-based formulation. Based on extensive numerical tests with differently-characterized benchmark sets, the flow model is shown [i] to improve the state-of-the-art approach for general instances, and [ii] to be competitive with a matching formulation tailored for a special case.",Exact and Heuristic Solution Approaches for Busy Time Minimization in Temporal Bin Packing,"[48992, 42815]",674,"[23, 109, 129]",2476,Cutting and Packing 5 - related topics,81,7,07,Cutting and Packing [ESICUP],"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,1019 [building - 202],"['Cutting and Packing', 'Programming, Integer', 'Scheduling']",TA-07
"The purpose of this paper is the formulation of an enhanced index replication model with second order stochastic dominance constraints.
The replicating portfolio is constructed minimizing an asymmetric deviation measure via quantile regression. 
According to the investor's risk aversion preference, the resulting linear optimization problem can be solved for different quantiles.
In order to take into account the increasing attention to ESG issues, the strategies can be ehnanced, via hard and soft constraints, focusing on the sustainability of the replicating portfolio.
The performance of the optimal portfolios are tested both in-sample and out-of-sample, and are compared to the solutions of well-known models.",ESG enhanced tracking portfolio with quantile regression,"[62503, 3141]",412,"[83, 0]",2478,Quantitative methods for systemic and climate risk,9,4,51,Risk management in finance,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,M5 [building - 101],['Optimization in Financial Mathematics'],MC-51
"The aim of the Water-Energy-Food-Ecosystem Nexus [WEFE Nexus] approach is to overcome the traditional “siloed approach to natural resources management – i.e. based on sectorial perspectives – ultimately achieving an integrated resources management. 
In this study, we particularly focus on the support to policy-making in complex Nexus systems, proposing a highly participatory methodology to co-design potential policy interventions capable of achieving sectoral security, while optimizing the Nexus.
 A System Dynamics Model [stock and flow] that describes the security dimensions was built. We then selected the variables representative of the concept of security, and built the underlying equations, with the idea of evaluating their variation in different political scenarios. Multi-objective optimization [MOO] has been used for identifying and analyzing existing and potential implications and trade-offs in intervention policies. 
In our framework, we compared two distinct a posteriori MOO methods - pattern search [PS] and the Evolutionary algorithms [EA] - to provide the Pareto front for two intervention areas, i.e., Donana and Pinios [LENSES project]. Some hints are given on how this tool turns out to be very useful as a support to decision-makers, to allow accounting for their preferences in the process.",Multi-objective optimization to support decision making in policy interventions,"[67823, 54579, 78605, 78620, 78619, 78618, 67835, 51066]",131,"[133, 77, 31]",2480,OR Innovations in Policy Making - A,26,3,13,Soft OR and Problem Structuring Methods,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,15 [building - 116],"['Soft OR', 'Multi-Objective Decision Making', 'Dynamical Systems']",MB-13
"We study the tactical transportation planning problem of a Norwegian aluminium producer. Today, products are shipped from the production plants in Norway to customers in Europe using a combination of maritime and land-based transportation. On the first leg from the production plants to terminals on continent, products are shipped using vessels on a fixed, weekly schedule. From the terminals, products are transported by truck to the customers when needed. Each customer is served through a dedicated terminal. The current planning process has resulted in challenges as vessel capacities are not sufficiently accounted for, often resulting in the need to manually reassign shipments from dedicated routes to alternative routes. We present a mathematical model for supporting decision-makers when setting up the transportation plan. The goal is to minimize the total transportation costs while ensuring that all customers are served within the delivery deadline. We further analyze the benefits of changing the current system with dedicated terminals. The analysis intends to provide insights regarding the interplay between flexibility in the transportation system, transportation costs, vessel capacity utilization, and system complexity
",Tactical transportation planning for a Norwegian aluminium producer,"[62559, 77861, 77863]",445,"[65, 143, 70]",2487,Supply Chain Network Optimization,6,15,55,Transportation,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S02 [building - 101],"['Logistics', 'Transportation', 'Maritime applications']",WD-55
"More than 3100 million people [42%] could not afford a healthy diet in 2021, resulting in malnutrition and immediate and long-term health issues. Regional and national governments can promote nutrition policies [e.g., subsidizing nutrient-rich products], often targeting vulnerable population groups [e.g., children under 5, pregnant or lactating women]. Due to the different nutritional composition of the food items and the varying nutritional requirements of the population [with age and sex], selecting the overall best policy is not straightforward. This study combines bi-objective optimization and multi-criteria assessments, providing comprehensive quantitative tools to assist in defining diets and assessing the impact of interventions on the nutrition intake of the population. It defines three stages - 1] a bi-objective optimization model to compute Pareto-optimal solutions for daily diets concerning cost and nutritional adequacy; 2] a multi-criteria procedure to determine the best diet for each population group, considering economic, social [equity], and environmental factors [water and CO2 footprint]; 3] a multi-criteria procedure to evaluate interventions and select the most effective based on economic, social, and institutional factors [promoting local economy]. Regions of Cambodia experiencing barriers to adequate dietary access are selected as a case study. Results can aid decision-makers in identifying diets and interventions to improve the nutrition of the population.",Preventing malnutrition - quantitative tools for decision-making in optimizing diets and interventions for regions with limited economic access to adequate diets,"[71292, 77503, 71137, 77851, 77855, 71494]",419,"[77, 58, 100]",2489,Humanitarian Aid,78,13,13,Secure & Sustainable Food Supply,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,15 [building - 116],"['Multi-Objective Decision Making', 'Humanitarian Applications', 'OR in Sustainability']",WB-13
"Manufacturing systems aim to be streamlined and produce innovative and high-quality products just in time to meet consumer needs. Just-in-time manufacturing processes have suffered significantly from disruptions in the supply chain, demand uncertainty, shop floor break downs, product quality failures, etc. To be responsive to all possible disruption scenarios in the future, first we need to recognize and assess the consequences of any changes we will have to make under these disruptions. Then, by use of an optimization method, we need to minimise the impact of these unexpected disturbances. For this purpose, we observe all these disruptions under several possible scenarios and develop a stochastic programming formulation. The objective function of this formulation minimises the total expected costs of a production operation that consists of production costs, inventory holding costs, inspection costs, and the penalty costs for the uncompleted orders. The uncertainties in this formulation stem from machine break downs, product quality loss, and demand fluctuations. Since the developed stochastic quadratic integer formulation is not efficient to solve large instances with a large set of scenarios, we study a heuristic solution approach based on scenario reduction logic. The generated optimization tool will support people in production industries to observe the consequences of different scenarios with alternative solutions and to make the best decision in terms of imposed costs.",Stochastic programming for responsive production operations under uncertain disruptions,"[51101, 62405]",807,"[69, 105, 136]",2490,Stochastic lot-sizing problems,32,7,49,"Lot Sizing, Lot Scheduling and Production Planning","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,M1 [building - 101],"['Manufacturing', 'Production and Inventory Systems', 'Stochastic Optimization']",TA-49
"Every week, planning teams in Amazon need to allocate the resources [e.g., schedule trucks, plan for labor] required to deliver customer orders in a timely and cost effective manner. This planning process is based on demand forecasts, that estimate the packages to be shipped across the network. As one gets closer to the delivery date, real-time signals on the progress of the plan execution become available. This opens the door to update the forecast and refine the resource planning continuously to improve cost and increase effectiveness. However, regular plan changes need to account for uncertainty and end-to-end impact across the network to avoid plan fluctuations and local decision making. In this talk, we explore different methodologies to generate the demand forecast and its refreshes with focus on accuracy, coherency, uncertainty quantification and reduced variability, as well as its combination with different resource planning strategies for regular updating.",Demand forecasting for Resource Planning and Dynamic Decision Making ,"[76695, 77850, 77852, 77853, 77854]",434,"[47, 12, 143]",2491,Forecasting for the middle mile,92,14,57,Optimization at Amazon,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S06 [building - 101],"['Forecasting', 'Capacity Planning', 'Transportation']",WC-57
"We investigate the benefits of relating reinforcement learning [RL] with risk-sensitive control. Our starting point is the duality between free energy and relative entropy, see e.g. Dai Pra et al. [1996]. It establishes an equivalence between risk-sensitive control and standard stochastic control problems with an entropy regularization term.
This approach has two major advantages:
i] it does not require a preliminary change of measure à la Kuroda and Nagai [2002];
ii] it is naturally consistent with the use of a regularization/penalization term in the literature that connects reinforcement learning with stochastic control, e.g. Wang et al [2020]. In this sense it also allows for a risk-sensitive interpretation of the entropy regularization in RL.

We furthermore show how this connects to the existing literature on risk-sensitive investment management [Kuroda and Nagai, 2002, Davis and Lleo, 2008, 2020, 2021], whereby cases with unknown parameters or with partial observation showcase the advantages of reinforcement learning.",Reinforcement Learning Methods in Risk-Sensitive Investment Management,"[77608, 71595]",126,"[83, 82, 66]",2493,"Dynamic portfolio selection - stochastic optimization, filtering, and learning techniques",74,4,57,Modern Decision Making in Finance and Insurance,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S06 [building - 101],"['Optimization in Financial Mathematics', 'Optimal Control', 'Machine Learning']",MC-57
"Decision making in Transportation, including network design, reactive adjustments during execution, or post-run analysis, requires an understanding of the vehicle transit time, carbon impact, and risks. In this talk, we explore a unified approach for the estimation of such factors that leverages the information available from vehicle sensors and other geospatial data at each time horizon. We propose a model that combines routing and predictive algorithms to provide transit estimates for both connections between Amazon locations as well as unseen routes between third-party vendor connections. We describe how we deal with noisy telematic signals and sparse ground truth, the machine learning approaches to work with road-level data, and the mechanisms to represent the variability inherent to a prediction that depends on the road conditions and driver behaviour.",Transit Estimation Models for Transportation Planning,"[77837, 77866, 77988, 77864, 78421, 77895]",434,"[143, 66, 145]",2495,Forecasting for the middle mile,92,14,57,Optimization at Amazon,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S06 [building - 101],"['Transportation', 'Machine Learning', 'Vehicle Routing']",WC-57
"Kidney exchange programmes [KEPs] increase the potential for living kidney donation, often using operational research techniques, and drastically improving the quality of life of recipients. This is achieved by identifying an optimal set of transplants that can be performed within a pool of donor-recipient pairs via exchange cycles and chains.  However, determining appropriate optimality criteria to use is not trivial, and only with historical real data or realistic synthetic data can simulations show how different criteria affect the performance of the KEP.  However, historical datasets can be difficult to obtain.

In this talk we walk through the process of producing a realistic synthetic data generator from real-world data using state-of-the-art techniques. We include full Jupyter notebooks that can easily be followed by researchers associated with real-world KEPs. We then follow up by demonstrating how the data generator can be used to simulate the effects of a number of different optimality criteria on the KEP.",Synthetic data generation for kidney exchange programme simulation,"[62409, 37829]",951,"[56, 134, 7]",2496,Kidney Exchange II,3,10,10,OR in Health Services [ORAHS],"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,11 [building - 116],"['Health Care', 'Software', 'Analytics and Data Science']",TD-10
"We introduce a comprehensive framework for addressing multicriteria portfolio decision analysis challenges, particularly in situations where between-projects independence or within-project independence may not be assumed. The Choquet integral preference model, a widely adopted non-additive integral in multicriteria decision analysis, is employed to consider potential interactions between projects and between criteria. In this context, to streamline the preference model and maintain problem manageability, we opt for the 2-additive Choquet integral, which assigns values only to individual entities and pairs of entities. An illustrative example demonstrates the application of our approach to a multicriteria portfolio decision analysis problem.",Modeling Criteria and Project Interactions in Portfolio Decision Analysis with the Choquet Integral,"[32182, 36935]",116,"[25, 77, 26]",2497,MCDM for project portfolio problems,44,4,44,Multiple Criteria Decision Analysis,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,20 [building - 324],"['Decision Analysis', 'Multi-Objective Decision Making', 'Decision Support Systems']",MC-44
"Given a set of points in the space of an arbitrary dimension, the hyper-rectangular clustering problem with axis-parallel clusters [HRC-AP] consists in grouping the points into clusters, each cluster being determined by a hyper-rectangle in the corresponding space. HRC-AP has been proposed as a model for explainable clustering, as it is straightforward to describe the clusters by the bounds defining each hyper-rectangle - if each coordinate corresponds to a relevant parameter in the application generating the points, then a cluster is specified by its bounds on each parameter.
A variant of HRC-AP considering the existence of outliers [called HRC-APO], allows a certain number of points to be discarded and not included in any cluster. In previous works, we explored integer programming techniques for HRC-APO proposing several formulations and algorithms. We showed that extended formulations [solved via branch-and-price] outperformed classical compact formulations. Nevertheless, these techniques fall short as soon as the number of points to be clustered is increased to medium-sized real-life instances.
In this work we propose an innovative strategy to solve the pricing problem by modeling it as a flow problem. This allows us to develop a polynomial-time routine for the pricing, qualitatively improving the performance of previous approaches. We compare the efficiency of the new method against the existing ones by providing an exhaustive computational experimentation.",Improving pricing strategies on a branch-and-price algorithm for the hyper-rectangular clustering problem with axis-parallel clusters,"[28518, 73638, 32736]",140,"[13, 66, 109]",2498,Mathematical Optimization for Trustworthy Machine Learning,15,9,27,Mathematical Optimization for XAI,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,047 [building - 208],"['Column Generation', 'Machine Learning', 'Programming, Integer']",TC-27
"The ITF Transport Outlook [2021] projects a 60% increase in carbon dioxide emissions [CO2] by 2050, primarily from the transport sector. Emission reduction, crucial for sustainable development goals [SDG13], is emphasised by the UK Department for Business Energy & Industrial Strategy [2021] reporting a swift rise in CO2 emissions from 97.7 MtCO2 in 2020 to 107 MTCO2 in 2021. Urgency is stressed by the UK Department of Transport [2021], urging a shift to a low-emission transport system with a 50% reduction by 2030. Despite this, EU transport strategies, as per the European Union's examples, have not effectively reduced CO2 emissions. Researchers focus on methods like Data Envelopment Analysis [DEA] to assess transport performance, revealing a literature emphasis on road and passenger car CO2 emissions. The study identifies gaps, suggests future directions, and recommends assessing maritime and aviation sectors.",DEA in Environmental Performance Evaluation for the Transport Sector - A Survey,"[77849, 77856]",391,"[24, 138, 139]",2499,DEA and its application,89,2,48,Data Envelopment Analysis and its Application,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,60 [building - 324],"['Data Envelopment Analysis', 'Supply Chain Management', 'Sustainable Development']",MA-48
"Healthcare delivery in resource constrained environment is a big challenge for both healthcare professionals and policy makers. This is mainly due to higher levels of interdependencies among healthcare providers. Of late, healthcare professionals are adopting the management science approaches for improving quality, efficiency and timely delivery of delivery of care at low cost. This mainly includes supply chain management approaches which is built based on theory of coordination and collaboration with suitable incentives. In developing countries where resources are scarce such coordination failures can have potentially severe impact on patient health. However, there is limited understanding about how coordination takes place across and within the different healthcare service providers and how this influence emergency care delivery. This presentation focuses on coordination and collaboration challenges in trauma care delivery in India using a patient survey. The findings suggest mechanisms to better integrate the processes in the care delivery which include setting-up referral processes, ambulance services, using third party coordinators and process improvement within the hospital following lean principles and suggestions for policy making are also highlighted.

",Approaches for Improving coordination in emergency care delivery in emerging economies context,[41272],774,"[56, 65, 143]",2503,Infectious diseases and pandemics 2,38,14,21,OR in Humanitarian Operations [HOpe],"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,49 [building - 116],"['Health Care', 'Logistics', 'Transportation']",WC-21
"In this talk, we present a unified approach for constructing efficient methods for solving Variational Inequalities, presented in a composite form [CVI]. This class of problems is close to the maximal one, which can be efficiently treated by numerical methods. At the same time, it is more difficult than the class of Convex Optimization Problems. All efficient methods for VI use an additional “extra-gradient” step. We propose a new interpretation of this step as a cutting plane for the optimal solution, reflecting the interaction of the monotone operator with the boundary of the feasible set. Contrary to the existing approaches, we introduce a universal extragradient step, which does not depend on the particular class of CVI. Consequently, our framework can be used for developing optimal methods for CVI, which are based on high-order oracles. ",High-Order Reduced-Gradient Methods for Composite Variational Inequalities,[25671],258,"[19, 21]",2504,Advances in Complexity of Convex and Nonconvex Problems,84,4,32,Advances in large scale nonlinear optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,41 [building - 303A],"['Continuous Optimization', 'Convex Optimization']",MC-32
"This presentation addresses the development of a supply chain customized for Carbon Capture and Storage [CCS] processes. Carbon Capture and Storage technology entails capturing CO2 emissions at significant sources, transporting the captured greenhouse gas to designated storage sites, and permanently storing it within underground geological formations. While pipelines and maritime transport are being investigated in various studies due to their ability to transport large volumes of CO2, vehicle transportation offers viable alternatives for the initial phase of CCS deployment or to address unavoidable emissions at a lower volume. We focus on the characteristics of emission sources, transportation modes and conditions, and permanent storage locations within the supply chain. Every element of the supply chain incurs annual investment and operational costs. The size of the network is determined by an annual emission capture target based on the existing CO2 reduction targets. The problem is formulated as a mixed-integer linear program that aims to minimize the net present value over a specified planning horizon. The model is solved by CPLEX. Preliminary findings from a case study are presented, with analyses centered on costs associated with network participation, utilization of shared transportation infrastructure contingent upon mode selection and transport conditions, as well as storage expenses.",Exploring Transportation Alternatives in Carbon Capture and Storage Supply Chains,"[72706, 13086]",545,"[138, 79, 139]",2506,Clean Energy Supply Chains,19,4,24,Sustainable Supply Chains,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,83 [building - 116],"['Supply Chain Management', 'Network Design', 'Sustainable Development']",MC-24
"Energy communities [EC] using renewable sources are a way to achieve energy transition, assigning an active role to end-users [called prosumers]. Every prosumer is an electricity consumer and producer, exchanging electricity with other prosumers and the grid. EC are gaining attention in Europe among researchers and the industry, thanks to their economic and legal recognition within the national regulations. The design and operation of EC is a complex combinatorial optimisation problem, where several objectives are to be accounted for - economic, to ensure cost savings, environmental, to take profit of renewable sources, and social, to achieve equity between prosumers. In this work, a multiobjective [MO] mathematical model is developed for the design and operation of EC, determining the location and size of equipment [renewable generators and batteries] as well as the electricity exchanges between prosumers and the grid. A correlation analysis between the solutions obtained from the individual optimisation of several economic, environmental and social objectives is performed in order to select a reduced set to be included in the MO model. Through a simple approach based on ε-constraints, an approximation of the Pareto front can be identified, constituting a set of alternative configurations among which decision-makers may select their preferred one.",Multiobjective approach for the design and operation of energy communities,"[77395, 77862, 71292, 43610, 77397]",844,"[36, 77, 93]",2507,OR in Electricity Markets,23,3,19,OR in Energy,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,44 [building - 116],"['Electricity Markets', 'Multi-Objective Decision Making', 'OR in Energy']",MB-19
"We address the Green Commodity-Constrained Split Delivery Vehicle Routing Problem [GC-SDVRP], which involves designing efficient and environmentally friendly delivery routes that reduce the carbon dioxide [CO2] emission associated with the transport of multiple commodities. The GC-SDVRP considers that different types of commodities are available at the depot, and each customer has specific demands for different types of commodities. Multiple vehicles may visit a customer to deliver different types of commodities, as long as all commodities of the same type are delivered by the same vehicle, thus reducing customer inconvenience caused by split deliveries. Despite its practical relevance, the green variant of this problem has not been studied in the literature so far.  In this talk, we introduce the GC-SDVRP and propose a branch-and-cut approach that is based on the solution of a relaxed formulation that provides a lower bound on the optimal value. Additionally, we adapt two other formulations from the literature. The results of computational experiments using benchmark instances indicate the effectiveness of our proposed method, which outperforms the results of other approaches. Our computational experiments show that minimizing emissions does not necessarily imply minimizing the travel costs, and vice versa. Moreover, it is computationally easier to solve the problem with CO2 emissions than with the traditional objective of minimizing travel costs.", Exact approaches for the green commodity-constrained split delivery vehicle routing problem,"[74184, 23193, 28518, 2168, 30176]",861,"[14, 145, 11]",2508,Combinatorial optimization approaches for freight deliveries,64,3,52,Combinatorial Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,8003 [building - 202],"['Combinatorial Optimization', 'Vehicle Routing', 'Branch and Cut']",MB-52
"Facilitated modelling [FM] is an intervention process by which operational research models are jointly developed with a group tasked with addressing a situation that requires attention and action. The process requires operational researchers to act both as analysts and facilitators, and is designed to help the group make progress with regards to the situation of interest. The aim of this paper is to propose a conceptual framework that can act as a guide for the study of FM processes and outputs, and the design and use of FM interventions. The framework is comprised of three elements - groups as motivated information processors, models as boundary objects, and facilitation as intervention. Each of these elements is discussed with working propositions formulated for core FM processes and outputs. Implications of the framework for the study and practice of FM are discussed, and directions for future research proposed.",Facilitated modelling revisited - A conceptual development,"[8371, 23081]",105,"[10, 55, 151]",2509,Behavioral OR general papers,13,8,11,Behavioural OR,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,12 [building - 116],"['Behavioural OR', 'Group Decision Making and Negotiation', 'Practice of OR']",TB-11
"Within the needed global energy transition, hydrogen presents a potential avenue for achieving deep decarbonization. The expanding role of hydrogen in the global energy landscape is underscored by governmental interest. Recently, new national strategies have been adopted, taking the total number of countries with hydrogen strategies to 41 countries in 2022. To assess the decarbonization potential of a hydrogen economy, one may use an integrated assessment approach, a comprehensive framework that incorporates insights across diverse sectors like energy, land use, and the broader economy, along with their associated greenhouse gas emissions. Integrated assessment models [IAM] are mathematical tools to conduct an integrated assessment. This presentation proposes a survey of 12 distinct family of IAM from 50 different studies, to assess the potential of a hydrogen economy for different scenarios developed within the IPCC Sixth Assessment Report WGIII. This presentation also reports on our ongoing efforts at GERAD to model hydrogen production pathways in our AD-MERGE integrated assessment model.",Modeling of a Hydrogen Economy in Integrated Assessment Models - A Review,"[18748, 77642, 77900, 78985, 77901, 78986]",252,"[37, 0]",2511,Impacts of transitioning to green gases,22,2,14,Energy Markets,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,16 [building - 116],['Energy Policy and Planning'],MA-14
"When looking at preventive approaches for disturbance management, the focus is on designing robust timetables that are able to absorb secondary delays. Following the recovery robustness approach, we use the concept of timetable fragility to analyze the resilience of a nominal timetable, i.e., its ability to recover from primary delays with optimal corrective actions. Given a timetable and suitable objective functions, the fragility defines a special class of scenarios associated with each pair train/section and use them to identify the pairs where a primary delay is more likely to generate large knock-on delays. In particular, the objective function should reflect real-time decision-making criteria adopted by dispatchers. Since no single, universally accepted measure exists, we explore some of the most common objective functions, e.g., the minimization of the max delay, or the minimization of the sum of consecutive train delays at the last station of their routes. Building on the concept of fragility maps, we propose additional analyses, including exploring the impact of precedence constraints in optimal dispatching and sensitivity analysis of the optimal solution. Considering real-life scenarios from a busy railway line in Norway, we discuss several potential uses of the fragility to improve decisions at different levels of the railway planning process, including dispatching, timetable design, and network design.",Analyzing Timetable Fragility,"[68086, 41018, 62586, 37212, 2154]",818,"[122, 129]",2513,Timetabling 2,85,12,51,Public Transport Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,M5 [building - 101],"['Railway Applications', 'Scheduling']",WA-51
"Behavioral Decision Research has provided a deep understanding of how humans form judgments and make choices, since its emergence in the 1950s. Underlying this body of research is the contrast of actual decision making with the paragons of rationality, which provide the idealized model on how humans should decide. Starting with the Ellsberg and Allais paradoxes and followed by Tversky & Kahneman’s seminal research, it has been clear that no real person is fully rational. Exploring deviations from rationality has been a major focus of Behavioral Decision Research. This research has two quite distinct branches - descriptive and prescriptive. Descriptive Behavioral Decision Research examines deviations from rationality and strives to develop theories or models to explain these deviations. Prescriptive Behavioral Decision Research also starts from observed deviations from rationality, but rather than developing theories of models to explain these deviations, it develops and tests tools or analytical methods to correct these deviations. The distinction between the descriptive and prescriptive perspectives in Behavioral Decision Research has often been implicit and blurred in the Decision Sciences literature. In this paper we aim to address this conceptual lacuna, proposing a taxonomy for these two perspectives in Behavioral Decision Research and describing their main achievements. 
",Behavioral Decision Research - Descriptive and Prescriptive Perspectives,"[52314, 5167]",557,"[25, 10, 27]",2514,Behavioral Decision Analysis II,13,4,11,Behavioural OR,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,12 [building - 116],"['Decision Analysis', 'Behavioural OR', 'Decision Theory']",MC-11
"In solving inverse problems with subsampled forward operators, a crucial aspect is choosing an efficient regularizer. The isotropic Total Variation [TV] functional is often used uniformly, to get gradient-sparse solutions, but it tends to remove small details and smooth edges. To address this, we propose a space-variant weighted isotropic TV regularization, whose weights can be determined by a neural network. This talk includes a rigorous theoretical analysis and presents experiments confirming the potential of the proposed approach to enhance the quality of undersampled CT reconstructions.",Space-Variant Total Variation boosted by learning techniques for subsampled imaging problems,[71978],423,"[72, 66, 5]",2516,Optimization and learning for data science and imaging [Part III],84,4,34,Advances in large scale nonlinear optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,43 [building - 303A],"['Mathematical Programming', 'Machine Learning', 'Algorithms']",MC-34
"We consider a nonadditive probability in the weight vector space considered by Stochastic Multicriteria Acceptability Analysis. We show that this allows us to represent uncertainty with respect to the weights to be assigned to the considered criteria as well as the Decision Maker’s [DM’s] optimism or pessimism in evaluating alternatives. After a motivating didactic example, we introduce our methodology, which is based on the definition of a nonadditive probability as a transformation of an additive probability in the weight vector space. To this end, we consider specific families of probability distributions and transformation functions discussing the results they provide, proposing also a methodology to induce them from DM’s preference information. We discuss the results obtained through our methodology in the domain of composite indicators, considering an application in the framework of sustainable development of European Countries.",Stochastic Multicriteria Acceptability Analysis with Nonadditive Probability,"[58022, 36935, 5550]",135,"[26, 25]",2517,Preference Learning 1,44,2,44,Multiple Criteria Decision Analysis,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,20 [building - 324],"['Decision Support Systems', 'Decision Analysis']",MA-44
"In electrical impedance tomography [EIT], the aim is to recover the unknown conductivity of a target by injecting currents and measuring boundary voltages through electrodes. It is a nonlinear and severely ill-posed inverse problem. We introduce a new reconstruction algorithm for EIT, which provides a connection between EIT and traditional X-ray tomography, based on the idea of virtual X-rays. We divide the exponentially ill-posed and nonlinear inverse problem of EIT into separate steps. We start by mathematically calculating so-called virtual X-ray projection data from the measurement data. Then we perform explicit algebraic operations and one-dimensional integration, ending up with a blurry and nonlinearly transformed Radon sinogram. We use neural networks to remove the higher-order scattering terms and perform deconvolution. Finally, we can compute a reconstruction of the conductivity using the inverse Radon transform. We demonstrate the method with experimental data. This is a joint work with Melody Alsaker, Fernando Moura, Juan Pablo Agnelli, Rashmi Murthy, Matti Lassas, Jennifer Mueller, and Samuli Siltanen.",Real data EIT reconstruction using virtual X-rays and deep learning,[77874],423,"[5, 66, 18]",2518,Optimization and learning for data science and imaging [Part III],84,4,34,Advances in large scale nonlinear optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,43 [building - 303A],"['Algorithms', 'Machine Learning', 'Computer Science/Applications']",MC-34
"In this talk, we focus on bi-objective mixed integer linear programming problems [BOMILPs]. Due to their structure, the detection of their Pareto frontier can be hard to compute from an accuracy perspective. 
We propose a new criterion space algorithm able to deal with this difficulty, detecting the exact non dominated set of a BOMILP.
The algorithm alternates the resolution of single objective mixed integer linear problems and bi-objective linear problems. 
During its execution a filtering procedure, based on a tree data-structure called BoT [bi-objective tree], stores in ordered manner all non-dominated points and segments found.
The performance of the algorithm is further improved by the use of suitably defined cuts.
Experimental results on a testbed of instances and a comparison with an existing algorithm will be presented.",On the accurate detection of the Pareto frontier for bi-objective mixed integer linear problems,"[60903, 77878]",51,"[112, 111]",2521,Multiobjective Combinatorial Optimization,34,12,37,Multiobjective Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,33 [building - 306],"['Programming, Multi-Objective', 'Programming, Mixed-Integer']",WA-37
"We present a bi-objective multi-period mixed integer linear programming formulation based on a time-space network for a delivery problem with an automated mothership and a fleet of drones. The proposed formulation integrates not only the routing and the scheduling problems underlying the delivery, but also the charging cycles of drone batteries with the movements of the mothership. The two objective functions are the minimization of the mothership and UAVs energy consumption and the maximization of the number of served customers. Indeed, it is assumed that not all customers can be served within the time horizon under consideration. Experimental results on a testbed of artificial instances will be presented.",A time-space network model for a truck and drones delivery system,"[2042, 60903, 5876, 68754]",217,"[14, 112, 129]",2522,Routing for hybrid fleets of vehicles,64,10,26,Combinatorial Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,012 [building - 208],"['Combinatorial Optimization', 'Programming, Multi-Objective', 'Scheduling']",TD-26
"In today’s business context, employees increasingly experience workload pressure, which impacts operational performance. To alleviate the workload burden and mitigate its potentially negative effects, automation is frequently seen as a solution. However, the mere implementation of automated systems is insufficient, and it is essential to understand employees’ behavior in terms of how they interact with digital automation systems, a topic still understudied in current literature. Our study aims to expose the key role of human-automation interaction in relation to workload and consequential operational performance. For this purpose, we analyze a rich dataset of digital control rooms at a major European rail system operator. This unique and novel setting allows us to examine the impact of controllers' automation intervention willingness on their workload and operational performance. On the one hand, we find that a higher workload increases train delays and that employees who have a tendency to adjust the decision from the automation system experience a significant increase in workload. On the other hand, we find that employees with a tendency to adjust the automated system's decisions exhibit greater resilience to mitigate the negative effects of workload on train delays. These findings underscore the necessity to consider the dual effect of human-automation interaction in relation to workload management and operational performance and the value of studying new settings.",Workload and Operational Performance in Digital Control Rooms - The Dual Role of Human-automation Interaction,"[77872, 62482, 41201, 65425]",567,"[10, 26]",2523,Behaviour and decision support,13,10,07,Behavioural OR,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,1019 [building - 202],"['Behavioural OR', 'Decision Support Systems']",TD-07
"Modern manufacturing keeps on introducing research challenges in terms of both problem structure and instance size. The Hybrid Flexible Flowshop [HFFS] represents a setting where a set of jobs needs to be processed on a set of sequential stages, each containing a different number of parallel [and typically identical] machines; each job will be processed at one machine at a time and it is possible to skip some stages. Additional structure is introduced by i] machine-dependent transportation times between the consecutive stages, ii] limited capacity buffers on the entry and on the exit of either each machine or each stage and, iii] two different types of renewable resources - the first responsible for the transportation of jobs between stages and the second affecting job processing in specific machines. To handle the above on instances of several hundred of jobs, while sustaining the ability to calculate optimality gaps, we examine both exact methods [MILP and CP] and [meta-]heuristic algorithms. Since exact methods commonly struggle with large instances, we examine also whether MILP-based and CP-based formulations can solve parts of the problem or sequentially handle parts of the instance. These components are combined with constructive heuristics and evolutionary schemes to offer a method versatile in the sense of offering multiple solutions for diverse time horizons. The results of the methods are compared in computational terms on both benchmark and real-life instances.",Exact methods and [meta-]heuristic solution methods for the Hybrid Flexible Flowshop problem,"[68992, 77923, 77677, 23864]",834,"[129, 69, 14]",2524,Flow shop scheduling problems,32,13,49,"Lot Sizing, Lot Scheduling and Production Planning","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,M1 [building - 101],"['Scheduling', 'Manufacturing', 'Combinatorial Optimization']",WB-49
"Optimization over the quantum relative entropy [QRE] cone has many applications in quantum information processing, for example, calculating the key rates for quantum key distribution [QKD] protocols. Recently, a new version of our convex optimization software package Domain-Driven Solver [DDS] was released with modified numerical approaches for solving QRE programming problems, potentially combined with many other convex function/set constraints. In this talk, we propose a preprocessing and two-phase approach for QRE programming to reduce the size and improve the conditioning of a given instance. Phase-I of this two-phase approach is a well-behaved convex optimization problem, such as minimizing a self-concordant barrier or an SDP. The reformulated QRE programming problem can be solved more efficiently and faster by DDS or other optimization algorithms. We finish the talk by presenting numerical results of using DDS for solving many classes of problems, including calculating the QKD rate for different protocols. ",Efficient Quantum Relative Entropy Programming with SDP Preprocessing,"[77875, 48461]",270,"[21, 60, 134]",2525,"Semidefinite Programming and implementations, Quantum Information Theory and other applications",68,10,38,"Conic Optimization - Theory, Algorithms, and Applications","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,34 [building - 306],"['Convex Optimization', 'Interior Point Methods', 'Software']",TD-38
"Sales and operations planning [S&OP] problems are rarely addressed with stochastic methodologies since available real-world data often limits applicability. We address this issue in a case study with a global electronics manufacturer and develop a hybrid approach to an S&OP problem under demand and supply uncertainty for multiple products. Decisions cover the full range of S&OP, i.e. raw materials, production, transport to distribution centers, inventories, and demand fulfillment. To tackle demand uncertainty, we use scenario-based two-stage stochastic programming. Scenarios are created with readily available data such as point forecasts and correlated forecast errors. To overcome data limitations, we employ shrinkage covariance estimation. Furthermore, we address supply lead time uncertainty with parametric cost function approximation for a central raw material’s stock level. We thus capture the extent to which raw material safety stock should be altered beyond the level implied by our demand-based stochastic program. We chose this approach instead of extending our scenarios since companies tend to have no regular lead time review process, hardly store related data in a structured way, and do not forecast lead time. Another reason is the higher computational burden if we extend scenarios by supply uncertainty. While conducting out-of-sample rolling horizon evaluation, we optimize the parametrization online and compare our hybrid approach to its deterministic counterparts.",A hybrid approach for sales and operations planning under demand and supply uncertainty,"[77877, 909]",807,"[135, 117, 138]",2526,Stochastic lot-sizing problems,32,7,49,"Lot Sizing, Lot Scheduling and Production Planning","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,M1 [building - 101],"['Stochastic Models', 'Programming, Stochastic', 'Supply Chain Management']",TA-49
"It is estimated that one in six adults in England have a mental health disorder and would benefit from a course of psychological therapy. The NHS Talking Therapies Programme treats over a million patients with common mental health problems each year according to the principle of stepped care, where effective but less resource intensive treatments are delivered to patients first and higher intensity interventions are then provided if required. 

In order to meet NHS targets, the programme will need to increase access rates, while meeting service standards for waiting times and recovery rates. Mental healthcare providers are therefore looking at all opportunities to increase capacity and productivity. The aim of this project is to develop innovative, advanced, analytical tools to help improve understanding and management of NHS Talking Therapies services’ demand and capacity. 

This study analyses data from iaptus, the leading digital care record for psychological therapy services, to investigate and model patient flows through talking therapy care pathways using process mining and other data-driven methods, augmented by simulation modelling to evaluate pathway performance and explore relationships between performance issues. The major impacts of this project are expected to be improved access to services, improved utilisation of resources, resulting in reduced waiting times, better recovery rates and reduced patient drop out. ",Data driven exploration and modelling of patient flows through NHS Talking Therapies care pathways,"[72771, 72772, 45846]",608,"[7, 56]",2529,Healthcare Analytics,3,4,15,OR in Health Services [ORAHS],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,18 [building - 116],"['Analytics and Data Science', 'Health Care']",MC-15
"For perishable products, it is common industry practice to resort to substitution in case of stock-outs. In this work, we investigate the stochastic capacitated lot-sizing problem for perishable products under dynamic demand substitution. We consider a single-echelon lot-sizing setting with multiple products that require the same limited production capacity. We assume that products have fixed shelf-lives and constant production lead times.  To represent real-life conditions, we allow for correlated product demands that evolve dynamically over time. We formulate this problem as a multi-stage stochastic program and propose a scenario tree approximation. We solve the resulting problem in a rolling horizon procedure. Our preliminary numerical results demonstrate the value of our proposed formulation for production and inventory planning of perishables and the value of substitution.",Multi-stage Stochastic Programming for Lot Sizing with Perishables under Demand Substitution,"[68197, 909]",807,"[138, 117]",2531,Stochastic lot-sizing problems,32,7,49,"Lot Sizing, Lot Scheduling and Production Planning","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,M1 [building - 101],"['Supply Chain Management', 'Programming, Stochastic']",TA-49
"This work studies a class of adaptive methods for structured convex optimization that use large stepsizes on par with linesearch methods without employing any backtracking procedure. We show that the convergence of such methods extends beyond Lipschitzian setting and encompasses the case of local Hölder gradient continuity. Unlike conventional approaches that often resort to epsilon-oracles or linesearch procedures to address the absence of local Lipschitz continuity, we leverage plain Hölder inequalities without approximation or linesearch. 
Our analysis establishes exact convergence results without the need for prior knowledge of local Hölder constants or the order of Hölder continuity.",Adaptive linesearch-free proximal gradient methods under Hölder gradient continuity,"[50785, 77883, 74806, 50060, 50245]",507,"[19, 81, 21]",2532,Convex optimization algorithms,70,12,41,Nonsmooth Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,97 [building - 306],"['Continuous Optimization', 'Non-smooth Optimization', 'Convex Optimization']",WA-41
"To enhance buildings' energy independence and carbon neutrality, technologies like energy storage and local renewables are increasingly deployed. Among these, long-term geothermal storage for residential buildings is emerging and poses new challenges in multi-timescale optimization.
We investigate the operation of a novel residential smart grid, based on a real project. It integrates electricity and heat systems via a heat pump and includes short-term electricity storage, long-term thermal storage, photovoltaics, and solar thermal assets. While the operation is optimized daily, seasonal considerations are crucial, especially to ensure that the thermal storage fills during summer. The question then is how to account for long-term effects in the daily optimization.
Our study relates to two key domains of operations research - inventory management and resource allocation. Utilizing a year-long real dataset, we explore different methods to guide the energy storage levels by day's end. These methods range from historical data-based thresholds to rolling-horizon strategies, assessing the impact of optimization window length. We also evaluate the relaxation of end-of-horizon constraints using penalties.
We analyze the strengths and weaknesses of each method, acknowledging the challenge of unreliable long-term forecasts. These insights facilitate decision-making for residential energy systems and provide valuable implications for capacitated warehouse problems in other contexts.",Optimal operation of a residential smart grid with short-term and long-term storage,"[73165, 78463, 13391, 80052, 80053, 80054]",453,"[38, 93, 111]",2536,Optimization of energy storage systems,21,7,22,Energy Management,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,81 [building - 116],"['Engineering Optimization', 'OR in Energy', 'Programming, Mixed-Integer']",TA-22
"The demand for sustainable urban mobility has surged with the increasing focus on climate protection. Consequently, ride-sharing services powered by electric vehicles [EVs] are gaining unprecedented attention. While EVs increase environmental benefits, they also present challenges due to the scarcity of charging infrastructure. Moreover, standard dial-a-ride settings, with their broad time windows and lengthy ride times, fall short of the service quality needed to persuade the broader public to switch from private cars.

Our study tackles a special Dial-a-Ride Problem [DARP], deploying EVs to meet dynamically incoming customer requests within a framework of limited charging stations and non-linear charging functions. To enhance appeal and utility, customers can opt for higher-priced premium options, such as non-shared rides, reduced ride time, or reduced delay at drop-off. The service provider must immediately decide whether to fulfill these premium requests or decline them and execute the customer request with standard service.

We conduct a comparative analysis of various charging strategies, evaluating their efficacy and relative impact on operational quality. Moreover, we tailor routing and scheduling strategies to accommodate the mixed standard and premium request nature. In this way, environmental considerations of EV usage are combined with enhanced service quality through premium options, resulting in a pragmatic approach to upgrading urban mobility.
",A Dynamic Electric Dial-a-Ride Problem with Optional Service Upgrades,"[69555, 9524]",181,"[145, 143]",2539,Ridehailing & Ridepooling,85,4,54,Public Transport Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S01 [building - 101],"['Vehicle Routing', 'Transportation']",MC-54
"Escalating human displacement resulting from conflict, natural disasters, and climate change, termed complex crises, represents a major global challenge today. Refugees, the second-largest displaced group, situated in locations experiencing protracted crises with declining living conditions, represent a major hurdle for humanitarian efforts. There exists low interest in research on Humanitarian Operations on the overlap of Humanitarian Logistics and Political Science, essential for addressing issues in conflict-induced displacement interventions. This area lacks exploration of viable operations management strategies in conflict zones, beyond access and security, for durable solutions realization. Decisions regarding refugees are subject to the political discretion of host countries, influenced by foreign policy negotiations and minimum standards set by the United Nations High Commissioner for Refugees for basic survival needs. This often neglects long-term welfare needs of food security and livelihood recovery. We aim to create a framework using System Dynamics to analyze trade-offs between temporary and permanent solutions in crises causing displacement, incorporating protractedness and conflict sensitivity. The result expected is a codification of realities and insights gained from multiple case analysis, and their inclusion into a versatile operational decision framework for various displacement contexts and future integration with traditional Operations Research models. ",Rethinking Durable Solutions - A System Dynamics Approach for Enhanced Decision-Making in Displacement Contexts,"[76824, 35962]",657,"[15, 140, 25]",2540,Simulation and Modelling for Decision Support,45,14,45,Decision Support Systems,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,30 [building - 324],"['Complex Societal Problems', 'System Dynamics and Theory', 'Decision Analysis']",WC-45
"The green transition has accelerated the expansion of variable renewable energy generation, most prominently wind generation. This significant penetration of wind power amplifies the need for reserve capacity and adjusting commitments close to dispatch. While these needs are recognised, whether current market prices incentivise generation companies to participate in reserve and intraday markets is unclear.

This work investigates whether price signals provide adequate incentives in these markets and how multi-market participation affects trading on the primary market for electricity, the day-ahead market, from a single-agent perspective. To that end, we propose a profit-maximising offer-strategy optimisation model for a price-taker in the Nordic day-ahead market and a reserve market. Uncertainties in market outcomes, intraday trading opportunities, renewable generation, and imbalance settlement are accounted for.

Incentives for and the effects of multi-market participation were assessed in a case study. Our results indicate that current price signals incentivise multi-market participation, which is seen from an increase in expected profits. We also observed that anticipating reserve and intraday market opportunities had an impact on day-ahead trading. Furthermore, a bias according to expectations regarding the system imbalance was identified. In fact, under certain circumstances, day-ahead offer curves in multi-market offer strategies resembled the use of market power.",The Effects of Multi-Market Participation on Day-Ahead Trading of an Electricity Producer,"[68862, 66980, 57798]",644,"[36, 136, 93]",2542,Electricity Market Design,22,3,09,Energy Markets,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,10 [building - 116],"['Electricity Markets', 'Stochastic Optimization', 'OR in Energy']",MB-09
"Exploring scenario design as an anticipation of future actions, the purpose of this research is to examine the issues raised by the incorporation of stakeholders’ behaviours in the process as well as in the content of scenario and foresight practices. Scenario literature may either refer to the engagement of multiple stakeholders in the scenario process [Bryson et al., 2016, Crawford, 2019, Mukherjeea et al., 2020] or to the incorporation of stakeholder analysis in scenario design [Bradfield et al., 2015, Marchais-Roubelat and Roubelat, 2016]. 
We discuss the lessons from an action-research on the engagement of industries in climate change adaptation [Roubelat and Marchais-Roubelat, 2022]. The scenario design process implied the making of groups of stakeholders with members of various industries to explore their futures from four perspectives - crises, value chains, networks, temporalities. The content of scenarios was designed from prospective stakeholders’ acts engaging industries in climate change adaptation, and to be ruled and challenged over time together with questioning stakeholders’ capacities to act.
Results stress on the process perspective the interest for a foresight collaborative process between different industries rather than focusing on a specific one to share experiences. On the content one, this participatory work beyond business silos produced community-based engagement scenarios empowering disempowered or supporting role stakeholders.",Incorporating stakeholders’ behaviours in foresight practices. Insights from corporate engagement in climate change adaptation action-based scenarios,"[77886, 77894]",569,"[10, 0]",2543,Scenarios and foresight practices - Behavioural issues II,13,13,11,Behavioural OR,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,12 [building - 116],['Behavioural OR'],WB-11
"A mathematical model for the analysis of an Emergency Medical Service [EMS] system with a specific number of advanced life support ambulances [ALS] and a specific number of basic life support ambulances [BLS] is presented in this work. The system admits incoming emergency calls which are divided into two classes - [i] urgent, high-priority calls for which the patient’s life is potentially at risk and [ii] less urgent, low-priority calls. Advanced life support units can serve both calls, while basic life support units handle only low-priority calls. The system either serves the incoming calls or forwards them to other systems if there are not enough ambulances for serving. Under a suitable cost structure, the system is modeled using an appropriate Markov decision process in continuous time for which we seek a stationary policy that minimizes a predefined optimality criterion for vehicle mixes over a set of candidate ambulance fleets. Based on this formulation, it is possible to implement standard Markov decision algorithms, such as the standard value-iteration algorithm and the standard policy- iteration algorithm.  ",Optimal decision control for a mixed fleet emergency medical services system with dispatch protocols for ALS ambulances using semi-Markov decision processes.,"[30714, 77955, 50993, 49511]",593,"[56, 135, 130]",2545,EMS logistics,3,2,10,OR in Health Services [ORAHS],"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,11 [building - 116],"['Health Care', 'Stochastic Models', 'Service Operations']",MA-10
"We miss Jakob for his entertaining speeches, humour, friendship, warmth and enthusiasm, for 50 years.  
Jakob was a 'people person'.  He was larger than life.  He was humble.  He worked quietly in the background to ensure that great people such as Jean-Pierre Brans got the recognition they deserved.  And he was great himself.
His Christmas cards, birthday greetings, photographs of his family and home were always something to look forward to.  He had a big heart.  
We had looked forward to celebrate him as our popular king in his home of Denmark in the EURO conference in 2024.  Sadly he is gone.  Now we can only say - “The King is dead. Long live the memory of Jakob Krarup”.  Jakob, you will never be forgotten. Thank you for your great engagement for Operations Research, for EURO, and for being such a wonderful happiness-spreading person.","Long live the memory of Jakob Krarup, you will never be forgotten",[5121],983,"[132, 0]",2548,Memorial Session for Jakob Krarup,66,5,65,Memorial Session for Jakob Krarup,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,R021 [building - 358],['Social Networks'],MD-65
"This work investigates efficient solution to two fundamental problems in topology optimization of frame structures. The first one involves minimizing structural compliance under linear-elastic equilibrium and weight constraint, while the second one minimizes the weight under compliance constraints. These problems are non-convex and generally challenging to solve globally, with the non-convexity concentrated in a polynomial matrix inequality. Recently, these problems where tackled using the moment-sum-of-squares [mSOS] hierarchy. However, only  smaller instances can be solved globally. Here, we aim to improve the scalability of solution to these problems by using the mSOS hierarchy supplemented with the Term Sparsity Pattern technique [TSP]. Due to the unique polynomial structure of our problems in which the objective and constraint functions are separable polynomials, we further improve scalability by adopting a reduced monomial basis containing non-mixed terms only. From extensive numerical testing, we conclude that these techniques allow for a global solution to larger instances and accelerate the solution of the problems significantly.",Term-sparse polynomial optimization for the design of frame structures,[77887],903,"[52, 63, 38]",2549,Advances in polynomial optimization and its applications,68,15,38,"Conic Optimization - Theory, Algorithms, and Applications","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,34 [building - 306],"['Global Optimization', 'Large Scale Optimization', 'Engineering Optimization']",WD-38
"Within the realm of football analytics, an objective is to enhance the predictive capabilities and exploit the emerging tool known as the expected goal [xG] model for pratical needs. The xG model aims to classify each shot, predict the final score of a game and evaluate strikers and goalkeepers performance. In previous works we proposed to match event data and composite indicators of player performance derived through Partial Least Squares - Structural Equation Model [PLS-SEM]. 
The aim of this work is twofold - leveraging a dataset comprising tracking data from the Italian “Serie A” related to 2022/2023 season, we introduce to the existing model some original features, such as the Kos angle. Our results showcase intriguing findings in comparison to a benchmark [Understat], particularly in certain metric indices. Furthermore, the significance of performance composite indicators from PLS-SEM and specific tracking variables for the xG model is established.
Afterwards, we exploit the performance of the model proposing a combination of machine learning and game theoretical approaches to assess the marginal contribution of single players in determining the expected goal and the final game score. In particular, to achieve this latter goal, we adopt our xG as the cohesion function of a [generalized] Shapley value. By doing so, an instrument to jointly estimate the probability to score the goal and evaluate the role of each player in determining such probability is proposed.
",Some novelty on the xG model for Football Analytics,"[77889, 77898]",665,"[99, 66]",2550,Football analytics,37,9,16,OR in Sports,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,19 [building - 116],"['OR in Sports', 'Machine Learning']",TC-16
"Synthetic populations are datasets which are created to be statistically representative of a chosen population using census data. They have been used in multiple contexts where researchers are interested in modelling effects on a population or individual level, including health related agent-based models and epidemiology. The SynthEco project aims to provide a platform for researchers to create synthetic populations in the form of an open source Python package available on GitHub through an iterative proportional fitting approach. The package allows for synthetic population creation from any census data and on different geographic levels through a simple plug-in system. Plugins exists for census data from the USA and from Canada, and researchers are invited to contribute their own plugins to create other populations. The use of SynthEco will be demonstrated through several application cases in Montreal, Canada, such as modelling access to healthy food and spatial inequality in financial wellbeing. These are made possible through dataset linkage of the synthetic population with geo-referenced discovery and population cohorts.",SynthEco - A digital system for analyzing multi-dimensional mechanisms of human behaviour in a multi-layered and dynamic geospatial environment ,"[77890, 77891, 77892, 72979, 77896, 77897]",408,"[7, 26, 134]",2551,"Advancements of OR-analytics in statistics, machine learning and data science 4",16,5,28,"Advancements of OR-analytics in statistics, machine learning and data science","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,065 [building - 208],"['Analytics and Data Science', 'Decision Support Systems', 'Software']",MD-28
"Achieving sustainable transformations, in areas such as climate change, mobility, agriculture and food, health and ageing, has become a imperative goal of societies worldwide. Participatory OR methods are often used to support group decision-making in the context of sustainable transformations. It brings multiple stakeholders with different incentives and aims together and potentially results in a shared vision, perspective or decision. However, evidence about the impact of participatory OR methods on sustainability transformations is scarce. 
Evaluation of participatory OR methods happens in various ways and is generally performed in an ad-hoc manner. Moreover, evaluation takes place at various levels including the behavior of the individual, dynamics of the group, consequences for the organization, or impact on the larger system in which the transformation takes place. 
In this talk, I present a number of concepts which can be used to evaluate the process of decision-making in reaching sustainable transformations. Hereby, I distinguish between the different levels [individual, group, process, system] at which assessment can take place. In particular, I focus on the question - How do these concepts assess the “transformative potential” of OR-supported decision-making processes? To illustrate this I use case studies from different domains where participatory OR methods are used to support sustainable transformations. 
",Evaluating the impact of supporting group decision-making with participatory OR methods,[75744],568,"[10, 55, 15]",2553,Behavior in group decision-making ,13,9,11,Behavioural OR,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,12 [building - 116],"['Behavioural OR', 'Group Decision Making and Negotiation', 'Complex Societal Problems']",TC-11
"Maintaining a complex transportation network at global scale involves a large number of planning processes, each requiring visibility on how expected demand from customer orders will flow through local regions in the transportation network. For example, planning labour to process warehouses requires visibility on packages arriving at each warehouse. While traditional forecasting approaches rely on statistical or machine learning models independently optimised to forecast for a single granularity and horizon, they can require significant historical data for the granularity in question and cannot adapt well to new time series, inhibiting the expansion of the network to new countries or demand streams. In this work we explore a new approach to time series forecasting, where a single deep neural network trained on large collections of time series datasets is used to provide robust forecasts across many granularities and horizons. Inspired by Large Language Models and the ability of Transformer architectures to model long range dependencies in textual data; we hypothesise that adapting the Transformer architecture to numerical data and training them on large time series datasets can allow a single model to reliably forecast across multiple granularities and horizons and even outperform the traditional multiple-model approach, particularly in cases where little historical data is available.",Foundation Models for Time Series Forecasting,"[77895, 80018, 80019, 78666, 80020]",434,"[143, 3, 66]",2555,Forecasting for the middle mile,92,14,57,Optimization at Amazon,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S06 [building - 101],"['Transportation', 'Agent Systems', 'Machine Learning']",WC-57
"Python and its vast ecosystem are great for data pre-processing, solution analysis, and visualization, but Python’s design as a general-purpose programming language makes it less than ideal for expressing the complex optimization problems typical of prescriptive analytics. AMPL is a declarative language that is designed for describing optimization problems and that integrates naturally with Python.

This presentation shows how the combination of AMPL modeling with Python environments and tools has made optimization software more natural to use, faster to run, and easier to integrate with enterprise systems. We demonstrate how AMPL and Python work together in a range of contexts:

- Installing AMPL and solvers as Python packages anywhere
- Fast data transfer from/to Python data structures such as Pandas and Polars dataframes
- Deploying models to the cloud quickly and easily

We also present our new open-source library for C/C++ and Python that makes it easy to connect modeling systems to AMPL-enabled solvers. In addition to our solver drivers, which have been open-source for many years, we are now offering open-source access to all of the deployment tools that we offer. Currently only AMPL takes full advantage of the functionalities that we provide, but the ability to solve problems using the full features of AMPL-enabled solvers is open to other systems as well.","AMPL - Advances in Python Integration, Cloud Deployment, and Open-Source Tools",[57228],240,"[76, 63, 72]",2557,Modeling Languages,76,8,30,Software for Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,53 [building - 208],"['Modeling Systems and Languages', 'Large Scale Optimization', 'Mathematical Programming']",TB-30
"Imaging diagnostic companies primarily rely on specialized medical workforce to deliver their services. However, there has been a stagnation in the number of specialist physicians over the years, making the workforce scarcer and more expensive. Another factor negatively impacting imaging diagnostic companies is health insurance providers' current financial status due to the sector's costs. Given this scenario, optimizing the allocation of medical resources in shifts is mandatory to achieve optimal outcomes, maximizing revenue and reducing costs associated with idleness. Idleness occurs when the demand is low for one examination unit, and physicians are over-allocated. In contrast, the demand for examinations in another unit can be high, but there is a need for more physicians. An integer linear optimization model was developed to allocate physicians in shifts and medical units, allowing physicians to be allocated in nearby units with available rooms. Additionally, two objective functions were analyzed - minimizing the number of unmet examinations, maximizing the lowest coverage considering units and shifts. Instances from an imaging diagnostic company were used for computational tests. When comparing the results obtained with data from the company, the model demonstrated a significant reduction in unmet demand and superior overall coverage, proving to be a practical approach to optimizing the allocation of medical resources.
",Optimizing physician allocation in an imaging diagnostic company,"[23969, 77909, 64435, 77911]",607,"[109, 84, 56]",2559,Staffing and workforce planning and scheduling,3,9,15,OR in Health Services [ORAHS],"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,18 [building - 116],"['Programming, Integer', 'Optimization Modeling', 'Health Care']",TC-15
"To be effective, heart attack treatment procedures must start as soon as possible after the onset of symptoms. Therefore, heart attack treatment centers should be strategically located to maximize timely accessibility for potential patients. To achieve this goal, health care decision makers need to have accurate predictions of the number of heart attack patients in high spatial resolution. We develop interpretable, as opposed to black box, models for predicting the number of heart attack patients in granular geographical areas. Our models are interpretable as they satisfy some meaningful constraints - older people have a higher probability of experiencing heart attack, predictions are non-negative, and that more populous geographical areas have a higher expected number of heart attack incidents. Our predictions are granular enough so that decision makers can use them to obtain accurate estimates of demand within any desired travel time radius from care providers. The proposed models are extensions of the Poisson and linear regression models. We use 10 years worth of empirical heart attack data from Alberta, Canada, and show that our interpretable models outperform black box models.",Interpretable prediction of heart attack incidence using demographic data,"[18338, 77984, 671, 77985]",530,"[47, 56, 66]",2560,"Advancements of OR-analytics in statistics, machine learning and data science 11",16,15,28,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,065 [building - 208],"['Forecasting', 'Health Care', 'Machine Learning']",WD-28
"In this work, a stochastic cost frontier approach is presented to compare the efficiency of Chilean ports and Mexicans ports. This approach is based on the work of several authors,the authors who presented an activity-based operational method for estimate CO2 emissions from container shipping considering repositioning of empty containers.Among other findings they demonstrate that the repositioning Efficient disposal of empty containers can reduce emissions in maritime transport. Based on this background, the following subsection presents an econometric estimation model of CO2 emissions from Chilean and Mexicans ports based on in empty containers and the activity of export and import ships.The theoretical framework that supports the work corresponds to the theory of the stochastic frontier of costs introduced by Farrel. The most used frontier models are production and cost models. In this approach, cost frontier models will be applied. The linearized functional form of the Cobb-Douglas production function is the origin of the development of the cost frontier stochastic, which will be used in this work to characterize the technical efficiency of each of the ports. A cost frontier model with panel data will be used. We will use the standard form of the Cobbs-Douglas type cost frontier due to its flexibility to be   estimated with a linear function with panel data. 

",Comparing the technical efficiency of Chilean and Mexicans ports using a stochastic frontier approach,"[78615, 25269, 78612, 79432]",154,"[135, 35, 100]",2561,Port Performance,52,15,62,OR in Port Operations,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S12 [building - 101],"['Stochastic Models', 'Efficiency Analysis', 'OR in Sustainability']",WD-62
"The Job Shop Scheduling Problem [JSSP] has been extensively studied in operations research for decades, resulting in the development of various solution methods. Recently, deep reinforcement learning [DRL] has emerged as a promising approach to automatically learn generalized construction heuristics from simulations. Construction heuristics iteratively generate solution sequences in which operations are integrated into a schedule. The neural networks of DRL-agents predict the probabilities per operation, that integrating it next in sequence will lead to the shortest schedule. Often multiple solutions are sampled stochastically from the predictions of trained agents. However, due to the symmetry of the JSSP, many of these sequences result in the same makespan or even the same schedule. This motivates the use of more sophisticated search strategies that cover a wider range of solutions and utilize trained agents effectively. 
This study compares theoretical and practical aspects of integrating learned priors into depth-wise search strategies, such as stochastic sampling and Monte-Carlo tree search, aiming to find the shortest makespan in limited computational time. While predictions for sampling are most efficiently parallelized, other methods effectively prune the search tree to require fewer predictions in total. Our results with state-of-the-art DRL agents indicate that variations of stochastic sampling perform best, considering realistic time and hardware constraints.
",Harnessing the Power Trained Reinforcement Learning Agents in Job Shop Scheduling Problems,"[77235, 77910, 77908]",320,"[8, 129, 42]",2562,Machine Learning in Applied Optimization,14,8,03,Data Science Meets Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,1005 [building - 202],"['Artificial Intelligence', 'Scheduling', 'Expert Systems and Neural Networks']",TB-03
"In recent discussions on reasoning about data using the rough set concept, a class of many-valued logics has emerged to differentiate between vagueness resulting from imprecision and ambiguity arising from coarseness. These logics, including the renowned Belnap four-valued logic, stem from a comprehensive seven-valued logic framework. This study explores the application of the seven-valued logic and its derivatives in multicriteria decision aiding [MCDA]. Specifically, we demonstrate how our approach effectively addresses common challenges in MCDA, such as uncertainty, imprecision, ill-determination, and related robustness concerns. ",A Seven-Valued Logic for Multicriteria Decision Aiding,"[1473, 5550]",135,"[25, 26, 27]",2563,Preference Learning 1,44,2,44,Multiple Criteria Decision Analysis,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,20 [building - 324],"['Decision Analysis', 'Decision Support Systems', 'Decision Theory']",MA-44
"The growing number of Shared Mobility-On-Demand services has the potential to contribute to a more socially and environmentally sustainable mobility provision. However, this potential may not be fully exploited due to possible conflicts with the objectives of the service providers. Thus, political discourses address regulatory instruments to influence providers’ operational planning. This paper analyzes the effects of two currently debated instruments, i.e., the introduction of a minimum pooling rate and a minimum spatial acceptance rate. This analysis is based on mathematical optimization models that we formulate as generalizations of the selective dial-a-ride problem. More precisely, the problem is first captured by a single-period model formulation and then generalized to a multi-period horizon to implement different regulatory strategies. In a comprehensive computational study, we solve the regulated model variants exactly both for artificial and real-world instances provided by our industry partner FLEXIBUS. We evaluate different levels of regulation for both instruments regarding their feasibility and their impact on the Shared Mobility-on-Demand system, and thereby identify their contribution to a conflicting multi-objective environment.",The Potential of Governmental Regulation on Shared Mobility-on-Demand Systems,"[75577, 72928, 14031, 16305]",587,"[119, 143]",2565,Demand-responsive public transport 2,85,14,54,Public Transport Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S01 [building - 101],"['Public Local Transportation Systems', 'Transportation']",WC-54
"The Vehicle Routing Problem with Delivery Options [VRPDO] extends the classical Vehicle Routing Problem [VRP] by incorporating multiple alternative delivery locations per customer with potentially distinct time windows, ordered according to customer preferences. These alternative destinations, such as homes, workplaces, or lockers, introduce additional complexities, including varying capacity and time window constraints at shared delivery points. Ensuring customer satisfaction involves adhering to minimal service level requirements, reflecting preferences for alternative delivery locations. The VRPDO calls for determining optimal routes and delivery locations while minimizing total routing costs and satisfying all time window, capacity and service level constraints. We propose a minimum insertion heuristic for generating initial solutions, followed by a local search framework to refine these solutions. The framework incorporates traditional routing operators alongside a novel service level-related operator capable of adjusting delivery locations to satisfy customer preferences. Initial findings demonstrate the effectiveness of our approach in achieving high solution quality while maintaining computational efficiency.",A Local Search metaheuristic for the last-mile Vehicle Routing Problem with Delivery Options,"[62378, 77920, 78477, 35735]",623,"[145, 65, 74]",2566,Vehicle routing I,6,2,60,Transportation,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S09 [building - 101],"['Vehicle Routing', 'Logistics', 'Metaheuristics']",MA-60
"The expansion of renewable energy sources is destined to increase in the future, under the impulse of the 2015 Paris Agreements, which aims to mitigate climate change by reducing greenhouse gas emissions. In particular, hybrid production plants allow the exploitation of different climatic sources to generate electricity. We aim to analyze the electricity generation of a mixed wind-photovoltaic system, considering a model that involves the required climatic variables. We also include the spot price of electricity in the model to evaluate the system's profitability through the expected income. Regarding the practical application, we considered a hypothetical wind-photovoltaic plant located in Italy with the relevant climate series. We have set specific technical hypotheses on the plant to determine the quantity of energy produced by knowing the climatic variables. Regarding the price of electricity, we considered the price of the Italian market. The time series used were modeled through machine learning techniques, compared with more traditional statistical and econometric models. On the practical side, the dataset is divided into estimation windows and test windows, allowing the simulated values of electricity production and expected income to be compared with the empirical values. Finally, we apply a backtesting technique to assess the model. Having jointly modeled the climate variables and the spot price of electricity constitutes one of the innovative aspects of this work.",Forecasting wind-photovoltaic energy production and income with traditional and ML techniques,[49102],605,"[36, 47]",2568,Renewable Energy Challenges,21,12,22,Energy Management,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,81 [building - 116],"['Electricity Markets', 'Forecasting']",WA-22
"Within many multi-attribute decision-making [MADM] methods, inter-criterial preferences of the decision-makers [DMs] are expressed via criteria weights. Due to its convenience and simplicity, the revised method of Simos is widely applied to elicit criteria weights interactively with the DMs. Especially in group decision-making situations it provides procedural ease and offers a means to engage the dialogue with and in between DMs.
In Simos’ method, each DM orders a set of criteria cards according to their relative importance to derive criteria weights. However, this can lead to ambiguous preference mappings and thus ambiguous criteria weight vectors. Therefore, the robust extension of Simos' method determines all possible weight vectors from an ordered set of criteria cards via linear programming, where the set of feasible solutions forms a hyper-polyhedron. Although the concerns about robustness are expressed in the scientific literature on Simos' method, there is currently no methodological approach to process this information for consensus-reaching in decision problems with multiple DMs. 
We present an optimization model that determines a consensual alternative by utilizing the ambiguity of Simos’ method. The model is based on the preference ranking organization method for enrichment evaluation [PROMETHEE] and demonstrated at hand of a case study for the assessment of seawater desalination plant concepts in Jordan.
",Resolving Ambiguous Preference Information in Group Decision-Making - A Novel Optimization Model for Consensus-Reaching,"[24622, 64524, 63613, 73903, 69695]",887,"[94, 25]",2569,Robustness analysis in MCDA 2,44,7,44,Multiple Criteria Decision Analysis,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,20 [building - 324],"['OR in Environment and Climate change', 'Decision Analysis']",TA-44
"We consider a ratio-based efficiency analysis in view of achieving performance targets by Decision Making Units. The targets refer to the efficiency scores or ranks and the truth of pairwise preference relations. These outcomes can be instantiated for at least one or all feasible weights associated with inputs and outputs. We discuss Multiple Objective Optimization problems that minimize the required reductions of consumed resources or increases of produced results needed to reach the pre-defined aims. The proposed models offer great flexibility to the Decision Maker, who can indicate which factors should be modified and to which extent. The evolutionary optimization algorithm computes a set of solutions that exhibit the trade-offs between expected modifications of various factors. They can serve as the basis for interactively selecting the most preferred solution to be implemented in practice. The use of the proposed framework is illustrated in the problem of analyzing the efficiency of Polish airports.", Attaining Robust Performance Targets in Data Envelopment Analysis with Application to Efficiency Evaluation of Airports,"[71943, 19484]",388,"[24, 35, 77]",2570,Robustness analysis in MCDA  1,44,5,44,Multiple Criteria Decision Analysis,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,20 [building - 324],"['Data Envelopment Analysis', 'Efficiency Analysis', 'Multi-Objective Decision Making']",MD-44
"Sustainable travel modes are considered as one of the main directions for moving towards more sustainable living in urban areas. The technology advancements of electric vehicles [EVs] have been receiving growing attention on global market. Still, the support of public authorities in pushing towards sustainable mobility paradigms and designing solutions for the deployment of charging infrastructures is needed. 
There are several papers in the literature that have dealt with the estimation of EVs charging demand, both on macro and micro level, the placement of EVs charging facilities and management of real-time operations of EVs. In this context, we propose a novel optimization model for the efficient placement of EVs charging stations by considering the influence of existing points of interest [such as restaurants, supermarkets, touristic places, schools, hospitals, etc.] and service radius of potential EVs facilities. Generally, the coverage of a charging point, expressed in terms of average time or distance to reach a charging station, is correlated with the willingness-to-use of EVs drivers. Also, the presence of a particular category of points of interest, within the predefined service radius, shows the attractiveness of charging location. The proposed model has been applied to a real network of the Lombardy region. Preliminary results are encouraging and push towards further developments.
",Optimal placement of electric vehicle charging stations in urban areas ,"[70608, 77915, 19432, 78738, 78740]",332,"[111, 0]",2571,MOST - MaaS & Innovative Services for Sustainable Mobility,6,12,55,Transportation,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S02 [building - 101],"['Programming, Mixed-Integer']",WA-55
"Systems with components operating under load conditions are vulnerable to failure, becoming more difficult the maintenance decision-making. Such fragility increases under structural dependency, which is commonly used for configuring systems as a cost-effective technique that optimize the reliability and availability. 
In this study, we develop a stochastic distribution to model lifetimes of systems with structural dependence under preventive maintenance policies. Further, a real-world case study is provided to illustrate the effectiveness of the proposed family.
",Modelling lifetimes of systems operating under load conditions,"[58953, 58995]",797,"[123, 135]",2572,Reliability Models,50,13,39,Stochastic Modelling,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,35 [building - 306],"['Reliability', 'Stochastic Models']",WB-39
"Lagrangian relaxation is a common and often successful way to approach computationally challenging single-objective discrete optimization problems with complicating side constraints. Its aim is often two-fold; first, it provides bounds for the optimal value, and, second, it can be used to heuristically find near-optimal feasible solutions, the quality of which can be assessed by the bounds. We consider bi-objective discrete optimization problems with complicating side constraints and extend this Lagrangian bounding and heuristic principle to such problems. The Lagrangian heuristic here produces non-dominated candidates for points on the Pareto frontier, while the bounding forms a polyhedral outer approximation of the Pareto frontier, which can be used to assess the quality of the candidate points. As an illustration example we consider a facility location problem in which both CO2 emission and cost should be minimized. The computational results are very encouraging, both with respect to bounding and the heuristically found non-dominated solutions. In particular, the Lagrangian bounding is much stronger than the outer approximation given by the Pareto frontier of the problem’s linear programming relaxation.",A Lagrangian Bounding and Heuristic Principle for Bi-Objective Discrete Optimization,"[25562, 67141, 77917]",117,"[77, 14]",2573,Discrete Multiobjective Optimization,34,13,37,Multiobjective Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,33 [building - 306],"['Multi-Objective Decision Making', 'Combinatorial Optimization']",WB-37
"Nowadays, companies are increasingly confronted with shortages and uncertainty in supply of raw materials. At the same time, in light of sustainability, supply chains are increasingly interested in returning used products from customers and remanufacturing them. We develop an analytical model in a closed-loop supply chain context to study how shortages and uncertainty in supply impact different supply chain decisions. We focus on a supply chain with a [re]manufacturer and collector as decision-makers. The manufacturer offers products [produced from scratch or through re-manufacturing] and then sells them to the customers [i.e., forward flow]. They also make product design investment decisions which affect remanufacturing costs, and set the products’ selling prices. The collector incentivizes consumers to return used products, collects them [i.e., return flow], and sells them to the [re]-manufacturer who uses them for the purpose of remanufacturing. We provide managers with insights on these decisions under a decentralized and centralized setting under supply shortage and uncertainty, and conduct a sensitivity analysis with respect to various parameters.",The impact of supply shortage and uncertainty on product design for remanufacturing in a closed-loop supply chain,"[77795, 41201, 77880]",924,"[125, 50, 138]",2574,Remanufacturing and refurbishing operations,18,10,23,"Circular Economy, Remanufacturing and Recycling ","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,82 [building - 116],"['Reverse Logistics / Remanufacturing', 'Game Theory', 'Supply Chain Management']",TD-23
"Neural networks and the data sets used to train them continue to grow in size. Traditional training methods like SGD and Adam require extensive hyperparameter tuning, which is increasingly incompatible with today's demands as current applications require methods that ideally do not need hyperparameter tuning. With this in mind, we investigate additive domain decomposition methods for neural network training. Due to their underlying additive decomposition, our methods are parallelizable, and owing to a trust-region strategy, they largely obviate the need for hyperparameter tuning. Our test results suggest that the investigated methods are potential candidates for efficient neural network training.",Parallel Neural Network Training via Nonlinearly Preconditioned Trust-Region Method,[77914],447,"[66, 5, 102]",2575,Preconditioning for  Large Scale Nonlinear Optimization,84,5,34,Advances in large scale nonlinear optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,43 [building - 303A],"['Machine Learning', 'Algorithms', 'Parallel Algorithms and Implementation']",MD-34
"We analyze call center data on properties such as agent heterogeneity, customer patience and breaks. Then we compare simulation models that are different in the ways these properties are modeled. We classify them according to the extend in which they approach the actual service level and average waiting times. We obtain a theoretical understanding on how to distinguish between the model error and other aspects such as random noise. We conclude that modeling explicitly breaks and agent heterogeneity is crucial for obtaining a precise model.",Call center data analysis and model validation,"[1082, 71561, 77916]",15,"[130, 131, 121]",2576,Advances in Stochastic Modelling and Applied Probability Ι,47,3,40,Advances in Stochastic Modelling and Learning Methods,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,96 [building - 306],"['Service Operations', 'Simulation', 'Queuing Systems']",MB-40
"The Shapley value equals a player's contribution to the potential of a game. The potential is a most natural one-number summary of a game, which can be computed as the expected accumulated worth of a random partition of the players. This computation integrates the coalition formation of all players and readily extends to games with externalities. We investigate those potential functions for games with externalities that can be computed this way. It turns out that the potential that corresponds to the MPW solution introduced by Macho-Stadler et al. [2007, J. Econ. Theory 135, 339-356] is unique in the following sense. It is obtained as a the expected accumulated worth of a random partition, it generalizes the potential for games without externalities, and it induces a solution that satisfies the null player property even in the presence of externalities.","Random partitions, potential of the Shapley value, and games with externalities","[45934, 68551, 45947]",385,"[26, 0]",2578,"Game Theory, Solutions and Structures I",88,2,36,"Game Theory, Solutions and Structures","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,32 [building - 306],['Decision Support Systems'],MA-36
"Which harvesting technology to use is a crucial decision for farmers, particularly when dealing with perishable products. Renting machinery is customary for many farmers, with costs fluctuating based on the machines’ features, yearly conditions, and the producer's willingness to pay. Negotiations on rental discounts often occur, especially when renewing machine rentals.

Employing simulation and decision rules, we assess the problem of choosing between different technologies that a wine grape farmer might use when harvesting. Besides the cost differences, the machines behave differently in rainy conditions. The occurrence of rain and its intensity is difficult to predict when the farmer must choose which machine to use. The farmer might also decide not to rent any equipment, so they won’t be able to harvest for some time. 

The farmer navigates harvesting decisions through a multidimensional utility function applied across numerous scenarios spanning several years, with rainfall intensity as the critical uncertainty. We performed numerical experiments using Monte Carlo simulation to evaluate the use of different technologies. We analyzed the results to determine how well the proposed approach can consider the operational complexity of harvesting using different types of machines and under uncertain weather conditions. This can help make a better decision when choosing which machine to use in the initial year and the switching costs in subsequent years.",Valuing harvesting technologies rental cost using a simulation approach,"[1917, 53805, 65176, 77928]",590,"[26, 89, 131]",2579,OR in Agriculture,20,5,12,OR in Agriculture and Forestry ,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,13 [building - 116],"['Decision Support Systems', 'OR in Agriculture', 'Simulation']",MD-12
"Despite the impressive numerical performance of the quasi-Newton and Anderson/nonlinear acceleration methods, their global convergence rates have remained elusive for over 50 years. This study addresses this long-standing issue by introducing a framework that derives novel, adaptive quasi-Newton and nonlinear/Anderson acceleration schemes. Under mild assumptions, the proposed iterative methods exhibit explicit, non-asymptotic convergence rates that blend those of the gradient descent and Cubic Regularized Newton's methods. The proposed approach also includes an accelerated version for convex functions. Notably, these rates are achieved adaptively without prior knowledge of the function's parameters. The framework presented in this study is generic, and its special cases include algorithms such as Newton's method with random subspaces, finite differences, or lazy Hessian. Numerical experiments demonstrated the efficiency of the proposed framework, even compared to the l-BFGS algorithm with Wolfe line-search.",Adaptive Quasi-Newton and Anderson Acceleration Framework with Explicit Global [Accelerated] Convergence Rates,[77924],363,"[19, 5, 21]",2581,Beyond First-Order Optimization Methods,84,13,32,Advances in large scale nonlinear optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,41 [building - 303A],"['Continuous Optimization', 'Algorithms', 'Convex Optimization']",WB-32
"In response to rising climate change challenges, the evolution of Integrated Assessment Models [IAMs] has been essential for a sustainable future. IAMs integrate environmental, technological, and socio-economic insights, to provide insight to sustainable planning. The recent significant cost reductions in Variable Renewable Energy [VRE] technologies have enhanced their adoption.  However, VREs introduce complexities in modeling concerning system flexibility and grid stability. This study aims to assess the interplay between technological advances, grid integration, and market dynamics of VREs—elements fundamental to the decarbonization of the power sector. To achieve this aim, we incorporate a wider spectrum of power generation technologies into the AD-MERGE IAM, including updated options for hydro, fossil fuels, and nuclear, with a special focus on refining the integration and dynamics of VREs. Additionally, more energy storage solutions are introduced to enhance the model’s technological representation. The improved storage options can enable the mitigation of the variability of VREs, ensuring a more reliable and stable energy supply. By evaluating the impacts of these advancements within the Shared Socioeconomic Pathway 2 scenario, our research addresses VRE integration challenges, seeks to overcome intermittency, and supports decarbonization targets.",Sustainable Energy Transitions - Addressing Variable Renewable Energy Challenges with the AD-MERGE Integrated Assessment Model ,"[77642, 18748, 36054, 77929, 54412]",605,"[37, 93, 40]",2583,Renewable Energy Challenges,21,12,22,Energy Management,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,81 [building - 116],"['Energy Policy and Planning', 'OR in Energy', 'Environmental Management']",WA-22
"An important attribute of globalization is the escalation of the movement of goods across international borders, heightening the need for trade facilitation. In 2007, the World Bank released the maiden version of the Logistics Performance Index [LPI] as a numerical yardstick for gauging progress towards trade facilitation. The LPI is an equally weighted six-component composite index that lies between 1 [the worst-case scenario] and 5 [the best-case scenario]. The LPI provides valuable information to many stakeholders around the world, including supply chain and logistics professionals, academics, and trade agencies, among others. The purpose of this paper is to examine the ability of the individual components of the LPI to predict trade openness [where trade openness is defined as the sum of exports and imports, expressed as a percentage of GDP]. The empirical analysis of the latest LPI data is conducted using Classification and Regression Trees [CART], which does not only have the advantage of capturing potentially complex relationship between the individual LPI components and trade but also lead to the identification of the most important components based on the trimming algorithm. Our empirical results indicate a strong potential of CART in enhancing the usefulness of the LPI.",The International Logistics Performance Index as A Useful Predictor of Trade openness - A Classification and Regression Tree [CART] Analysis,"[5314, 77931]",625,"[65, 138, 143]",2584,Advancing mobility towards sustainable solutions IV,6,13,56,Transportation,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S04 [building - 101],"['Logistics', 'Supply Chain Management', 'Transportation']",WB-56
"A popular approach for regularization involves replacing the original problem with an optimization problem that minimizes the sum of two terms - an l^2 term and an l^q term with q smaller than or equal to one. The first penalizes the distance between the measured and reconstructed data, while the second imposes sparsity on certain coefficients of the computed solution.

In [1], we propose to use the fractional Laplacian of a properly constructed graph in the l^q term to compute extremely accurate reconstructions of the desired images. Furthermore, we propose automatic approaches to determine the involved parameters so that the proposed method is completely plug-and-play. We show that the algorithm, under some reasonable assumptions, is a regularization method. Some selected numerical examples show the performances of our proposal. 

[1] S. Aleotti, A. Buccini, M. Donatelli, Fractional graph Laplacian for image reconstruction, Applied Numerical Mathematics, 2023.",Fractional graph Laplacian for image reconstruction,"[77378, 77352, 77935]",421,"[72, 19, 5]",2585,Optimization and learning for data science and imaging [Part I],84,2,34,Advances in large scale nonlinear optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,43 [building - 303A],"['Mathematical Programming', 'Continuous Optimization', 'Algorithms']",MA-34
"Network Data Envelopment Analysis [NDEA] is an extension of Data Envelopment Analysis [DEA] that takes into consideration the internal structure of Decision Making Units [DMUs] in the efficiency assessment. In NDEA, every DMU is conceived as a network of several sub-processes arranged into a series, parallel or mix of series and parallel [complex] structures. In the literature, most of the studies focus on series structures while the drawbacks of the methods developed for the efficiency assessment of parallel structures are neglected. In this paper, we discuss the pitfalls of the most established NDEA method for parallel structures, and we develop a new method to treat these issues. Our method relies on Multi-Objective Programming [MOP] techniques and can be generalized to any type of internal structure. We compare the introduced method with other prominent methods in the literature highlighting the differences and the advantages of the proposed method. Finally, we employ this method to evaluate the teaching and research efficiency of Polish higher education institutions.",A generalized composition approach in Network Data Envelopment Analysis for complex structures - An application to higher education institutions in Poland,"[31462, 71383, 31329, 224]",938,"[24, 35, 72]",2586,DEA applications in Education and Health I,89,5,48,Data Envelopment Analysis and its Application,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,60 [building - 324],"['Data Envelopment Analysis', 'Efficiency Analysis', 'Mathematical Programming']",MD-48
"In this work, we study the problem of transporting temperature-sensitive products in refrigerated trucks, where maintaining temperatures within specified ranges is crucial to prevent quality degradation. Innovative routing models are characterized by several limitations, such as the lack of a direct link between temperature and the variables influencing its dynamics and the assumption of deterministic scenarios. Our work introduces a novel model that treats temperature as a dynamic variable influenced by multiple factors, including cooling unit operation, and incorporates uncertainties such as door opening durations and initial product temperatures. Our approach involves optimizing a high-frequency control policy for cooling units by casting the problem as a multistage stochastic program, with each client stop representing a stage. This method poses computational challenges due to the extensive number of stages and the complexity of thermodynamic modeling. To address this, we employ the Stochastic Dual Dynamic Programming algorithm and validate our methodology through a case study with real-world data from a specialized Dutch logistics company. The findings demonstrate that our strategy surpasses both deterministic lookahead strategy and current industry practice in maintaining optimal product temperatures while minimizing energy consumption. This advancement holds significant potential for enhancing the efficiency and sustainability of refrigerated transport logistics.",Real-Time Temperature Control for Refrigerated Trucks - A Multistage Stochastic Programming Approach,[77930],800,"[117, 143, 100]",2587,Analysis of Stochastic Models II,50,15,39,Stochastic Modelling,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,35 [building - 306],"['Programming, Stochastic', 'Transportation', 'OR in Sustainability']",WD-39
"Consider a two-person zero-sum search game between a Hider and a Searcher. The Hider chooses to hide in one of n discrete locations [or boxes] and the Searcher chooses a search sequence specifying which order to look in these boxes until finding the Hider. A search at box i takes t_i time units and finds the Hider - if hidden there - independently with probability q_i, for i=1,...,n. The Searcher wants to minimize the expected total time needed to find the Hider, while the Hider wants to maximize it. It is shown in the literature that the Searcher has an optimal search strategy that mixes up to n distinct search sequences with appropriate probabilities. This work investigates the existence of optimal pure strategies for the Searcher - a single deterministic search sequence that achieves the optimal expected total search time regardless of where the Hider hides. We identify several cases in which the Searcher has an optimal pure strategy, and several cases in which such optimal pure strategy does not exist. An optimal pure search strategy has significant practical value because the Searcher does not need to randomize their actions and will avoid second guessing themselves if the chosen search sequence from an optimal mixed strategy does not turn out well.  This is joint work with Thuy Bui and Kyle Lin.",Optimal Pure Strategies for a Discrete Search Game,[32908],645,"[50, 14, 129]",2588,"Game Theory, Solutions and Structures VII",88,9,36,"Game Theory, Solutions and Structures","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,32 [building - 306],"['Game Theory', 'Combinatorial Optimization', 'Scheduling']",TC-36
"Our study delves into a platform supply chain that comprises a manufacturer and an e-platform, alongside a secondary marketplace. Drawing inspiration from real-world industrial scenarios, we explore the roles of the marketplace and the retailing channel within the e-platform. We outline the optimal operational strategies for this platform supply chain, considering various supplier market encroachment strategies. Furthermore, we conduct a comparison of supplier encroachment strategies, developing an analytical framework to show how to choose the optimal strategy and the critical factors influencing this decision within the platform supply chain. Additionally, we analyze the dynamics of optimal collect and resell strategies for the e-platform, along with the corresponding operational decisions, indicating how the e-platform should adapt its strategies based on the supplier’s chosen encroachment strategy.",Strategic analysis of e-platform supply chain decisions,[77313],560,"[50, 138]",2589,Game Theory in Sustainable Supply Chains,19,14,24,Sustainable Supply Chains,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,83 [building - 116],"['Game Theory', 'Supply Chain Management']",WC-24
"We study the design of two-echelon distribution networks with facility sizing decisions over a multi-period planning horizon amidst demand uncertainty. This research is based on a real-world problem faced by a large consumer-packaged goods manufacturing and distribution company seeking to implement a multi-tiered distribution strategy via satellite facilities strategically located across the service area. The problem is formulated as a two-stage stochastic program with discrete recourse, where the first stage defines both the location and size of satellites, and the second stage defines which satellites to operate and how to serve customers. To solve the two-stage stochastic program, we develop an exact solution approach combining the integer L-shaped method and sample average approximation. Through extensive numerical experiments, we show the effectiveness of the proposed exact approach and present several managerial insights regarding facility location, the value of accounting for stochasticity, and the effects of explicitly incorporating facility sizing decisions when designing two-echelon distribution networks.",Designing Multi-Period Two-Echelon Last-Mile Delivery Networks with Multiple Capacity Levels under Uncertainty,"[74658, 77956, 58557]",637,"[79, 117, 64]",2591,Retail Distribution I,30,12,50,Retail Operations,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,M2 [building - 101],"['Network Design', 'Programming, Stochastic', 'Location']",WA-50
"Operations research involves mathematical modeling and analytic techniques.  Some people learn modeling and then analytics, some learn the other way around, and some learn them simultaneously. Most modern textbooks seem to teach them together and center the modeling. Are there problems  with that? I will explain why I think so. Can we do better? I will discuss how I think we can.  ",Pedagogy and prescriptive analytics,[22923],102,"[106, 7, 10]",2592,Behavioral Decision Analysis IV,13,7,11,Behavioural OR,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,12 [building - 116],"['Profession of OR', 'Analytics and Data Science', 'Behavioural OR']",TA-11
"This talk aims to showcase the latest developments in RAPOSa, a global solver for polynomial optimization problems, with a primary focus on its extension to address mixed-integer problems. The talk will delve into various facets of the key challenges encountered in this extension, along with some numerical experiments designed to assess the performance of different approaches, both relative to one another and also relative to other state-of-the-art global solvers for MINLP problems.

Regarding the extension to mixed integer problems, we will discuss approaches targeted to improve the quality of the final bounds returned by the algorithm. In particular, for the upper bounds we will discuss the integration of the calls to auxiliary local solvers [discrete and continuous] and the trade-offs that arise when trying to do it efficiently.
",RAPOSa - A free global solver for mixed integer polynomial optimization problems,"[43831, 66640, 77942, 72252]",238,"[134, 52, 111]",2593,MINLP Solvers,76,3,30,Software for Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,53 [building - 208],"['Software', 'Global Optimization', 'Programming, Mixed-Integer']",MB-30
"In recent years, increased awareness of gender bias in machine learning has led developers to focus more on gender equality in model outcomes. This study investigates how fairness norms and accuracy thresholds impact developers' model choices and reporting decisions. The experiment design in this study replicates the machine learning algorithm development process. Results from 604 programmers participating in this experiment show that fairness norms led to the selection of fairer models [Hypothesis H1]. Moreover, developers in companies valuing accuracy less prioritized fairness when informed of its norm [Hypothesis H2]. Interestingly, we found that high accuracy thresholds with fairness norms encouraged participants to prioritize fairness, even at the expense of personal benefits. As an exploratory analysis, we study the reporting provided by programmers and the effect of their backgrounds and demographics on their models' performance. This study highlights the importance of fairness norms in guiding machine learning development and reporting, aiding gender equality in AI applications.",Shaping Programmer Practices - Effective Strategies for Mitigating Bias in Machine Learning Algorithm Development,"[77939, 63174, 77943, 53593]",567,"[66, 41, 25]",2595,Behaviour and decision support,13,10,07,Behavioural OR,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,1019 [building - 202],"['Machine Learning', 'Ethics', 'Decision Analysis']",TD-07
"In the Analytic Hierarchy Process [AHP], a Consistency Index [CI] is used for detecting inconformity of a Pairwise Comparison Matrix [PCM]. An original idea of T. Saaty is to compare value of CI with an average value of CI obtained for a large amount [e.g. 100,000] of random PCMs. The inventor of AHP idea has specified percentage thresholds for CI values, which should not be exceeded. However, as A.Z. Grzybowski has shown, very small number of random PCMs is even close to consistent in Saaty's sense [https://doi.org/10.1016/j.eswa.2012.04.051]. Furthermore, according to Grzybowski’s research, for PCMs of the size larger than 5, the probability of gaining PCM, that satisfies Saaty's consistency condition is close to 0. Hence, evaluation of the relationship between random PCMs and consistent PCMs is rather pointless. However, much higher percentage of randomly matrices are consistent, when transitive PCMs are taken into account. Moreover, PCMs given by real decision makers usually are transitive [despite of their inconsistency]. Therefore, it was decided to investigate mostly that kind of PCMs. Thus, Monte Carlo simulations for various CIs have been designed. The intent is to discuss the result of that examination, in particular certain characteristics of CIs distribution obtained for transitive PCMs. It is believed that, on the bases of these results, one can more credibly classify PCM as enough or not enough consistent.",A Simulation Analysis of Inconsistency Indices Values for Transitive Pairwise Comparison Matrices,"[76954, 77946]",893,"[6, 26, 7]",2598,Pairwise comparisons and preference relations 3,44,12,44,Multiple Criteria Decision Analysis,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,20 [building - 324],"['Analytic Hierarchy Process', 'Decision Support Systems', 'Analytics and Data Science']",WA-44
"The disruptions caused by the COVID-19 pandemic have revealed the necessity for innovative last-mile delivery solutions in urban logistics. Thus, automated parcel lockers offer benefits for courier companies and customers by enhancing flexibility and reducing mileage, delivery time, and emissions. This work introduces a novel approach by integrating a System Dynamics Simulation Model to forecast e-commerce demand in Pamplona, Spain, and assesses its scalability for other cities. Additionally, a bi-objective Facility Location Problem [FLP] is rooted within the simulation environment to test various scenarios of demand coverage. Simulation and demand forecasting are conducted using Anylogic software with the CPLEX optimization solver for the FLP, featuring an ε-constraint approach based on economic and demand criteria. Results draw optimal APL networks, derived from the non-dominated solution set of the bi-criteria mathematical programming model through ε parameterization. That leads to the identification of the Pareto Frontier and the trade-offs between the number of lockers and parcel demand satisfied. Finally, we encourage the utility of simulation-optimization in addressing dynamic environments.",Enhancing Last-Mile Delivery Efficiency - A Hybrid Simulation-Optimization Approach for Automated Parcel Locker Network,"[3531, 77944, 40991, 23407, 72366]",868,"[65, 143, 131]",2599,Combinatorial Optimization models and applications in Logistics and Transportation II,64,3,29,Combinatorial Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,157 [building - 208],"['Logistics', 'Transportation', 'Simulation']",MB-29
"The optimal use of critical resources, particularly train paths and rolling stock, plays a pivotal role in enhancing the operational efficiency and cost reduction of railway freight companies. Determining the right timetables for train paths is essential when integrating the selection of train paths and the assignment of the rolling stock. In this research, we explore heuristic approaches for this integrated problem where practical operational constraints such as maximum load, maximum weight, delivery time and freight car shunting in classification yards are taken into account. We propose an Adaptative Large Neighborhood Search [ALNS] heuristic based on a Pickup and Delivery Problem with Time Windows and Transfers [PDPTWT] to define the train path catalog that is required for the routing decisions of the rolling stock.
A column generation approach is then used for the assignment of individual demands to train paths. Real-world data on the French railway network is used to validate the proposed approach, and good solutions are obtained in reasonable computational times. Our integrated approach is also compared with a sequential approach.
",Integrated Planning of Train Path and Rolling Stock Assignment in Railway Freight Transportation,"[77278, 16259]",763,"[122, 142, 145]",2600,Public Transport,5,8,58,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S07 [building - 101],"['Railway Applications', 'Timetabling', 'Vehicle Routing']",TB-58
"Turnaround apron operations involve a diverse array of interconnected services required for aircraft handling.  Short-term planning of these services demands comprehensive coordination between the participating organisations to effectively manage the multiple interactions arising. This includes coordination efforts between aviation service providers [SPs] which are subcontracted by airlines for turnaround services and a coordinator such as the airport operator. Traditional coordination approaches largely focus on the priorities of a single stakeholder, typically airlines and airports. This paper introduces four cross-organisational coordination strategies for apron operation planning with varying levels of SP participation in decision-making. A simulation-optimisation approach based on Constraint Programming is used to model and solve the apron planning within the proposed coordination settings.  Experiments suggest that equitable central coordination enhances robustness and resource efficiency of plans, while more pragmatic strategies can address specific challenges of SPs such as resource scarcity, still yielding positive outcomes in terms of schedule stability.",Cross-organisational Coordination in Apron operations,"[62529, 36741, 77949, 76049, 77948]",650,"[143, 145, 4]",2601,Airline Applications II,6,5,55,Transportation,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S02 [building - 101],"['Transportation', 'Vehicle Routing', 'Airline Applications']",MD-55
"Circular supply chains are key enablers to implement the circular economy [CE], however they are yet to be established in industries with significant environmental impacts. Refurbishing is ranked high among circular strategies as it extends the lifespan of products and preserves product functionality across multiple lifecyles. Nevertheless, it requires a challenging coordination of forward and reverse supply chains.
This work investigates the optimal circular supply chain re-design for the refurbishment of modular products under three scenarios - [i] cost minimisation, [ii] CO2e emissions minimisation, [iii] cost and CO2e emissions minimisation in an eco-efficient configuration. A mixed-integer linear programming model was developed and applied to a case study of a three-tiers global mining equipment supply chain, contextually determining the location of the refurbishment facilities. Findings highlight that supplier selection has a significant impact on the overall supply chain re-design, while locating the facilities in customers’ proximity yields environmental benefits.
This work contributes to the CE literature by designing optimal circular supply chains, which integrate a high-R imperative, such as refurbishing, while simultaneously considering product modularity, multi-modal transportation, economic and environmental objectives. The model could further support practitioners aiming to re-design circular supply chains based on the refurbishment of modular products.
",Circular Supply Chain Design for Refurbishing Modular Products,"[47017, 77950, 78011]",922,"[125, 138, 100]",2602,Supply chain design in the circular economy,18,8,23,"Circular Economy, Remanufacturing and Recycling ","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,82 [building - 116],"['Reverse Logistics / Remanufacturing', 'Supply Chain Management', 'OR in Sustainability']",TB-23
"Inland water transport terminals, especially those in developing countries, are commonly used for handling multiple cargo types. Berth allocation in such multipurpose terminals has special features different from that in specialized seaport terminals. While existing berth allocation research primarily focused on specialized terminals, this study investigates the discrete berth allocation problem in multipurpose inland water terminals considering the diverse cargo types and unique operational constraints involved. The problem is first formulated as a mixed integer linear programming model with the objective of minimizing the waiting and completion times of barges. A memetic algorithm is developed to obtain near-optimal solutions efficiently. Real-world data from an Indian inland water terminal are used to conduct computational experiments which demonstrate the model’s effectiveness and the algorithm's efficiency. The developed method has the potential to be incorporated into the operations planning system of inland water terminals in developing nations to help improve their productivity.",Model and a Memetic Algorithm for Berth Allocation in Multipurpose Inland Waterway Terminals,"[7214, 77951, 77953]",670,"[65, 74, 84]",2603,Seaside Planning II,52,5,62,OR in Port Operations,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S12 [building - 101],"['Logistics', 'Metaheuristics', 'Optimization Modeling']",MD-62
"As market competition escalates, retailers are compelled to understand consumer preferences more deeply to discern needs accurately. However, limited to internal databases, they lack access to cross-channel consumption data for a comprehensive understanding. This study utilizes electronic invoice records to analyze consumer behavior, predicting product choices in supermarkets. Employing the Self-Organizing Map [SOM] algorithm for consumer segmentation, it groups consumers with similar behavior, enhancing predictive accuracy. Using variables like expenditure and purchase frequency, it constructs Long Short-Term Memory [LSTM] models for channel forecasting. Empirical results show exceptional accuracy in product category prediction. This research aids effective segmentation, providing insights into consumer needs for inventory optimization. Cluster analysis facilitates targeted marketing, benefiting both retailers and consumers. Further validation and updates are vital for practical applications.",Enhancing Retail Strategies through Electronic Uniform Invoice Analysis - A Study on Consumer Behavior and Market Segmentation,"[59123, 77969, 59157]",528,"[71, 7, 8]",2605,E-Commerce,30,14,50,Retail Operations,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,M2 [building - 101],"['Marketing', 'Analytics and Data Science', 'Artificial Intelligence']",WC-50
"The growing adoption of Electric Vehicles [EVs] creates new challenges for EV Charging Station Operators [CSOs] due to the increasing charging demand, but also opportunities to leverage EV user flexibility.
We consider an EV CSO offering several charging power rates at different prices, and we explore the design of a price menu. In our setting, EV users view charging more as an opportunity than as an indispensable need. They are inflexible in their parking duration, which depends on their on-site activities, but they are flexible in terms of their energy demand, and choose the option that maximizes their welfare, i.e., their utility minus their cost of charging.
We formulate the optimal price menu design problem of a profit maximizing CSO as a mixed integer linear programming problem, and we compare against the outcome of social welfare maximization. We further account for the provision of demand response by the CSO, i.e., lowering its power consumption for a certain period given a certain price for remuneration, by adjusting the price menu [in real time], to incent EV users to choose lower power rates. Our numerical demonstrations provide useful insights on the construction of the optimal price menu.",Optimal Price Menu Design of Electric Vehicle Charging Stations,"[77952, 77958, 77959, 61976]",685,"[124, 136, 33]",2606,Charging Infrastructure toward Sustainable Transport,80,7,53,Sustainable and Resilient Systems,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,8007 [building - 202],"['Revenue Management and Pricing', 'Stochastic Optimization', 'Economic Modeling']",TA-53
"Most stores offering clothing, electronics, phones, and similar shopping goods, agglomerate in clusters —malls or shopping districts. This location pattern facilitates multipurpose shopping, which consists of purchasing more than one good during the same trip, and comparison shopping, the process of comparing products of different stores, but serving the same purpose, before purchasing. 
Clustering leads to a two-stage [or sequential] destination choice by customers. They decide first on a cluster to visit, and then, the stores to visit within the cluster. The two stages are not independent, as the first decision depends on the stores in the cluster - if a consumer is in search of a product serving a purpose, the number of alternative stores that fulfill that purpose in the cluster, and the number of stores offering other products in the same cluster, affect the selection of the chosen cluster. 
This agglomeration and the correlation are not represented in the customer choice rules common in location models [Multinomial Logit, Gravitational rules], as they do not take into account the spatial distribution of stores, although modifications to gravitational and Logit rules have been suggested by marketing researchers. 
We explore a Nested Logit model fit for the spatial choice process and later use it to formulate, for the first time, a location model that includes the two-stage process and the spatial agglomeration. 
Preliminary computational experience is also presented.",Facility location under agglomeration and two-stage customers’ spatial choice,"[1646, 66185, 54824, 12245]",43,"[64, 109, 43]",2609,Advances in Location Analysis ,29,2,61,Locational Analysis,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S10 [building - 101],"['Location', 'Programming, Integer', 'Facilities Planning and Design']",MA-61
"Among the United Nations' 17 Sustainable Development Goals, a critical aim is to provide affordable, sustainable, and clean energy for all. Additionally, the rising urgency to ensure energy security and address global warming has highlighted the need to explore alternative energy sources beyond fossil fuels. Leveraging agricultural waste for bioenergy production has emerged as a promising solution, offering an opportunity to mitigate environmental impacts while creating renewable energy. The success and attractiveness of this initiative to stakeholders and investors significantly depend on designing efficient supply chain networks, particularly in selecting optimal locations for biorefineries. This research sets out to design a supply chain network that prioritizes sustainability, encompassing economic, environmental, and social considerations. Initially, it employs a two-pronged approach, utilizing Geographic Information System [GIS] and MCDM, to pinpoint viable biorefinery locations, taking into account both environmental and economic constraints.
",Sustainable design of agricultural waste supply chain based on circular bioeconomy,"[75817, 72274]",919,"[138, 93, 139]",2611,Sustainable food supply chains,18,4,23,"Circular Economy, Remanufacturing and Recycling ","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,82 [building - 116],"['Supply Chain Management', 'OR in Energy', 'Sustainable Development']",MC-23
"We report the progress of our project of solving large-scale Quadratic Assignment Problems [QAPs]. We present a method that efficiently computes global lower bounds for QAPs by employing the Newton-bracketing method along with Lagrangian doubly nonnegative [DNN] relaxation. The integration of auxiliary information derived from the Newton-bracketing method, combined with the checkpoint mechanism of the parallelization framework Ubiquity Generator [UG], enables the determination of strong global lower bounds for large QAPs. This method has led to the improvement of lower bounds for several unsolved problems listed in QAPLIB, the standard benchmark for QAPs.",Improving Lower Bounds for Large Scale QAPs,"[57568, 6984, 68285, 23761, 29543]",191,"[109, 63, 114]",2613,Parallel Solvers,76,7,30,Software for Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,53 [building - 208],"['Programming, Integer', 'Large Scale Optimization', 'Programming, Quadratic']",TA-30
"We study a multistage sales and production problem. At each stage a market price is observed, sales decisions are made, and resources are available for a production decision which yields products to be sold at later stages. The prices of each product evolve randomly. To address model ambiguity, we study two distributional robustifications of the sample average version of this problem. The model is applied to a problem arising in New Zealand's dairy industry. We first compare the out-of-sample performance of the sample average policy to that of a model predictive control policy which is shown to be distributionally robust with an ambiguity set of distributions with matched means. We then compare this with a distributional robustification using discrete Wasserstein distance-based ambiguity sets. The models are solved using stochastic dual dynamic programming in SDDP.jl.",Multistage Sales and Production,"[77125, 24031, 4275]",379,"[136, 127, 82]",2615,Trends and Open Problems in Robust Optimization,49,10,34,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,43 [building - 303A],"['Stochastic Optimization', 'Robust Optimization', 'Optimal Control']",TD-34
"Many online retailers start to fulfill orders within one hour or even minutes to guarantee a good customer experience. This study analyzes the on-time order fulfillment problem for an autonomous order picking and delivery system in online retailers. We draw inspiration from Meituan’s pioneering solution, Autonomous micro-warehouse and delivery, in which micro-warehouse robots are used for order picking, and delivery robots are used for transporting orders to destinations. We investigate the dynamic priority policy to facilitate efficient task allocation for different order classes, which ensures prompt service for high-priority orders and attentive handling of low-priority orders with extended waiting times.

We develop a dynamic priority-based queueing network to model the system and propose an AMVA-based approximation method to solve it. Numerical experiments validate the analytical model, providing valuable insights into operational efficiency. This paper makes the following contributions - [1] We are among the earliest to investigate autonomous order picking and delivery systems using queueing networks. We propose a new approximation method to solve the proposed nested queueing network with dynamic priority. [2] We outline procedures for determining the optimal numbers of micro-warehouse robots and delivery robots, considering the order fulfillment time requirements. [3] We find that the use of dynamic priority can improve the on-time order fulfillment rate.",Enhancing on-time order fulfillment for autonomous order picking and delivery systems of online retailers,"[77968, 64285]",860,"[32, 121, 65]",2616,"Discrete, continuous or stochastic optimization and control in networks, transportation and design IV",64,5,25,Combinatorial Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,011 [building - 208],"['E-Commerce', 'Queuing Systems', 'Logistics']",MD-25
"We studied the order acceptance and scheduling problem on a single machine in additive manufacturing[AM] environment, considering a group of part orders with different sizes, profits, arrival date, due date, and delivery deadlines to be decided,manufactured and scheduled on an single AM machine. In our research, the AM machine is modeled as a batch processing Machine, processing  part orders in batch sequences.Part orders are either accepted and assigned to a batch or rejected, the accepted orders are subject to dual constraints of time window and processing space.The goal is to maximize total net revenue considering orders revenue,processing costs, and penalties for delayed delivery and order rejection.

In this talk, we :
1.develop an Adaptive Variable Neighborhood Search Algorithm [AVNSA] to obtain a high-quality order acceptance list and part processing sequence which satisfies all technical constraints. 
2.present a heuristics methods for initialization.
3.propose 9 kinds of move operators for neighborhood search and 2 kinds of backtracking mechanism to prevent the algorithm from getting stuck in local optima
4.make the comparisons between the proposed AVNSA and several methods in previous research in 135 randomly generated instances.

The preliminary experimental results illustrates that,within shorter CPU time,the AVNSA is capable to obtain better solutions with higher net revenue,fill rate and less delay.



",An adaptive VNS optimization algorithm for order acceptance with scheduling problem in additive manufacturing,[77971],881,"[129, 69, 14]",2618,Topics in scheduling [Contributed],64,14,26,Combinatorial Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,012 [building - 208],"['Scheduling', 'Manufacturing', 'Combinatorial Optimization']",WC-26
"As an important home health care mode in China, the home-community service mode focuses on the impact of healthcare centers in each community and coordinates the medical resources between communities. In this paper, we propose a new collaborative home health care routing and scheduling problem in multiple service centers [C-HHCRSP]. In addition to the traditional routing and scheduling decisions, C-HHCRSP also assigns a working center for each highly qualified caregiver who can provide health services across multiple communities. This problem is challenging due to the presence of complex, realistic constraints such as mandatory lunch breaks, synchronized visits, and downgrading of services. We first formulate the problem as a mixed integer programming model, which is solved by CPLEX. Then, we propose an adaptive large neighborhood search [ALNS] algorithm that integrates new problem-specific destroy and repair operators. To further improve the performance of ALNS, we propose two post-optimization techniques, which are based on heuristic strategies and a set partitioning model, respectively, resulting in the enhanced algorithms ALNS-HS and ALNS-SP. Tested on 104 benchmark instances, ALNS-HS shows a competitive performance with CPLEX on small instances. For large instances, we compare our algorithms with a classic large neighborhood search algorithm to demonstrate their superiority. Additional analyses are performed to verify the roles of the components of our algorithms.",Collaborative Scheduling and Routing of Home Healthcare Service across Multiple Communities,"[77807, 40508, 65596, 78008]",598,"[56, 111, 74]",2620,Home Health Care and Operating Room Scheduling,3,12,15,OR in Health Services [ORAHS],"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,18 [building - 116],"['Health Care', 'Programming, Mixed-Integer', 'Metaheuristics']",WA-15
"We model tactical changes in association football as a Markov game. The pitch is discretised into nine zones and the states of the Markov game are defined according to the zone in which the ball is located in play, the team in possession, and the score. We first model tactical changes in a Markov decision process framework, wherein one team maximises their probability of winning. Then, we model tactical changes in a two-person zero-sum Markov game framework, wherein both teams maximise their probability of winning. Fundamental to our modelling is the notion that tactical changes impact upon transition rates. We verify the models using data from matches in a season of the Japan Professional Football League. We define a change in transition rates that can be realised by changes in tactics, and illustrate an example of optimal tactical changes when both teams can vary their tactics. The models we develop in the paper can support managers who are considering important decisions about substitutions and changes to formation, for example, when a match is in-play.",Modelling tactical changes in association football using a Markov game,"[10082, 77976, 77979, 77980]",665,"[99, 0]",2622,Football analytics,37,9,16,OR in Sports,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,19 [building - 116],['OR in Sports'],TC-16
"We propose and construct an automated essay scoring system based on rubric to reduce the workload on teachers. This system calculates thirteen items, including written content, document style, writing skills, and output the overall evaluation results. We collected and analyzed human score data to improve accuracy of overall evaluation. Specifically, we asked 6 faculty members to score 25 essays manually using our automated scoring system rubric to identify the characteristics of items that affect the overall evaluation. The results revealed that low-rated essays had similar evaluation results among faculty members. Correlations were also obtained between the automated and human scores for document style, skill, and contents among the rubric items. However, we could not obtain the common evaluation items among the teachers for high-rated essays and established the absence of an effective automatic calculation method for the overall evaluation. Therefore, we classify low-rated essays and others using the accumulated low-rated essay data and present a comprehensive evaluation of low-rated essays by automated scoring. On the other hand, for others including high-rated essays we suggest that a feedback system highlighting essential evaluation contents rather than the overall evaluation would be beneficial.",Analysis of characteristics of human score and improvement of feedback of evaluation results of automated essay scoring system,[62029],210,"[7, 0]",2626,Analytics and the link with stochastic dynamics II,17,8,31,Analytics,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,54 [building - 208],['Analytics and Data Science'],TB-31
"Multimodal public transit is an integral part of urban passenger transport in rapidly urbanizing modern economies. Public transit systems need strong, tightly-coupled last mile connectivity for continued patronage. In this paper, we deploy a spatial analytics and optimization-based approach to create a replicable framework which identifies and addresses gaps in last-mile connectivity to a planned urban public transit system such as the metro rail. We draw on classic transportation literature on the feeder network design problem and more recent research on last mile transportation systems, while adding to optimization literature on NP-Hard variants of the generic spanning tree problem for an undirected graph to provide an optimal set of last-mile stops and route structures.  
We demonstrate our framework through a systematic deconstruction of publicly available spatial/demographic data on an urban agglomeration in India [Bengaluru, Karnataka] and its metro rail infrastructure under development. Our findings show that the proposed long-term metro rail covers about 49% of the current population of 5.8 MM who reside within a walking distance from the proximal station. We identify stops and potential route structures for a fixed last-mile feeder network system which can potentially add a further 33% to the coverage. We also identify zones and segments of the city and its population for consistent planning of the built environment to facilitate better last mile connectivity.",A Framework for building Last Mile Connectivity to Urban Public Transit Systems in India,"[75616, 77987, 71999]",822,"[119, 143, 53]",2627,Demand-responsive public transport 3,85,10,54,Public Transport Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S01 [building - 101],"['Public Local Transportation Systems', 'Transportation', 'Graphs and Networks']",TD-54
"Our research proceeds in three ways. Firstly, we identify patterns of member interaction and key influencing factors via brings together developments in the literature over the past several years. Second, key influencing factors are included in the team decision model to build interactive team decision rules containing the individual member decision rules and the overall team decision rules. Finally, the simulation model of team decision is built on the Anylogic platform to verify the dynamic evolution of innovative team interactive decision in the digital platform system.
Our principal contributions can be highlighted as follows:
[1] Literature research has found that the factors that affect the interactive decision-making of innovation teams include the individual cognitive level of members, risk preferences, and member satisfaction.
[2] If there are significant differences in risk preferences among team members, they may find it difficult to reach consensus quickly during decision-making and exhibit conflicting behaviors in repeated decision-making, resulting in low innovation value in the decision-making outcomes.
[3]A team with a high level of cognitive ability is more likely to form higher innovative value in decision-making. 
[4]In repeated interactive team decisions, satisfaction can affect members' willingness to communicate and change their choices, thereby affecting the team's decision-making outcomes. ",Decision-making Modeling and Simulation of Digital Platform Innovation Team based on Member Interaction,"[77644, 53530, 77656]",565,"[3, 25, 10]",2629,Simulation in innovation,77,7,43,"Agent-based Models in Management, Economic and Organisation Sciences","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,99 [building - 306],"['Agent Systems', 'Decision Analysis', 'Behavioural OR']",TA-43
"The rapid advance of AI has created risks that are difficult to foresee, let alone quantify, making AI risks an area of deep uncertainty. Despite this, researchers have begun applying probabilistic methods to AI risks at a coarse level; e.g., existential catastrophe. This type of analysis may be useful for supporting strategic policy-level considerations, but is not particularly useful for design or operational decisions.  

We argue that many practical AI problems could be addressed by drawing from a toolkit of non-probabilistic risk-management methods. There exists a large class of strategies for managing risks that could be utilized to improve AI safety, using methods from fields such as safety engineering, product management, and military planning. These techniques could be used both to anticipate AI risks, and to mitigate AI risks even when they are imperfectly understood or cannot be quantified. These methods range from qualitative fault trees and event trees, to simple but effective solutions like checklists, what-if thinking, and pre-deployment testing [even by non-expert users].

We distinguish between safe design vs. rapid reaction to undesired behaviors. Reactive solutions include contingency planning, monitoring, and anomaly detection. These strategies can also operate in parallel, creating defense in depth. Drawing on these non-probabilistic methods should make it possible to develop safer AI applications, while allowing the field to advance.","Risk, Uncertainty and AI - non-probabilistic methods for anticipating and preventing AI risks ","[77442, 77991]",558,"[8, 126]",2630,Behavioral Decision Analysis I,13,3,11,Behavioural OR,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,12 [building - 116],"['Artificial Intelligence', 'Risk Analysis and Management']",MB-11
"The rapid development of ICT [Information and Communication Technology] in recent years has led to the problem of wireless bandwidth depletion. One of the technologies to alleviate the problem is to make use of CRNs [Cognitive Radio Networks]. This is a technology for efficient and dynamic use of frequencies. In CRNs, users are divided into PUs [Primary Users] and SUs [Secondary Users]. PUs are licensed users to occupy the bandwidth. On the other hand, SUs are not licensed users and can use the bandwidth only if SUs know that PUs are not present by sensing because PUs have preemptive priority over SUs. Therefore, spectrum handoff in CRNs critically affects performance of SUs.

This paper considers CRNs and analyzes performance measures of SUs. In particular, we take into account of spectrum handoff delay in CRNs and compare performance of SUs with and without the handoff delay. We construct system models by continuous-time Markov chain having transition rate matrix with block tridiagonal structure, which is called QBD [quasi-birth-death] process, to efficiently compute the stationary distribution. We show numerical examples of the performance of SUs in terms of delay in system. We discuss the effects of spectrum handoff delay on the performance measures for the model with the spectrum handoff delay and the conventional model without the handoff delay.
",Effects of spectrum handoff delay on secondary users in cognitive radio networks,"[77989, 77982, 67036]",510,"[135, 121, 141]",2631,Advances in Stochastic Modelling and Applied Probability II,47,4,40,Advances in Stochastic Modelling and Learning Methods,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,96 [building - 306],"['Stochastic Models', 'Queuing Systems', 'Telecommunications']",MC-40
"The Public Distribution system [PDS] distributes essential food and non-food items to the economically disadvantaged population. The distribution is often facilitated through a network of state warehouses and fair price shops. Each year, the Government of India spends a huge amount of money on the distribution of these grains across the nation, from surplus to deficient regions, which puts a heavy burden on financial resources. The present study aims to optimise the PDS operations to improve logistical decision-making and thereby reduce the cost to the government. The mathematical model is developed using mixed integer linear programming, which cost. The constraints considered in the model are demand constraints, supply constraints, and capacity constraints. The parameters includes capacity, demand, geo-coordinates, and district name. The outputs are optimal tagging and associated amounts of food commodities to be transported and optimal clusters for storage facilities significant. The above model has been implemented for 3 months in one of the states where the state has reported a reduction in the cost to the government. Efforts are underway to scale the exercise to all other states.",OR based solution for distribution of grains within a state,"[77995, 43014]",852,"[84, 65, 111]",2632,Food Supply Chains,78,12,13,Secure & Sustainable Food Supply,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,15 [building - 116],"['Optimization Modeling', 'Logistics', 'Programming, Mixed-Integer']",WA-13
"In a supply chain with a manufacturer ordering from two competing suppliers, this study examines how information quality affects information sharing's value and supplier competition. Market demand follows a time-series model. All members use periodic inventory reviews, order-up-to policy, and MMSE forecast method. We analyze various combinations of three information-sharing situations and four information-quality levels. Results show:
1. Supplier demand rises with competitive power, but increased orders mean higher inventory costs and potential savings through sharing. Suppliers should balance these to find optimal competitiveness and gain advantage.
2. Different information errors impact supplier competition. For instance, transmission errors might give one supplier an edge when the manufacturer shares data with both, while source errors do not.
3. Differentiation, including information status and quality, leads to competitive advantage. Suppliers should negotiate for exclusive sharing when no errors or only source errors exist. For transmission errors, enhancing information quality management is advised.",Competition and Cost in a Dual Sourcing Supply Chain,[77996],795,"[61, 138]",2633,Inventory Models,50,9,39,Stochastic Modelling,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,35 [building - 306],"['Inventory', 'Supply Chain Management']",TC-39
"In the platform economy, e-commerce platforms serve as distribution channels and financing providers through online loans. This study focuses on platform finance, where credit lines or interest rates are set, offering limited or unlimited loans to capital-constrained sellers. The optimal sellers’ channel strategy [single, online-dominant, or offline-dominant dual channels] depends on the platform’s credit line, consumers’ channel preferences, and switching behavior. With unlimited [or limited] loans, the seller’s dominated channel’s quantity decreases [or increases] with initial capital. Preferences for unlimited or limited loans depend on the dominant channel and capital constraints. Loan offerings lead to Pareto improvements for both sellers and platforms. Compared to bank finance, platform finance increases sellers' threshold quantities without stockouts in each channel, encouraging higher quantities in both channels. The study explores the channel substitution effect on sellers' quantity decisions, offering insights for optimal financing and channel structures to boost profits.",Platform Finance and Channel Strategies of Dual Channels,[65247],895,"[32, 138, 50]",2634,"Game Theory, Solutions and Structures IX",88,12,36,"Game Theory, Solutions and Structures","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,32 [building - 306],"['E-Commerce', 'Supply Chain Management', 'Game Theory']",WA-36
"This study considers a pricing problem in a duopoly market where customers have different preferences with respect to congestion. Customers choose a product based on the quality and price of the product sold by each firm, as well as the degree of congestion [aggregate demand] for that product. Customers who prefer congestion are more likely to choose a store if the store is more congested in order to obtain utility. Such a phenomenon is called a positive network effect [customer type P]. On the other hand, customers who do not like congestion will use a store when it is less congested. This is called a negative network effect [customer type N]. In this study, we formulate a price competition model under a linear demand function in order to clarify how the intensity of the network effect and the ratio of customer types affect competition among firms. Our analysis reveals that The equilibrium price increases with the number of type P, and the maximum revenue of both firms increases accordingly. The higher the intensity difference between positive and negative network effects, the higher the profit of the firm with good quality increases and that of the firm with poor quality decreases. Furthermore, comparing the case where both firms jointly determine prices and the competitive situation, we show that when the ratio of type N and the negative network effect are strong, the profit from competition exceeds that of joint.",Optimal Pricing with Customer Heterogeneity in Duopoly Markets - Insights from Network Effects,"[51011, 78001]",698,"[124, 0]",2635,Pricing Strategies,11,9,59,Pricing and Revenue Management,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S08 [building - 101],['Revenue Management and Pricing'],TC-59
"In large-scale projects, such as transport infrastructure developments, the cost planning phase stands out as one of the most crucial for the project success. Cost and time overruns, during the construction phase, are often the cause of project failure. A meticulous, conscious, and accurate ex-ante analysis of cost and time assessment can greatly contribute to the efficient and effective completion of a project. Various approaches have been developed to estimate and mitigate cost overruns. Such mitigation is subject to careful analysis of project risks, encompassing construction, environmental, social acceptance, and market risks. In this contribution, we propose the implementation of the risk assessment tool suggested by the National Anti-Corruption Authority [ANAC] using fuzzy logic to enhance its effectiveness. This application can provide public authorities with a tool to assess expected risk during the design and implementation phases of the project. By associating probability matrices of occurrence and expected impact with fuzzy prioritizations, this tool can facilitate risk assessment and management during the design and construction phases as well as consensus creation process.",A Fuzzy Logic Application to Manage Costs in Large-Scale Projects,"[48052, 10666]",906,"[6, 26, 49]",2637,"How to support complex decisions. Negotiating the trade-off between Social, Environmental and Economic values 3",44,7,47,Multiple Criteria Decision Analysis,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,50 [building - 324],"['Analytic Hierarchy Process', 'Decision Support Systems', 'Fuzzy Sets and Systems']",TA-47
"Portfolio optimization has a mixed reputation among investment managers, with some being so skeptical that they believe it is almost useless due to the inherent parameter uncertainty. It is undeniable that portfolio optimization problems are sensitive to parameter estimates, especially the expected returns that are arguably also the hardest parameters to estimate. However, most practitioners still attempt to build mean-risk optimal portfolios, albeit in implicit ways. Resampled optimization is a popular mathematical heuristic to tackle the parameter uncertainty issue. It computes optimal portfolios using sampled parameter estimates and calculates a simple average of the portfolio exposures across samples. The unsatisfactory aspect of the resampled approach is that there is no mathematical justification for using the average of portfolio exposures, it just works well in practice. This article provides perspectives for understanding the resampling approach by analyzing the portfolio exposure estimation process from a bias-variance trade-off. We show that the traditional resampled optimization corresponds to a naive version of stacked generalization. Finally, we introduce a stacked generalization approach that can be used to handle both parameter uncertainty and combine optimization methods in full generality. We coin the new method Exposure Stacking.",Portfolio Optimization and Parameter Uncertainty,[77670],372,"[126, 21, 45]",2638,Advanced Options Strategies Using O.R. and Machine Learning,4,9,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S14 [building - 101],"['Risk Analysis and Management', 'Convex Optimization', 'Financial Modelling']",TC-63
"In this study, we examine price volatility among energy companies listed on European capital markets during an energy crisis. Our objective is to explore how these firms navigate turbulent market conditions, offering insights into price fluctuation dynamics within the challenging energy sector landscape. We analyze 57 companies listed on stock exchanges across Austria, Belgium, Czech Republic, Finland, France, Germany, Hungary, Italy, Norway, Poland, Portugal, Romania, Spain, and Switzerland from June 2021 to December 2021. Our findings indicate that incorporating informed trading activity into volatility forecasts can be advantageous, particularly in high-volatility scenarios within the energy market.
",Predicting Realized Volatility - Insights from EU Energy Listed Firms During Crises,"[57997, 55475]",511,"[33, 25, 37]",2641,Innovations in Digital Assets - IDA,17,14,31,Analytics,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,54 [building - 208],"['Economic Modeling', 'Decision Analysis', 'Energy Policy and Planning']",WC-31
"Optimization algorithms contain parameters that greatly influence their behavior, such that finding good parameters with automated algorithm configuration [AC] tools has become a critical component in the algorithm design process. Optimization algorithms often possess the anytime property, meaning they can be stopped at any time during their execution and provide a feasible solution. However such algorithms are not specifically targeted by current AC techniques. Setting the parameters of anytime algorithms is especially difficult, as the parameters ought to provide robust performance across varying execution times. Traditional AC methods address this challenge by finding a one-size-fits-all parameter configuration, however finding a schedule of configurations, each targeted to a different runtime, can lead to better overall performance. We introduce a novel AC method for configuring anytime algorithms that produces viable configuration schedules that assign different configurations to different runtimes. Our proposed method harnesses an early termination mechanism for unpromising configurations using a machine learning model, and uses two novel MIP formulations to discard configurations and to create the configuration schedule, respectively. The resulting schedules can be used by decision makers to choose a suitable configuration given a specific runtime for an anytime target algorithm.",Anytime algorithm configuration,"[71156, 27003]",730,"[66, 109]",2644,Optimization and Machine Learning - Methodological Advances,14,9,03,Data Science Meets Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,1005 [building - 202],"['Machine Learning', 'Programming, Integer']",TC-03
"Forecasting and assortment planning for fashion products have been a challenging area in Operations Research due to different factors, particularly the high percentage of new products that lack any historical sales data. This problem gets more interesting when we incorporate the possibility that products can be ordered and fulfilled in different channels – what we commonly call as Omnichannel. Our study focuses on assortment optimization for products in fashion retail across multiple channels in omnichannel retail. The demand for a combination of attributes is predicted from the existing sales data. Then, from a universe of feasible attribute combinations available across channels - both online and offline - under asymmetric integration, we identify the right mix of products for each channel. We model this decision as a profit maximization function considering predicted sales revenue and the fulfilment cost for different products across multiple channels.",Assortment Optimization for Fashion Products in Omnichannel Retail,"[77998, 78414]",934,"[138, 0]",2645,"Online, Omnichannel, and Pricing",30,15,61,Retail Operations,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S10 [building - 101],['Supply Chain Management'],WD-61
"Mexico like other developing countries has been battling low outcomes in innovation, technology, and entrepreneurship projects and policies, to understand that phenomenon a multi-level and network density analyses based on a literature review of innovation systems models, in order to analyze how many levels and agents are interacting in these models as well as the relationships between agents. The innovation systems analyzed are the National Innovation System, Regional Innovation System, Triple Helix, Quintuple Helix, Innovation Ecosystem, Entrepreneurship Ecosystem, National Innovation Ecosystem, and Regional Innovation Ecosystem. The results show that for all models only two levels are operating, one government level and the institutional level, and therefore not all levels are articulated; the number of agents considered by each model is relevant but not determinant of the model’s quality; network density better reflects the conceptualization of each model; Entrepreneurship and Innovation Ecosystems models have higher network density, and therefore are the recommended ones to be used by planners, strategists, and decision-makers in order to obtain better outcomes; the most likely path to follow by the innovation systems literature and researchers is complex adaptive systems.","Innovation, Technology, and Entrepreneurship Low Development - a Literature Review based Multi-level and Network Density Analysis.","[74672, 37306]",221,"[15, 140, 53]",2646,"AI in Knowledge, Technology, and Innovation ",54,15,08,"Knowledge, Technology, and Innovation","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,1020 [building - 202],"['Complex Societal Problems', 'System Dynamics and Theory', 'Graphs and Networks']",WD-08
"A rental platform offering customised reusable transport items [RTIs] grapples with placement challenges impacting RTI availability and usability. Notably, RTIs often stay idle in customer warehouses, adding no value. Thus, incentivising the business customers’ sharing and optimising the platform placement decision are worth further research, but have not been well discussed in previous literature. Therefore, this study introduces an optimisation decision framework aimed at encouraging B2B sharing and refining RTI placement and customer incentives, innovatively using empirical and optimisation models. A quantitative model evaluates business customers’ willingness to share [WTS] under varied incentive schemes and RTI needs. Incorporating WTS, we develop a bi-objective optimisation model with a large-scale heuristic algorithm to determine optimal RTI purchasing and sharing incentives pricing. Our real-case analysis illustrates that an RTI-sharing strategy reduces operational costs by 18.5% and boosts customer welfare, achieving a win-win scenario. Additionally, our findings suggest the platform's strategies on sharing incentives and resource procurement should adapt based on customer WTS preference, RTI reusability, and holding durations. This research could extend to similar rental networks or sharing platforms, underlining the significance of strategic resource placement and sharing incentives.",Incentivising-sharing-enabled optimal placement strategy on a reusable transport item rental platform ,"[76085, 62173, 78012]",921,"[10, 77, 25]",2649,Policy and legislation for a circular economy,18,7,23,"Circular Economy, Remanufacturing and Recycling ","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,82 [building - 116],"['Behavioural OR', 'Multi-Objective Decision Making', 'Decision Analysis']",TA-23
"The Indian outstation tourism sector, valued at $66 billion, sees substantial bus travel [50% of outstation journeys]. The online bus market, currently at 5% [$1.2 billion], is projected to reach 9.5% by FY25. One of the driving factors for this growth is the shift in customer behavior. Hence, our research focuses on studying customer behavior. The research objective is to evaluate the applicability of the choice model in predicting bus booking patterns and further solve discrete dynamic pricing problem. The primary hurdle is constructing an appropriate choice model, which involves various options that a customer contemplates while making their decision and identifying relevant attributes influencing their choices. To model choice behavior, the widely used Multinomial Logit model is adopted. The second challenge lies in the incomplete available data, with access only to booking records and no insight into browsing behaviors. To address this, we utilize an Expectation-Maximization [EM] method. In the first stage, we apply this estimation approach to simulated data, and then to the real bus booking data. The accuracy of our estimation is verified through calculations of asymptotic standard errors and goodness-of-fit tests, comparing the observed and expected booking counts predicted by our model. The estimates are then used to solve the discrete dynamic pricing problem of the choice-based network RM model.",Estimating customer choice model and discrete dynamic pricing in the Online Bus Market ,"[77999, 64539]",414,"[124, 0]",2651,Pricing and learning 2,11,4,59,Pricing and Revenue Management,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S08 [building - 101],['Revenue Management and Pricing'],MC-59
"In this study, we focus on Fujita-Ogawa [FO] model, which is known to be one of equilibrium models to analyze the agglomeration patterns of cities. FO model has a scheme of population game, in which the numbers of firms and households for each location serve as variables, and their distance-decay interactive benefits and commuting costs serve as payoffs. In this study, we assume the heterogenous attribute for each firm, and formulate its equilibrium by means of complementary conditions. We also calculate some equilibrium solutions in a graphical manner.",Complementarity problem formulation of Fujita-Ogawa model with heterogeneous players,[24044],549,"[19, 33, 84]",2652,Recent advances on Variational Inequalities and Equilibrium Problems II,51,14,43,Variational Inequalities and Equilibrium Problems - From Theoretical Advances to Real World Applications,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,99 [building - 306],"['Continuous Optimization', 'Economic Modeling', 'Optimization Modeling']",WC-43
"We suggest a dynamic pricing model for selling empty seats - seat reservations for extra space in addition to regular reservations. Such extra space tickets share the main product’s resources and are a significant revenue-generating opportunity when coaches, trains, or airplanes frequently depart with many empty seats. We formulate the problem of a transportation company that sells tickets in the same compartment [1] without a seat reservation, [2] with a seat reservation, and [3] with a seat reservation and extra space as a Markov decision process. To address the resulting curse of dimensionality, we follow two approaches. First, we use approximate linear programming [ALP] with an affine value function approximation. We solve the resulting linear problem via a column generation algorithm. The result can be shown to provide both a tighter bound than previously known as well as a simple heuristic for the control problem. To allow for more general basis functions and possibly good value function approximations and corresponding heuristics, we also discuss simulation-based approximate dynamic programming [sb-ADP]. In a numerical study, we show that our sb-ADP heuristic typically obtains expected revenues close to our theoretical upper bound and better than a simple ALP-based policy. The quality of the sb-ADP, however, highly depends on the allotted computation time. For increasingly complex customer preferences, the time required for strong performance can be prohibitive.
",Dynamic Pricing of Empty Seats in the Transportation Industry under the Nested Logit Model,"[60093, 25676, 22994]",697,"[124, 16, 108]",2655,Pricing and Capacity Management,11,8,59,Pricing and Revenue Management,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S08 [building - 101],"['Revenue Management and Pricing', 'Complexity and Approximation', 'Programming, Dynamic']",TB-59
"The matrix spectral norm and nuclear norm appear in enormous applications. The generalization of these norms to higher-order tensors is becoming increasingly important, but unfortunately they are NP-hard to compute or even approximate. Although the two norms are dual to each other, the best-known approximation bound achieved by polynomial-time algorithms for the tensor nuclear norm is worse than that for the tensor spectral norm. We bridge this gap by proposing deterministic algorithms with the best bound for both tensor norms. The main idea is to construct a selection of unit vectors that can approximately represent the unit sphere, in other words, a collection of spherical caps to cover the sphere. For this purpose, we explicitly construct several collections of spherical caps for sphere covering with adjustable parameters for different levels of approximations and cardinalities. These readily available constructions are of independent interest, as they provide a powerful tool for various decision-making problems on spheres and related problems. Our results are generalized to the ℓp-sphere covering and approximating spectral and nuclear p-norms.",Sphere covering and approximating tensor norms,[42330],917,"[16, 19, 21]",2656,New Algorithms for Nonlinear Optimization,84,7,34,Advances in large scale nonlinear optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,43 [building - 303A],"['Complexity and Approximation', 'Continuous Optimization', 'Convex Optimization']",TA-34
"
Participating in sports events has a rich history, from the ancient Olympics to today's global mega-events. In real-world scenarios, tourists attending sports events also desire to visit attractions during their stay. Tourist planning systems often integrate road and pedestrian networks to minimize transportation costs and greenhouse gas [GHG] emissions. Indeed, reducing annual tourism carbon emissions is a challenging objective due to infrastructure lock-in effects. In our research, tourists move to various Points of Interest [PoIs] or sports events, either by walking or driving, according to their preferred periods, considering time windows, and then are picked up from nearby locations. This variant of the delivery and pickup problem allows tourists to switch between walking and driving in different periods. Our contribution aims to develop a decision support system for tourists attending sports mega-events, offering opportunities to explore PoIs and participate in sports events. We present a mixed-integer linear programming formulation for the multi-period green delivery and pickup problem with time windows, aiming to maximize profit and minimize GHG emissions. We solve this problem using a new Hybrid ALNS algorithm and evaluate its performance against NSGA-II on real tourism data for Paris, focusing on the 2024 Paris Olympics. Finally, we offer insights for tourist agency managers through various analyses.
",Developing a Sustainable Walk-and-Drive Tourism Recommendation System for Sports Mega-Events,"[78010, 78023]",217,"[111, 145, 139]",2658,Routing for hybrid fleets of vehicles,64,10,26,Combinatorial Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,012 [building - 208],"['Programming, Mixed-Integer', 'Vehicle Routing', 'Sustainable Development']",TD-26
"Raman Spectroscopy is a technique based on the inelastic scattering of a monochromatic light beam used to observe low-frequency modes in a target molecular system. The scattering pattern can be viewed as a sort of ”fingerprint” that encodes information about the chemical composition of a given target, including presence, concentrations, and interactions among molecules.
Recently, Raman spectra of biofluids have been used for medical diagnosis, and Machine Learning and Deep Learning models have been proposed for the development of noninvasive tests for early diagnosis.
However, the lack of interpretability of these models makes decoding and interpreting the Raman spectral fingerprint a challenging task.
In this work, we explore the application of eXplainable Artificial Intelligence [XAI] methods to extract explanations aimed at the possible identification of new biomarkers. 
Among the possible approaches, feature attribution methods seem to be the most promising for analyzing spectroscopic data, and we will focus our attention on Shapley value methods based on cooperative game theory. The computational results of a comparative analysis will be presented and discussed.",Shapley value analysis for revealing molecular signature in Raman Spectra,"[78013, 23990, 35427]",648,"[66, 73, 50]",2659,Feature attribution and selection for XAI,15,10,27,Mathematical Optimization for XAI,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,047 [building - 208],"['Machine Learning', 'Medical Applications', 'Game Theory']",TD-27
"The operation and services of logistics companies can be heavily disrupted due to disasters and crises, requiring them to be more resilient. Digitisation innovation has advanced considerably in the last decade and motivated supply chain resilience [SCR]. Given the high level of socialised services and strong conductivity and linkage roles of logistics firms, the impact of any supply chain disruptive events on the logistics industry will be transmitted and significantly affect the production and operation of upstream and downstream enterprises such as manufacturing and retailer companies However, it is still unclear how logistics firms systematically facilitated SCR by digital innovation. This study identifies digital innovation as exploratory and exploitative digital innovation. Combined with factors of resourcefulness and dynamic capability, this study applied Fuzzy-set Qualitative Comparative Analysis [fsQCA] and Artificial Neural Network to analyse the digital innovation configuration effect on supply chain resilience. Data from 167 logistics firms reveals three digital innovation model designs, each with its performance implications. Moreover, this study found that exploratory digital innovation is the core condition for motivating SCR. It provides researchers with a new line of inquiry and strategy suggestions on the role of digital innovation in SCR.",Analysis of digital innovation configuration effect on supply chain resilience：case of logistics firms,"[77694, 62173]",221,"[138, 30, 126]",2661,"AI in Knowledge, Technology, and Innovation ",54,15,08,"Knowledge, Technology, and Innovation","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,1020 [building - 202],"['Supply Chain Management', 'Disaster and Crisis Management', 'Risk Analysis and Management']",WD-08
"In this talk, we address the problem of learning multivariate distribution from empirical data aimed at scenario generation. Capturing the complex spatio-temporal relationship among multiple variables is a challenging task. Copula based models are usually applied in this context for their ability to separate the multivariate structure from its marginal distributions. However, when considering real case studies choosing the right family of copula models may be difficult.
To overcome these difficulties, in this work, we propose a data-driven [or model-free] approach by adopting generative methods. In particular, we explore the use of Variational Auto-encordes and Generative adversarial Networks, for learning the multi-variate joint probability distribution of link speeds on a road network, using real sensor data.
Experimental results, conducted on three distinct benchmark datasets, highlight the potential of the proposed model in generating new scenario samples of multivariate variables hat preserve correlations among variables, while producing samples that faithfully represent the empirical marginal distributions.
",Variational Auto-Encoders and Generative Adversarial Networks for scenario generation,"[23990, 78086, 78016]",273,"[66, 7, 117]",2662,Urban Logistics and sustainable TRAnsportation - OPtimization under uncertainTY and MAchine Learning,49,4,35,"Stochastic, Robust and Distributionally Robust Optimization","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,44 [building - 303A],"['Machine Learning', 'Analytics and Data Science', 'Programming, Stochastic']",MC-35
"The Location Routing Problem arises in many logistic settings where strategic and operational decisions are combined to improve effectiveness.
We study a two-stage Robust Location Routing Problem [2SRLRP], where, in the first stage, the decision maker determines the set of depots to open based on their fixed costs, and, in the second stage, a Team Orienteering Problem is solved, managing uncertain demands in a Robust Optimization [RO] fashion.
To the best of our knowledge, the most recent work where a variant similar to this problem is solved is [Sen Huang, Kanglin Liu, Zhi-Hai Zhang, Column-and-constraint-generation-based approach to a robust reverse logistic network design for bike sharing, Transportation Research Part B, 2023]. The problem studied there arises from the bike-sharing setting, where faulty bikes must be recollected and delivered to recycling centers, considering uncertainty in the demands; the problem was solved with a column-and-constraint-generation-based approach applied to a set-partitioning reformulation of the problem. 
In our work, we devise different reformulations of the 2SRLRP where various uncertainty sets are considered, affecting the objective function only. Traditionally, this class of problems is solved using column-and-constraint generation algorithms. We propose alternative methods exploiting dualization and tailored cut-generation techniques. Preliminary results are promising.",Exact Algorithms for the Two-Stage Robust Location Routing Problem,"[73393, 76660, 51181, 19719]",741,"[127, 145, 64]",2665,Vehicle Routing Under Uncertainty 1,5,5,64,VeRoLog - Vehicle Routing and Logistics,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S16 [building - 101],"['Robust Optimization', 'Vehicle Routing', 'Location']",MD-64
"Impatience of customers plays a crucial role in modelling service systems. In many situations, any arriving customer is informed about the current congestion level. In this way they can obtain a sense of their prospective waiting time, based on which they can decide whether or not to join. If this waiting time exceeds their patience threshold, they balk; otherwise they join. In this talk we discuss queues with impatient customers, by in particular considering a relevant related decision problem - how to set a fixed admission price so as to maximize revenues? We discuss approaches based on applying existing techniques such as Stochastic Gradient Descent via Infinitesimal Perturbation Analysis and Stochastic Approximation methods such as the Kiefer-Wolfowitz algorithm.",Learning and pricing in queues with unobserved balking,"[77184, 68609, 68596]",884,"[121, 135, 131]",2666,Queueing Models with Strategic Customers,47,7,40,Advances in Stochastic Modelling and Learning Methods,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,96 [building - 306],"['Queuing Systems', 'Stochastic Models', 'Simulation']",TA-40
"Simulating physical dynamics to solve hard combinatorial optimization has proven effective for medium- to large-scale problems. The dynamics of such systems is continuous, with no guarantee of finding optimal solutions of the original discrete problem. We investigate the open question of when simulated physical solvers solve discrete optimizations correctly. Having established the existence of an exact mapping between CIM dynamics and discrete Ising optimization, we report two fundamentally distinct bifurcation behaviors of the Ising dynamics at the first bifurcation point - either all nodal states simultaneously deviate from zero [synchronized bifurcation] or undergo a cascade of such deviations [retarded bifurcation]. For synchronized bifurcation, we prove that when the nodal states are uniformly bounded away from the origin, they contain sufficient information for exactly solving the Ising problem. When the exact mapping conditions are violated, subsequent bifurcations become necessary and often cause slow convergence. Inspired by those findings, we devise a trapping-and-correction [TAC] technique to accelerate dynamics-based Ising solvers, including CIMs and simulated bifurcation. TAC takes advantage of early bifurcated “trapped nodes” which maintain their sign throughout the Ising dynamics to reduce computation time effectively. Using problem instances from open benchmark and random Ising models, we validate the superior convergence and accuracy of TAC.",Bifurcation behaviors shape how continuous physical dynamics solves discrete Ising optimization,[77465],297,"[31, 63, 14]",2667,Advances in Optimization for Industrial Applications,64,12,29,Combinatorial Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,157 [building - 208],"['Dynamical Systems', 'Large Scale Optimization', 'Combinatorial Optimization']",WA-29
"Public authorities use policies to encourage freight forwarders to shift towards more sustainable modes of transportation. In an integrated freight and passenger transportation setting, freight forwarders can choose between unimodal [truck] or multimodal [truck and scheduled train/barge] transport, minimizing their operational costs. To promote more sustainable transport, public authorities levy taxes on road transport and subsidize scheduled services. In this study, we propose a bi-level model where the authority sets road-access taxes and rail/water-access charges while freight forwarders select the modes of transportation. Due to the inherent computational complexities, we solve the problem using a bi-section algorithm for the upper level and Adaptive Large Neighbourhood Search for the freight-forwarder's lower level problem. We find that it is usually optimal to make the scheduled line free and to set the tax sufficiently high to stay within a predefined budget. However, increasing subsidies and taxes gives marginal returns on modal shift and mileage reduction. Due to decreased routing efficiency, a budget-neutral policy results in considerable cost increases for the freight forwarders. The policy maker can allocate a budget to keep tax at an acceptable level while still promoting modal shift.   ",Policy Setting for Integrated Freight and Passenger Transportation,"[71086, 70471, 53556, 55094]",763,"[145, 50, 74]",2668,Public Transport,5,8,58,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S07 [building - 101],"['Vehicle Routing', 'Game Theory', 'Metaheuristics']",TB-58
"We investigate the possibility to unify the different approaches considered in the framework of vector optimization with variable ordering structures. As consequence, we analyze the incompatibility principle between optimality and openness and deduce new optimality conditions in this context.",Vector optimization with variable ordering structure - a unifying approach,[35782],49,"[112, 19, 81]",2671,Vector and Set Optimization I,33,2,41,Vector and Set Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,97 [building - 306],"['Programming, Multi-Objective', 'Continuous Optimization', 'Non-smooth Optimization']",MA-41
"Finding a timely and minimum cost truck schedule to transport packages closer to the customer through the Amazon Middle Mile Network is a big challenge. We breakdown it down into steps, where here we will focus in the combinatorial problem of deciding the equipment [truck and trailer] type, their departure time and the commodity demand mix carried, while the fulfilment paths and commodity demand are predetermined. We model the middle mile network as a time expanded graph, where nodes describe facilities, as well as their inbound and outbound, and arcs are either fixed and capacitated according to the availability of a shared resource, or schedulable connecting different facilities, with different options of trailer capacities and costs. Since a commodity will flow through a predetermined sequence of sites, each commodity traverses a very limited fraction of the network. We call this problem variation Capacitate Fixed-Charge Network Design for Multicommodity Flow Problem with fixed arcs and sparse commodities. We present a scalable integer linear-programming based matheuristic for the problem and show how many existing results from the broad network design literature can be adapted to the problem we are solving.",Capacitated Network Design for Multicommodity Flow problems with Sparse Commodities,"[74761, 78022]",290,"[63, 14, 143]",2676,Scheduling and sustainability,92,13,57,Optimization at Amazon,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S06 [building - 101],"['Large Scale Optimization', 'Combinatorial Optimization', 'Transportation']",WB-57
"In the past, urban parcel delivery was dominated by diesel-powered vans. However, with society’s growing awareness of sustainability, parcel delivery services are now encouraged to adopt innovative and environmentally friendly systems. As a result, cargo bikes, robots, and drones are investigated as potential alternatives. But relying on a single delivery system may not always be practical, as not all customer locations are suitable for these new technologies. However, it could prove beneficial to combine them with the traditional method to ensure an efficient and cost-effective delivery process. To address these issues, a mathematical optimization model for managing urban parcel logistics with two echelons is proposed, which can be used both independently and in combination with alternative means of transportation. An Adaptive Large Neighborhood Search is presented to analyze realistic urban delivery areas, and insights into its performance and the practicality of integrating combined fleets are provided. A case study from Hamburg, Germany, shows that labor costs are a key factor influencing the effectiveness of delivery vehicles. In addition, the strategic value of integrating robots and drones into delivery systems becomes clear, even if only a proportion of customers opt for these new means of transportation. These findings provide valuable recommendations for stakeholders considering the introduction of innovative parcel delivery systems tailored to urban environments.",Optimizing Urban Parcel Logistics - An ALNS-based approach to combine tradition and technology,"[69272, 13086]",627,"[65, 145, 74]",2678,Freight transportation and logistic II,6,9,55,Transportation,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S02 [building - 101],"['Logistics', 'Vehicle Routing', 'Metaheuristics']",TC-55
"Traditionally, wholesale electricity prices were driven by fossil power plants' fuel costs. Optimising dispatch in five scenarios of fully decarbonised, sector-coupled European power systems, we show how future electricity prices continue to be driven by fuel prices. In contrast to today’s market, their impact can also result from opportunity costs of cross-sectoral demand-side technologies. Particularly, we find that electrolysers become the main price-setting technology in Europe in the presence of a significant domestic hydrogen demand and abundant low-variable-cost electricity generation. Consequently, future electricity prices highly depend on these uncertain parameters. While flexible electricity consumption is able to adapt to varying price levels, investors in solar and wind power are similarly exposed to price risks as inflexible electricity consumers. Therefore, they could mutually benefit from price-mitigating instruments, such as Contracts for Difference. In the hydrogen sector, producers and consumers do not share such a common interest as hydrogen consumers’ final energy consumption costs are highly uncertain, but electrolysers robustly recover their investment costs across all scenarios and countries.

This work has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement no. 864276.",Price formation and intersectoral distributional effects in a fully decarbonized European electricity market,"[77565, 62349, 58607, 33470]",403,"[36, 93, 37]",2679,New Market Designs & Models for 100% Renewable Power Systems,22,4,09,Energy Markets,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,10 [building - 116],"['Electricity Markets', 'OR in Energy', 'Energy Policy and Planning']",MC-09
"The basic idea of Federated Machine Learning is to collect models rather than data centrally. We study the feasibility and the potential of the rather young research area, which has been proposed by Google in 2016, for digital Covid-19 diagnosis based on German multicenter data of 3,670 patients. Therefore, we compare Federated Machine Learning to traditional testing methods such as antigen or PCR [Polymerase chain reaction] tests on economical and operational dimensions. We aim to inform essential decisions regarding the choice of diagnostic methodology during the progression of a pandemic. Accordingly, we run a time dependent simulation for Federated Machine Learning to digitally diagnose Covid-19 and find a significant potential. The federated deep learning model with six clients and full access to all datapoints achieves an F1-score of 89.9 percent. For comparison, the centrally trained model reaches up to 92.5 percent. Our results highlight the potential of applying Federated Machine Learning to Covid-19 diagnosis. The study might therefore function as a benchmark for hospital managers to contribute to future research while maintaining governance of their data.",Enhancing Pandemic Evolution - A Simulation Modeling Study Utilizing German Multicenter Data to Unveil the Value of Federated Machine Learning ,[77708],595,"[66, 56, 67]",2681,COVID-19,3,13,15,OR in Health Services [ORAHS],"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,18 [building - 116],"['Machine Learning', 'Health Care', 'Management Information Systems']",WB-15
"In this work, we introduce a bi-objective Vehicle Routing Problem that minimizes both the routing cost and the impact on citizens evaluated  in terms of pollution, noise, and safety [social impact]. To measure the latter we consider the number of traversals multiplied by the population living on each street. Formulated on a multi-graph, the problem considers different paths between each pair of nodes. The two objective functions are solved lexicographically. First, the routing cost is minimized, then the social impact is optimized by allowing a predefined degradation of the routing cost. Several K-Sum compact formulations as described in [1] and [2] in addition to a non compact one are tested and compared.  A comprehensive sensitivity analysis is also conducted on multiple realistic instances, considering different k values.
References
[1] Ogryczak, Wlodzimierz, and Arie Tamir. “Minimizing the sum of the k largest functions in linear time.” Information Processing Letters 85.3 [2003] - 117-122. 
[2] Blanco, Victor, Justo Puerto, and Safae El Haj Ben Ali. “Revisiting several problems and algorithms in continuous location with ℓ τ norms.” Computational Optimization and Applications 58.3 [2014] - 563-595.",Last-Mile Logistics - Comparing Alternative Formulations of a Bi-Objective Vehicle Routing Problem Based on K-Sum Minimization,"[77518, 67738, 5876, 68095]",51,"[77, 72, 145]",2682,Multiobjective Combinatorial Optimization,34,12,37,Multiobjective Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,33 [building - 306],"['Multi-Objective Decision Making', 'Mathematical Programming', 'Vehicle Routing']",WA-37
"In the scope of the EMBRACER Project, the study explores integration of connectivity and automation in transport, emphasizing the challenges of managing the coexistence of conventional vehicles and connected and automated vehicles [CAVs]. Optimising traffic flow in a mixed environment [different vehicle classes, automation levels, propulsion technologies, and kinematics] is crucial to realize the full potential of CAVs. Intercity corridors, with parallel routes, offer a controlled environment to examine the complex interplay of traffic flow, demand, network capacity, and demand management strategies.
The study explores how optimising traffic distribution across parallel inter-urban routes can reduce travel time and environmental impacts, considering current traffic scenario and projecting effects of a 30% penetration of electric CAVs. The study employs field data [traffic volume, fleet composition, travel time, GPS data], microscopic traffic modeling to simulate existing conditions and incremental demand, and VSP methodology for emissions modeling. External costs are monetized based on greenhouse gas emissions and air pollution, enabling multi-objective optimisation.
Results show a nonlinear impact of CAV integration, with benefits influenced by route characteristics and traffic demand. While widespread electric vehicle adoption significantly reduces emissions, optimised traffic distribution is essential for minimising traffic-related externalities in intercity corridors.
",Traffic Distribution for Sustainable Intercity Corridors - A Mixed Conventional and eCAV Scenario,"[63261, 78025, 40251]",352,"[131, 143, 40]",2684,Advancing mobility towards sustainable solutions I,6,9,56,Transportation,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S04 [building - 101],"['Simulation', 'Transportation', 'Environmental Management']",TC-56
"Conveyor belts are often used in conjunction with robotic arms for automated manufacturing processes. For moving and packing, the robotic arms automatically pick the items on the conveyor and place them in containers. This is however essentially a TSP, and efficiency thus requires a sophisticated control scheme. Increasing the look-ahead is a way to introduce more information to the system, however this might lead to a decreased throughput as the system waits for the perfect item and sub-par items run off or are recycled into the system. A key factor will be intelligently incorporating the look-ahead information in the constraints of the optimization problem, to limit the negative impact on throughput of the system while maintaining the benefits of the larger solution space. This paper proposes to extend the objective function to include the look-ahead and incorporating the associated time penalty in the problem constraints using a temporal-distance scoring metric. Furthermore, the decision cycle will be included in the problem formulation as it directly relates to the problems’ temporal nature. ",Information/Throughput Tradeoff Mitigation in Automated Sorting,"[77495, 78027]",66,"[69, 131, 77]",2685,Emerging Trends in Decision Analysis,45,5,45,Decision Support Systems,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,30 [building - 324],"['Manufacturing', 'Simulation', 'Multi-Objective Decision Making']",MD-45
"The increasing number of home deliveries paired with various delivery modes and channels require retailers to establish omnichannel networks, integrating stores for direct customer deliveries. This enables versatile shopping opportunities but also increases logistic complexity. The innovative truck-and-robot concept poses an alternative to address the resulting challenges. The concept relies on robots carried and released by trucks to serve customers in predefined time windows. We extend this concept by integrating stores and corresponding customer requests for direct supply from stores, resulting in a new concept that offers robots as a service to pick up and deliver goods to the customers.
The problem described is formalized as the Truck-and-Robot Pickup-and-Delivery Problem [TnR-PDP] and solved using a tailored solution approach, the Adaptive Genetic Algorithm [AGA]. The AGA is based on a recombination-based search framework but tailored to the problem-specifics [e.g., no or multiple visits per location] using specialized recombination operators and an adaptive search strategy for location and operator selection. Our numerical experiments show that our algorithm works efficiently, outperforming existing benchmark approaches concerning runtime by up to 83%, while improving the solution quality on benchmark instances. Furthermore, an in-depth analysis regarding store integration is applied to provide managerial insights and highlight the benefits of the proposed concept.",Robots at Your Service - Covering the Last-Mile in Omnichannel Retail by Robot,"[75228, 49049]",735,"[65, 145, 74]",2687,Routing Unmanned Aerial Vehicles 1,5,3,64,VeRoLog - Vehicle Routing and Logistics,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S16 [building - 101],"['Logistics', 'Vehicle Routing', 'Metaheuristics']",MB-64
"Time-varying causality tests robust to mild explosivity, bubbles and crashes are used to provide a new, evidence-based perspective on commodity financialization, where money flows associated with index investors are conjectured to predict changes in commodity futures prices. We show that Singleton’s [2014, Management Science] finding that money flows helped predict changes in crude oil futures prices is almost entirely attributable to the atypical behaviour of oil prices around the Global Financial Crisis, an aspect we show a standard Granger causality test applied over the whole sample is unable to pick up. Our approach helps settle what in the literature was an important but controversial finding. More generally, we corroborate some of the recent results by Gilbert [2018] suggesting there is some causality between index investment and non-ferrous metals and some agricultural commodities, albeit commodity-specific and in a milder form concentrated in the aftermath of the GFC.  The robustness of the test herein offers some clarity to an academic literature that has been settling against the efficacy and wider applicability of Singleton’s results but has not been persuasive enough to see the same views become pervasive among finance practitioners. ",The Role of Investor Sentiment in Commodity Price Behavior - Clarifying Evidence Based On Time-Varying Causality Tests,"[78029, 78032]",190,"[33, 45, 44]",2688,Natural Resource Management and Commodity Markets,4,4,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S14 [building - 101],"['Economic Modeling', 'Financial Modelling', 'Finance and Banking']",MC-63
"Food banks play a crucial role in facing food insecurity that affects millions of people worldwide. These organisations receive donations and distribute them to vulnerable families, usually through standardised food baskets. Food banks increasingly prioritise nutritional adequacy, food assortment diversity, and equitable product allocation. However, the food baskets may lack sufficient and balanced nutrients, and the family nutritional requirements, which depend on the sex and age of its members, are not considered in detail. In this study, we propose a multi-objective linear program to optimise the food allocation problem from food banks to beneficiaries. We employ a Thresholded Lexicographic Ordering approach to consider the following objectives, in order of importance - 1] maximise the nutritional coverage of the least covered nutrient-beneficiary pair, 2] maximise the variety of products in food baskets, and 3] minimise the total inequity. The model also considers the specific nutritional needs of the beneficiaries according to their sex and age, as well as the incompatibility between certain food products and beneficiaries due to dietary restrictions. We applied the model to a Catalan food bank case study, and our solutions led to significant improvements in all three objectives compared to the baseline solution.","Optimising Food Bank Operations - A Multi-Objective Approach for Improved Nutrition, Equity, and Variety","[77503, 71292, 71848, 71494, 71845]",911,"[58, 72, 77]",2689,"Assessment Methods for Shaping the Green, Inclusive, and Digital Cities II",79,3,18,Sustainable Cities,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,42 [building - 116],"['Humanitarian Applications', 'Mathematical Programming', 'Multi-Objective Decision Making']",MB-18
"Consistent solutions are vital in multi-period facility location problems, especially for long-term planning. In this work, we present various exact solution approaches 
on the nested p-center problem, a recently introduced variation of the well-known [vertex] p-center problem, which can be defined as follows - 
Given a set of locations and a finite time horizon, in each time period, a set of open facilities needs to be opened, where the set of a time period, needs to be a subset of 
the set of open facilities in the following period. The goal is to minimize the sum of the maximal distance between any customer and its nearest open facility over the time horizon.  

In our work, we present different mixed integer linear programming formulations on the nested p-center problem, which are solved using exact 
solution methods like Bender's Decomposition or branch-and-bound. Furthermore, we have developed a solution framework to increase the performance in terms 
of runtime on all formulations by adopting a diverse set of methods, like cut separation, lifted optimality cuts or preprocessing. 

The different formulations and methods used are analyzed in a computational study on benchmark 
instances, known from previous literature and the managerial implications of the nesting property are analyzed. 
",Exact solution approaches for the nested p-center problem,"[77501, 36277]",43,"[64, 111, 11]",2690,Advances in Location Analysis ,29,2,61,Locational Analysis,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S10 [building - 101],"['Location', 'Programming, Mixed-Integer', 'Branch and Cut']",MA-61
"Optimization under uncertainty has been a topic of large debate in Operations research [OR] and Machine Learning [ML] communities for a long time. Recently, many authors have turned to distributional robust optimization [DRO] in search of a solution; however, fewer studies have focused on enhancing fairness under uncertainty. This is of great interest in those applications of ML that can have a direct damaging impact on the population, credit scoring [CS] being one of the most potentially damaging fields according to regulatory bodies. 



The paper presented explores the effects of using a DRO-based logistic regression [LR], one of the most commonly used classifiers in CS, across multiple CS datasets. This study will show how robustness has a greater impact on fairness than the fairness constraint, and that the impact on performance is negligible and, in some datasets, positive. We will also provide an empirical analysis of the effect of the different hyperparameters that are unique to DRO-based LR. Furthermore, we will argue that the level of robustness narrows the dispersion of the probability of default distribution and that the parameters in charge of the ground metric have an unnoticeable impact. On a side note, we suggest traditional fairness metrics used in credit scoring are not best suited for the task.",A distributionally robust optimisation approach to fair credit scoring,"[76108, 78034, 10232]",211,"[7, 44, 66]",2693,Analytics and the link with stochastic dynamics III,17,9,31,Analytics,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,54 [building - 208],"['Analytics and Data Science', 'Finance and Banking', 'Machine Learning']",TC-31
"Plug-and-Play methods are obtained replacing with off-the-shelf denoisers, the proximal operator in many first order proximal optimization algorithms. Under suitable hypothesis, it is possible to derive the functional whose proximal operator corresponds to a particular class of denoisers, referred to as Gradient Step denoisers. This characterization result allows for the interpretation of Plug-and-Play schemes as the minimization of an underlying non-convex cost function, enabling the study of the convergence of such methods. In particular, this analysis can be further extended to ensure the convergence of several accelerated methods. The non-convexity of the cost function strongly limits any improvemnt in the theoretical convergence rate. However, the numerical experiments empirically demonstrate the benefits of this acceleration, that allows to reduce the computational demand and workload required to compute the reconstruction of an image.",Accelerating convergent Plug-and-Play methods,"[78031, 78045, 78046]",423,"[81, 66, 73]",2694,Optimization and learning for data science and imaging [Part III],84,4,34,Advances in large scale nonlinear optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,43 [building - 303A],"['Non-smooth Optimization', 'Machine Learning', 'Medical Applications']",MC-34
"Planning the cold storage and transport requirements throughout the fresh fruit supply chain is essential for decision makers, in order to preserve fruit quality and reduce fruit loss. This study presents a multi-period mixed integer linear programming [MILP] model for supporting cold storage and transport decisions of the fresh fruit supply chain, aiming to minimize total costs. For solving the proposed model, a hybrid solution method is developed, which combines a constructive heuristic algorithm and an exact method to build solutions incrementally, that is, period by period. A real case study from a Chilean fruit company is used for demonstrating the suitability of the hybrid solution method, which requires much lower computational time than the exact method for finding near optimal solutions [about 0.1% of the time required by the exact method]. The results show that the hybrid method finds solutions with gaps in objective function values ​​less than 2.0% compared to the objective function values ​​obtained by the exact method. These results provide more alternatives for planning cold storage and transportation requirements in the fresh fruit supply chain.",A Hybrid Solution Approach to Support Fresh Fruit Supply Chain Cold Storage and Transportation Decisions,"[74100, 12838, 58816, 243]",589,"[89, 111, 74]",2698,Agrifood supply chain decision problems,20,4,12,OR in Agriculture and Forestry ,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,13 [building - 116],"['OR in Agriculture', 'Programming, Mixed-Integer', 'Metaheuristics']",MC-12
"Nowadays, customers as well as retailers look for increased sustainability. Recommerce markets - which offer the opportunity to trade-in and resell used products - are constantly growing and help to use resources more efficiently. To manage the additional prices for the trade-in and the resell of used product versions challenges retailers as substitution and cannibalization effects have to be taken into account. An unknown customer behavior as well as competition with other merchants regarding both sales and buying back resources further increases the problem's complexity. Reinforcement learning [RL] algorithms offer the potential to deal with such tasks. However, before being applied in practice, self-learning algorithms need to be tested synthetically to examine whether and which work in different market scenarios. We evaluate and compare different state-of-the-art RL algorithms within a recommerce market simulation framework. We find that RL agents outperform rule-based benchmark strategies in duopoly and oligopoly scenarios. Further, we investigate the competition between RL agents via self-play and study how performance results are affected if more or less information is observable. Using an ablation study, we test the influence of various model parameters and infer managerial insights. Finally, to be able to apply self-learning agents in practice, we show how to calibrate synthetic test environments [digital twins] from data to be used for effective pre-training.",AI meets Sustainability - Using Digital Twins to Leverage Reinforcement Learning – Based Dynamic Pricing in Circular Markets under Competition,[48742],696,"[124, 7, 100]",2701,Learning and pricing,11,7,59,Pricing and Revenue Management,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S08 [building - 101],"['Revenue Management and Pricing', 'Analytics and Data Science', 'OR in Sustainability']",TA-59
"This study presents a novel approach to online retail sustainability, embedding eco-labeled products alongside conventional offerings to stimulate sustainable consumer behavior. Inspired by the EU eco-label strategy, our framework aims to empower consumers while promoting sustainable purchasing decisions. We examine the nexus between consumer utility, environmental awareness, and online retailers’ marketing efforts, strategically enhancing the visibility of eco-labeled products. Central to our analysis is the certification landscape, where retailers navigate between developing their own certifications and utilizing external standards. Employing game theory, we analyze retailers' optimal strategies under different certification policies. This research not only introduces a new paradigm in online retail sustainability but also provides strategic insights crucial for future sustainability initiatives in the digital marketplace.",Green Shopping Online - Boosting Eco-Friendly Choices Through Integrated Product Strategies,[73402],689,"[32, 50, 138]",2702,Sustainability in Consumer Systems & Industry,80,9,53,Sustainable and Resilient Systems,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,8007 [building - 202],"['E-Commerce', 'Game Theory', 'Supply Chain Management']",TC-53
"In the realm of Customer Relationship Management, customer retention is pivotal for sustained growth. The positive correlation between customer satisfaction, retention, and profitability guides traditional Customer Churn Prediction models to rank high-value customers lower on average than their low-value counterparts. Ranking based on churn propensity without considering the profitability of retaining specific customers may lead to suboptimal decision-making. This is especially pronounced in Business-to-Business [B2B] industries. Various profit-driven metrics and algorithms address this, but their integration and extension demands considerable effort, hindering widespread adoption.

This paper proposes a streamlined approach, incorporating pre-processing techniques inspired by fairness literature, including Massaging, Reweighting, and Resampling. These techniques rectify the underrepresentation of high-value customers within the targeted group, improving churn campaign profitability. Preliminary results on two B2B datasets show these techniques outperforming baselines.

The research contributes by proposing techniques to amplify the often-underrepresented high-value customer fraction within the targeted group. Framing the challenge as one involving the sensitivity of high-value customers provides a simpler interface for improving churn campaign profit on par with existing profit-driven techniques.
",Enhancing B2B customer retention - a streamlined profit-driven approach using fairness-inspired pre-processing techniques,"[71962, 67909, 56999]",235,"[7, 8, 66]",2704,Fairness and responsible AI,16,7,28,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,065 [building - 208],"['Analytics and Data Science', 'Artificial Intelligence', 'Machine Learning']",TA-28
"Identification of market abuse is an extremely complicated activity that requires the analysis of large and complex datasets. We propose an unsupervised machine learning method for contextual anomaly detection, which allows to support market surveillance aimed at identifying potential insider trading activities. This method lies in the reconstruction-based paradigm and employs principal component analysis and autoencoders as dimensionality reduction techniques. The only input of this method is the trading position of each investor active on the asset for which we have a price sensitive event [PSE]. After determining reconstruction errors related to the trading profiles, several conditions are imposed in order to identify investors whose behavior could be suspicious of insider trading related to the PSE. As a case study, we apply our method to investor resolved data of Italian stocks around takeover bids.",Dimensionality reduction techniques to support insider trading detection,"[78033, 78039, 78040, 78041, 78042, 78043]",713,"[66, 44]",2706,Experimental economics and game theory 3,73,15,40,Experimental economics and game theory,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,96 [building - 306],"['Machine Learning', 'Finance and Banking']",WD-40
"This paper considers how we recognise the impact of qualitative analysis within interventions. First, empirical data is used to illustrate the integration of qualitative and quantitative within the context of an everyday workplace, considering patterns of integration and their perceived effects. Second, the historical basis for valuing the integration of qualitative and quantitative is explored, considering the longer-term evolution and use of Soft OR and Problem Structuring Methods [PSMs]. Finally, I reflect upon the broader questions these observations raise about the relevance of qualitative analysis and how we promote this relevance within and beyond our immediate community. ",Empirical and historical explorations of integrating qualitative and quantitative analyses,[53756],129,"[149, 133, 55]",2707,Approaches for Integrating Quantitative Modelling,26,4,13,Soft OR and Problem Structuring Methods,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,15 [building - 116],"['Problem Structuring', 'Soft OR', 'Group Decision Making and Negotiation']",MC-13
"It is well known that Cournot games involving heterogeneous players with affine and decreasing demand functions and convex quadratic production cost functions can equivalently be cast as a mixed linear complementarity problem or a quadratic optimization problem [QP]. Market equilibria can thus be computed using a complementarity solver or, more efficiently, a convex optimization solver. When studying such games, it is common to aggregate the consumers and substitute their inverse demand function to eliminate the market price. We argue, however, that the market price should be treated as an explicit variable that is central to the game. We show that the problem can be reformulated as a fixed-point problem [FPP] for the price only, thereby greatly reducing its dimensionality. The FPP can therefore be solved even more efficiently than the QP and also yields a more accurate solution. The FPP can be generalized to spatial games and games involving multiple commodities. We envision that this could vastly improve solution efficiency and accuracy when incorporated into dynamic or multilevel games. We finally consider a Cournot game across the markets for two commodities that may be converted into each other. The FPP formulation reveals the structure of the equilibrium and allows an analytical stability and sensitivity analysis. We find that market imperfections in either of the markets extend to the opposite market and may significantly reduce the welfare gains from market coupling.",A New Strategy for Solving Cournot Games,"[72300, 36601]",175,"[50, 72, 5]",2708,"Energy sector coupling, optimization and equilibrium",23,3,21,OR in Energy,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,49 [building - 116],"['Game Theory', 'Mathematical Programming', 'Algorithms']",MB-21
"Integrated power and gas energy system optimization models [ESOMs] can be a valuable tool for planning decarbonized energy systems. In this context, pipeline gas transmission is a particularly complex problem due to the nonlinear and nonconvex relation of [bidirectional] gas flows and gas pressure and the pipeline’s capacity to be used for short-term gas storage [linepack]. One method to deal with this complexity is piecewise linearization, which implies formulating the ESOM as mixed-integer linear program [MILP]. In the literature, several general piecewise linearization methods with the desirable characteristics of tight formulations have been applied. However, these methods are not tailored to this specific problem and thus tightness is harmed. Against this background, we propose a novel piecewise linearization method for the pipeline gas transmission problem with linepack. By computing the tightest possible polyhedra equivalent to its linear systems of constraints, we show that the proposed formulation is tight per se while others are not. The resulting computational advantages are demonstrated in a case study of an integrated 24-bus power and 12-node gas system interlinked by gas-fired generators [based on discrete unit commitment decisions]. ",A Tight Approach on Piecewise Linear Pipeline Gas Flow Modeling with Linepack,"[76299, 61600, 56946, 40094]",397,"[93, 37]",2709,Decarbonized energy systems & markets,22,12,09,Energy Markets,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,10 [building - 116],"['OR in Energy', 'Energy Policy and Planning']",WA-09
"We present the congested facility location problem with partial coverage under the assumption of uncertain customer demand. First, we formulate the deterministic problem and its robust counterpart as mixed-integer quadratic problems. Second, since the size of the robust counterpart grows with the number of customers, which could be significant in real-world contexts, we investigate the use of Benders decomposition to effectively reduce the number of variables by projecting out of the master problem all the variables dependent on the number of customers. We discuss single-tree and multi-tree Benders approaches and introduce a perturbation technique to deal with the degeneracy of the Benders subproblem efficiently. Our tailored Benders approaches outperform the state-of-the-art solver Gurobi on adapted instances from the literature.",Benders decomposition for robust and congested partial set covering location,"[72098, 22042, 65962]",254,"[127, 5, 64]",2710,Mixed Integer Optimization I,64,7,52,Combinatorial Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,8003 [building - 202],"['Robust Optimization', 'Algorithms', 'Location']",TA-52
"This paper proposes robust type models of de novo programming [R-DNP] using cardinality-constrained robustness via interval-based and norm-based uncertainty sets. The R-DNP has not been researched, and we aim to fill this literature gap. In particular, we develop the robust counterpart of the weighted DNP [W-DNP], Chebyshev DNP [C-DNP], and extended DNP [E-DNP] models to consider different uncertainty sets and incorporate the decision makers’ preferences. Methodologically, the proposed approach extends the conventional DNP model to enable it to solve uncertain coefficients for each decision variable on the left-hand side of each objective function and on the total budget, overcoming a limitation of the current multi-criteria solution procedure of the DNP approach. The proposed methods provide decision makers more flexibility to express their level of conservatism and preferences by setting aspiration levels. The proposed method’s usefulness over the standard DNP is demonstrated by providing an illustrative example. Moreover, we validate the proposed formulations for solving real-world problems through a hypothetical application - optimizing the renewable portfolio for electricity generation in Morocco. The results confirm the proposed methodologies and show that they can assist decision makers in determining the optimal system design for sustainable electricity generation under uncertain conditions.",Robust De Novo Programming Under Different Uncertainty Sets and its Application to Renewable Energy Sector,[55438],116,"[77, 127, 93]",2711,MCDM for project portfolio problems,44,4,44,Multiple Criteria Decision Analysis,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,20 [building - 324],"['Multi-Objective Decision Making', 'Robust Optimization', 'OR in Energy']",MC-44
"Designing moulds for injection moulding is a decisive step given the complexity of some plastic parts produced by this process. The correct design principles must be followed to prevent the existence of defects that make the parts worthless. This includes short shots, flash, sink marks, warpage, weld lines and air traps, which can be avoided or minimized by the adoption of various measures, including optimizing the process parameters and using high-quality moulds. 
However, the presence of a high number of design variables related to the different phases of the process makes this a difficult task, principally because these variables are of different types - i] operating conditions; ii] mould design, including filling channels, number of gates and cavity; and iii] variables that depend on the machine used.
Therefore, the use of numerical modelling programs was usually adopted to obtain relevant information on the injection moulding process and to enable the engineer to increase the speed of the development process. Given the high quantity of data generated and the high number of decision variables and objectives involved more sophisticated data analysis is required. 
The objective of this work is to develop and implement data mining techniques and surrogate models based on Artificial Neural Networks to optimize the cooling phase. Different methodologies will be tested and the hyperparameters of the methods used will be assessed to select the best model to use.",Artificial intelligence techniques for designing conformal cooling channels for injection moulding using Moldex3D,"[45822, 78625, 78628, 78050]",215,"[38, 8, 112]",2713,Applications of Machine Learning in Optimization,64,10,25,Combinatorial Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,011 [building - 208],"['Engineering Optimization', 'Artificial Intelligence', 'Programming, Multi-Objective']",TD-25
"The Ecosystem Services [ES] refer to a decision-making condition in which the existence of many and different aspects [regulatory, provisioning and cultural] imposes a reciprocal trade-off rules that gives some greater legitimacy to others. Dealing with ES in a complex decision-making system encourages evaluations to achieve a short-medium-long-term equilibrium between ES supply and demand in the social, economic, and environmental context of reference.
This is especially exacerbate in the urban settings, where types guidelines of trade-offs alongside those of ecosystem services take action, e.g. in cases to design and program urban land use via Nature-based Solution [NbS]. 
It intends to present an integrated approach based on multi-criteria logics, with the goal of minimizing the ecosystem service supply and demand trade-off for Nbs allocation across urban areas. The approach's substance lies in the formalization of an optimization algorithm based on ES accounting and the AHP protocol, as well as the financial, economic, environmental, social, and cultural constraints that aid in the case-specific personalization of urban planning and environmental NbS design strategies. An description of the Italian case study, which was examined testing the proposed optimization algorithm, rounds up the presentation.",Optimising NbS Allocation to Negotiate the Trade-off Between Ecosystem Service Supply and Demand  in Urban Areas,[78006],158,"[84, 77, 100]",2714,"How to support complex decisions. Negotiating the trade-off between Social, Environmental and Economic values 1",44,4,47,Multiple Criteria Decision Analysis,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,50 [building - 324],"['Optimization Modeling', 'Multi-Objective Decision Making', 'OR in Sustainability']",MC-47
"Recent technological advancements have significantly impacted consumer behavior and the operational models of physical retail businesses. With the rise of data science and artificial intelligence, scholars are increasingly utilizing deep learning methods to predict consumer behavior in traditional channels. This study addresses a gap in the literature by focusing on predicting consumer brand choices based on electronic uniform invoice data. Specifically, it examines consumers purchasing cigarette brands in physical channels. Using Long Short-Term Memory [LSTM], the study predicts the cigarette brands consumers with smoking habits might choose in their next purchase. Five machine learning approaches are also employed for comparison. The results indicate that factors such as consumption time, location, and product prices significantly influence prediction accuracy. LSTM outperforms other models, demonstrating superior performance in predicting consumer brand choices. These findings provide valuable insights for channel operators to optimize marketing strategies, product offerings, and operational efficiency, ultimately enhancing market competitiveness. This study offers practical implications for cigarette companies and retailers in understanding and satisfying consumer needs in physical channels using electronic invoice data.",Predicting Electronic Invoice Consumer Product Brand Choices Using Deep Learning Methods,"[59157, 59123]",634,"[8, 47, 71]",2716,Retail Optimization,30,15,50,Retail Operations,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,M2 [building - 101],"['Artificial Intelligence', 'Forecasting', 'Marketing']",WD-50
"The EMBRACER Project focuses on improving the interconnection with urban and rural areas and achieve seamless intelligent climate-resilient regional and local intermodal mobility. Under this framework, digitalisation and traffic/transport data are essential to promote more sustainable mobility alternatives and satisfy citizens mobility needs. Data-driven knowledge allows for more informed and effective decision-making, both at a strategic and operational level. In this context, data analysis tools play a relevant role. We will focus on the potential of the Two-Step-SDP methodology for analysing vehicle dynamics data and gaining insights regarding traffic-related impacts. The technique is suitable for performing data clustering and dimensionality reduction, has shown superior performance to analogous methodologies, and it is implemented as an open-source tool. In particular, it considers the clustering problem of objects and attributes formulated as nonlinear SDP-based models and, due to their nature, it requires the use of Linear SDP relaxations and an approximation algorithm to obtain their solution. This methodology was applied to different sets of vehicle dynamics data and it revealed to enable a more detailed analysis of the traffic-related impacts. Analising traffic-related data allows policymakers to make evidence-based decisions to improve the sustainability of transport systems and can then, implement targeted interventions to more sustainable mobility solutions.",Enhanced analysis for more sustainable traffic systems through Two-Step-SDP,"[40251, 63259, 63261]",352,"[143, 139, 7]",2717,Advancing mobility towards sustainable solutions I,6,9,56,Transportation,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S04 [building - 101],"['Transportation', 'Sustainable Development', 'Analytics and Data Science']",TC-56
"With the proliferation of Distributed Energy Resources [DERs] in power grids, the transition to bidirectional power flow challenges traditional unidirectional energy systems. The uncertain nature of renewable DERs further escalates the complexity of energy management. Despite these challenges, DERs offer valuable energy and flexibility services to power grids. This study proposes a novel tri-level decentralized transactive energy market framework to address these complexities. In this framework, Distribution System Operators [DSOs] are positioned as key distribution-level local energy market coordinators and serve as crucial intermediaries, facilitating communication between Transmission System Operators [TSOs] and DERs through network-aware pricing and transactive control mechanisms. By allowing DER participation through optimized bidding strategies in local energy markets [and indirectly in wholesale markets via DSOs], the framework aims to optimize system operations while accounting for uncertainty through distributionally robust optimization methods. Thus, it presents a promising avenue for enhancing grid security and reliability through the proposed distributionally robust unit commitment and optimal power flow problems. By considering the dynamic interactions among TSOs, DSOs, and DERs, the proposed coordination approach aims to lay the groundwork for a more resilient and efficient energy system management with optimized grid operations.","Empowering Energy Exchange - A Tri-Level Decentralized Coordination Framework for TSOs, DSOs, and DERs","[61683, 58087, 37449]",472,"[36, 93, 84]",2718,Distributed energy systems,21,10,22,Energy Management,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,81 [building - 116],"['Electricity Markets', 'OR in Energy', 'Optimization Modeling']",TD-22
"In the globe, human and non-human systems are highly connected as complex networks. Complexity is understood as the property that is defined by the nature of interconnectedness, links and size of the network. As example, business and society are made of supply chain and societal community network. These networks are susceptible to attacks; we have considered two scenarios - [A] purposeful attack and [B] random attack. This study aims to investigate the robustness of complex networks during the attacks and how the complexity of networks changes under the attacks. And suggests algorithms and analytics to identify vulnerable complex networks. The application is shown on two different data sets for complex networks. The first is for the supply chain with materials flow and contractual relationships networks. The second is for community networks. Results and analysis show that some complex networks are less vulnerable to attacks, and some are more vulnerable. Further, the results show the characterisation of the robustness measure for different complex networks during attacks and the changes in the network property under the attacks. ",Characterisation of robustness measures for complex networks under attacks,"[77328, 78057]",539,"[15, 79, 5]",2720,"Advancements of OR-Analytics in Statistics, Machine Learning and Data Science 13",16,8,06,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,1013 [building - 202],"['Complex Societal Problems', 'Network Design', 'Algorithms']",TB-06
"In today's globalized world, supply chains have global characteristics as well. Instead of geographically separated supply chains, there exists a global single supply chain in the world. Because of this circumstance, disruptions even at geographically distant locations can have severe impacts on all businesses globally. This nature of business increases the importance of supply chain risk management [SCRM] more than ever. Manufacturing plants require accurate tools to measure and manage risks effectively and profitably. In this study, an enhancement to Material Requirements Planning [MRP] type production planning is proposed such that supply chain risk can be measured precisely and risk management policies can be developed accordingly. Initially, supply chain disruptions are defined and categorized and buffers used against these uncertainties are introduced to the MRP optimization model. With this new model and a risk function defined in terms of backlogs, an SCRM framework in the form of buffer management is proposed. Afterwards, the performance of the model with different settings is observed with simulation experiments and the statistical analysis is conducted on the data. Although the most significant buffering policy was found to be the supplier lead times, many findings were discovered on the effects of buffers on the supply chain [SC] risk and the profitability of the business.",Learning the effects of different buffering strategies in supply chain risk management - A regression approach,"[78044, 3769]",11,"[138, 126, 66]",2721,Sustainable Supply Chain Management,19,12,24,Sustainable Supply Chains,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,83 [building - 116],"['Supply Chain Management', 'Risk Analysis and Management', 'Machine Learning']",WA-24
"With telemedicine coming to the forefront during the COVID-19 pandemic, flexibility in terms of modes of care delivery has emerged. We are working with a partner within NHS Wales, TEC Cymru, who worked to rapidly implement video consultations within Wales during the pandemic and continue to promote the use of telemedicine in Wales. In this paper, we consider the scheduling of patients’ appointments via three different modes of delivery - traditional face-to-face, video conferencing platforms, and telephone. The solution of the model not only has an impact on satisfying patient and clinician preferences, but also could potentially reduce travel for patients and staff. We model the problem as a multi-mode resource constrained project scheduling problem, with the aim of maximising patient and clinician preferences for delivery method. Two model formulations are presented to solve this problem. The first assigns appointments to timeslots while the second formulation assigns an ordering of appointments. We compare these two models, evaluating which is more useful in different scenarios.",Mathematical Programming for Scheduling Telemedicine Appointments,"[72565, 63586, 47682, 28046, 75275, 75274]",596,"[56, 129]",2722,Appointment planning,3,8,15,OR in Health Services [ORAHS],"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,18 [building - 116],"['Health Care', 'Scheduling']",TB-15
"Data Envelopment Analysis [DEA] is a non-parametric technique for measuring the relative efficiency of a set of decision making units [DMUs], on the basis of multiple inputs and multiple outputs. Performing a typical analysis with DEA requires to solve a series of linear programs, one for each DMU. Therefore, DEA suffers from the curse of dimensionality, i.e., on big data the computational load is very high. This issue is commonly treated in the literature with the adoption of Machine Learning [ML] algorithms. Nevertheless, even though the selection of the training dataset is of crucial importance in such algorithms, in the DEA literature this factor is neglected and all methods rely on random sampling. In this paper, we built on the existing literature and we introduce a clustering-based data preprocessing technique to select the training dataset in a way that it represents the entire dataset as much as possible. We use simulated data to test this new technique against random sampling under different ML algorithms, number of netputs and standard DEA models. We further test it on a network DEA model for two-stage series structures in which the efficiency scores are represented in a two-dimensional vector. In all cases, the results highlight that the proposed technique increases the accuracy of the ML algorithms, whereas it may even decrease the required computational load.",Simulating Data Envelopment Analysis with Machine Learning - A Clustering-Based Data Preprocessing Technique for Training Set Selection,"[77723, 31462]",936,"[24, 35, 66]",2723,DEA and Machine Learning,89,3,48,Data Envelopment Analysis and its Application,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,60 [building - 324],"['Data Envelopment Analysis', 'Efficiency Analysis', 'Machine Learning']",MB-48
"Allocating resources to various tasks is a critical decision in practices. One of the difficulties arises from the intangible nature of task values or importance, making it challenging for decision makers to express them explicitly. Additionally, the value is nonlinearly linked to the level of allocated resources. This work proposes a method to estimate the non-linear value function by the historical decisions. Simulated studies are used to demonstrate the effectiveness of the proposed method.",Non-linear Task Valuation in resource allocation problems,[78052],55,"[105, 72, 84]",2724,Sustainable Economics in Developing Countries  ,67,13,18,OR for Development and Developing Countries,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,42 [building - 116],"['Production and Inventory Systems', 'Mathematical Programming', 'Optimization Modeling']",WB-18
"General quality assessment of 3D RNA structures predicted in silico is crucial to identify native or near-native 3D RNA folds. Nowadays, there are many, freely available computational methods for 3D RNA structure prediction, however without the reference - experimentally determined 3D RNA structure - it is difficult to reliably rank them in terms of practical usefulness. Here, we propose to apply graph neural networks to precisely infer the interatomic relationships commonly observed in 3D RNA structures and therefore reliably predict the quality of RNA structures. We started with the preparation of a high-quality training dataset based on experimentally determined structures retrieved from a non-redundant 3D RNA structures repository. Due to the scarcity of the available structures, we had to extend the dataset by 3D RNA models predicted using all state-of-the-art methods for 3D RNA structure prediction. We confirmed that graph NNs can harness a wide range of conformational space. We also solved the problem of variable-length 3D RNA structure representation crucial for the proper application of machine learning techniques by constructing local 3D motifs and applying an ensemble of local quality scores to assess the whole 3D RNA structure quality. A novel general quality assessment approach that does not need the reference 3D structure to reliably rank RNA 3D structures, we believe will be a breakthrough in the field of RNA structural bioinformatics.",Quality assessment of 3D RNA structures using graph neural networks,"[20145, 73720, 11193]",309,"[17, 8, 42]",2726,Integrative Approaches in Health and Disease - From Molecular Structures to Clinical Outcomes,2,9,20,"Computational Biology, Bioinformatics and Medicine","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,45 [building - 116],"['Computational Biology, Bioinformatics and Medicine', 'Artificial Intelligence', 'Expert Systems and Neural Networks']",TC-20
"In pursuit of climate neutrality and increased energy security, Switzerland's transition to renewable energy needs to be steered by key energy policies. However, socio-political barriers, such as social acceptance, rather than technical constraints, mostly hinder the successful implementation of these policies. Such socio-political aspects need to be better understood, examined and accounted for during energy policy analysis and consultation.  To address this need, we propose a hybrid multi-criteria decision analysis [MCDA] framework that integrates the fuzzy best-worst method [BWM] with PROMETHEE II and incorporates a comprehensive assessment of energy policy risks and benefits by an expert. This approach not only elicits the socio-political dimensions of policy performance, but also employs a strategic sorting technique for targeted action plans. The methodological framework is coupled with a robustness analysis that validates the reliability of the results under different future scenarios, ensuring the resilience of policy recommendations under uncertainty. The findings prove the efficacy of the framework to address uncertainties and include the social element in robust policy assessment and strategic planning.",Strategic Categorization of Swiss Energy Policies - A Robust Hybrid MCDA Approach for Socio-Political Assessment,"[64524, 36647, 53365]",887,"[25, 37]",2727,Robustness analysis in MCDA 2,44,7,44,Multiple Criteria Decision Analysis,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,20 [building - 324],"['Decision Analysis', 'Energy Policy and Planning']",TA-44
"Due to the high complexity and relevance of cybersecurity, efficient management of cyber attacks and threats is becoming a strategic technology trend. It elevates the resilience of ecosystems through the use of reactive and proactive strategies, which rely on standardized incident response procedures and the implementation of suitable mitigation countermeasures to prevent cyber attacks. The selection of actions is a multi-criteria problem encompassing technical and organizational aspects that must deal with benefits, costs, and the cascading effects between dependent assets. A group decision-making setting is required to incorporate the opinions of different experts on the information, business, and organizational levels. We hence introduce a comprehensive group decision-making process that helps security operations centres assess and choose cybersecurity actions. It applies the Delphi technique to unify the judgments of decision-makers and is aligned with the MCDM model that equivalently uses the quantitative or qualitative value function based on the standard scoring system. The veto function is also aggregated to prevent the selection of actions with an insufficient improvement in efficiency. The model considers criteria from the common CIA, ISO/IEC, and NESCOR standards. The framework utilizes the mappings between compromised assets, vulnerabilities, attack techniques, and responses. It further enhances analytics through external data sources and sensitivity analysis.",Group MCDM Methodology for Cyber Attack Mitigation and Response,"[29689, 79326, 79348]",889,"[25, 55, 126]",2728,MCDA applications in Engineering and Management 2,44,3,47,Multiple Criteria Decision Analysis,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,50 [building - 324],"['Decision Analysis', 'Group Decision Making and Negotiation', 'Risk Analysis and Management']",MB-47
"Quasi-Newton methods form an important class of methods for solving nonlinear optimization problems. In such methods, first order information is used to approximate the second derivative. The aim is to mimic the fast convergence that can be guaranteed by Newton-based methods. In the best case, quasi-Newton methods will far outperform steepest descent and other first order methods, without the computational cost of calculating the exact second derivative. These convergence guarantees hold locally, which follows closely from the fact that if the objective function is strongly convex it can be approximated well by a quadratic function close to the solution. Understanding the performance of quasi-Newton methods on quadratic problems with a symmetric positive definite Hessian is therefore of vital importance. In the classic case, an approximation of the Hessian is updated at every iteration and exact line search is used. It is well known that the algorithm terminates finitely, even when the Hessian approximation is memoryless, i.e. requires only the most recent information. This talk will address ways in which the reliance on exact line search and dependence on conjugate search directions can be relaxed, and how these changes affect the behavior of quasi-Newton methods.",On the behavior of limited-memory quasi-Newton methods for quadratic problems,"[77355, 78508]",363,"[19, 21, 63]",2731,Beyond First-Order Optimization Methods,84,13,32,Advances in large scale nonlinear optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,41 [building - 303A],"['Continuous Optimization', 'Convex Optimization', 'Large Scale Optimization']",WB-32
"Countless algorithms have been developed to tackle transportation planning problems. However, there is no one-size-fits-all algorithm that can ensure absolute superiority in all scenarios. To improve solving resilience across various instances and demands, we take Service Network Design Problem [SND] as a starting point. A Convolutional Neural Network-based method is proposed to distill and represent the feature of the instance and demand. Meanwhile, historical solving data are used to train a machine learning module for selecting the best candidate algorithm. This module is embedded into an algorithm framework, employing several algorithms to solve diverse instances under various solving demands. Numerical experiments reveal that the proposed method has superior solving resilience compared to individual and random algorithms utilization.",Scenario-driven Algorithm Selection for Transportation Planning Problem,"[78055, 75779, 66472, 78091]",817,"[5, 66, 143]",2732,Network Design for Public Transport,85,10,51,Public Transport Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,M5 [building - 101],"['Algorithms', 'Machine Learning', 'Transportation']",TD-51
"Data is increasingly vital to deal with the increased intermittency and limited controlability of renewable and distributed energy resources. Concurrently, data sharing raises privacy concerns motivating the need for privacy-enhancing techniques such as differential privacy. Data markets provide a means to enable wider access as well as determine the appropriate privacy-utility trade-off. Existing data market frameworks either require a trusted entity to perform computationally expensive valuations or are unable to capture the combinatorial nature of data value and do not endogenously model the effect of differential privacy. We address these shortcomings by proposing a valuation mechanism based on the Wasserstein Distance for differentially-private data, and two procurement mechanisms leveraging incentive mechanism design theory for task-agnostic data procurement, and task-specific joint task and data optimisation. The latter are reformulated into tractable mixed-integer second-order cone programs. The framework is applied to develop a joint energy and data market. We consider the retailer energy procurement problem where consumers’ demand is uncertain and historical demand data is differentially-private. This is modelled as an integrated forecasting and optimisation problem providing a means of valuing data directly rather than forecasts or accuracy. The value of joint energy/data clearing are highlighted through extensive numerical studies using real smart meter data.",Wasserstein Distance Based Market for Differentially-Private Data Trading,"[72435, 78056]",430,"[36, 47, 38]",2734,Data Valuation from Data-driven Optimization,49,12,35,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,44 [building - 303A],"['Electricity Markets', 'Forecasting', 'Engineering Optimization']",WA-35
"This paper explores strategic optimization in updating essential medical kits crucial for humanitarian emergencies. Recognizing the perishable nature of medical components, the study emphasizes the need for regular updates involving complex recovery, substitution and disposal processes with the associated costs. The goal is to minimize costs over an unpredictable time horizon. The introduction of the kit-update problem considers both deterministic and adversarial scenarios. Key performance indicators [KPIs], including updating time and destruction costs, are integrated into a comprehensive economic measure, emphasizing a strategic and cost-effective approach.
The paper proposes an innovative online algorithm utilizing available information at each time period, demonstrating its 2-competitivity. Comparative analyses include a stochastic multi-stage approach and four other algorithms representing former and current MSF policies, a greedy improvement of the MSF policy, and the perfect information approach.
Analytics results on various instances show that the online algorithm is competitive in terms of cost with the stochastic formulation, with differences primarily in computation time. This research contributes to a nuanced understanding of the kit-update problem, providing a practical and efficient online algorithmic solution within the realm of humanitarian logistics.
",An efficient 2-competitive online algorithm for kit update at MSF Logistique,"[72911, 49266, 76749]",551,"[5, 136, 58]",2735,Demand Forecasting in Humanitarian Operations,38,8,21,OR in Humanitarian Operations [HOpe],"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,49 [building - 116],"['Algorithms', 'Stochastic Optimization', 'Humanitarian Applications']",TB-21
"Researchers and countries are turning to alternative energy sources as the existing energy sources are insufficient to meet the demand. Hydrogen energy is one of the alternative energy sources and one of its main potential contributions will be through the Hydrogen Fuel Cell Vehicles [HFCVs]. To ensure the widespread use of HFCVs, hydrogen fuel must be easily accessible. The number and location of hydrogen refueling stations [HRSs], therefore, have a crucial role for the usage of the hydrogen energy via HFCVs.
The subject of this study is to determine of the number and locations of HRSs for Istanbul, Türkiye’s most crowded city. The adaptation to hydrogen technology for each district is modelled using a measure for life quality called human development index which is then used to determine the HFCVs’ demand based on traffic flow data.  We employ three different approaches for modeling the problem in a multi period setting - p-median model, a set covering model, and a hybrid model of both.  It turns out that ignoring the transition of adopting hydrogen technology may result in a significant loss.  Moreover, instead of spreading from the center to the city boundary, the stations appear at both low populated and high populated regions from early periods of the 30 years planning horizon. Finally, very few numbers of HRSs are sufficient to meet high demand, but the number increases significantly if all demand should be satisfied.
",Hydrogen Refueling Station Allocation Problem - A Case Study for İstanbul,"[77094, 63036, 63048]",712,"[64, 37, 84]",2736,Empowering Energy Access,21,15,22,Energy Management,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,81 [building - 116],"['Location', 'Energy Policy and Planning', 'Optimization Modeling']",WD-22
"In this paper, we offer a theoretical framework to rationalize choices of commuting and traveling for leisure conducted by individuals under uncertainty about the future level of emissions. The main focus is analyzing technology's role in this process. The model is estimated for a sample of surveyed individuals across US counties during the years 2017 and 2018. The role of technology is measured through the availability of fixed broadband and fiber-to-the-home for each individual. We find positive elasticities of substitution suggesting that as the cost of commuting increases, the demand for broadband services increases. This  implies that individuals are substituting commuting with activities that require broadband such as working from home and therefore digital technology is an emission reducer and produces productivity gains in travelling for work and leisure.  More commuting time for work will cause an increase in emissions volatility across counties, and the total effect on expected emissions will be positive as individuals do suboptimal consumption of the two activities.","Emissions Uncertainty, Technology and Optimal Commuting and Leisure Demands",[78054],711,"[33, 37, 143]",2737,Environment and climate change,21,14,22,Energy Management,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,81 [building - 116],"['Economic Modeling', 'Energy Policy and Planning', 'Transportation']",WC-22
"     In this study, we focus on the material selection and feeding schedule of four kinds of lamination films for the multi-layer automatic film dispensing problem in the polarizer industry process. The material selection is to select the appropriate size of each material film for lamination. The feeding schedule is to find the best sequence of automatic film dispensing for the selected materials. 
    The scheduler in the plant has arranged the film dispensing manually, which has a great impact on the loss of materials. We develop a particle swarm optimization algorithm and two improved hybrid differential evolutionary algorithms to handle the material selection. The variable neighborhood search is designed to deal with the feeding schedule. The near-optimal solution can be found by iterative search. A series of experiments are designed to compare the search performance of these three swarm search algorithms. Three algorithms can achieve satisfactory results. In the future, these algorithms can be expected to develop the decision support system to the multi-layer film material selection and scheduling problem in the polarizer industry.
",Optimization of automatic film dispensing of multi-layer films - The method of establishing material selection and scheduling,"[49098, 78066]",622,"[74, 23, 129]",2742,Cutting and Packing 4 - 3D irregular,81,5,07,Cutting and Packing [ESICUP],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,1019 [building - 202],"['Metaheuristics', 'Cutting and Packing', 'Scheduling']",MD-07
"FMCEA [Failure Modes Causes and Effect Analysis] has long been utilized as an effective risk management approach across various industries for examining and assessing the failure modes of diverse products and processes. Nonetheless, this method exhibits several shortcomings associated with traditional RPN [Risk Priority Number] calculations, such as duplicate identification, gaps, and equal weighting of experts, as well as the lack of consideration for various factors like risk parameter weights, interrelationships, cost, and time and efficiency notions in risk assessment.

To overcome these limitations, a five-stage methodology integrating Analytic Hierarchy Process [AHP], Technique for Order of Preference by Similarity to Ideal Solution [TOPSIS], Decision Making Trial and Evaluation Laboratory [DEMATEL], and Data Envelopment Analysis [DEA] with FMCEA is proposed. First, AHP assigns weights to experts, while FMCEA assesses failure causes considering occurrence, detection, severity, cost, and time. DEMATEL weights risk factors and explores their interrelationships. TOPSIS calculates RPNs. DEA measures efficiency and establishes interrelation matrices. DEMATEL ranks sub-failure modes based on their interrelationships. Finally, an aggregation procedure combines related sub-failure modes. The RFID system is used as an example to illustrate the methodology to show the advantages of the proposed approach.
","Identifying Root Causes and Effects of RFID System Failures with Integration of AHP, DEMATEL, TOPSIS, DEA and FMCEA","[71986, 78074]",889,"[77, 126, 24]",2744,MCDA applications in Engineering and Management 2,44,3,47,Multiple Criteria Decision Analysis,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,50 [building - 324],"['Multi-Objective Decision Making', 'Risk Analysis and Management', 'Data Envelopment Analysis']",MB-47
"The resilience of power systems is challenged by increasingly frequent and severe environmental events. Anticipation of threats and vulnerabilities is crucial to ensure adequate response and recovery from extreme events. In this context, the deployment of local generation can help absorb the impact of system disturbances. Size and location of distributed energy resources are critical factors in determining their economic and technical viability. In this paper, we develop robust and distributionally robust optimization models among others, under load and supply uncertainty, and random contingencies. An illustrative case study is presented to demonstrate the models’ performance on a benchmark distribution network with synthetic data combining a variety of representative days and failure scenarios in order to explore the trade-offs between reliability, costs, and resilience. System performance assessed under in-sample and out-sample scenarios constitute the resilience metrics.",Resilience enhancement of distributions networks with robust optimal sizing and location under uncertainty and random contingencies,"[77339, 78059, 78060]",573,"[127, 72, 93]",2745,Location and transportation problems under uncertainty,49,10,35,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,44 [building - 303A],"['Robust Optimization', 'Mathematical Programming', 'OR in Energy']",TD-35
"Ground handling operations at airports include a wide range of services such as baggage handling, aircraft maintenance, and passenger services. The complex associations and dynamic nature of these operations demand not just efficiency but also adaptability and precision in resource management. 

This research introduces a novel approach for scheduling ground handling operations, by integrating Resource Constrained Project Scheduling Problem [RCPSP] and Hopcroft-Karp algorithm to match ground handling operations and multi-skilled personnel. First, the task scheduling is addressed, which involves the adapted RCPSP model to the ground handling operations, considering various constraints including time windows, personnel availability, and task dependencies. Second, the allocation of multi-qualified personnel is tackled, which introduces a modified Hopcroft-Karp algorithm to match the scheduled tasks with the applicable personnel based on their skills and availability. The overall objective is the minimisation of delays, i.e. deviations from the planned activities.

We demonstrate the performance of this approach under different conditions such as delays or resource shortage for an exemplary data set of an airport. We compare different scenarios and present the benefits of this approach.
",Optimising Airport Ground Handling Operations Considering Multi-Skilled Personnel,"[73668, 33284, 78064, 41723]",650,"[84, 5, 143]",2746,Airline Applications II,6,5,55,Transportation,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S02 [building - 101],"['Optimization Modeling', 'Algorithms', 'Transportation']",MD-55
"With Gurobi 9.0 we introduced an API to define univariate nonlinear convex and nonconvex constraints, which Gurobi would then automatically approximate using piece-wise linear functions. With Gurobi 11.0, the user can now choose to solve models with nonlinear constraints directly in order to get a globally optimal solution to the mixed integer nonlinear program [MINLP] at hand.

In this talk, we review the algorithmic technology employed in Gurobi 11 to address nonlinear constraints. We highlight the strengths and limitations of our approach and give an outlook on further developments that will be part of Gurobi 12.
",Global MINLP in Gurobi,[12336],238,"[72, 113, 111]",2747,MINLP Solvers,76,3,30,Software for Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,53 [building - 208],"['Mathematical Programming', 'Programming, Nonlinear', 'Programming, Mixed-Integer']",MB-30
"We explore the potential of implementing a dual supply chain system to replace  a conventional one driven by a typical large Consumer Packaged Goods [CPG] firm. The dual supply chain would deliver finished products both through distribution centers and through direct dispatch to retailers.   Demand information and the cost structure from the company's production facilities and existing supply chain were analyzed to build and validate a model for the dual supply chain within planning constraints. 

The objective is to reduce operating costs by using the opportunity for direct dispatch, considering network restrictions. Direct dispatch possibilities were identified, based on customer segmentation, across time and across locations.  Aggregation is possible both temporally [combining orders over multiple days, subject to inventory and service level constraints] and spatially [combining orders at nearby locations, subject to vehicle movement constraints]. Heuristic approaches and exact optimization models were applied.  The detailed example considers 243 customers [retailers] of a plant of the CPG firm. Of these, 24 feasible direct dispatch customers were identified, yielding a Direct Dispatch Opportunity Rate of 14.41% for the plant resulting in cost reduction.  The model is extendable to multi factory situations, and leads to interesting modeling issues in fixed cost allocation and consideration of different aspects of service in retail supply chains.",Direct Dispatch and Order Fulfillment Models in Retail Supply Chains,"[28300, 78067, 78069]",635,"[138, 0]",2748,Warehouse Management,30,5,50,Retail Operations,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,M2 [building - 101],['Supply Chain Management'],MD-50
"Waste collection management has seen an increasing interest in the OR community these past years. This is due to the wide range of Capacitated Vehicle Routing Problems [CVRP] studied and the growing trend in studying sustainability-related problems.

For waste collection companies, it can be challenging to identify the type of strategy most suitable for a given situation. On the one hand, the complexity of the state-of-the-art algorithms presented in the literature; on the other hand, the data needed for these algorithms can be challenging to obtain and encode. 

In partnership with Alpenluft, a Swiss waste collection consulting company, and the Innosuisse agency supporting R&D projects, we developed the WasteLogs application, a user-friendly strategic waste collection decision tool. The application offers interfaces allowing the encoding of the collection points, the amounts of waste to collect, and the collection strategy in different features that can be combined to generate a routing for collection vehicles. There are currently three state-of-the-art collection strategies implemented in the tool. Each algorithm minimizes the CO2 emissions through heuristic methods; the user can then identify what collection strategy is the most suitable and extract the information needed to import them into GPS systems.  WasteLogs also allows importing existing collection tours to evaluate whether they can be improved.",WasteLogs - a decision support tool for strategic waste collection,"[77394, 62164, 50839]",68,"[26, 100, 145]",2750,Decision Support for Sustainable Operations,45,10,45,Decision Support Systems,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,30 [building - 324],"['Decision Support Systems', 'OR in Sustainability', 'Vehicle Routing']",TD-45
"This research proposes a comprehensive approach to skill-based staff scheduling and training, aiming to optimize workforce efficiency and minimize costs. The focus lies on efficiently assigning skilled workers to shifts within a defined time horizon and scheduling intermediate training sessions increasing the productivity of workers. The training of workers may encompass on-the-job or off-the-job training sessions. Importantly, both staff scheduling and training decisions are made concurrently during this phase, with training costs integrated with other expenses. The proposed algorithm encompasses a meta-heuristic procedure, which is initialised by creating baseline schedules without training, prioritizing task assignments and workforce availability. These initial schedules can be efficiently resolved using exact Mixed Integer Programming mathematical methods in a reasonable computational time. The further optimisation aims to improve the costs via introducing training and is performed via dedicated local search operators and the combination of high-quality schedules following the principles of evolutionary processes.",Integrated scheduling of work and training activities for a multi-skilled workforce,"[78028, 19342, 56999]",227,"[129, 57, 111]",2753,Combinatorial Optimization in Scheduling,64,13,26,Combinatorial Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,012 [building - 208],"['Scheduling', 'Human Resources Management', 'Programming, Mixed-Integer']",WB-26
"This work aims to support the decision-making process in the scope of Chemical, Biological, Radiological and Nuclear [CBRN] Defense, looking for the minimum radiation accumulated dose along the route for an aerial vehicle involved in rescue or transportation missions on situations of exposure to ionizing radiation by overflight within radioactive or nuclear plumes. A previous methodology was proposed to address the level of the radiation dose, the autonomy of the vehicle and the obstacles, when the vehicle is flying at low altitude or in urban areas where buildings or mountains are obstacles to the navigation. In this work, this methodology is improved by also taking the temporal evolution of the radiation dose into account, since the radiation dose may be different at the same location, depending on the specific moment it was measured due to the plume evolution. The proposed methodology will be presented and applied to illustrative instances.",An Air Route Planning Under Radioactive Plume Scenarios With Temporal Evolution,"[1880, 78072, 67759, 78073, 6537]",781,"[145, 26, 30]",2755,Routing Unmanned Aerial Vehicles 2,5,4,64,VeRoLog - Vehicle Routing and Logistics,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S16 [building - 101],"['Vehicle Routing', 'Decision Support Systems', 'Disaster and Crisis Management']",MC-64
"In response to Europe's climate objectives, this study investigates a novel distributed coordination framework for cross-sectoral integration in multi-operator energy systems, critical for achieving sustainability. Incorporating digitalization and decentralized resources, our method overcomes the challenge of efficiently coordinating highly interconnected systems while preserving data confidentiality and promoting renewable integration. We  address nodal pricing efficiencies through a unique distributed algorithm that is based on autonomous agents—corresponding to market participants and network operators. This innovation ensures flexibility and efficiency, adhering to strict European regulations. Our approach not only handles the complexity of multi-energy carriers and the intricacies of highly interconnected power systems but also enables scalable, privacy-aware coordination avoiding major drawbacks of existing market designs. Based on a careful evaluation of decomposition techniques, we derive a distributed framework using the alternating direction method of multipliers [ADMM]. Demonstrating the advances of this distributed algorithm, our research contributes to operational efficiency in multi-energy systems, enabling an enhanced integration of decentralized resources in a cross-sectoral energy system to meet Europe's ambitious climate targets.",Cross-sectoral coordination in highly interconnected multi-operator energy systems - A distributed framework with autonomous agents,"[75238, 14845]",343,"[93, 36, 3]",2757,Uncertainties in the Energy Transition,22,5,09,Energy Markets,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,10 [building - 116],"['OR in Energy', 'Electricity Markets', 'Agent Systems']",MD-09
"The problem of perishable goods logistics in the fast food sector in the city of Bogota is the shortage and logistics costs of transportation routes for the supply chain carried out by logistics operators, there is a need to analyze the current situation to identify the variables that affect the current results and how to measure the IRP models [Programming, Multi-Objective] for the scheduling and routing of vehicles transporting perishables, in order to satisfy fast food customers [QSR - Quick Service Restaurant], Thus, the delivery of perishable food products [perishables] must have an adequate supply chain to avoid food waste, improve the performance of the agri-food supply chain logistics [AFSC] sector through different optimization models [LIRP, PIRP, IRP], In this research, a mathematical model IRP is defined where G=[N,A], N is the set of vertices and A is the set of arcs, vertex 0 is the supplier, applying the optimization defined by Archetti [2022] IRP [Inventory Routing Problem] whose objective function is to minimize the total inventory cost at the supplier, customers and transportation, where this problem is applied to the QSR sector in order to minimize nonconformities for the FFR [Fast Food Restaurant]. The factual problem has 72 restaurants, with 3 multi-temperature products [Frozen, refrigerated and dry], the SLOAD+MCF model was chosen, which incorporates additional multi-product flow constraints with the ability to model the respective demands and capacities.","Optimization with a multi-objective IRP model of the perishable supply chain for fast food restaurants, case of 3PL in Bogota D.C.","[76674, 38291, 35588]",520,"[65, 112, 61]",2758,"Advancements of OR-analytics in statistics, machine learning and data science 10",16,14,28,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,065 [building - 208],"['Logistics', 'Programming, Multi-Objective', 'Inventory']",WC-28
"This paper deals with the park-and-multi-loop routing problem, in the context of same-day delivery, in which a fleet of traditional vehicles equipped with several autonomous delivery robots [ADRs] leave from a depot to service a set of customer requests characterized by a delivery location and a time window. Each delivery route involves a main tour of a traditional vehicle, which may stop at one or more parking slots, from which one or more subtours are carried out by the driver and the ADRs on the pedestrian network. The goal is to minimize the overall distance traveled by vehicles for traditional deliveries, as well as the walking distance covered by drivers and ADRs. For this problem, we propose an Adaptive Large Neighborhood Search [ALNS], in which the destroy and repair operators are parameterized, and the most appropriate parameter setting is constantly updated. The update phase is carried out by a reasoner, which analyzes the solutions and, according to some performance measures, identifies the most suitable routes to be destroyed and the parameterization of the repair operator. Computational results on a set of instances of varying size, derived from real data of several major cities, certify the quality of the approach, compared to an ALNS with classic destroy and repair operators.

Acknowledgement
The work is supported by the research program Sustainable Mobility Center [Centro Nazionale per la Mobilità Sostenibile - CN MOST], project code CN00000023, Spoke 7",Park-and-Multi-Loop Routing for Last-Mile Delivery,"[30716, 47603, 11649, 4762, 78075]",747,"[145, 0]",2759,Last-Mile Delivery,5,12,64,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S16 [building - 101],['Vehicle Routing'],WA-64
"This study examines the dynamics of the exploitation of a natural resource distributed among and flowing between several nodes connected via a weighted, directed network.
The network represents the locations and interactions of the resource nodes. A regulator decides to designate some of the nodes as natural reserves where no exploitation is allowed. The remaining nodes are assigned [one-to-one] to players, who exploit the resource at the node.
This study extends that in Fabbri et al. [2024] to nonlinear dynamics of the capital stock evolution at the nodes, taking into account evolution equations of logistic or Solow type, establishing the existence of a Markovian equilibrium which is linear in the capital stock and confirming that, consistently with the case of a linear evolution, the equilibrium exploitation and resource stocks depend on the productivity of the resource sites, the structure of the connections between them, and the number and preferences of the agents. The best locations to host nature reserves are identified per the model's parameters and correspond to the most central [in the sense of eigenvector centrality] nodes of a suitably redefined network that considers the nodes' productivity.

",Symmetric equilibria in spatially distributed extraction games with non linear growth,"[23529, 67583, 51465]",846,"[20, 50, 40]",2760,Heterogeneity in optimal control problems,90,7,33,Optimal Control Theory and Applications,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,42 [building - 303A],"['Control Theory', 'Game Theory', 'Environmental Management']",TA-33
"The planning of waste collection operations typically involves a series of tactical and operational decisions. This work is concerned with studying tactical decisions related to waste collection schedules and the use of intermediate facilities for unloading waste, while optimizing route planning at the operational level. The real-world inspired waste collection problem at hand can be modelled as a periodic vehicle routing problem with intermediate facilities. Vehicles based at a central depot collect waste from different locations and visit incinerators en route, when they are full, and at the end of each tour to replenish their capacity. The planning horizon is several days or weeks and, as plans should be repeatable, a periodic plan is established. In order to provide decision support to practitioners at both operational and tactical levels, we investigate more flexible collection schedules and flexibility in the use of intermediate facilities and their allocation to vehicle routes, considering minimum quotas and maximum capacities. The aim is to minimize overall transportation costs while avoiding overflows of the waste bins. The problem is solved using an adaptive large neighborhood search with problem-specific operators and a local search strategy. We present computational results on benchmark instances from the literature and on a set of real-world inspired instances from the city of Vienna showing the effects of allowing more flexibility in planning.",Tactical Planning Considering Different Levels of Flexibility for a Waste Collection Routing Problem,"[74087, 61518, 2769]",777,"[145, 65]",2761,Waste Collection,5,9,58,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S07 [building - 101],"['Vehicle Routing', 'Logistics']",TC-58
"The problem of forecasting demand for seasonal products became even more challenging for retailers amid the Covid-19 pandemic. They are now faced with the difficult task of forecasting demand in the “new normal”. On the one hand, the pandemic’s future trajectory is hard to predict. On the other hand, an increase in supply constraints forces retailers to confirm orders several seasons in advance. Given this increased uncertainty in future demand, we develop a forecasting model that combines de-biased expert judgment to utilize their knowledge of shifts in consumer behavior, the effects of inflation and other global factors on product demand, and statistical methods well suited for extrapolating repeated patterns from the past. Using data from a premium bicycle manufacturer, we show that accurate demand forecasts for the next three years can be obtained by integrating experts’ estimates of the category growth rate with a seasonal decomposition of the pre-pandemic demand curve. Our method yields a forecast error [MAPE] of 18.05%, on average, on data from early 2022. Moreover, our integrated human-algorithm forecasts perform better than statistical time-series forecasts, indicating that experts play a decisive role in predicting demand for seasonal products during and after the pandemic.",Long-term Forecasting of Seasonal Goods using De-biased Expert Judgment - Beyond the Pandemic,[23418],428,"[7, 138]",2763,Retail Analytics,30,7,50,Retail Operations,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,M2 [building - 101],"['Analytics and Data Science', 'Supply Chain Management']",TA-50
"
Governments employ diverse strategies, including financial disincentives, restrictions, and subsidization, to shape urban environments and achieve specific objectives like reducing CO2 emissions. In the Netherlands, policies like time windows and vehicle restrictions are implemented to enhance urban transportation. Time windows reduce inconvenience caused by large trucks during peak hours, improving social sustainability, while vehicle restrictions alleviate traffic problems and pollution, promoting a cleaner and safer urban environment. 
This research research investigates the effectiveness of these regulations. 
Specifically, we focus on retailers and their distribution and replenishment operations to examine the effects of regulations. 
So, the concept involves optimizing the operational aspects of the retailer, such as distribution and replenishment, within the framework of specified regulations.
The output of this optimization process guides us through gauging the city measures to figure out if they are in line with the goals of regulations.
We also explore whether city hubs may be an efficient addition under a given regulatory framework.
In the case of confirming the effectiveness of city hubs, subsidizing the city hubs is also an option to address urban transport issues.
The result of this study may offer valuable managerial insights into the evaluation of the effectiveness of urban policies and regulations. ",Exploring Urban Regulation Effectiveness - Investigating Retailers' Distribution Costs,"[67481, 65884, 2069]",912,"[138, 143, 139]",2764," Enhancement of circularity, inclusivity, and smartness in cities II",79,5,18,Sustainable Cities,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,42 [building - 116],"['Supply Chain Management', 'Transportation', 'Sustainable Development']",MD-18
"The importance of hydraulic studies is undeniable in understanding the behavior of fluids in natural and constructed environments, such as channels, rivers, and streams. The Saint-Venant equations, a widely adopted one-dimensional model, are crucial for this understanding. However, ensuring the representativeness of these equations requires considering all boundary conditions, which includes estimating the Manning roughness coefficient. This estimation, essential for accounting for fluid friction with different surfaces, is a notable challenge due to the coefficient's variability along the channel. In this context, we propose an approach that utilizes the Augmented Lagrange Method to minimize errors in solving differential equations at different points. Additionally, we employ the PRIMA software [BOBYQA] with space reduction based on interpolation to estimate the Manning coefficient. This methodology was applied to the East Fork river, and even with few days of training data, we achieved relevant results in describing the behavior of this river.
",Estimation of Hydraulic Parameters Using the Augmented Lagrange Method,"[73883, 14337]",285,"[19, 84, 38]",2766,Large Scale Constrained Optimization - Algorithms and Applications,84,2,32,Advances in large scale nonlinear optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,41 [building - 303A],"['Continuous Optimization', 'Optimization Modeling', 'Engineering Optimization']",MA-32
"In this talk, we consider multi-stage robust optimization problems of the minimax type. We assume that the total uncertainty set is the cartesian product of stagewise compact uncertainty sets and approximate the given problem by a sampled subproblem. Instead of looking for the worst case among the infinite and typically uncountable set of uncertain parameters, we consider only the worst case among a randomly selected subset of parameters. By adopting such a strategy, two main questions arise - [1] Can we quantify the error committed by the random approximation, especially as a function of the sample size? [2] If the sample size tends to infinity, does the optimal value converge to the true'' optimal value? Both questions will be answered. An explicit bound on the probability of violation is given and chain of lower bounds on the original multi-stage robust optimization problem provided. Numerical results dealing with a multi-stage inventory management problem show that the proposed approach works well for problems with two or three time periods while for larger ones the number of required samples is prohibitively large for computational tractability. Despite this, we believe that our results can be useful for problems with such small number of time periods, and it sheds some light on the challenge for problems with more time periods.",Sampling methods for multi-stage robust  optimization problems,"[24015, 62825, 3122]",273,"[127, 16, 21]",2767,Urban Logistics and sustainable TRAnsportation - OPtimization under uncertainTY and MAchine Learning,49,4,35,"Stochastic, Robust and Distributionally Robust Optimization","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,44 [building - 303A],"['Robust Optimization', 'Complexity and Approximation', 'Convex Optimization']",MC-35
"Heuristic algorithms are commonly used in practical applications of combinatorial optimization to achieve efficient problem-solving. However, these heuristics have limitations when applied to interleaved and computationally hard problems. To address these shortcomings, we propose enhancing decision-making by using Machine Learning [ML] models. We demonstrate the effectiveness of this approach on a complex example of a serial-batch scheduling problem, which involves deciding the grouping of jobs [i.e. batching] and sequencing of batches [i.e. scheduling]. The maximum potential of ML models can be exploited by strategically applying them at various junctures within the problem-solving process. We analyze three stages to integrate ML models to improve decision-making. Firstly, ML models are used to anticipate solutions for base-level nesting problems posed by the batching decisions. Secondly, the models are used to tune the parameters of the top-level problem's solution method circumventing time-intensive search approaches. Finally, the most suitable solution method is selected for a problem instance by creating ordinal preferences between solution methods based on model predictions. The effectiveness of using ML models to improve decision-making at different stages of the scheduling problem is highlighted by our empirical evaluations.",Applying Machine Learning in Machine Scheduling - Improving the Decision-Making of a Serial-Batch Scheduling Problem,"[59576, 27800, 1609]",932,"[14, 66, 129]",2771,Machine Learning in Machine Scheduling,35,14,60,Project Management and Scheduling,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S09 [building - 101],"['Combinatorial Optimization', 'Machine Learning', 'Scheduling']",WC-60
"The question of why and how humans choose to help others without expecting anything in return has puzzled philosophers, psychologists, and economists for centuries. This becomes even more important during Sudden Onset Disasters [SOD], where understanding donor behavior plays a critical role in efficient management of Humanitarian Supply Chains.
Demographics, trust, and personal history all have an impact on the individual’s donation decisions. Another factor is the perceived closeness to the victims. “Psychic Distance“ [PD], a concept borrowed from international business literature, is adapted to describe donor-victim relationship in terms of shared personal history and cultural closeness.
We propose another dimension to the PD under SOD conditions - donors’ temporary mental, physiological and financial states. Since decisions during SODs are taken within a short time frame, we expect these contextual factors to play a significant role in the donation decisions as well.
Through conducting an experimental design, we study donation decisions of the subjects while their psychology and physiology are manipulated via different priming methods. The objective is to observe changes in intention to donate, and donation preferences, while the perceived contextual PD is modified. 
",Understanding donation patterns in the immediate aftermath of sudden onset disasters,"[78076, 28554, 3292]",692,"[58, 30, 28]",2774,Developing Countries and Sustainable Humanitarianism ,67,15,18,OR for Development and Developing Countries,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,42 [building - 116],"['Humanitarian Applications', 'Disaster and Crisis Management', 'Developing Countries']",WD-18
"The EU-funded SOLUTIONSplus project aims at promoting e-mobility in 9 cities around the world, including Kathmandu, Nepal. This paper develops a systematic approach for selecting the fleet of electric vehicles [among 8 different types including buses, shuttle vans, 3-wheelers, trucks, and waste collectors] that maximizes societal benefit within a specific budget. It introduces a framework combining Multi-Criteria Decision Making [MCDM] and optimization techniques. The MCDM aspect employs over twenty key performance indicators to assess the impact of various alternatives against specific attributes that include financial viability, alignment with institutional framework, effects on climate change and environment, effects on society and the wider economy. A set of weights defines the values and priorities of the local stakeholders. As this problem is non-linear and the objective function is discontinuous, a metaheuristic is needed to find a good solution. It appears that the Evolutionary Algorithm produces good solutions at reasonable computational times. The proposed framework can identify an optimal solution configuration for problems with discontinuous objective functions and predefined preferences. In practical terms, this research not only advances the field of urban mobility but also offers a scalable model for other cities facing similar challenges.",Optimization of electric mobility investments in Kathmandu,"[78078, 78084, 78083, 78085, 12015]",956,"[25, 74, 100]",2775,Optimization of sustainable urban mobiltiy II,79,8,18,Sustainable Cities,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,42 [building - 116],"['Decision Analysis', 'Metaheuristics', 'OR in Sustainability']",TB-18
"Cancer is one of the main public health problems worldwide. Most cancer patients are treated with radiation. This treatment uses a source of ionizing radiation that destroys the tumor cells, in which it is possible to vary the intensity of the radiation fluence and the radiation beam can be shaped in such a way to radiate only around the tumor. Computed tomography scans are performed in order to define the target volumes and organs at risk, and then define the patient's treatment plan, in which sufficient amounts of dose are prescribed for the tumor to cure, while minimizing the inevitable dose to the other regions. The prescription is normally made by the oncologist, which may be associated with uncertainties. Therefore, the dosage values are considered as fuzzy numbers. In this context, the dose distribution problem in radiotherapy planning translates into a fuzzy optimization problem in which mathematical programming techniques are used for the solution. Fuzzy constraints are mathematically translated by the surprise approach. The proposed methodology was applied in several cases of cancer using the TROTS database. The results obtained are compared by means of Dose-Volume Histograms and contour graphs and dose surface. In conclusion, the approach using fuzzy numbers produced viable treatment plans, inferring that the model is an important tool in decision-making in the planning of radiotherapy treatment.",Fuzzy optimization in beam Intensity Modulated Radiotherapy Treatment,"[73881, 1795]",609,"[110, 56, 49]",2776,Radiotherapy and chemotherapy planning,3,7,10,OR in Health Services [ORAHS],"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,11 [building - 116],"['Programming, Linear', 'Health Care', 'Fuzzy Sets and Systems']",TA-10
"In this presentation, we present a novel scheduling algorithm tailored specifically for the shipbuilding industry, with a key focus on integrating additional resource input to potentially minimize makespan. While prior research has addressed various facets of shipyard scheduling, most efforts have centered on optimizing schedules based solely on existing resource capacities, often neglecting the critical issue of alleviating bottlenecks caused by resource limitations. Our research fills this gap by introducing a novel mathematical model designed to augment existing shipbuilding scheduling systems. This model strategically incorporates the option of supplementing resources to alleviate bottlenecks, all while accounting for the associated acquisition costs. Complementing this mathematical framework, we developed a heuristic algorithm meticulously crafted to enhance efficiency. Through extensive computational experiments, we demonstrate the efficacy and robustness of our proposed approach.",Optimizing workspace scheduling for shipyard manufacturing under additional resource input consideration,"[69291, 78082, 64382]",878,"[129, 138, 74]",2777,Heuristic Algorithms for Combinatorial Optimization Problems II [Contributed],64,15,52,Combinatorial Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,8003 [building - 202],"['Scheduling', 'Supply Chain Management', 'Metaheuristics']",WD-52
"Dealing with post-consumption clothing products is key to make the fashion industry circular. This will require effective reverse logistics to support the Re-x processes of the circular economy. The design of effective reverse logistics networks is a multifaceted task that need to cater for the different stakeholders and multiple decision criteria. This paper provides a systematic overview of the literature in the context of the fashion industry in three key areas - [i] reverse logistics, [ii] circular economy, and [iii] policy interventions. The focus of the first is optimisation and operations research models as well as game theory models. The second area looks into the application of the circular economy and sustainability. In the third area, the main policy interventions include extended producer responsibility [EPR], taxation, eco-labelling, and waste management. A comprehensive search was conducted on Web of Science. The managerial insights are proposed as follows - Designing operations research models assists decision-makers to optimise transportation, profit, waste, and location-allocation of facilities. Game theory models drive stakeholders to compete strategically, finding equilibrium in decision-making for resource allocation, costs, and pricing. Using game theory models can also exhort players [retailers, recyclers, charities, and customers] to collaborate, earning profit and fostering a sustainable supply chain, minimising waste and maximising resource reusing. ","A review of the synergistic role of optimisation, game theory, and policy interventions in making the fashion industry circular","[77219, 32980]",921,"[50, 125, 84]",2778,Policy and legislation for a circular economy,18,7,23,"Circular Economy, Remanufacturing and Recycling ","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,82 [building - 116],"['Game Theory', 'Reverse Logistics / Remanufacturing', 'Optimization Modeling']",TA-23
"In recent years, AI has witnessed a shift towards the inclusion of participatory approaches. This transition highlights the need of integrating AI with various methodologies, e.g. citizen science, crowdsourcing or digital democracy platforms. However, adopting a participatory approach means engaging communities and activating their Collective Intelligence [CI] across all stages of the AI system. Grounded in Activity Theory, this research aims to explore the potential of AI in gaining CI, facilitating the elicitation of collective values within communities and nurturing awareness and trust in AI tools. The research focuses on the Archivio Atena project, a culture-led initiative in Atena Lucana, in the South of Italy. Using image data collected during the project activities, the study encourages the community to participate in a collective supervised learning process for the AI, by tagging them with value-based labels. This iterative process enables participants to recognize collective value patterns, supporting data clustering and furnishing an evaluation tool for impact metrics within the future projects. The integration of AI with participatory methodologies offers a promising approach to address subjective and collective issues, beyond quantitative and objective realms. By leveraging AI in the context of PSMs, this research outlines both the limitations and potentials of AI in facilitating the promotion of CI and defining useful criteria for impact evaluation processes.",Exploring Participatory Approaches - Integrating AI with Collective Intelligence in the Archivio Atena project,"[78079, 78092, 51402, 48194]",128,"[8, 41, 149]",2779,Impact of AI on Soft OR - A,26,8,13,Soft OR and Problem Structuring Methods,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,15 [building - 116],"['Artificial Intelligence', 'Ethics', 'Problem Structuring']",TB-13
"Loyalty programs are widely popular amongst firms and consumers. These programs usually offer reward points for purchases that can be redeemed at the firm to pay for future transactions. Firms may use reward points as an incentive in promotions instead of offering a price discount. Offering reward points as incentive in a promotion may address challenges associated with price discounts, such as the direct margin impact and failing to boost repeat purchases. In using reward points from their loyalty program as an incentive in promotions [reward points coupons] instead of immediate discounts [price discount coupons], firms seek to optimize the gross profit from the promotion by reducing costs and strengthening repeat purchase behavior. We compared both coupons in a field experiment with a retailer, finding - [i] Both boost short-term sales significantly. [ii] Compared to immediate discount coupons, future-credit coupons cut redemption revenues by 50%, despite higher average transaction spends, due to fewer redemptions. Despite cost benefits, they yield less profit than immediate discounts. [iii] Redemption of future-credit coupons fails to increase total spend or repeat purchases post-promotion in comparison to the redemption of an immediate discount coupon. Retailers' hopes for loyalty points over immediate discounts in promotions don't pan out.",The Shift from Immediate Discount Coupons to Reward Points Coupons - Does It Promote Short- and Long-Term Buying?,"[78081, 23418, 12752]",428,"[71, 124]",2780,Retail Analytics,30,7,50,Retail Operations,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,M2 [building - 101],"['Marketing', 'Revenue Management and Pricing']",TA-50
"The aquaculture industry contributes considerably to CO2 pollution in Norway, whereas about 40% of the aquaculture sector emissions are caused by aquaculture vessels. The aquaculture industry is expected to grow in the coming years. However, emissions must be reduced by 55% by 2030 to meet the goals set in the Paris Agreement. Therefore, it is critical to decarbonize the aquaculture operations. The transition of the aquaculture vessels to zero-emission energy carriers such as hydrogen is a promising solution. However, the adoption of hydrogen solutions is hindered by non-existing hydrogen infrastructure. 

To address this challenge, we formulate a location routing problem, considering the location of hydrogen fuelling stations and the routing of aquaculture vessels. The aim is to decide the location of fuelling stations, which routes to serve, and the number of necessary vessels to satisfy demand. The objective is to locate hydrogen fuelling stations so that opening costs of fuelling stations, vessel costs and routing costs are minimized. We solve the problem using a path flow model formulation with a priori route generation.

Our case study is inspired by the real case from Norway considering a location routing problem of well-boats in Lofoten and Vesterålen.   
",Locating Fuelling Stations for Zero-Emission Aquaculture Vessels,"[78080, 62559, 78088, 78087]",580,"[95, 109, 64]",2781,Location of Alternative Fuel and Charging Stations,29,3,61,Locational Analysis,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S10 [building - 101],"['OR in Fisheries', 'Programming, Integer', 'Location']",MB-61
"This study introduces a decision-making methodology employing probabilistic control to reduce the bullwhip effect within recycling-inclusive closed-loop supply chains. Replenishment policies are refined to incorporate recycling operations, and probabilistic controllers are utilised to manage inventory and information flow dynamics. We construct a state space model that encompasses inventory levels, information flow, and recycling activities, capturing their complex interrelations. This model is analysed under stationary conditions, with stochastic customer demand characterised using conditional probability density functions, providing insights into supply chain behaviours.
 
We explore the characteristics of the bullwhip effect and assess the overall stability of the supply chain. Simulations are conducted to evaluate the effectiveness of our probabilistic control strategy in managing and stabilising the supply chain during such disturbances. The ultimate goal is to enhance supply chain efficiency and resilience in the face of uncertainties through this innovative control approach.
",Probabilistic Control for Reducing the Bullwhip Effect in Recycling-Enhanced Supply Chains,"[50586, 78090]",928,"[20, 138, 125]",2782,Information sharing in sustainable supply chains,18,15,23,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,82 [building - 116],"['Control Theory', 'Supply Chain Management', 'Reverse Logistics / Remanufacturing']",WD-23
"In this talk we consider a fleet composition and delivery routing problem, performed by a heterogeneous fleet of both conventional and electric vehicles under uncertain demand. Multi-trips for recharging operations of the electric vehicles to the central depot are considered. The problem is modeled as a two-stage stochastic mixed integer program where the first stage decisions are related to the selection of the vehicles which compose the fleet while second stage decisions concern the routing to satisfy the uncertain customer demand. The aim is the minimization of total operation costs due to the initial acquisition of fleet components, the travel time of each vehicle and an extra penalty cost in case of unserved customers. A tradeoff between cost and emissions is evaluated to analyze the impact of the selection of electric vehicles in the delivery fleet. Scenarios representing the stochasticity in customers’ demand are generated through a kernel density estimation approach. Computational experiments are carried out on instances based on real data of a large Italian delivery company. The impact of stochasticity on demand is examined through stochastic measures. Some managerial insights are finally discussed.",A two-stage stochastic programming approach for an electric fleet composition and mix vehicle routing problem.,"[76647, 24015, 67662, 78093]",573,"[136, 145, 137]",2783,Location and transportation problems under uncertainty,49,10,35,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,44 [building - 303A],"['Stochastic Optimization', 'Vehicle Routing', 'Strategic Planning and Management']",TD-35
"Recently, there have been a growing number of challenges related to processes that are taking place thanks to interactions between people [their communities]. Society creates a set of social behavioural phenomena [e.g., the conduct of voters during elections; the behaviour of the crowd during a terrorist attack; people’s behaviour during pandemics; institutions’ social capital development processes; the behaviour of social network users; etc.], whose knowledge of mechanisms is essential for building a sustainable society. We aim to review the operationalisation of such phenomena as the expression of logical and verbal communication in order to develop a paradigm of rational choice for the efficient understanding of their nature and relevant decisionmaking, thus bridging OR with social sciences and humanities. The creation of such a paradigm on the basis of the theory of structural equation modelling [SEM], multi-agent modelling and game theory, together with data science and mathematical sociology methods, allows the development of data-driven operationalisation for evidence-based solutions.",Modeling&Simulation of Social-Behavioral Phenomena in Creative Societies,[65736],469,"[25, 100, 50]",2785,Modelling social-behavioural phenomena in creative societies,13,15,07,Behavioural OR,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,1019 [building - 202],"['Decision Analysis', 'OR in Sustainability', 'Game Theory']",WD-07
"Seaports, as substantial consumers of energy, aim to adopt energy management systems in response to rising energy costs and a commitment to sustainable operations. Many ports are shifting to electricity as their primary energy source, opting for fully electrified equipment like electric vehicles [e.g. battery AGVs] instead of carbon-intensive alternatives. This paper presents a mixed integer linear programming model designed to address the integrated planning and energy management challenges faced by seaports utilizing a port microgrid. The planning module of the model determines the allocation of electric vehicles and related equipment, as well as the scheduling of EV charges, specifying the energy usage for each one-hour period. Additionally, the planning module decides the berthing duration for each ship, influencing the hourly energy consumption of the port. The energy management aspect involves aligning energy demand and supply, considering various energy pricing schemes, bidirectional energy trading among different sources [such as the utility grid, renewable energy sources], and integrating energy storage systems. The study's findings suggest that the implementation of smart grid technology, represented by the port microgrid, can yield substantial cost savings compared to traditional setups. Notably, efficient charge planning and battery management within microgrid systems enable ports to achieve significant cost reductions.",Optimising electric vehicles charging and energy management in port microgrids,[75745],173,"[70, 143, 93]",2788,Energy Management in Ports and Shipping I,52,8,62,OR in Port Operations,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S12 [building - 101],"['Maritime applications', 'Transportation', 'OR in Energy']",TB-62
"Chemical recycling represents a new option for addressing challenges related to mixed and contaminated plastic waste. The chemical composition of the resulting feedstock is equivalent to that of virgin feedstock. Consequently, a product created from a blend of recycled and virgin feedstock exhibits the same quality as a product exclusively derived from virgin feedstock. Thus, the sole distinction between a “green” and a “brown” product lies in the claimed origin of the feedstock.

This opens the option to detach recycling credits from the physical product, allowing for credit transfer without the necessity for physical transport. However, this practice, known as multi-site attribution, is controversial.  While industry claims attribution flexibility to be necessary for the economic viability of plastics recycling non-governmental organizations [NGOs] strongly oppose it and position it as green-washing. Our research addresses this controversy and seeks to unravel the intricate dynamics of chemical recycling, attribution policies, and the evolving landscape of the recycling industry. Our aim is to assess the effects of various attribution policies on the environmental and economic performance of manufacturers through the application of an analytical model. Additionally, we aspire to provide policymakers with valuable insights into the ramifications of the transfer of recycling credits.
",Economic and Environmental Implications of Transferring Green Feedstock Credits,"[72746, 4229]",926,"[100, 50]",2790,Recycling,18,13,23,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,82 [building - 116],"['OR in Sustainability', 'Game Theory']",WB-23
"This study examines a firm adopting offshoring, the strategic relocation of business processes abroad. Motivated by cost benefits like cheaper labor and regulatory advantages, offshoring may enhance competitiveness and market access.
However, in today's dynamic market, the strategic advantage of offshoring can quickly transform into a liability for firms. As consumers grow more discerning and socially conscious, factors like the MADE IN effect [the preference/aversion toward products based on their country of origin] alongside rising ethical concerns about labor exploitation and environmental sustainability, may challenge the efficacy of offshoring. Moreover, unforeseen circumstances such as war, pandemics, or social unrest can disrupt operations abroad, prompting companies to reassess their production strategy.
Given these factors, we consider an external disruption of social, economic, or geopolitical origin, occurring unexpectedly and leading to a regime shift within the firm's operational environment.
The objective of this research is to investigate how the firm should adjust its offshoring strategy in anticipation of and in response to such regime shifts. The problem is structured and addressed as a 2-stage optimal control problem with a stochastic switching time. The study aims to provide insights into proactive measures firms can employ to adapt their offshore operations effectively, mitigating risks and maintaining competitiveness in a dynamic global environment.",Offshoring and reshoring under social and economic uncertainty,"[78063, 16165]",897,"[82, 137, 33]",2793,Optimal control and resilience,90,9,33,Optimal Control Theory and Applications,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,42 [building - 303A],"['Optimal Control', 'Strategic Planning and Management', 'Economic Modeling']",TC-33
"Value at Risk is one of the most widely implemented methods in the financial sector for assessing the traditional market risk associated with a portfolio. We focus our attention on nonlinear pay-off derivatives such as options, proving the effectiveness of the proposed methodology also for non-standard options. To calculate the future projections of the analyzed portfolio, we implement a full-repricing forward-looking Monte Carlo method. Based on data from the last five years, we compute for each of the eleven stocks, on which options are written, the volatility implementing six different techniques [Close-to-Close, High-Low, High-Low-Close, EWMA, GARCH[1,1] methods and Implied Volatility], the dividend using two alternatives [Historical and Implied measures] and the drift using both traditional econometric approaches like ARIMA and  innovative like LSTM technique. Subsequently, having estimated the volatility and the drift, we have implemented those for simulating the underlying prices for the next periods using a Geometric Brownian Motion. Doing this, we have used only the most prudential scenarios for the projection of the assets. We have consequently considered all the possible combinations for these inputs in the option pricing model, estimating the most reliable forward-looking fair value distributions of each option over the next days through 20,000 daily simulations. Value at risk has then computed taking the percentiles of this distribution.",Value-at-Risk of an Option Portfolio Under Different Scenarios. A Proposal of a More Reliable Market Measure,"[77787, 78570, 78572, 78574]",277,"[44, 45, 126]",2794,Risk Management and Cryptoassets,4,8,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S14 [building - 101],"['Finance and Banking', 'Financial Modelling', 'Risk Analysis and Management']",TB-63
"Decision problems are often characterized by complex criteria dependencies, which can hamper the development of an efficient and theoretically sound multicriteria decision aid model. These criteria interactions have the form of either redundancy or synergistic effect and require arduous and demanding preference statements for their quantification. In this paper, criteria interactions in decision models are addressed with the proposition of an MCDA framework, coupling the preference elicitation protocol of the method of cards and the Choquet integral preference model, which is approached as an importance index. An interactive robustness control algorithm ensures the concurrent acquisition of a stable decision model and satisfactory evaluation results. Robustness is assessed with a portfolio of robustness indicators, spanning from the variability of the preference parameters to the reduction of the model’s feasible space and rank acceptability indices. At the algorithm's core, a heuristic module generates pairwise elicitation questions and selects those delivering the highest expected information gain. The whole framework is stress-tested with a small-scale decision problem, where three versions of the heuristics are automatically applied, with the machine randomly answering the questions. Subsequently, the same problem is approached with the involvement of a real decision-maker, to appraise the required cognitive effort and receive valuable feedback. ",Integrated Assessment of a Robust Choquet Integral Preference Model for Efficient Multicriteria Decision Support,"[36647, 71822, 53365, 75114]",388,"[77, 26, 25]",2796,Robustness analysis in MCDA  1,44,5,44,Multiple Criteria Decision Analysis,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,20 [building - 324],"['Multi-Objective Decision Making', 'Decision Support Systems', 'Decision Analysis']",MD-44
"In this talk we shall explain the reasons why interior point methods
[IPMs] deliver particularly attractive features when they are applied
in the context of decomposition methods, cutting plane schemes
and column generation techniques.
These features include - 
[1] generating epsilon-subgradients in Benders and Dantzig-Wolfe decomposition,
[2] finding [stable] well-centred solutions of restricted master problems,
[3] delivering on-demand accuracy in column generation.
It goes without saying that these features cannot be delivered
by the simplex-based solvers.
Some of the advantages will be illustrated when solving 
very large discrete optimal transport problems. 

References - 
J.Gondzio, 
Interior Point Methods in the Year 2024, 
Technical Report, February 28, 2024.  
Submitted for publication.

","Decomposition, Cutting Planes and Column Generation with Interior Point Methods",[9890],239,"[13, 5, 60]",2797,Continuous Solvers,76,4,30,Software for Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,53 [building - 208],"['Column Generation', 'Algorithms', 'Interior Point Methods']",MC-30
"This study proposes and analyzes a last-mile retail supply chain model. Consumers follow an Economic Order Quantity model to decide their quantity and frequency of shopping from the closest retail store. The retailer determines its store density to minimize the total logistics costs per unit of goods sold. In anticipation of the decisions from consumers and the retailer, the government imposes a carbon tax to reduce emissions. In contrast to the literature, we find that the impact of the carbon tax on the last-mile supply chain is not monotone. Depending on the parameters, a carbon tax may lead to a sparser or denser retail network. Surprisingly, there may even be a win-win scenario where a carbon tax incentivizes the retailer, who is primarily focused on cost minimization, to adopt a supply chain design that simultaneously minimizes costs and emissions [the win-win]. Through numerical analysis using data from the United States and Germany, we find that denser last-mile retail networks are currently necessary to reduce emissions in both countries. The win-win scenario could potentially be achieved through, e.g., innovations in electrification. ",The impact of carbon tax on last-mile retail supply chain,[78098],540,"[100, 138, 151]",2802,Sustainable Logistics,19,8,24,Sustainable Supply Chains,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,83 [building - 116],"['OR in Sustainability', 'Supply Chain Management', 'Practice of OR']",TB-24
"The evolution of mathematical programming has revolutionized our ability to address once-deemed intractable real-world problems on a large scale. Despite the efficiency of modern optimization techniques, the reluctance to accept provably optimal solutions persists, largely attributed to the perception of optimization software as a black box by many stakeholders. While well-understood by the scientific community, this lack of transparency poses a barrier to practitioners. We advocate for a paradigm shift by emphasizing the importance of incorporating aspects of interpretability and explainability in mathematical optimization. By clarifying the concepts of explainability and interpretability, we aim to bridge the gap between the advanced techniques understood by scientists and the accessibility required by practitioners. We will showcase initial steps taken in this direction and engage in a discussion on potential future directions.",Explainability and Interpretability in Mathematical Optimization,[50791],431,"[7, 62, 8]",2804,Interpretable Optimization Methods and Applications,14,13,03,Data Science Meets Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1005 [building - 202],"['Analytics and Data Science', 'Knowledge Engineering and Management', 'Artificial Intelligence']",WB-03
"This study explores the efficiency of integrated water- and land-based transportation [IWLT] systems in addressing the needs of growing urban populations. This has become more challenging under the regulations limiting the sizes and the reach of freight vehicles in cities to improve mobility and reduce carbon footprint. In the proposed systems, vessels can provide flexible and efficient means of transporting goods to supply light electric freight vehicles in the cities. The cost of an IWLT system depends on the design of transshipment facilities, where goods are transferred from the water to the road network. Besides allocating resources in these facilities, this study also examines flexible IWLT systems using public spaces as on-demand meeting points like parking areas and public transportation stops. Such systems introduce more complexity for the logistics services due to the interdependence between interacting vehicles requiring synchronization. We develop a decomposition model to formulate the optimization problems at both networks as well as at transshipment points under the real time resource capacities. Experiments on the benchmark instances show that on average, flexible IWLT services provides a savings of 5% in terms of total travelled distances, 10% less on the roads with fewer vehicles and 30% more over waters, compared to the dedicated systems. Moreover, we assess these systems based on a case study we designed for a delivery service in Amsterdam, the Netherlands.",Optimizing City Logistics - Assessing the Efficiency of Integrated Water- and Land-based Transportation Systems,"[74378, 66802, 73411]",540,"[143, 79, 77]",2805,Sustainable Logistics,19,8,24,Sustainable Supply Chains,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,83 [building - 116],"['Transportation', 'Network Design', 'Multi-Objective Decision Making']",TB-24
"Discrete choice models [DCMs] provide probabilities for individuals choosing a certain alternative when faced with a set of limited options. DCMs can be parametric or non-parametric. Parametric models are easier to estimate but require assumptions about individuals' preferences, while non-parametric models rely solely on training data without any assumptions. Ranked-list methods are popular non-parametric models and capture individuals’ behavior by associating them with preference lists of options sorted in decreasing order of preference. Individuals are assumed to always choose the option best placed in their preference list when confronted with an alternative.  Despite the generality and simplicity of ranked-list methods, a major drawback associated with them is the exponential increase in the number of potential lists. Column generation [CG] can be employed to address this issue, with the CG subproblem being modeled as a generalized linear ordering problem [GLOP]. In this work, we propose a dynamic programming algorithm to solve GLOPs. The proposed method is generic and capable of handling different settings without requiring drastic changes in its implementation. When incorporated into maximum likelihood and minimum L1 estimators, our algorithm efficiently generates preference lists. The algorithm performs well when facing instances with several observations, which is crucial as non-parametric choice models heavily rely on data volume for accurate estimations.",Modern column generation for the estimation of non-parametric discrete-choice models,"[77398, 50537, 73478, 913]",695,"[124, 7, 13]",2806,Customer behaviour,11,5,59,Pricing and Revenue Management,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S08 [building - 101],"['Revenue Management and Pricing', 'Analytics and Data Science', 'Column Generation']",MD-59
"In today’s consumer-driven society, the issue of product repairability and the Right-to-Repair [RTR] movement have seen significant attention and debate. As these RTR laws and regulations are very recent, there is no evidence of their impact on the desired issues associated with the repair. It is not even clear how well known RTR legislation is among the general public, and what consumers expect from such legislation. Those expectations and whether as well as how they are eventually satisfied will shape the effectiveness of the legislation. 
To provide a comprehensive insight into the factors influencing expectations of RTR legislation, we conducted focus group interviews and a quantitative online survey with participants from both Austria and the United Kingdom [UK]. Our main finding is that [in general] past experiences only indirectly affect the expectations about RTR legislation, namely via repair intention. These results are surprisingly robust when we zoom into the different types of repair scenarios, DIY and use of a repair service. Finally, comparing Austria and the UK, the results are strikingly robust. This research contributes to the growing body of knowledge on the RTR legislation and its implications for consumer behaviour, sustainability, and public policy. Moreover, this study offers guidance for policy-makers and repair companies in understanding the factors that influence consumer expectations of RTR legislation.
",How do past repair experiences affect expectations about right-to-repair legislation?,"[77825, 9703, 78104, 78105]",7,"[139, 69, 138]",2809,"Circular Economy, Remanufacturing and Recycling",18,2,23,"Circular Economy, Remanufacturing and Recycling ","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,82 [building - 116],"['Sustainable Development', 'Manufacturing', 'Supply Chain Management']",MA-23
"Within the context of optimization on manifolds, a research direction of particular interest is the investigation of algorithms fit to optimize non-smooth objectives. This research area is relevant since the need for optimizing non-smooth objective functions arises in many real-world problems and applications such as image and signal restoration, denoising, inpainting, etc. In this talk, we introduce the convex bundle method to solve convex, nonsmooth optimization problems on Riemannian manifolds. Each step of our method is based on a model that involves the convex hull of previously collected subgradients, parallely transported into the current serious iterate. This approach generalizes the dual form of classical bundle subproblems in Euclidean space. Several numerical examples implemented using the Julia package Manopt.jl illustrate the performance of the proposed method and compare it to other non-smooth optimization algorithms.",The Riemannian Convex Bundle Method,"[78100, 61690, 78102]",58,"[81, 5, 21]",2810,Optimization on Manifolds,69,8,41,Optimization on Geodesic Metric Spaces - Smooth and Nonsmooth,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,97 [building - 306],"['Non-smooth Optimization', 'Algorithms', 'Convex Optimization']",TB-41
"Analyses of transport systems require knowledge of the interactions between travel demand and transport system performance. In general, the travel demand depends on transport system performance, and in turn the transport system performance depends on how many users use the same element at the same time.
In the literature, rail and metro systems are generally considered uncongested [i.e. performance does not depend on the travel demand] at least as regards train running times.
This contribution examines the dependence of train running times on the number of transported users. Indeed, in rail and metro contexts, the weight of carried passengers in the case of a full load has the same order of magnitude as the unloaded train weight. Hence, variation in the maximum traction effort and the motion resistance between a fully loaded and a completely empty condition implies variations in train running times. Moreover, in frequency services, even in the case of constant demand [i.e. rigid demand assumption], the number of users boarding each train depends on the service frequency. Hence, since service frequency depends on the running times through the cycle time, it is possible to identify a fixed-point problem between the number of boarding passengers and train running times.
The application to the case of Line 1 of the Naples metro system has shown the extent of these variations and the extension times conditions that allow defining a cycle time independent of demand variability.",The influence of passenger weight on rail and metro running times - Theoretical formulation and applications to the case of Line 1 of the Naples metro system [Italy],"[1891, 78107, 78109, 62183]",975,"[143, 122, 119]",2811,Transportation Network Modelling and Optimization II,6,3,55,Transportation,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S02 [building - 101],"['Transportation', 'Railway Applications', 'Public Local Transportation Systems']",MB-55
"Renewable energy generation plays a crucial role in the energy transition. Nevertheless, as many Renewable Energy Sources [RES] are intermittent, specifically photovoltaic [PV] generation, there are often mismatches between energy production and consumption. These mismatches can be partially reduced by storage systems.
In this paper we investigate the investment decision in a PV power plant coupled with a Battery Energy Storage System [BESS], namely an Energy Storage System [ESS]. In our setting, the BESS is connected to the national grid and the PV plant - energy can be produced, purchased from the grid, stored, self-consumed, and fed into the national grid.
PV production and energy consumption loads evolve stochastically over time. In addition, BESS are costly, and energy stored has an opportunity cost, which depends on the prices of energy purchased from the national grid and energy fed into the grid, respectively. Nonethless, BESS can significantly contribute to increase ESS managerial flexibility and, in turn, ESS value. In detail, we investigate the optimal BESS capacity that maximizes ESS expected net present value. We also analyze ESS management costs to provide insights on ESS optimal management strategy.",Investment decisions in Energy Storage Systems - a real options approach,"[63387, 10666, 78103, 47803]",854,"[20, 37, 136]",2812,Dynamics of the Firm II,90,4,33,Optimal Control Theory and Applications,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,42 [building - 303A],"['Control Theory', 'Energy Policy and Planning', 'Stochastic Optimization']",MC-33
"In this talk, I will provide a high-level overview of recent principled approaches for constructively analyzing and designing numerical optimization algorithms. The presentation will be example-based, as the main ingredients necessary for understanding the methodologies are already present in the analysis of base optimization schemes, such as gradient descent. Based on those examples, I will discuss how those techniques can be leveraged for constructing Lyapunov-based analyses and optimal convex optimization algorithms. The methodology can be accessed through easy-to-use open source packages [including PEPit - https://github.com/PerformanceEstimation/PEPit], allowing to use the framework without the modelling pain. This talk is based on joint works with great colleagues that I will introduce during the presentation. ",Constructive approaches to the analysis and construction of optimization algorithms,[66635],366,"[19, 72, 81]",2815,Computer-Assisted Proofs in Optimization,84,14,32,Advances in large scale nonlinear optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,41 [building - 303A],"['Continuous Optimization', 'Mathematical Programming', 'Non-smooth Optimization']",WC-32
"With the massive growth of app usage and smartphone penetration, an increasing number of third-party developers are offering their complementary apps aiming to target the massive audience of the leading core apps, and thereby enhance user experience. At the same time, leading developers launch their own complementary apps, which compete with the third-party apps in terms of quality. The effect of the third-party developer on the first-party developer is twofold - On the one hand, the former creates added value for the core app, which translates into additional profit for the first-party developer, generated by the core app. On the other hand, the competition between first- and third-party substitutable apps threatens the first-party developer's profit gained from its own substitutable app. Similarly, the third-party developer rides on the big market of the core app, while its own market share may be eroded due to the competition between the two substitutable apps. In this study, we develop a game-theoretical model of a market consisting of three apps - a core app, and two partially substitutable apps, both of which complement the core app. We investigate the app developers' R&D strategies, and derive conditions for launching their complementary apps. In addition, the core-app developer's M&A strategy is analyzed.","First- and third-party complementary apps - R&D investment, launching, and M&A strategies","[69462, 48531]",60,"[138, 32]",2816,Applications of knowledge and innovation in finance,53,4,08,AI & Innovation in Sustainable Finance,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,1020 [building - 202],"['Supply Chain Management', 'E-Commerce']",MC-08
"Low birth weight [LBW] is defined when newborns weigh less than 2,500 grams,and is considered at term when born at greater than or equal to 37 weeks of gestation. The incidence of LBW is considered a significant public health issue, closely associated  with an increased risk of mortality and future morbidity. Furthermore, it is important to highlight that between 15% and 20% of children born worldwide have LBW. Therefore, to overcome this situation, researchers started to develop and use the Geographic Information System to identify areas at risk of health problems and create solutions for these places. Based on this, this study aimed to utilize spatial analysis to verify the distribution of LBW at term and identify high-risk areas within the 645 municipalities in the state of São Paulo from 2012 to 2021. Specifically, this study used the global Moran index to measure the degree of spatial association in the study area and the local Moran index to identify risk areas with high prevalence. The findings indicate potential presence of significant clusters of municipalities revealed by the global moran index, which was substantiated by the local moran index, highlighting three small clusters in the northern part of the state. This approach can assist health professionals and governmental bodies to formulate precise strategies and initiatives to mitigate LBW in these localities. 
",Spatial analysis techniques for diagnosis situational of low birth weight at term in the state of São Paulo,"[77434, 77441, 23231, 77429]",538,"[7, 0]",2817,"Advancements of OR-analytics in statistics, machine learning and data science 12",16,7,06,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,1013 [building - 202],['Analytics and Data Science'],TA-06
"We study an Attended Home Delivery Problem with Recovery Options [AHDP-RO] as a supporting tool for companies that want to ensure timely services within the complex attended home delivery setting. Since missed deliveries can negatively impact customer satisfaction and delivery costs, the problem considers customers' availability through different profiles identifying the probability of them being at home for each time slot of the delivery day. When a missed delivery occurs, the problem implements different recovery policies based on the customer's preferences [e.g., leaving the package in a safe spot, scheduling a second delivery attempt on the same day, or directing the package to a designated collection point]. To optimize the scheduling of deliveries and minimize routing costs, we present a Mixed-Integer Linear Programming formulation in which the cost of a missed delivery is proportional to a penalty [determined by the chosen recovery option] and to the probability of not finding the customer at home during the selected time slot. Moreover, different solution approaches [based on various exact or heuristic technologies] are proposed to solve real-size instances and, in turn, support a multi-attribute analysis for obtaining useful economic and managerial insights on the decisional process we are facing.  ",On the Attended Home Delivery Problem with Recovery Options - Economical analysis and solution approaches,"[42989, 67518, 67738, 68095]",273,"[145, 111, 77]",2818,Urban Logistics and sustainable TRAnsportation - OPtimization under uncertainTY and MAchine Learning,49,4,35,"Stochastic, Robust and Distributionally Robust Optimization","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,44 [building - 303A],"['Vehicle Routing', 'Programming, Mixed-Integer', 'Multi-Objective Decision Making']",MC-35
"We study optimal water management of many sources which are partitioned into surface and groundwater, with water transportation through natural processes being possible among water bodies. Water stock dynamics involve natural inflows and outflows from water users’ abstractions as well as natural processes. Sustainability constraints are introduced by applying terminal conditions on the water stocks. The net benefits of the users are quadratic with concave gross benefits and convex costs based on the amount of water used from each water source. We focus on developing two sequential water abstraction solutions based on serial dictatorship under explicit resource dynamic constraints. The first is when each sequential user commits to a path for the whole planning horizon, and the second is when the serial dictatorship water abstraction decisions are completed in each time period. Simultaneous water management is also studied in terms of cooperative solutions and non-cooperative open-loop and feedback Nash equilibrium solutions. The sequential and simultaneous solutions are then compared in terms of aggregate and individual benefits, and in terms of meeting the sustainability constraints. The results of the comparison are used to characterize water management policies.",From many sources to many users - water allocation mechanisms under resource dynamics,[68811],900,"[147, 84, 139]",2819,Equilibrium detection in applications,63,12,40,"Interfaces Between Optimization, Hierarchical Problems and Equilibrium Detection with Applications","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,96 [building - 306],"['Water Management', 'Optimization Modeling', 'Sustainable Development']",WA-40
"To enhance production planning through a proactive lead time management approach, the Workload Control [WLC] concept encompasses a robust framework with multiple hierarchical planning layers. Despite the considerable body of research on Workload Control [WLC] and hierarchical production planning, there exists a noticeable gap in incorporating behavioral operational research [BOR] into these domains. This present study explores the relationship between instructions from the upper planning layer and the human response behavior of a lower hierarchical level within a comprehensive WLC system. The article particularly emphasizes that the arousal level, a human factor, influences the response behavior of shop floor operators. Through laboratory experiments with a real effort task and mediation analysis, this research reveals that tense arousal significantly impacts operators' response behavior in terms of productivity within a WLC environment with hierarchical planning layers. This outcome contradicts the assumption of the standard hierarchical model of rational behavior at the base level. Furthermore, the study contributed to the theory underpinning the WLC concept and its implementation issues with regard to a human-operated shop floor.",Incorporating Human Factors and Human Behavior into Research on Workload Control,[45470],105,"[10, 105, 138]",2820,Behavioral OR general papers,13,8,11,Behavioural OR,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,12 [building - 116],"['Behavioural OR', 'Production and Inventory Systems', 'Supply Chain Management']",TB-11
"Wildfires impose significant health, environmental, and social costs annually.   This paper presents the development of a predictive and prescriptive framework for deploying three wildfire suppression resources - air tankers, helicopters, and firefighters.  Leveraging fifteen years of historical data on wildfires in Alberta, Canada, we initially train machine learning models to predict both the daily number of fire occurrences and the required hours of suppression resources.  Subsequently, we utilize the forecasts in an optimization procedure to ascertain the necessary number of suppression resources at various bases, aiming to minimize response time.  The optimization procedure employs a queueing model to compute the response time probabilities for fires.  Given resource shortages, the historical level of resource usage serves as a lower bound for the required resources.  To address this, we incorporate survival analysis techniques into the forecasting models to account for data censoring.  Our results can guide wildfire managers and decision-makers in enhancing the acquisition and allocation of fire suppression resources.",Data-driven optimization of wildfire resource deployment,"[54839, 78251, 73986]",336,"[7, 47, 48]",2821,Analytics for Combinatorial Problems from Health Care to the Food Industry,17,10,31,Analytics,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,54 [building - 208],"['Analytics and Data Science', 'Forecasting', 'Forestry Management']",TD-31
"In this work, we show that the heavy-ball [HB] method provably does not reach an accelerated convergence rate on smooth strongly convex problems. More specifically, we show that for any condition number and any choice of algorithmic parameters, either the worst-case convergence rate of HB on the class of L-smooth and μ-strongly convex quadratic functions is not accelerated [that is, slower than 1 − O[κ]], or there exists an L-smooth μ-strongly convex function and an initialization such that the method does not converge. To the best of our knowledge, this result closes a simple yet open question on one of the most used and iconic first-order optimization technique. Our approach builds on finding functions for which HB fails to converge and instead cycles over finitely many iterates. We analytically describe all parametrizations of HB that exhibit this cycling behavior on a particular cycle shape, whose choice is supported by a systematic and constructive approach to the study of cycling behaviors of first-order methods. We show the robustness of our results to perturbations of the cycle, and extend them to classes of functions that also satisfy higher-order regularity conditions.
",Provable non-accelerations of the heavy-ball method,"[78113, 66635, 75405]",366,"[19, 21, 5]",2822,Computer-Assisted Proofs in Optimization,84,14,32,Advances in large scale nonlinear optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,41 [building - 303A],"['Continuous Optimization', 'Convex Optimization', 'Algorithms']",WC-32
"A Cutting Stock Problem [CSP] consists of determining the best way in which a set of larger units [objects] should be cut in order to fulfill the demand for a set of smaller pieces [items] while minimizing a certain objective that can be, for instance, the number of objects or material waste. In industrial contexts, automatic cutting machines have a limited number of unloading stations and, thus, it is essential, in mathematical modeling, for accurate representation to consider these constraints added to the CSP. Such problem is known as the Cutting Stock with Limited Open Stacks Problem [CS-LOSP]. In this research, we present an integer linear programming formulation for the CS-LOSP, and some computational experiments carried out on randomly generated instances.",The cutting stock problem with limited open stacks,"[27729, 78116]",674,"[23, 59]",2826,Cutting and Packing 5 - related topics,81,7,07,Cutting and Packing [ESICUP],"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,1019 [building - 202],"['Cutting and Packing', 'Industrial Optimization']",TA-07
"This contribution presents a linear mixed-integer optimization framework designed to tackle the two-dimensional variable-sized cutting stock problem [2D-VSCSP], particularly incorporating guillotine cuts. In the 2D-VSCSP, the objective extends beyond merely identifying cutting patterns; it involves determining optimal dimensions [width and length] for the panels to be manufactured, aiming to minimize material usage. The Cutting Stock Problems with Variable-Sized Stock were recently introduced in various sectors, notably the textile and cardboard industries. This paper introduces a model relevant to the cardboard industry, accommodating cutting patterns with multiple item types, a feature not previously explored. This enhances the model's flexibility and widens its range of applications. The impetus for this research stems from the honeycomb cardboard sector and is developed in collaboration with a company based in Spain. Furthermore, the model's effectiveness is validated using real-world data characterized by significant variability, demonstrating substantial reductions in material utilization compared to the company's current operational practices.",2D Variable Size Cutting Stock Problem - a real application in the  honeycomb cardboard industry,"[152, 73765, 29039]",874,"[23, 59, 111]",2827,Applications of combinatorial optimization II,64,8,25,Combinatorial Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,011 [building - 208],"['Cutting and Packing', 'Industrial Optimization', 'Programming, Mixed-Integer']",TB-25
"The evolution of automated machining centers, especially in sectors such as aerospace and automotive manufacturing, has been significantly driven by the adoption of Automatic Tool Changers [ATCs] in computerized numerical control [CNC] machines. These ATCs are pivotal for the efficient execution of diverse machining tasks, facilitating rapid tool changes without manual intervention. Central to enhancing the operational efficiency of these centers is the challenge of optimizing the ATC indexing process. This problem, known as the ATC Tool Indexing Problem, is a challenging combinatorial optimization problem. The ATC Tool Indexing Problem without Tool Duplication is a variant of this problem, which can be formulated as a specialized instance of the Quadratic Assignment Problem, with a distinctive structure in its distance matrix. We present a characterization of optimal solutions to the ATC Tool Indexing Problem without tool duplication. We also show that this problem is NP-Complete.",Characterization and NP-Completeness of ATC Tool Indexing Problem without Tool Duplication,"[67429, 220]",882,"[14, 72, 16]",2828,Topics in Combinatorial Optimization I [Contributed],64,14,25,Combinatorial Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,011 [building - 208],"['Combinatorial Optimization', 'Mathematical Programming', 'Complexity and Approximation']",WC-25
"In the last few years, Total Generalized Variation [TGV] regularization has proved to be a valuable tool to remove blur and noise from an image while avoiding the staircase effect and preserving the sharp edges. The TGV model depends on two regularization parameters whose values must be appropriately selected to obtain good-quality restored images.
In this work, we use the Balancing Principle [BP] to formulate the TGV-based image restoration problem as a constrained minimization problem whose objective is an implicit function of the two regularization parameters depending on the image to be restored. The values of the regularization parameters, and the corresponding restored image, satisfying the optimality condition of the formulated problem guarantee that the data fidelity and regularization terms are balanced. A Scaled Gradient projection method is proposed specifically tailored to the BP-based optimization problem. The numerical results show that the proposed approach can effectively restore input images corrupted by several kinds of noise.",A Scaled Gradient Projection method for the realization of the Balancing Principle in TGV-based image restoration,"[78114, 50552, 78119]",133,"[81, 0]",2829,"Nonsmooth optimization and applications, Part I",84,7,32,Advances in large scale nonlinear optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,41 [building - 303A],['Non-smooth Optimization'],TA-32
"Since its introduction, cumulative prospect theory [CPT] has emerged as a significant approach for modeling the irrationality of the decision makers’ behavior in complex real situations. Moreover, the interplay between rational finance theories and behavioral finance framework is becoming an intriguing stream of research. To harmonize these two paradigms, a recent contribution in the literature proposed a modification of the PT value function and several alternative probability weighting functions to extend the classical CPT model.
The contribution of this paper is twofold. On the one hand, our study delves into these alternative value functions and proposes a CPT-based portfolio optimization model that aligns with rational dynamic asset pricing models while ensuring Environmental, Social, and Governance [ESG] compliance in the investment process. Furthermore, our portfolio design integrates cardinality, box, and budget constraints, while an upper bound for the portfolio turnover maintains control of the transaction costs during the rebalancing phases. On the other hand, to tackle the ensuing optimization problem, we propose a novel hybrid constraint-handling technique embedded into a level-based learning swarm optimizer. Finally, using data from the STOXX Europe 600 Index, we test the profitability of the proposed portfolio optimization strategy.
This is a joint work with Professor Massimiliano Kaucic, who will be co-presenter of the paper at the Conference.",Behavioural portfolio optimization with sustainable attitudes - an application to the STOXX Europe 600,"[78097, 78125]",245,"[83, 100, 26]",2833,Portfolio optimization and sustainability,53,2,08,AI & Innovation in Sustainable Finance,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,1020 [building - 202],"['Optimization in Financial Mathematics', 'OR in Sustainability', 'Decision Support Systems']",MA-08
"Efficiently allocating treatments while accounting for operational constraints constitutes an important challenge across various domains. In marketing, e.g., using promotions to target potential customers and boost conversions is limited by the available budget. While much research focuses on estimating causal effects, there is limited work on learning how to optimally allocate treatments. Existing methods for uplift modeling or causal inference that estimate treatment effects do not consider how the estimated effects relate to decisions made based on these estimates. Therefore, a potential downside of these methods is that the resulting predictive model may not be aligned with the operational context, resulting in prediction errors being propagated to the optimization problem and, subsequently, a suboptimal allocation policy. We explore an alternative approach based on learning to rank. In this approach, the idea is to directly learn an allocation policy that prioritizing instances for treatment in terms of their treatment effect. We explore different ranking metalearners and propose an efficient sampling procedure for the optimization of the ranking model to scale our methodology to large-scale data sets. We validate our methodology empirically and show its effectiveness in practice through a series of experiments on both synthetic and real-world data.",Exploring Learning to Rank for Optimal Treatment Allocation,"[69331, 46180, 79401, 79400]",63,"[7, 8, 26]",2834,Causal Machine Learning,17,4,31,Analytics,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,54 [building - 208],"['Analytics and Data Science', 'Artificial Intelligence', 'Decision Support Systems']",MC-31
"Teleoperated vehicles are a promising concept for increasing the attractiveness and profitability of car-sharing services. Such vehicles can be remotely steered by an operator to the location of an on-demand vehicle requesting user. It eliminates the need for users to walk to a car-sharing vehicle and the need for providers to relocate vehicles with drivers on site to meet the temporal and spatial vehicle demand.
To ensure an acceptable service level, an effective utilization of the vehicle fleet and the number of operators is required. The decision process involves deciding which vehicle should be steered next by which operator to fulfil which user request. This is challenging as the future demand and the future availability of vehicles are uncertain, since the rental duration and return location are unknown.
We tackle this problem with a two-step allocation algorithm that combines a cost function approximation with the prediction of future vehicle returns to minimize the waiting time for current and potential future users. The first step identifies effective vehicle-user matchings which balance waiting times and driving efforts. In the second step, the sequence of identified matchings to be executed by the operators is determined, anticipating potential improvements by predicted future vehicle returns. We demonstrate the merits of our approach in comparison to intuitive benchmark policies in a computational study highlighting the characteristics of this novel problem.",Dynamic Routing and Scheduling Optimization of Teleoperated Car-Sharing Service,"[74993, 72509, 46258, 69880]",745,"[145, 143]",2835,Dynamic Vehicle Routing 1,5,9,64,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S16 [building - 101],"['Vehicle Routing', 'Transportation']",TC-64
"The system operators usually need to solve large-scale unit commitment problems within limited time frame for computation. In this talk, we will discuss how by learning and predicting the on/off commitment decisions of conventional units, there is a potential for system operators to speed up their computation significantly. Additionally, we propose a data-driven unit commitment model enabling the system operator to utilize available contextual information in the unit commitment model to enhance decision-making efficiency. We explore whether, and if so to what extent, our proposed data-driven model outperforms stochastic models.

",Unit Commitment Predictor,[77303],317,"[72, 22, 47]",2836,Data Science and Optimization,14,12,03,Data Science Meets Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1005 [building - 202],"['Mathematical Programming', 'Critical Decision Making', 'Forecasting']",WA-03
"We develop and implement a model-free, deep learning-based approach to tracking the performance of any chosen financial strategy in the presence of market frictions. Our method provides the optimal dynamic allocation over a - possibly time-varying - set of trading instruments. To achieve the optimal replication strategy, we introduce a novel architecture called Universal Allocators, inspired by Transformer networks. Its number of parameters is independent of the dimension of the investment universe and the length of the training path, allowing it to handle large portfolios without additional computational cost. We implement an adversarial training scheme using the Wasserstein-1 distance to solve the replication task. To the best of our knowledge, this is the first time that an adversarial training scheme using the Wasserstein-1 distance has been used to solve such a problem. To illustrate our method, we replicate a highly non-linear strategy, similar to a collar index, which provides the daily performance of an underlying asset whose values have been clipped below and above a given threshold. The replication set consists of a large number of vanilla options whose strike prices and expiration dates are defined over a discrete and sparse lattice, making the market incomplete. Using both simulated and S&P500 data, we show that our method provides financially meaningful and interpretable allocations for replicating such a strategy at moderate cost.",Deep learning-based allocation for financial strategy replication in incomplete market.,"[78124, 78128, 78135, 78132]",372,"[44, 66]",2838,Advanced Options Strategies Using O.R. and Machine Learning,4,9,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S14 [building - 101],"['Finance and Banking', 'Machine Learning']",TC-63
"As the field of precision medicine grows, accurately forecasting patients' health trajectories is essential for developing data-driven treatment models. Yet researchers must still contend with high levels of missingness due to issues such as missed bedside readings, data entry error, or in the case of outpatient monitoring, inconsistent patient follow-ups. When forecasting patient health trajectories, even small imputation errors can compound over the given time horizon, complicating the task of missing data imputation and jeopardizing model accuracy. To address this, we develop a novel on-training imputation algorithm - Generative Iterative Multiple Imputation [GIMI] - which estimates missing health data by directly maximizing the accuracy of patients' predicted health trajectories. Rather than separating the imputation and forecasting steps, GIMI leverages state-of-the-art generative neural networks to simultaneously forecast patients' missing and observed health trajectories, and is trained in an end-to-end fashion to maximize forecast accuracy. Using a large clinical dataset of intensive care unit [ICU] patients, we show that GIMI outperforms commonly used benchmarks in forecasting patient health trajectories in the presence of missing data. Notably, we show that the imputation errors of several existing imputation techniques compound over the health trajectory time horizon, whereas GIMI's prediction accuracy remains stable.",GIMI - Generative iterative multiple imputation for improving health trajectory estimation in the ICU,"[77600, 78130, 78133, 78134, 78137, 78138]",210,"[7, 56, 66]",2841,Analytics and the link with stochastic dynamics II,17,8,31,Analytics,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,54 [building - 208],"['Analytics and Data Science', 'Health Care', 'Machine Learning']",TB-31
"A partial [i.e., partially filled] latin square PLS is called premature, if it is not completable to a full latin square [of the same order] but completable to such a latin square after deletion of any of its symbols. The complete knowledge of this kind of PLS would give an answer to the completability problem for latin squares in the following sense - a PLS can be completed if and only if it does not contain a premature PLS as a subsquare. One may also think of a hypergraph whose maximal independent sets are given by the full latin squares of a given order and whose hyperedges are to be described explicitly.   
We survey some known examples for a premature PLS and present a new class obtained from a combination of two well-known types.","A new class of premature, partial latin squares",[22040],202,"[14, 53]",2842,Applications of combinatorial optimization I,64,7,25,Combinatorial Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,011 [building - 208],"['Combinatorial Optimization', 'Graphs and Networks']",TA-25
"   In e-commerce, a robust logistic service provides a competitive edge, necessitating significant investment. Many small businesses opt for third-party logistics to cut costs. Our study explores enhancing revenue for such platforms by examining the benefits of sharing logistic resources. Key questions addressed are:
   a] Is it advantageous for a food delivery platform [FDP] to collaborate on logistic systems with another e-commerce platform, considering its current operations? 
   b] What is the impact of logistic sharing between a food delivery platform and an e-commerce platform on restaurant behavior?
   We developed a non-cooperative game model involving the FDP, a restaurant, an e-commerce platform, and third-party logistics. Both restaurant and e-commerce demands were considered stochastic variables. Two scenarios were evaluated - [1] Without sharing logistic resources between the food delivery and e-commerce platforms, and [2] With shared logistic resources. The aim was to determine the feasibility of resource sharing. The study identified the optimal commission rates from the restaurant and e-commerce platform to the FDP, the optimal marketing index and the optimal service level offered to the restaurant and e-commerce platform, respectively by the FDP, through solving a Stackelberg game-theoretic model. By comparing outcomes from both scenarios, we determine the viability of sharing logistic resources for each player.
",Logistic Service Sharing between a Food delivery platform and an E-com platform,"[78126, 65891, 78334]",108,"[124, 32, 50]",2844,Revenue Management in Sharing/Platform Economy,11,3,59,Pricing and Revenue Management,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S08 [building - 101],"['Revenue Management and Pricing', 'E-Commerce', 'Game Theory']",MB-59
"In this talk the existence of solutions for a system consisting of two inequalities of variational type is discussed. Each inequality is formulated in terms of a nonlinear bifunction and a coupling functional. We consider two sets of assumptions for each of the functional involved in the formulation of the system and show that if the constraint sets are nonempty, closed, convex and bounded then the system possesses at least one solution regardless of the assumption imposed on each functional. If one of the constraint sets is unbounded, then a coercivity condition is needed to ensure the existence of solutions. We provide two such conditions.
The theoretical results are then employed to establish the existence of weak solutions for a mathematical model which describes the antiplane shear deformation of a cylinder made of a nonlinear elastic material and a rigid foundation.",Existence results for coupled systems of variational inequalities,[78122],855,"[21, 81]",2848,Recent advances on Variational Inequalities and Equilibrium Problems III	,51,15,43,Variational Inequalities and Equilibrium Problems - From Theoretical Advances to Real World Applications,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,99 [building - 306],"['Convex Optimization', 'Non-smooth Optimization']",WD-43
"Natural Language Processing [NLP] introduces computational representations of language with the word vectors as a potent approach. This work focuses on Sentiment Analysis for classifying the emotional intent of text. A variety of different types and layers of Neural Networks [NN] are used. 
A group of Deep Learning [DL] algorithms are used for classification starting from a Simple Dense Neural Network with Embedding layer [DNE] and continuing with the Convolutional Neural Network [CNN], the Long-Short Term Memory [LSTM], the LSTM with Dropdown [LSTMD], and the Ensemble model with LSTM and CNN[LSTM_CNN]. Hyperparameters tuning for the NNs [i.e. learning rate, etc.] are also used. The use of hyper parameters will be able to boost the accuracy levels even higher. The grid search method is used for discovering the LSTM hyperparameters for better accuracy.

All approaches are tested for accuracy with Large Movie Review Dataset [IMDB].
In DNE, misclassification of false positive and false negative is noticed.
CNN, with one-dimensional sequence of words, improves DNE performance at classifying film reviews. The filter of the CNN layers obtained better results at learning short sequences. The LSTM surpasses LDTMD and LSTM_CNN.

",Classification for natural language processing,[51716],538,"[66, 7, 62]",2849,"Advancements of OR-analytics in statistics, machine learning and data science 12",16,7,06,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,1013 [building - 202],"['Machine Learning', 'Analytics and Data Science', 'Knowledge Engineering and Management']",TA-06
"We introduce and investigate the multi-product pickup and delivery problem with time windows and alternative destinations. In this problem, we consider a fleet of vehicles with a predetermined capacity for serving multiple customer orders simultaneously. Each order consists of a collection of products, possibly with different pickup locations, and a set of potential delivery points with specific time windows for customer service. To comprehensively investigate the problem, we construct an extensive set of instances that build upon the well-known CVRPLIB dataset. We provide a formal problem description formulated as a MILP and obtain optimal solutions using a branch-and-bound approach. Finally, considering the problem's inherent complexity, we propose a hybrid adaptive large neighborhood search. Our algorithm demonstrates effectiveness in obtaining satisfactory solutions for medium and large-scale instances, which typically pose challenges for exact methods.",The multi-product pickup and delivery problem with time windows and alternative destinations.,[70095],524,"[145, 14, 74]",2855,Freight transportation and logistic III,6,10,55,Transportation,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S02 [building - 101],"['Vehicle Routing', 'Combinatorial Optimization', 'Metaheuristics']",TD-55
"The construction industry is widely recognized as one of the most environmentally sensitive sectors, having a significant impact when adopting sustainable development practices. Environmental, Social and Governance [ESG] stand as the three primary pillars of a company’s sustainability performance, which contrast with the pillars defining sustainable development. Notably, ESG play a critical role in evaluating company performance by revealing inherent risks. Despite the widespread use of indicators, there exists a notable gap in those specifically oriented to assess company performance within the construction industry. However, research in how to achieve a standard framework to evaluate and report ESG performance in companies are still very limited. This paper aims to develop a framework of indicators relevant to the construction industry within a European context, serving as a tool to measure ESG performance. The methodology consists of the characterization of a set of indicators drawn from established protocols for assessing and reporting ESG criteria such as GRI and GRESB. Through a selection process, 169 indicators categorized into environmental, social and governance, enables construction companies to assess comprehensively ESG performance. Future research opportunities include in the definition of a methodology to identify material indicators for each company and the definition of benchmarks in the contextualization of the framework. ",An ESG Indicators Framework for Construction and Real Estate Companies,"[77761, 53128, 6297]",905,"[139, 126]",2856,"How to support complex decisions. Negotiating the trade-off between Social, Environmental and Economic values 2",44,5,47,Multiple Criteria Decision Analysis,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,50 [building - 324],"['Sustainable Development', 'Risk Analysis and Management']",MD-47
"One of the most important aspects of mathematical optimization and Operations Research is getting your data into a form that optimization solvers can understand and work with. The art of modeling, as it is often referred to, can all too easily get in the way of actually solving the problem at hand.
Gurobi's open-source OptiMods are data-driven Python APIs for different common optimization use cases. They enable practitioners and learners alike to compute solutions without requiring extensive modeling experience. This session presents the goals and design of the project and explains how to use and extend it.",Gurobi OptiMods - Painless Optimization Templates,[32758],704,"[134, 72, 84]",2858,Optimization Tools,76,12,30,Software for Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,53 [building - 208],"['Software', 'Mathematical Programming', 'Optimization Modeling']",WA-30
"Image-to-Image translation is an area of computer vision that has become popular recently, with various studies providing different implementations. By using Image-to-Image translation with a large dataset, we can create different versions of the same image, and each version will allow us to see how the image looks in a different type of scene. This study aims to explore this field and provide a new implementation that could provide new insight into how images can be transformed. A famous image-to-image translation project inspires our implementation, which has a different generator architecture and normalization method. The implementation utilizes a Conditional Generative Adversarial Network, which consists of a generator and a discriminator. We have used a mixture of the U-Net architecture and the ResNet type architecture for the generator instead of only using a U-Net architecture. For the discriminator, we have used the PatchGAN architecture, which separates the image into different patches and classifies each patch of the image as either real or fake. Generated images are evaluated manually, and the calculated Structural Similarity Index is planned to be used in an automated evaluation system. We plan to continue improving our output and evaluate our results in other ways.",A new implementation with combining mixture of U-Net and ResNet type architectures for image-to-image translation,"[78141, 78144]",351,"[8, 66, 18]",2860,New Trends in Generative Adversarial Networks and Deep Neural Networks ,71,4,04,Recent Advancements in AI ,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,1001 [building - 202],"['Artificial Intelligence', 'Machine Learning', 'Computer Science/Applications']",MC-04
"We consider partial inverse optimization problems, which are bilevel optimization problems in which the leader aims to incentivize the follower to include a given set of elements in the solution of their combinatorial problem. For solving partial inverse combinatorial optimization problems with only weight increases, we present a new branch-and-bound scheme.
In this talk, we focus on the partial inverse shortest path problem with only weight increases. Branching on follower variables, the scheme relies on two different methods that are basically complete inverse shortest path problems on similar graphs, which are known to be solvable in polynomial time. Computational experiments suggest that for dense graphs our branch-and-bound scheme outperforms an MPCC reformulation as well as a decomposition scheme.",A Branch-and-Bound Scheme for a Class of Partial Inverse Combinatorial Optimization Problems,"[36110, 75093]",466,"[11, 14, 50]",2861,Robust and Multi-Level Optimization,86,10,04,MINLP,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,1001 [building - 202],"['Branch and Cut', 'Combinatorial Optimization', 'Game Theory']",TD-04
"The shift minimization personnel task scheduling problem concerns the optimal assignment of a multi-skilled workforce to skill-specific tasks and the objective is to minimize the number of shifts utilized. Due to its widespread applications in personnel scheduling, various efficient algorithms have been proposed for this problem which solves most of the benchmark instances to optimality. However, some of these instances remain extremely hard to solve. Various statistical experiments have been conducted to analyze what makes some of these instances hard and as a result, a new set of benchmark instances have been generated which offer a significant challenge to the state-of-the-art algorithms. 

In addition, this shift minimization problem translates as the list coloring problem on interval graphs, a classic coloring problem on graphs. Through this study, we also investigate what are those  graph properties that significantly affect the difficulty in coloring interval graphs. We observe that various scheduling problems in the literature can be polynomially reduced to the list coloring on interval graphs and therefore, our observations and insights can shed light on what makes certain interval scheduling problems and specifically the subsets of their benchmark instances hard. 
",Instance space analysis of the shift minimization personnel task scheduling problem,"[71999, 18783, 70803, 71177]",881,"[129, 14, 53]",2863,Topics in scheduling [Contributed],64,14,26,Combinatorial Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,012 [building - 208],"['Scheduling', 'Combinatorial Optimization', 'Graphs and Networks']",WC-26
"This study focuses on developing a mathematical model that addresses a train timetabling problem arising at Companhia Brasileira de Trens Urbanos [CBTU], a railway company in Brazil. The objective of the problem involves determining arrival and departure times at multiple stops along the network.

The operation of a railway system heavily depends on a well-organized timetable. Adherence to safety protocols regarding vehicle operations on tracks is also crucial. Therefore, the objective of this work is the allocation of routes and timetables for trains, while enhancing customer satisfaction and considering operational constraints.

The CBTU train line covers a 30-kilometer route from Santa Rita to Cabedelo, both cities located in the state of Paraíba, Brazil. Approximately 8 thousand passengers make use of the railway daily. Hence, determining a timetable impacts the lives of passengers needing safe, convenient travel in sync with their routines. Furthermore, trains must travel in opposite directions using a single track simultaneously, and collisions pose a significant risk if schedules are not carefully planned.

Preliminary results suggest that the developed formulation can handle small-scale instances. However, the current model could perform more effectively when dealing with real-life scenarios. The next phase of the research involves devising a stronger formulation and exploring [math]heuristic approaches capable of tackling larger instances.",A mathematical formulation for a real-life single-track train timetabling problem,"[74255, 73369, 74392, 67821]",243,"[65, 143, 142]",2864,Models and algorithms for real-life combinatorial optimization problems,64,12,52,Combinatorial Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,8003 [building - 202],"['Logistics', 'Transportation', 'Timetabling']",WA-52
"This paper reports on a novel software platform that aims to facilitate and enhance diverse open innovation practices in contemporary organizations. The proposed solution builds on and advances a series of state-of-the-art artificial intelligence and natural language processing tools and technologies to meaningfully cluster and aggregate stakeholders’ feedback. In addition, it adopts an argumentation-based collaboration approach to augment knowledge reification and co-creation. Through dedicated services, the platform augments knowledge management and informed decision-making in the associated data-intensive and cognitively complex settings. The proposed solution has been shaped through long collaboration among diverse types of stakeholders, through which a series of rich application scenarios have been designed and analyzed. Evaluation results are positive, justifying the rationale of our approach towards integrating machine reasoning features that can automatically organize, analyze and summarize the content of ideas and positions expressed in the settings under consideration.",Leveraging Open Innovation Practices Through Emerging Technologies,"[62178, 78154, 78153]",221,"[62, 8, 18]",2866,"AI in Knowledge, Technology, and Innovation ",54,15,08,"Knowledge, Technology, and Innovation","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,1020 [building - 202],"['Knowledge Engineering and Management', 'Artificial Intelligence', 'Computer Science/Applications']",WD-08
"In today's business landscape, mergers and acquisitions [M&A] are a crucial strategy for growth and development. However, M&A deals often fail due to discrepancies in valuation. To address this issue, this paper aims to present a unique approach to estimating M&A prices by combining fuzzy real options methodology and game theory. By calculating the sum of a company's intrinsic value and M&A incremental value with fuzzy real options, we arrive at an M&A price. However, we find that this price can be influenced by the negotiation process. To determine the final target price during bargaining, the game theory is employed. Our study applies this method to a real case and results show a range of M&A prices that closely match the actual transaction price. This pricing methodology has practical value for corporate decision-making and is particularly relevant in today's competitive and uncertain market environment.",Fuzzy Real Options and Game Theory Approach in M&A Pricing Issue,[78149],92,"[124, 49, 50]",2868,Real Option Analysis,8,15,57,Real Option Analysis,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S06 [building - 101],"['Revenue Management and Pricing', 'Fuzzy Sets and Systems', 'Game Theory']",WD-57
"The constantly-increasing concern on environment and sustainability and the recent events which have pushed energy prices to alarming levels have led manufacturing systems to try to achieve higher standards of energy efficiency. This is the reason behind the growing interest of the research community in the study of optimization problems in production systems that deal with energy-related constraints and/or criteria.  
Among them is the Simple Assembly Line Balancing Problem with Power Peak Minimization [SALB3PM]. The problem attempts to assign the tasks of a production process to the workstations of a paced straight machining line, and to determine their starting times so as to minimize the peak of the overall power consumption profile.  
This work focuses on the SALB3PM and proposes a new Mixed-Integer Linear Programming formulation in which a set of binary three-index variables models the core assignment and trigger decisions. We show that the proposed model outperforms the reference 01LP model on a benchmark instance set.  
We then test the model on new instances in which tasks have non constant power profiles and we propose some valid inequalities and preprocessing. Numerical results are presented and discussed.",A new MILP model for the Simple Assembly Line Balancing Problem with Power Peak Minimization,"[42217, 8293]",871,"[14, 111, 105]",2870,Exact Algorithms and Formulations for Combinatorial Optimization Problems,64,10,29,Combinatorial Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,157 [building - 208],"['Combinatorial Optimization', 'Programming, Mixed-Integer', 'Production and Inventory Systems']",TD-29
"In this work, we present a method for predicting car prices by employing clustering and prediction techniques in a hybrid way. The primary aim is to provide accurate car price predictions and offer valuable insights to potential buyers, sellers, and industry stakeholders. The dataset we use comprises various car attributes, including make, model, year, mileage, tax, and additional features. Before model training, we apply preprocessing and feature engineering to extract distinct patterns and relationships. Methodologically, we use clustering techniques to segment cars into homogeneous groups and utilize prediction algorithms within each group. This not only enhances prediction accuracy but also provides insights into pricing dynamics across different market segments. Experimental results show that the proposed method gives better results compared to traditional approaches.  The integration of clustering and prediction techniques represents a novel advancement in car price prediction, enhancing forecast accuracy and aiding informed decision making for buyers and sellers, thereby promoting a more efficient and competitive market environment.",Improving Car Price Prediction using Clustering,"[77331, 55783]",350,"[66, 25, 7]",2871,Hybrid Appraches in Deep Learning and Machine Learning,71,5,04,Recent Advancements in AI ,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,1001 [building - 202],"['Machine Learning', 'Decision Analysis', 'Analytics and Data Science']",MD-04
"Employees care about being treated fairly and will react strongly to perceived unfairness. As such, organizations are increasingly seeking ways to incorporate employee-centric principles in their decision-making processes. This presents the challenge of balancing two sets of trade-offs -  the organizations well-being versus their employees, and their employees' desire for equality versus equity. Ensuring these tradeoffs are consistently balanced according to the organization’s values at the scale and speed of modern business requires an algorithmic approach. The increasing use of algorithms to guide such value-driven decision-making necessitates a better understanding of how these elements should be integrated within decision-making models. This research explores these complex trade-offs computationally by modeling two representative case studies - a synthetic gig-economy package delivery problem and an empirical interhospital patient transport problem, both of which are NP-hard. One key observation is that certain combinations of functions in the model objective lead to opportunities for organizations to perform better while also representing fairness more precisely. We also find that the effects of trade-off parameters are partially determined by both their relative levels and the functions used in the objective. These and other observations offer guidance for practice while also motivating future research to help fully explore how these concepts shape decision-making.",Can justice be profitable? The benefits of balancing equity and equality in optimization models,"[78157, 58786, 78176]",104,"[41, 84, 145]",2873,Just and ethical sustainability transitions,28,3,20,OR and Ethics,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,45 [building - 116],"['Ethics', 'Optimization Modeling', 'Vehicle Routing']",MB-20
"Our research addresses the need for a realistic assessment of rail traffic management algorithms. We introduce a modular framework integrating demand and supply dimensions for algorithm evaluation. Our approach incorporates passenger simulation via an application programming interface [API] in the commercial rail traffic simulator OpenTrack, enabling realistic route choice modeling and passenger rerouting based on observed choices and traffic conditions. Trains are realistically simulated in OpenTrack, and historical distributions of traffic perturbations are applied via the API. When train delays increase, the API triggers passenger rerouting - depending on the current and planned traffic evolution, passengers decide whether to stick to their original plan or choose a different option. This mimics realistic decisions, where passengers react to updated traffic information. The proposed framework allows for the closed-loop assessment with a traffic control optimization algorithm, ensuring practical feasibility. We validate the framework using diverse scenarios in a Copenhagen suburban rail network, exploring variants of optimization algorithms with objectives including train delay minimization and passenger delay considerations. Our proposal represents the first comprehensive framework integrating passenger behavior and realistic rail traffic simulation, enhancing the evaluation of traffic management algorithms.",Realistic assessment of rail traffic management optimization - a demand-supply simulation framework,"[74028, 73747, 78158, 78159, 67197, 71916, 36073, 78189]",816,"[122, 26, 143]",2874,Railway Traffic Management,85,13,51,Public Transport Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,M5 [building - 101],"['Railway Applications', 'Decision Support Systems', 'Transportation']",WB-51
"The effective prevention of crime requires the availability of empirical data to optimise public resource utilisation and public safety outcomes. In this context, spatio-temporal incident reports can be used for newly developed variable-density cluster analysis approaches, for which empirical data collected over more than two decades in the City of Chicago is used. This provides insights into the evolution of crime type ratios in the twenty-first century so far, with particularly notable effects from the recent COVID-19 pandemic due to shifts in space occupancy, with primary functions of city areas playing an important role. An analysis of spatial autocorrelations at different distances, as a methodology transfer from cosmology, demonstrates variations in incident uniformity between clusters and outlier areas, and highlights the need to question the currently wide-spread practice of cluster epicentre policing. One of the points the discussion focusses on is the role of criminology-oriented optimisation as an aspect of community operational research. This is particularly relevant when it comes to acknowledging risks due to known and unknown data biases, for example due to extraneous and temporally constrained events as well as impacts from spatially varying police-community relations.",Spatio-temporal analysis of variable-density clusters in hot spot policing,[78150],538,"[66, 7, 15]",2876,"Advancements of OR-analytics in statistics, machine learning and data science 12",16,7,06,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,1013 [building - 202],"['Machine Learning', 'Analytics and Data Science', 'Complex Societal Problems']",TA-06
"We analyse a scenario where two competing airlines, are considering new routes to emerging airports. These airlines can design either direct routes or connect passengers via their respective hubs. The total market size for a route is influenced by two key factors - flight frequency and network connectivity, while passengers prefer direct flights over those with layovers. An airline’s revenue on a route hinges on both the overall market size and its capacity share. Therefore, each airline faces a strategic decision regarding the optimal frequency of flights to allocate on each new route. The cost of deploying capacity on a route encompasses several factors - direct aircraft operating costs, airport fees [landing, gate usage], and passenger handling expenses. Airport fees are typically higher at major hubs, which may also impose slot restrictions due to capacity constraints. Airlines’ objective is to maximize their profits. To analyse this frequency competition, we propose a game-theoretic model similar to developed in Wang et al. [2022]. By analysing the model’s equilibrium, we obtain insights, such as the conditions under which airlines might reallocate capacity from established, high-traffic routes to serve emerging airports. The validity of this model has been demonstrated using data from India’s Regional Connectivity Scheme [RCS], which aims to promote air connectivity to unserved and underserved airports.",On airline frequency competition for routes linking emerging airports,[63750],331,"[143, 50, 79]",2877,Airline Applications I,6,4,55,Transportation,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S02 [building - 101],"['Transportation', 'Game Theory', 'Network Design']",MC-55
"LSHADE-SPACMA stands out as a highly potent algorithm, amalgamating a refined version of LSHADE with a modified version of CMA-ES. In this study, we endeavor to augment the exploratory capability of LSHADE by incorporating a transfer operator. Additionally, an alternative mutation strategy is introduced to enhance overall diversity. The proposed optimizer, dubbed ILSHADE-SPACMA, undergoes a rigorous comparison with both the original LSHADE-SPACMA algorithm and LSHADE across 12 functions sourced from CEC2022. These experiments are conducted with varying numbers of evaluations—50,000 and 100,1000—while dimensions are set to 10 and 20. The results underscore the robustness of ILSHADE-SPACMA in swiftly converging towards near-optimal solutions within acceptable timeframes.",ILSHADE-SPACMA - Improved version of LSHADE-SPACMA,"[78162, 78164, 78163]",877,"[5, 19]",2878,Heuristic Algorithms for Combinatorial Optimization Problems I [Contributed],64,14,52,Combinatorial Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,8003 [building - 202],"['Algorithms', 'Continuous Optimization']",WC-52
"Our study investigates allocation strategies at a retailer warehouse that replenishes a multi-period perishable product in case packs to a set of different stores that face stochastic demand. The objective of the retail chain is to reduce food waste and increase freshness, while maintaining an agreed service level in the stores. Since the warehouse has a high service level target for replenishing the stores, it has to keep safety stock, which results in batches with different ages at the warehouse. We showed in a previous study that the waste at the stores can be significantly reduced by unpacking the case packs at the warehouse and/or increasing the remaining shelf life, especially for stores with a low demand. Unpacking at the warehouse allows the stores to order in smaller quantities, but this increases the order picking costs for the warehouse. We quantified in our new research the potential improvements of allocating only a fraction of the stores with unpacked and/or fresher products to limit the additional supply chain costs resulting from applying these strategies.",Quantifying the potential to reduce food waste and increase freshness in a two-echelon divergent single product supply chain.,"[3329, 3330]",424,"[100, 138, 61]",2879,Food Waste,30,2,50,Retail Operations,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,M2 [building - 101],"['OR in Sustainability', 'Supply Chain Management', 'Inventory']",MA-50
"The use of data to improve the performance and operation of sports institutions is increasingly expanding. 
Related to scouting, the most renowned case is that of Moneyball, which summarizes the successful use of data to improve the performance of a US professional baseball team that had a much smaller budget than that of most of its opponents. 
In this work we apply different data science techniques to improve the scouting of a professional soccer team. In particular, we attack the problem of similarity between players, so that a sports manager can have information about who is the player most similar to someone he has to replace, or who he cannot access due to a budgetary issue.
Various similarity schemes were employed to compare players by considering their specific individual characteristics as covariates. 
Some approaches adopted a non-supervised perspective, while others utilized supervised models that incorporated valuable information obtained from the players' known positions on the field as a target for supervision. 
The disparity between these contrasting approaches yielded complementary results.
We take as a case study that of the Real Racing Club de Santander, from Spain Second Division,",Data science models for football scouting - the Racing de Santander case study,"[74207, 78166, 78168, 32010, 74694, 78169, 78170]",668,"[99, 151, 66]",2880,Performance and scouting in football,37,10,16,OR in Sports,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,19 [building - 116],"['OR in Sports', 'Practice of OR', 'Machine Learning']",TD-16
"The assessment of micromobility in Paris is essential for aligning urban transportation practices with the  climate targets set by the European Union, fostering sustainable mobility solutions to mitigate greenhouse gas emissions and promote environmental stewardship.This study presents an innovative approach to evaluating micromobility modes in the urban context of Paris through the application of the Extracted Framework Element Effect [EFEE] methodology within a Multi-Criteria Decision Making [MCDM] framework. The EFEE approach analyzes the impact on overall evaluation when specific criteria are systematically extracted, revealing the relative importance of each criterion in Euclidian environment. Various micromobility options, such as e-scooters, bike-sharing, and emerging alternatives, undergo scrutiny based on criteria relating to their environmental impact, accessibility, safety, and efficiency. Moreover, the research delves into assessing the alignment of these evaluated micromobility modes with the European Union's 2050 climate targets, with a particular emphasis on reducing greenhouse gas emissions and fostering sustainable transportation practices. The findings provide insights into the efficacy of micromobility modes in mitigating climate change and advancing sustainable urban mobility. Ultimately, the study aims to inform policymakers, urban planners, and stakeholders in Paris about the role of micromobility in achieving the EU's long-term climate objectives.",Reviewing Micromobility Modes of Paris City by novel MCDM Approach - Extracted Framework Element Effect [EFEE] ,"[78165, 80024]",890,"[25, 143, 55]",2881,MCDA and urban planning 2,44,9,47,Multiple Criteria Decision Analysis,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,50 [building - 324],"['Decision Analysis', 'Transportation', 'Group Decision Making and Negotiation']",TC-47
"We explore a noncooperative game framework involving two hospitals, where treatment quality suffers under congestion. Recognizing that healthcare demand is significantly influenced by patient age, we incorporate a continuous age distribution into our model. Each hospital aims to determine the optimal treatment[age-structured] intensity that maximizes its objective - for a public hospital, this involves enhancing the cross life-expectancy as measure for the number and quality of treated patients [public hospital]; for a private hospital, the goal is to maximize profits based on public payments for treatment. The resulting problem leads to the introduction of differential games with the closed-loop information structure. The paper formulates conditions for verifying whether a given strategy profile constitutes a Nash equilibrium with the dual closed-loop information structure. The verification theorem is then used to develop a numerical algorithm for determining Nash equilibria in a finite number of steps. The numerical simulations demonstrate how the Nash equilibrium can shift in response to varying socio-economic factors.",Competition between hospitals with age-structured patients and negative congestion effects - a differential game approach,"[78167, 19669, 79377, 52070, 19666]",847,"[56, 50]",2882,Optimal control in organizations,90,5,33,Optimal Control Theory and Applications,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,42 [building - 303A],"['Health Care', 'Game Theory']",MD-33
"The Bertrand competition is an arena where firms compete in the price market. In Cournot games, firms compete by setting their quantities in the market. The attractiveness of Bertrand's games in economics is the visibility of prices to all competitors. 
Games are commonly used in markets with a few decision makers where the playing firms see downward demand functions. 
In this study, we analyze two firms in a duopoly market. The firms sell nonidentical products that satisfy the same customers' needs. 
The demand function for each firm is based on its price and the competitor's price, along with an interaction of both prices on each demand. We show that there is more than one Nash equilibrium point that leads to stability questions, 
The role of the cost function, its functional form, and parameters impact the number of potential equilibrium points. This is an essential issue in markets with technological and innovative products.
The dynamics of Bertrand's games can serve as a mechanism to reveal the costs of the competition.
Along with the optimal paths for the players under varying circumstances, we developed a simulator to handle various scenarios relevant to research.  
",Empirical and Theoretical Results on Bertrand Duopoly with Interaction,[2236],698,"[124, 33, 50]",2884,Pricing Strategies,11,9,59,Pricing and Revenue Management,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S08 [building - 101],"['Revenue Management and Pricing', 'Economic Modeling', 'Game Theory']",TC-59
"This study proposes a maintenance strategy for a degrading system under dynamic working conditions. Dynamic working conditions including environmental factors such as temperature, and humidity, alongside operating factors like production rate and operating speed, present challenges in accurately capturing the degradation process. In addition, the heterogeneity among observed components adds complexity to utilising the health monitoring data. To address these issues, we utilise Bayesian Linear regression and Bayesian Poisson regression to account for shock occurrence and magnitude. During system operation, regression parameters are updated upon the arrival of shocks at each decision epoch. We formulate the optimal maintenance problem as a Markov decision process, with decisions triggered by parameter updates converging towards underlying values. This paper not only establishes an analytically tractable degradation model incorporating component heterogeneity and dynamic working conditions but also theoretically explores the structure of optimal preventive maintenance thresholds. Additionally, to ease the computational burden due to the high state space, we introduce a heuristic algorithm that leverages the most likely distribution. Numerical experiments and a case study validate the effectiveness of the proposed maintenance policy, demonstrating its practical applicability and efficacy in real-world scenarios.",Condition-based Maintenance for a Degrading System under Dynamic Working Conditions,"[77284, 78172, 30977]",797,"[136, 82, 123]",2885,Reliability Models,50,13,39,Stochastic Modelling,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,35 [building - 306],"['Stochastic Optimization', 'Optimal Control', 'Reliability']",WB-39
"Quite often, scientific papers are the result of collaboration among many researchers. Each researcher focuses primarily on their part of the work, having different ideas about the parts provided by others. It leads to overestimating one's contribution to the research and underestimating the contribution and importance of others. In addition, those whose contribution to the study is relatively significant have a better understanding of the value of the work of the other team members. Therefore, participation is often presented as a verbal description of the work. Unfortunately, this is insufficient when the need is to distribute the reward fairly [e.g., a monetary bonus] for the achievement. 
In the work, we propose a group decision-making method that establishes some compromise numerical ranking based on pairwise comparisons of alternatives. The model will, on the one hand, consider all stakeholders' opinions and, on the other hand, will give these opinions the right weight. The presented model can be used in many situations where there is a need for proper reward distribution, and the rewarded team wants to avoid entrusting the reward distribution to one individual.",My Part is Bigger than Yours! A Proposal for a Consensus Finding Model in a Team of Peers,[47594],894,"[25, 27, 6]",2886,Pairwise comparisons and preference relations 4,44,13,44,Multiple Criteria Decision Analysis,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,20 [building - 324],"['Decision Analysis', 'Decision Theory', 'Analytic Hierarchy Process']",WB-44
"Many optimization models and algorithms have been proposed for the railway timetabling problem over the past decades. However, in practice, timetables are still constructed by planners with little to no optimization support. One step towards closing the gap between the state of art and state of practice is to research user experience aspects related to optimization-based timetable planning tools, and thereby better understand when and how the planners could gainfully use the algorithms in their everyday work.

In MOTIONAL WP6, RISE investigates user experience aspects that come into play when a timetable planner working for an infrastructure manager wants to use optimization/automation to solve conflicts for a train, or a set of trains, in the long-term planning process.  “Participatory Design” is used to design a timetable planning support tool with optimization. The purpose of participatory design is to give a variety of stakeholders who will use the tool the opportunity to provide input on function, interaction, and visualizations to create a high-quality tool that supports their daily work. At the same time, optimization researchers will ensure that the design suggestions are likely to be implementable. 

In this presentation we will share our current status of work, including knowledge gained from interviews, a hierarchical task analysis and a first sketch of the functionalities developed in the “Participatory Design” process.
",User experience and timetable optimization ,[78035],182,"[26, 122, 142]",2887,Europe's Rail MOTIONAL - Algorithms for railway planning,85,5,54,Public Transport Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S01 [building - 101],"['Decision Support Systems', 'Railway Applications', 'Timetabling']",MD-54
"Firm growth and profitability are typically achieved through new customer acquisition and loyal customer retention, often relying on non-discriminatory mass marketing. Given the rise of customer databases and firms’ natural desire to build lasting customer relationships, retailers increasingly employ direct marketing measures. Yet the associated marketing campaigns tend to be wasteful and myopic, leaving a vast potential untapped. Bespoke Direct Marketing Optimization [BDMO] comprises [i] campaign selection, [ii] campaign sequencing, and [iii] the targeting of [sub]segments for [re]activation while considering multiple objectives. We propose an optimization model that solves these decisions simultaneously while adhering to multiple business constraints, incorporating actual customer buying behavior, and accounting for the multi-period customer migration, which, despite its major implications, is often ignored by other DMO models. We also describe a heuristic that can be applied to larger problem instances. We dramatically reduce the complexity and runtime of the optimization problems while still obtaining nearly optimal results. Using a real-world planning problem, we perform a numerical study for testing and benchmarking. There is a limit to the performance effects of increasing the budget for direct marketing models; any improvement beyond that limit is possible only with models and/or measures that are more bespoke.",Bespoke Direct Marketing Optimization,"[78171, 23418]",428,"[71, 32, 113]",2888,Retail Analytics,30,7,50,Retail Operations,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,M2 [building - 101],"['Marketing', 'E-Commerce', 'Programming, Nonlinear']",TA-50
"We deal with the Standard Fractional Quadratic Quadratic Programming Problem [StFQQP], consisting of the minimization of the ratio of two quadratic functions over the standard simplex. We assume the denominator is strictly convex with a symmetric positive definitive [SPD] matrix, so the objective function is well-defined on the feasible set. 
Without loss of generality, we also assume the numerator is strictly convex with an SPD matrix. 
Contrary to the case where the numerator is a linear function, a stationary point [SP] of StFQQP is not necessarily a global minimum because the objective function is not quasi-convex.

We introduce a new sequential algorithm for the StFQQP, exploiting a reformulation as a mathematical program with complementarity constraints derived from the KKT conditions of the StFQQP, which are in the form of the so-called symmetric Eigenvalue Complementarity Problem [EiCP]. Solving an EiCP is then equivalent to computing an SP to the StFQQP.

Preliminary computational experiments with various test problems suggest that the sequential algorithm is a promising technique for solving the StFQQP.
",Solution of Fractional Quadratic  Programs on the Simplex,[77009],855,"[5, 19]",2889,Recent advances on Variational Inequalities and Equilibrium Problems III	,51,15,43,Variational Inequalities and Equilibrium Problems - From Theoretical Advances to Real World Applications,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,99 [building - 306],"['Algorithms', 'Continuous Optimization']",WD-43
"Bullwhip effect describes the increasing magnitude of fluctuations in orders in the upstream direction within supply chains that lack proper coordination due to various reasons. One of them is incomplete information sharing about demands, orders or inventory levels, which is caused by the reluctance of supply chain actors to share sensitive information that might increase their vulnerability in negotiations or leak to other actors, e.g. competitors. As a consequence, full information sharing is very often not applied in practice despite its positive effects confirmed by a vast amount of research papers.

As an alternative, we propose to use cryptographic methods, such as secure multi-party computation or homomorphic encryption, that enable to share necessary information [e.g. a certain demand signal or trend] without revealing the exact values of each individual partner. We implement this approach on a 4-stage supply chain with multiple retailers using order-up-to policy where the information about average market demand is shared horizontally between the retailers. In our test we consider factors such as demand variation, correlation of demand or number of retailers sharing the information between each other and we show the positive effect that this information sharing has on mitigating the bullwhip effect.
",Mitigating the bullwhip effect on supply chains with multiple retailers by horizontal information sharing,"[42824, 46489, 46997]",928,"[61, 105, 131]",2891,Information sharing in sustainable supply chains,18,15,23,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,82 [building - 116],"['Inventory', 'Production and Inventory Systems', 'Simulation']",WD-23
"Stochastic facility location problems with outsourcing costs [SFLPOC] optimize facility placement and customer assignment under demand uncertainty.  Excess demand beyond a facility's capacity incurs outsourcing costs.  This work  addresses SFLPOC, aiming to minimize overall expected costs [installation, servicing, and outsourcing].

We model SFLPOC as a two-stage stochastic program. While prior work focused on specific assumptions or small scenario sets,  we present methods suitable for general probability distributions. For discrete scenario sets, we improve upon classic Benders decomposition by exploiting the second-stage subproblem's structure.

To handle general distributions, we partition the probability space, enabling the computation of expected values with fewer scenarios.  Coupled with Benders cuts, this provides an exact solution method for common distributions [e.g., Bernoulli, Gaussian].

Additionally, we introduce a compact formulation specifically for i.i.d. demand distributions, allowing us to solve even continuous distribution problems to optimality.  Computational experiments on established benchmarks demonstrate that our compact formulation consistently finds optimal solutions, while the Benders approach provides strong solutions with proven optimality gaps for general distributions, outperforming sample average approximations. ",Stochastic facility location problem with outsourcing costs,"[32736, 22042]",871,"[117, 14, 64]",2894,Exact Algorithms and Formulations for Combinatorial Optimization Problems,64,10,29,Combinatorial Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,157 [building - 208],"['Programming, Stochastic', 'Combinatorial Optimization', 'Location']",TD-29
"The timely provision of emergency care has emerged as a significant challenge due to a consistent rise in the number of visits to emergency departments each year. However, there has been no corresponding expansion in hospital resources and infrastructure, resulting in a high level of overcrowding. One of the primary reasons can be attributed to non-urgent patient visits, which account for 80% of the total accesses. This work focuses on the patient reallocation strategy, illustrating how transferring non-urgent patients between EDs within the same multi-hospital network can effectively decrease waiting times. To implement this approach, we devise a novel multi-objective optimization model with an intrinsic two-stage structure, where arriving patients can be either admitted or diverted to another hospital according to system capacity and congestion, and subject to consistent vehicle routing and availability. The proposed model is applied to a real-case study involving a multi-hospital system in northern Italy, attaining an average reduction of over 35% in daily waiting times. Extensive numerical experiments are conducted to demonstrate the scalability of the model and quantify its benefits in various settings. Based on the experimental outcomes, we develop a machine learning algorithm which serves as a tool to quickly assess the potential of the reallocation in a given hospital network with minimal information and provides insights on the required fleet size.",Patient reallocation for waiting time reduction in emergency departments within multi-hospital networks,"[77414, 67662, 67661]",594,"[56, 77, 136]",2895,ED logistics,3,12,10,OR in Health Services [ORAHS],"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,11 [building - 116],"['Health Care', 'Multi-Objective Decision Making', 'Stochastic Optimization']",WA-10
"We use data from Danish and Swedish treasury auctions to analyze differences in bidding strategies and auction performance. Our analysis suggests that underreporting true valuations is more pronounced under discriminatory auction format than under uniform auction format. Analogously, underreporting of true demand quantity is more pronounced under discriminatory auction format than under uniform auction format. We find no significant difference in the expected government profit [relative to the market prices].",Bidding Strategies in Treasury Auctions,"[75468, 72196]",639,"[9, 7, 44]",2896,Market Design 1 - Auctions,87,10,43,Market Design,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,99 [building - 306],"['Auctions / Competitive Bidding', 'Analytics and Data Science', 'Finance and Banking']",TD-43
"Decision-Focused Surrogate [DFS] framework has been recently proposed to tailor parametrically tractable proxies for intricate optimization problems with nonlinear and/or nonconvex nature. In this work, we extend the frontier of DFS to a data-driven contextual stochastic decision-dependent setting where the endogenous uncertainty is affected by decision-dependent features. Moreover, the conditional distribution remains unknown, yet historical observations are available. Specifically, we substitute the unknown conditional distribution with a parametric linear model. To ensure the decision quality for specific tasks, we employ techniques from Decision-Focused Learning [DFL], including optimization differentiation and surrogate loss construction, to acquire gradient of decision quality concerning model's parameters. We examine the performance of DFS in a battery energy management problem within a renewable power system considering decision-dependent degradation. Our results demonstrate that compared to a surrogate model trained using standard regression loss, DFS can achieve better decision quality, especially when the model is mis-specified. Despite the higher computational expense of learning DFS, our empirical analysis reveals that the trained surrogate can generate high-quality solutions for instances under similar conditions.",Learning Decision-Focused Surrogate for Decision-Dependent Problems with Samples,"[76238, 78059, 69458]",72,"[27, 84, 93]",2898,"Advancements of OR-analytics in statistics, machine learning and data science 1",16,2,28,"Advancements of OR-analytics in statistics, machine learning and data science","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,065 [building - 208],"['Decision Theory', 'Optimization Modeling', 'OR in Energy']",MA-28
" 'Smart Predict and Optimize' is an emerging paradigm where the parameters and optimal solutions for an optimization problem are jointly determined. We present some recent results using this paradigm for mean-variance optimization. We show that the integrated [joint] approach can result in parameter estimations and optimal solutions that perform better out of sample than those obtained from the 'predict or estimate first, then optimize' approach. Computational challenges will be discussed which motivates the development of decomposition methods based on the alternating direction method of multipliers [ADMM] for the optimization layer in the neural network that represents the integrated approach.
",Recent Results in Contextual Portfolio Optimization,"[77912, 78183]",275,"[45, 66, 83]",2899,Optimization Model for Novel Risks in Finance and Climate,4,5,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S14 [building - 101],"['Financial Modelling', 'Machine Learning', 'Optimization in Financial Mathematics']",MD-63
"Literature shows that order-level inventory systems for perishable items are often modelled by parametric approaches e.g., a time-dependent [Weibull] deterioration rate. In practice however, product deterioration for perishable products is a complex process, highly product dependent and, crucially, depending on environmental conditions from origin to final destination in supply chains. We develop a generalizable concept for order-level inventory systems in multi-echelon supply chains in which product-specific deterioration is derived from biological sciences.
We take time-temperature-dependent deterioration, and multi-echelon supply chains into consideration and integrate both aspects into a generically applicable EOQ-based methodology. We demonstrate the concept from a real-life case study for cold chain management in floriculture. Two common multi-echelon cold chains for cut roses in practice are considered and compared. The results show that the optimal order levels for different actors in the supply chains are substantially different. In addition to the sojourn time, the temperature has a major impact on order levels, the total supply chain costs, and the remaining shelf-life at retail level. We demonstrate that the proposed concept for extending the EOQ-based model to time- and temperature-dependent deterioration can be easily generalized and applied to other specific causes of product-dependent deterioration.
",Integrating time-temperature dependent deterioration in an EOQ-based methodology for perishable products in multi-echelon supply chains,[10561],425,"[120, 138, 61]",2902,Retail Inventory Management I,30,3,50,Retail Operations,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,M2 [building - 101],"['Quality Management', 'Supply Chain Management', 'Inventory']",MB-50
"This research addresses the optimization of inspection station placement in production systems modeled as queueing networks, where defects may arise at multiple stages. Our proposed optimization model aims to minimize total costs and/or production lead time by determining the optimal number and placement of inspection stations within the network. By strategically positioning inspection stations, we mitigate the risk of defects propagating through the system, enhancing product quality and operational efficiency. We present numerical examples and evaluate various scenarios to identify the most effective configurations. This research contributes to advancing operational efficiency and quality management in complex production systems, offering practical insights for decision-makers in optimizing resource allocation and enhancing overall system performance.",Cost-Effective Quality Assurance - Optimizing Inspection Station Placement in Queueing Network Production Systems,[54129],159,"[105, 121, 84]",2904,Stochastic Models in Manufacturing,50,5,39,Stochastic Modelling,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,35 [building - 306],"['Production and Inventory Systems', 'Queuing Systems', 'Optimization Modeling']",MD-39
"Efficient Global Optimization [EGO] has been widely applied to simulation optimization problems, where it uses the Expected Improvement [EI] criterion to navigate the search space for optimal solutions, traditionally relying on Kriging for surrogate modelling and uncertainty estimation. Some existing efforts have attempted to broaden EGO's applicability by substituting Kriging with various machine learning models. However, approximating emulation uncertainty is limited in the absence of Kriging. In this paper, we propose the Expected Regret [ER] criterion, which captures uncertainty through data-driven insights without relying on Gaussian assumptions for uncertainty estimation and enables the direct utilization of machine learning regression models for surrogate modelling. We then apply the proposed algorithm to various simulators of different dimensionalities. Comparative performance evaluations with the classical EGO method demonstrate that our approach achieves competitive results while offering greater flexibility in selecting machine learning models, indicating its potential for addressing high-dimensional optimization problems.",Beyond Efficient Global Optimization - an Algorithm Minimizing Expected Regret,"[78187, 47510, 78191]",121,"[131, 84, 123]",2905,On Mathematical Optimization for Explainable and Fair Machine Learning,15,3,27,Mathematical Optimization for XAI,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,047 [building - 208],"['Simulation', 'Optimization Modeling', 'Reliability']",MB-27
"Energy system optimization models [ESOMs] are a helpful tool to design future energy systems. Although uncertainties in the input data assumptions can significantly influence the structure of the energy system, they remain unconsidered in many cases. Through the application of stochastic programming [SP] it is possible to obtain expansion decisions that offer risk hedging with regard to the uncertainty of future developments.
ESOMs become large in size when a broad regional scope and technological diversity for sector coupling are considered, even without taking uncertainties into account. Therefore, speed-up techniques are needed to keep the models solvable, especially if SP is additionally considered. Benders Decomposition [BD] is a widely used method to solve SP models since the second stage can be solved in independent subproblems, allowing for a parallelization along the stochastic scenario dimension. However, the cardinality of the scenario set is usually much smaller than the number of time steps.
In our analysis we expand the parallelization potential of stochastic ESOMs by additionally splitting the scenarios along the time dimension. We apply this decomposition technique to two methods - First, we consider time-splitting in BD in combination with MPI. This is then compared to the parallel high-performance computing solver PIPS-IPM++, which was recently extended to also consider stochastic optimization, allowing a decomposition along the scenario and time dimension.",Decomposing stochastic energy system optimization models – time-splitting in Benders Decomposition vs. PIPS-IPM++,"[72639, 54534, 43524, 4091]",841,"[93, 136, 12]",2906,Decomposition techniques applied to energy problems,23,10,19,OR in Energy,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,44 [building - 116],"['OR in Energy', 'Stochastic Optimization', 'Capacity Planning']",TD-19
"Global healthcare faces a significant challenge with missed appointments, leading to financial losses and decreased service efficiency. According to the UK’s National Health Service [NHS] reports over 300 million missed appointments annually, costing over £216 million. This paper introduces an innovative machine learning-based system for optimizing General Practitioner [GP] appointments, enhancing scheduling by analyzing patterns of cancellations and no-shows. Utilizing both real and hypothetical data, we propose a highly applicable system which employs fuzzy logic algorithm to dynamically adjust GP schedules, prioritizing urgent cases and allowing walk-ins. This approach significantly benefits healthcare providers by maximizing resource utilization and reducing idle time. From a patient’s perspective, it ensures better access to care and shorter wait times for urgent needs. Our findings demonstrate the system’s potential in offering practical recommendations for managing walk-ins and unforeseen consultations more efficiently, paving the way for improved patient satisfaction and operational effectiveness in healthcare settings worldwide.",Optimizing General Practitioner Appointments with Machine Learning - A Data-Driven Approach,[78192],966,"[56, 66, 101]",2908,Machine learning and analytics in healthcare,3,3,15,OR in Health Services [ORAHS],"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,18 [building - 116],"['Health Care', 'Machine Learning', 'OR/MS and the Public Sector']",MB-15
"Recently, challenges related to climate change have increased the need for ever more complex energy system models, covering more sectors, more technological detail, as well as high spatial and temporal resolution. Furthermore, step-wise decarbonisation goals [e.g., for the heat sector] may require contradictory system designs for different future years. Similarly, anticipated, but only slowly implemented, developments of the hydrogen system may necessitate certain transition technologies. With the inherent risks for lock-in effects or stranded assets, the timing of infrastructure investment decisions has increasingly gained importance. Current energy system models often focus either on designing this sector-coupled energy system in great detail, lacking knowledge of the pathway to achieve it, or on designing pathways to a decarbonised energy system, lacking a detailed consideration of the system's operation.

We give an overview of algorithms that help depict complex transition paths together with high operational details, e.g., adaptive temporal resolutions and ways to account for seasonal storage, classical decomposition algorithms [e.g., Benders], closely related algorithms such as stochastic dual dynamic programming [including extensions], as well as approaches based on simulation. We further discuss requirements and features, potential [dis]advantages, and present test cases to illustrate how these algorithms can be applied to a wide range of energy system models.",Modeling long-term energy transition pathways - Algorithmic approaches and their properties,"[72784, 62333, 69694]",177,"[93, 37, 63]",2909,Long-term energy system planning,22,10,09,Energy Markets,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,10 [building - 116],"['OR in Energy', 'Energy Policy and Planning', 'Large Scale Optimization']",TD-09
"The reverse mortgage [RM] contract allows elder homeowners to borrow money using their home as security for the loan maintaining the right to live in the house. The debt is repaid by the heirs when the borrower moves out or dies. This contract can constitute a valid support for the spending needs that may arise during retirement. The paper deals with a decision problem of a homeowner who is approaching old age and has to evaluate contracting a Reverse Mortgage or not. We built elders lifetime utility functions considering consumptions, bequest motivations and lifespan uncertainty and solve the maximization problem to find the optimal allocation of the wealth between housing/nonhousing,  consumptions and bequest with and without RM. Through our analysis, we expect to find that, in presence of long-term care expenses and house mainteinance costs, individual’s liquid wealth significantly increase with reverse mortgage. Moreover, homeowner with a higher bequest motivation may have lower utility gains from contracting RM plan as well as individuals with a lower house value respect to liquid assets. ",Utility based evaluation of Reverse Mortgages,[67250],189,"[25, 45, 139]",2911,Insurance Risk Management,4,3,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S14 [building - 101],"['Decision Analysis', 'Financial Modelling', 'Sustainable Development']",MB-63
"We consider the single-item single-stocking location stochastic inventory system under a fixed ordering cost component. A long-standing problem is that of determining the structure of the optimal control policy when this system is subject to order quantity capacity constraints; to date, only partial characterisations of the optimal policy have been discussed. 

An open question is whether a policy with a single continuous interval over which ordering is prescribed is optimal for this problem. Under the so-called “continuous order property” conjecture, we show that the optimal policy takes the modified multi-[s, S] form. Moreover, we provide a numerical counterexample in which the continuous order property is violated, and hence show that a modified multi-[s, S] policy is not optimal in general. 

However, in an extensive computational study, we show that instances violating the continuous order property do not surface, and that the plans generated by a modified multi-[s, S] policy can therefore be considered, from a practical standpoint, near-optimal. Finally, we show that a modified [s, S] policy also performs well in this empirical setting.",On the Stochastic Inventory Problem Under Order Capacity Constraints,"[30948, 76619, 6635]",833,"[61, 136, 105]",2914,Stochastic inventory systems,32,12,49,"Lot Sizing, Lot Scheduling and Production Planning","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,M1 [building - 101],"['Inventory', 'Stochastic Optimization', 'Production and Inventory Systems']",WA-49
"Container stacking strategy determines the positions of incoming containers in the storage yard, which plays a vital role in improving the efficiency and productivity of container terminals and reducing the energy consumption of equipment and vehicles. The smart stacking strategy for import containers aims to create relocation-free stacks – named smart stacks, by utilizing customer information, thus reducing the retrieval time of import containers. There are two variants of stacking policies under the smart stacking strategy - split policy and non-split policy. The split policy, where containers from the same customer are allowed to be split between smart stacks and non-smart stacks, is superior to the non-split policy. This paper investigates the import container stacking problem under the split policy. The problem is first formulated as a mixed-integer programming [MIP] model to minimize the total working time of the yard crane. By enhancing the representation of certain variables, we then develop an improved MIP model, which is computationally more efficient than the original model when using CPLEX as a solution tool. We also propose a column generation algorithm to solve the improved model. Extensive computational experiments are conducted to demonstrate the effectiveness of the proposed models and the efficiency of the proposed algorithm.",Column generation algorithm for smart-split stacking strategy at automated container terminals,"[78110, 35540]",173,"[72, 13, 143]",2915,Energy Management in Ports and Shipping I,52,8,62,OR in Port Operations,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S12 [building - 101],"['Mathematical Programming', 'Column Generation', 'Transportation']",TB-62
"This study focuses on solving a capable vehicle routing problem [CVRP]
using a genetic algorithm [GA]. The CVRP is a complex optimization problem that often arises in supply chains when routing vehicles with different populations. The study proposes a new solution that uses a metaheuristic to optimize the implementation between crossover and mutation of new operators. These operators use different strategies to search for better solutions generated from an initial population. The CVRP is formulated as a mixed integer programming [MIP]problem, where the genes represent decisions about which customers to visit and
which vehicles to use for the route.

The existing literature provides ample support for this issue, with many articles discussing the indicators and comparisons of experimental tests post-realizations that show the efficacy of the proposed algorithm. Through significant advances in the solution problem, the study obtained a GA-modified solution to the CVRP by adjusting different parameters to identify the most adaptable solution. The results were then compared with other solution methods as solvers focus on the same problem.",Hybrid Genetic Algorithm a new Perspective to Solving Capable Vehicle Routing Problem [CPVPR],"[78202, 50952, 50953]",786,"[5, 74, 145]",2918,Heuristics for Vehicle Routing 3,5,3,58,VeRoLog - Vehicle Routing and Logistics,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S07 [building - 101],"['Algorithms', 'Metaheuristics', 'Vehicle Routing']",MB-58
"The travel demand forecasting model plays a crucial role in evaluating large-scale infrastructure projects, such as the construction of new roads or transit lines. While combined modeling approaches have been explored as a solution to overcome the problem of input/output discrepancies in a sequential four-step modeling process, previous attempts at combined models have encountered challenges in real-world applications, primarily due to their limited behavioral richness or computational tractability. In this study, we propose a novel convex programming approach and present a key theorem demonstrating that the produced optimal solution coincides with the one arising from the hierarchical extended logit model. This model is specifically designed to capture correlations existing in travelers' choices, including similarities among transport modes and route overlaps. The convex property of our model ensures the existence of solutions and naturally offers computational efficiency. The advantages of our proposed model are twofold. First, it provides a single unifying rationale [i.e., utility/entropy maximization] that is valid across all steps. Second, its combined nature allows one to systematically handle observed data, enabling a better representation of reality. Our convex programming approach shows promise in enhancing the accuracy and applicability of travel demand forecasting, thereby aiding in the decision-making processes for infrastructure projects.",A Combined Convex Formulation Replacing Hierarchical Extended Logit Model for Travel Demand Forecasting,"[76804, 78206, 78209, 78210, 38073]",902,"[143, 21, 33]",2920,Advances in algorithms and applications for linear and convex optimization,68,14,38,"Conic Optimization - Theory, Algorithms, and Applications","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,34 [building - 306],"['Transportation', 'Convex Optimization', 'Economic Modeling']",WC-38
"Confronted with the urgent demand for medical supplies in COVID-19, governments encouraged manufacturers to invest efforts in expanding and transitioning their production to bolster the supply capacity of relief supplies. However, the limited supply capacity and the risk of interruption in the upstream supply chain of critical raw materials hinder the expansion of manufacturers' capacity. We address this challenge within a relief supply chain consisting of a supplier and two competing manufacturers under various government subsidy strategies. In this study, we develop a game-theoretic model where an incumbent and an entrant manufacturer engage in two-dimensional competition involving production investment efforts and sales volume. We examine three scenarios - [1] no subsidy, [2] per-unit sales subsidy, and [3] per-unit price subsidy to understand the effectiveness of subsidy strategies in different contexts. Furthermore, we evaluate the stability performance of the subsidies from an evolutionary game theoretical perspective. We find that when the entrant manufacturer has a high level of investment efficiency, they prefer the per-unit sales subsidy over the price subsidy strategy. Additionally, government subsidies can promote production quantities, investment efforts, and social benefits for all stakeholders. To demonstrate the practicality of our results, we apply them to the case of China's response to COVID-19 and compare it with strategies adopted by the UK government.",Manufacturing and Procurement Strategies of Relief Supplies Considering Disruption Risk in Response to COVID-19,"[77564, 32980, 78213]",554,"[12, 30, 50]",2923,Infectious diseases and pandemics,38,13,21,OR in Humanitarian Operations [HOpe],"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,49 [building - 116],"['Capacity Planning', 'Disaster and Crisis Management', 'Game Theory']",WB-21
"In response to the growth tendency of online sales, grocery retailers have embraced e-commerce, and committed to the required investments. It is a recognized fact that profitability is hard to attain given the costs associated with order fulfillment and last-mile distribution, aggravated by customers' unwillingness to pay additional fees for these services. We consider the case of an omnichannel grocery retailer, that provides home deliveries for a delivery fee, leveraging its Brick-and-Mortar network. Improving the operational efficiency of online order fulfillment is important, but larger opportunities lie in effectively managing demand within and across channels. We use customer-level activity over time to infer the influence of online channel features, such as delivery fees and assortment size, and of the store network on customer behavior. Our findings reveal that customers strategically adjust their shopping habits based on factors like fees, product variety, household needs, and proximity to physical stores. Building on these insights, we model customer behavior regarding store selection, shopping frequency, and expenditure patterns. By considering the cost to serve online orders and the value customers present to the retailer across channels, we prescribe solutions that optimize the retailer’s profits. In a series of numerical studies, we draw managerial insights on the path to omnichannel profitability.",Navigating Omnichannel Grocery Retailing - Bridging Customer Insights for Strategic Decisions,"[72112, 23114, 67228, 36154]",482,"[32, 10, 79]",2924,Omni-Channel Retailing ,30,10,50,Retail Operations,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,M2 [building - 101],"['E-Commerce', 'Behavioural OR', 'Network Design']",TD-50
"Hydrogen produced with low-carbon emissions plays a central role in European decarbonization strategies. Scaling up renewable hydrogen production is envisioned to speed up the energy system transition and reduce dependency on fossil fuel imports. To facilitate this scale-up, the European Union introduced guidelines outlining the conditions under which hydrogen is considered a renewable fuel. These guidelines specify that electrolyzers should only be powered by grid electricity during hours with excess renewables, or by electricity from dedicated renewable assets. With increasing shares of renewables in Europe, in recent years, about 85-100 TWh of renewable electricity has been curtailed annually. In this study, we use a linear optimization model to determine the optimal capacity and operation of electrolyzers for varying hydrogen prices and investigate the untapped potential of utilizing this excess renewable electricity for hydrogen production. While the option to deploy battery storage is included to store otherwise curtailed power for later use, it is not deployed in any scenarios considered due to high up-front investment costs. We show that by utilizing otherwise curtailed renewable electricity, about 30% of today’s European hydrogen demands from chemical industry and refineries could be met. The highest potentials are found in Germany, Denmark, and the Netherlands, where electrolytic hydrogen could displace over 60% of existing fossil hydrogen production.",Quantifying the potential of curtailed electricity to produce green hydrogen ,"[72717, 69378, 78214, 69248, 78215]",252,"[37, 93, 139]",2925,Impacts of transitioning to green gases,22,2,14,Energy Markets,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,16 [building - 116],"['Energy Policy and Planning', 'OR in Energy', 'Sustainable Development']",MA-14
"Parininihi ki Waitotara [PkW] is a New Zealand indigenous organisation lead by Māori that manages land and other assets on behalf of thousands of shareholders. Many of these shareholders are “lost” in the sense that PkW has no contact details for them. As a consequence, PkW has millions of dollars of dividends waiting to be paid if these shareholders can be found. We have developed a system to collect public social network data and search this for the missing PkW shareholders. This involves solving the entity resolution problem to determine whether multiple document records refer to the same person or people. Traditional entity resolution approaches treat all matching decisions independently and only compare attribute similarities between reference pairs. However, this independence assumption omits valuable relational information in situations where the references describe a rich network of relationships between people. Collective entity resolution - where entities are resolved jointly - incorporates this previously ignored information and can predict matches with greater accuracy. We use Markov logic networks to convert our domain knowledge and evidence data into a Markov network. Prediction is made by performing most probable explanation [MPE] inference on the network using integer programming formulations and a mixed IP solver. We report experiments that demonstrate how our collective entity resolution approach is able to resolve complex relational information.","Using integer programming, Markov logic networks and inference to help find missing shareholders in social networks for a New Zealand Māori indigenous incorporation","[4275, 78265, 12990]",211,"[132, 7, 109]",2927,Analytics and the link with stochastic dynamics III,17,9,31,Analytics,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,54 [building - 208],"['Social Networks', 'Analytics and Data Science', 'Programming, Integer']",TC-31
"Due to the emergence of robotics and autonomics, there has been a burst of interest in understanding the relationship between knowledge assets and firm performance. It is also observed that firms are keen to enhance performance by managing knowledge assets. On the research side, while much effort is going into this problem, clearcut answers have yet not come out. This research works on optimal answer to this problem by manipulating knowledge assets in their four prevalent forms, i.e., research and development, capital expenditure, selling, general, and administrative expenses, and property, plant, and equipment. We take performance as given by the rates of growth in seven most common metrics, i.e., revenue growth, cost of revenue, earnings from interest and taxes, profit, gross profit margin, cash flow, and times interest earned ratio. We formulate four hypotheses and conduct twenty-eight tests linking four knowledge assets to seven performance metrics. Using data from IT industries belonging to the SIC Code 737, we study the relationships and find that for different metrics, there are different knowledge assets. There is no one knowledge asset that will work for all performance metrics. It is true even for R&D. Therefore, to find the right knowledge asset for the desired performance metric, we design search methods employing selection maps to apply our findings to show how to strategically assign resources to the four knowledge assets for getting any targeted performance. We",Selecting and Switching Knowledge Assets for the Firm Growth and Performance,[1421],536,"[44, 137, 69]",2928,Applications of Knowledge and Technology,54,13,08,"Knowledge, Technology, and Innovation","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1020 [building - 202],"['Finance and Banking', 'Strategic Planning and Management', 'Manufacturing']",WB-08
"In the current scenario where energy crises are increasingly evident, the development of renewable energy has become a crucial pathway towards achieving sustainable development. The Pig-Electricity Synergy system emerges as an advanced energy solution, seamlessly integrating pig farming and solar energy storage technologies.The core advantage of this technology lies in its integrated application of solar energy and storage technology, forming a diverse and reliable energy management system.The uniqueness of the Pig-Electricity Synergy system lies in its successful realization of the integration of solar energy and pig farming, enhancing energy reliability through collaborative operations and significantly mitigating system risks. The resulting diverse and flexible energy management system not only meets the escalating demands of energy but also contributes to enhancing the resilience of the overall energy framework. This study employs DEMATEL analysis and ISM modeling to delve into the pivotal role of the Pig-Electricity Synergy system in sustainable energy development. Through a comprehensive analysis of the system's technology, diverse energy integration, and flexibility, the research aims to provide tangible and viable solutions for sustainable energy development. The application of DEMATEL and ISM contributes to a deeper understanding of the inherent relationships within the system, offering profound insights for sustainable development decision-making.",Pig-Electricity Synergy System in Sustainable Energy Development,"[78219, 79452, 29766, 53814]",682,"[139, 22, 77]",2930,Sustainable Energy,80,4,53,Sustainable and Resilient Systems,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,8007 [building - 202],"['Sustainable Development', 'Critical Decision Making', 'Multi-Objective Decision Making']",MC-53
"In response to the dynamic and ever-evolving landscape of network attacks and cybersecurity, this study aims to enhance
network security by identifying critical nodes and optimizing resource allocation within budget constraints. We introduce a
novel approach leveraging node centrality scores from four widely-recognized centrality measures. Our unique contribution
lies in converting these centrality metrics into actionable insights for identifying network attack probabilities, providing an
unconventional yet effective method to bolster network robustness. Additionally, we propose a closed-form expression correlating
network robustness with node-centric features, including importance scores and attack probabilities. At the core of our
approach lies the development of a nonlinear optimization model that integrates predictive insights into node attack likelihood.
Through this framework, we successfully determine an optimal resource allocation strategy, minimizing cyberattack risks on
critical nodes while maximizing network robustness. Numerical results validate our approach, offering further insights into
network dynamics and improved resilience against emerging cybersecurity threats.",Securing Network Resilience - Leveraging Node Centrality for Cyberattack Mitigation and Robustness Enhancement,[59566],705,"[18, 53, 110]",2933,Resilient Networks,80,10,53,Sustainable and Resilient Systems,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,8007 [building - 202],"['Computer Science/Applications', 'Graphs and Networks', 'Programming, Linear']",TD-53
"Rail is a good mode of transport especially for bulk commodities like cement and  steel, over medium and long distances. However rail services are subject to  delayed deliveries, especially due to unavailability of wagons [set of wagons, also known as rakes]. To address these issues, companies invest in railway wagons. This fleet is maintained by the shipper, but the movement is by the railway company with a rebate on transportation cost. Procuring such a fleet requires a significant financial investment. We consider the case where some in-house rakes are procured that are used along with outsourced resources. With this mixed fleet composition and considering the benefits offered by in-house rakes over outsourced rakes, an operational question is - how should we allocate in-house and outsource rakes to various demands? We examine several allocation policies considering 1] distance 2] cost rebate through use in-house fleet and 3] overall transportation cost. A simulation modeling tool is used to assess different policies under various scenarios considering demand patterns, turnaround time, waiting time of locomotives and availability of outsourced rakes. The study also explores how the randomness associated with these parameters affects these policies. The general question is that of optimal use of limited internal capacity along with outsourced resources in a cost effective way.
",Optimal use of in-house and outsourced vehicles,"[76912, 28300]",630,"[122, 131, 65]",2935,Railway Applications,6,14,56,Transportation,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S04 [building - 101],"['Railway Applications', 'Simulation', 'Logistics']",WC-56
"We study the minimum variance portfolio optimisation problem, wherein the covariance matrix is subject to a degree of uncertainty, possibly due to estimation errors and the presence of outliers. Classic solutions mitigate such errors through a rank-one shrinkage matrix, constructed using dual variables from a related constrained optimisation problem. Inspired by this, we propose a model that permits varying degrees of matrix shrinkage and identifies the most effective shrinkage scale based on the in-sample performance.Moreover, we identify sufficient conditions that guarantee that the updated [shrunk] covariance matrix remains positive semi-definite, and  obtain a practical range for the scale of the shrinkage.
Our numerical experiments on out-of-sample data demonstrate that portfolios generated by our model typically exhibit lower levels of short selling and higher Sharpe ratios compared to existing methods. Additionally, they outperform in key risk metrics, such as Value-at-Risk [VaR] and Conditional Value-at-Risk [CVaR]. This highlights our approach's effectiveness in addressing estimation uncertainties and its potential to redefine investment strategies in volatile markets.",Achieving robustness by searching for the right amount of shrinkage in minimum variance portfolios,"[78038, 79112]",470,"[83, 45, 127]",2936,Applications to Economics and Finance,4,14,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S14 [building - 101],"['Optimization in Financial Mathematics', 'Financial Modelling', 'Robust Optimization']",WC-63
"Large retailers such as Amazon offer a vast array of products, encompassing millions of unique items. The multitude of products creates a complex demand landscape where high-demand products are classified as head, moderate-demand as body, and low-demand as tail. To control complexity and achieve efficiency in the fulfillment network, Amazon regionalized its operations in the US. This process involved defining regions, assigning fulfillment centers accordingly, and aiming to meet customer demands using the inventory of the fulfillment centers within each region. This prompts the crucial question - What level of inventory decentralization is economically beneficial for each product category when considering the customer's sensitivity to delivery speed?
In this talk, we report on a large-scale case study focusing on the 48 contiguous US states. We present our methodology, which leverages adapting the Newsvendor model to incorporate transportation costs and order-to-delivery-time sensitivity. We finally present key insights from the study, notably that [1] profit maximization calls for decentralization of inventory for head and body products and more centralization of tail product inventory, and [2] a nuanced approach to inventory decentralization, and hence, region definition based on the product category, is essential.
",Inventory Decentralization Tailored to Product Demand in Delivery Speed-Sensitive Logistics Networks,"[78228, 78231, 78230, 62766]",289,"[32, 61, 105]",2937,Large-scale network optimization and inventory management,92,10,57,Optimization at Amazon,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S06 [building - 101],"['E-Commerce', 'Inventory', 'Production and Inventory Systems']",TD-57
"The traveling salesman problem with Job-Time [TSPJ] is a variant of the classic traveling salesman problem. In the TSPJ, the traveler visits a set of nodes, ensuring one visit to each of them while he initiates a job. The time of each job depends on the node where it is performed. Once started, the traveler moves to the next node, and the job continues autonomously. The aim is to minimize the maximum completion time, also known as the makespan. However, since the problem is NP-hard, the existing mixed integer linear programming [MILP] models cannot be efficiently solved for medium and large instances. Therefore, we propose a new  MILP model for the TSPJ, which is improved by the computation of valid lower and upper bounds. Moreover, we propose strengthened exponential-size formulations that explicitly incorporate subtour elimination constraints and additional valid inequalities. Based on the proposed model, we also develop an exact branch-and-cut [B&C] algorithm that is able to solve instances of the TSPJ. The new model and B&C algorithm are tested on benchmark symmetric instances from the literature, four sets of instances ranging in size from 17 to 1200 vertices. The computational results show that our B&C outperforms the state-of-the-art MILPs in the two smaller sets of instances. Meanwhile, for medium and large sets, our exact method achieves promising results for both instance sets.",Models for the Traveling Salesman Problem with Job-Time,"[74424, 59356]",220,"[14, 111, 84]",2938,Advanced Topics in Combinatorial Optimization,64,8,26,Combinatorial Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,012 [building - 208],"['Combinatorial Optimization', 'Programming, Mixed-Integer', 'Optimization Modeling']",TB-26
"The impact of variability on a production system in terms of flow time and work in progress is a well-known problem. Recent research works investigating the application of machine learning [ML] models on production data to improve process performance have shown promising results. Drawing inspiration from such works, in the first part of the study we do partial replicative research and apply existing ML models to identify the sources of variability such as machine parameters, defects, product routing and so on that lead to increased flow time in a semiconductor manufacturing plant.  Using the production data for a work center in the wafer production process, we identify the critical parameters in the work center that influence the flow time using ML algorithms such as random forest, neural networks and gradient boosting. To enhance the interpretability of the results, we apply explainable AI [XAI] approaches such as Shapley Additive Explanation [SHAP]. As a second step, the results will be presented to process experts to derive actions that can reduce the variability in the process and thus enhance the efficiency in resource utilization. Through our work, we expect, on the one hand, to reinforce the potential benefits of using ML models to understand and improve production processes, and on the other hand, to increase the trust of experts on the use of ML models and to leverage their knowledge to tackle the issues they face in manufacturing.",Variability reduction in semiconductor manufacturing using machine learning and explainable AI,"[67298, 78235, 17105, 80302, 80303]",920,"[66, 69, 105]",2941,Scheduling for sustainability,18,5,23,"Circular Economy, Remanufacturing and Recycling ","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,82 [building - 116],"['Machine Learning', 'Manufacturing', 'Production and Inventory Systems']",MD-23
"We study the problem of coalition formation and profit sharing among risk-averse agents who jointly undertake a project which requires management decisions and leads to uncertain outcomes. We use acceptability functionals to model the agents’ risk preferences and define the value of coalitions by the optimized sup-convolution of their acceptability functionals. We call such games coalitional acceptability games and provide conditions for the non-emptiness of the core for important subclasses of games such as network games, bankruptcy games, and generalized production games. We show that players agree on ex-ante contracts that are equivalent to a mutual exchange of standard options contracts. Finally, we illustrate our approach with two numerical examples - a water management application and a detailed case study of a virtual power plant.",Coalitional Acceptability Games - A Framework for Risk-averse Stochastic Coalitional Games,"[9082, 3542, 18480]",535,"[117, 50, 93]",2943,Risk Averse and Contextual Stochastic Optimization,49,8,35,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,44 [building - 303A],"['Programming, Stochastic', 'Game Theory', 'OR in Energy']",TB-35
"This research delves into the synergistic application of Multi-Criteria Decision-Making [MCDM] methodologies in Traditional Chinese Medicine [TCM] diagnosis, with a specific focus on Post COVID-19 Syndrome. The global aftermath of the pandemic has heightened the demand for nuanced diagnostic approaches, and this study aims to bridge the gap between traditional diagnostic principles and modern decision-making tools. Through a detailed exploration of MCDM techniques such as Analytical Hierarchy Process [AHP] and Technique for Order of Preference by Similarity to Ideal Solution [TOPSIS], the research integrates these methodologies into the TCM diagnostic framework. The case study on Post COVID-19 Syndrome serves as a practical application, examining how MCDM enhances diagnostic precision and effectiveness. The findings not only contribute to the evolution of TCM practices but also hold implications for a more integrated and responsive healthcare approach in the post-pandemic era.",Integrating Multi-Criteria Decision-Making [MCDM] in Traditional Chinese Medicine [TCM] Diagnosis - A Case Study on Post COVID-19 Syndromes,"[78238, 79176, 79177]",957,"[25, 0]",2944,MCDA in medicine,44,14,47,Multiple Criteria Decision Analysis,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,50 [building - 324],['Decision Analysis'],WC-47
"We use the slacks-based data envelopment analysis model to obtain the shadow prices of nitrogen surplus in a sample of Lithuanian family farms over 2014 and 2019. The variable returns to scale weak disposability technology is assumed. The results indicate that the shadow price obtained for Lithuanian cereal farms is relatively low compared to the earlier literature. This implies that the environmental performance of Lithuanian cereal farms needs to be improves [especially, the fertilizing regime].",Shadow Pricing of Nitrogen Surplus in Lithuanian Family Farms,"[30453, 74496]",941,"[89, 139, 24]",2945,DEA applications in Environment and Sustainability II,89,9,48,Data Envelopment Analysis and its Application,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,60 [building - 324],"['OR in Agriculture', 'Sustainable Development', 'Data Envelopment Analysis']",TC-48
"Joosten [1996] attempted to reconcile marginalism and egalitarianism by proposing the egalitarian Shapley values, which is a convex combination of the Shapley value and the equal division values. Abe and Nakada [2019] generalized the equal division values of egalitarian Shapley values to propose weighted-egalitarian Shapley values. Recently, Cheon and Choi [2024] introduced a new solution by generalizing the Shapley values to the weighted Shapley values within this weighted-egalitarian Shapley values framework.
In this study, we further advance the generalization of TU-values. This research delineates the axiomatic properties of solutions resulting from the convex combination of four classes of TU-values. These classes include convex combinations of the weighted division values with weighted Shapley values, positively weighted Shapley values, random order values, and the Harsanyi set. This research has been influenced by the parallel axiomatization of Besner [2020].  Besner [2020] axiomatized weighted, positively weighted, and multiweighted Shapley values, random order values, and the Harsanyi set, presenting the axiomatization of all individual classes as a single theorem for all classes. In our study as well, within the main theorem, the axiomatization of adjacent two classes differs by only one axiom, and the process of generalization of classes of solutions and weakening of corresponding axioms progresses in the same direction.",Parallel axiomatizations of generalized  weighted-egalitarian Shapley values,"[76967, 63696]",646,"[50, 0]",2946,"Game Theory, Solutions and Structures VIII",88,10,36,"Game Theory, Solutions and Structures","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,32 [building - 306],['Game Theory'],TD-36
"Robust combinatorial optimization has come a long way over the last 25 years. While many variants of decision criteria and uncertainty sets have been proposed, some cornerstones have emerged. These are min-max, min-max regret, two-stage, and recoverable robust problems with discrete, interval, continuous budgeted, and discrete budgeted uncertainty. In this talk, I take stock of the current state-of-the-art in these areas and point out the challenges that we face. This includes a list of promising open problems that I encourage the audience to consider.",Open Problems in Robust Combinatorial Optimization,"[29733, 50791]",379,"[127, 14, 16]",2947,Trends and Open Problems in Robust Optimization,49,10,34,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,43 [building - 303A],"['Robust Optimization', 'Combinatorial Optimization', 'Complexity and Approximation']",TD-34
"The importance of increasing efficiency and capacity for last-mile delivery prompts e-commerce companies to explore alternative solutions, including the adoption of parcel lockers and leveraging crowd-shipping platforms. This study develops a holistic framework for optimising a parcel locker-based crowd-shipping system assisting the courier company’s last-mile delivery. Our approach covers various aspects for operating such a system, including identifying the optimal locker locations, considering customers’ stochastic choices in the delivery locations, and accounting for uncertainties in demand and crowd-shippers’ availability. The proposed two-phase approach integrates generated locations improved by simulated annealing with a simulation-based mixed integer linear programming model for matching delivery tasks and optimizing vehicle routing. Specifically, once the locker locations are identified, a simulation-based approach is employed to determine the stochastic choices made by customers and their corresponding delivery tasks. The model then matches these tasks with either available crowd-shippers or the courier’s own delivery vehicles, subsequently devising the most efficient routes for these delivery vehicles. Finally, the model is evaluated using last-mile delivery data from a region in Copenhagen. Our results highlight how the use of a parcel locker-based crowd-shipping system can enhance the efficiency of delivery operations for the courier company. ",Optimising Crowd-shipping With Parcel Lockers for Last-Mile Delivery,"[78240, 19761, 77053, 74502]",747,"[65, 136, 145]",2948,Last-Mile Delivery,5,12,64,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S16 [building - 101],"['Logistics', 'Stochastic Optimization', 'Vehicle Routing']",WA-64
"Employee absenteeism occurs when an employee is not present at work during their scheduled hours. When this situation occurs, the absent employee's shift must be covered by another employee. This in turn may affect others in the organisation, creating an undesirable ripple effect that can propagate throughout the entire roster. Generating robust personnel rosters can proactively mitigate the negative consequences when employees are absent from work. One way of increasing roster robustness is by scheduling special on-call duties that can be converted into regular working duties whenever necessary. Machine learning models that predict when employee absences will occur provide an intuitive and regularly employed approach to determine where best to position on-call duties in a roster. In this talk we will propose a methodology to evaluate the circumstances under which such predictions can actually increase roster robustness. More specifically, we will analyze the results of a series of computational experiments to determine the prediction performance levels needed by a machine learning model to outperform a non-data-driven robust rostering method.",Using predictions on employee absenteeism to generate robust personnel rosters,"[32203, 67846, 37557, 66581, 23268]",300,"[128, 66, 135]",2949,Automated Timetabling,36,2,58,Automated Timetabling,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S07 [building - 101],"['Rostering', 'Machine Learning', 'Stochastic Models']",MA-58
"The evolution of tokenomics has been greatly influenced by Ethereum [Ether], one of the most prevalent cryptocurrencies [cryptos]. With the release of the Merge upgrade [EIP-3675] on September 15, 2022, Ether achieved a major turning point by transitioning from the PoW [proof-of-work] consensus mechanism to PoS [proof-of-stake]. This EIP  [Ethereum Improvement Proposal] marked the dawn of a new epoch in blockchain innovation, enabling the emergence of creative applications with reduced transaction costs. Meanwhile, Ether experienced a price surge prior to deployment, followed by a subsequent deflationary trend. This paper investigates the impact of EIPs such as the Merge on crypto supply and demand, discussing whether these proposals function similarly to traditional monetary policies and analyzing the characteristics of EIPs that influence substantially. Furthermore, we discuss the broader implications of technology adoption and upgrades for Ether, investigating how technological uncertainty influences pricing and functional performance. Through an event study examining Ether's EIPs, we identify event-driven opportunities and risks, providing valuable insights for investors and protocol designers [i.e. policy makers] alike.",The Chronicles of Ethereum - An Event Study on EIPs,"[77913, 78253, 78257, 55475]",511,"[7, 44, 45]",2951,Innovations in Digital Assets - IDA,17,14,31,Analytics,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,54 [building - 208],"['Analytics and Data Science', 'Finance and Banking', 'Financial Modelling']",WC-31
"We introduce the GIS-based optimization tool FlexiGIS-H2, applied to urban areas in both New Zealand and Germany to identify the potential for hydrogen to aid the energy transition within cities. Based merely on open-source datasets, we connect existing urban energy infrastructure, electricity generation and hydrogen production potential and the specific demand of a given city.  Land use and building types are used to calculate electricity demand, and available areas are analysed to estimate the potential electricity generation from solar and wind resources, alongside the potential production of green hydrogen. Through cost-minimization, we optimize storage scenarios including batteries and green hydrogen, and solve for the most techno-economic solution to meet electricity and hydrogen demand for urban applications. 

In this work, we present the model FlexiGIS-H2 applied to the cities Auckland, Christchurch, Karlsruhe, and Oldenburg, in New Zealand and Germany. Here, we delve into the application of FlexiGIS-H2, our modelling outcomes for different cities, and ongoing developments aimed at supporting decision-making for future energy system planning in urban areas. The resulting hydrogen portfolio and residual energy scenarios, and the optimisation of hydrogen as a flexibility option in urban settings, can be used as inputs to national capacity expansion tools, to provide local detail often lacking at the national level. ",GIS-based optimization tool for green hydrogen integration in cities - A comparative case study of New Zealand and German cities ,"[73784, 74115, 74116, 74117, 78245, 74118]",682,"[93, 38, 37]",2952,Sustainable Energy,80,4,53,Sustainable and Resilient Systems,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,8007 [building - 202],"['OR in Energy', 'Engineering Optimization', 'Energy Policy and Planning']",MC-53
"In the recent years, a lot of attention was given to branch-and-bound techniques for solving pure integer linear programs with an arbitrary number of objectives. Significant progress and contributions were made. On the other hand, the efficiency and practicality of branch-and-bound methods for bi-objective mixed-integer linear was shown through a variety of works in the past decade. Intuitively, the next step is to address mixed-integer linear programs with three or more objective functions. Again, linear-relaxation based branch-and-bound frameworks appear to be the methods of choice because of the continuous nature of some variables. Unfortunately, due to the complex structure of the non-dominated set of such problems, straightforward extension of existing branch-and-bound frameworks to solve multi-objective mixed-integer linear programs is not possible. In this talk, we identify some of these obstacles[representation and navigation of the non-dominated set, update of the upper bound set, dominance test, etc.], and present various solutions we explored to address these issues. Moreover, we highlight implementation-related and performance-related difficulties, and conclude with future works.",On the extension of branch-and-bound methods formixed-integer linear programs,"[67745, 20539]",200,"[112, 111, 110]",2953,Multi-objective Combinatorial Optimization,64,4,52,Combinatorial Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,8003 [building - 202],"['Programming, Multi-Objective', 'Programming, Mixed-Integer', 'Programming, Linear']",MC-52
"Bilevel optimization has gained a lot of attention in recent years as a data-driven learning technique for the estimation of unknown model operators or for the automatic selection of hyperparameters in highly parameterized models. Formally, a bilevel problem consists in  a nested optimization problem where a variational model acts as a constraint.
Starting from an outline of the main theoretical concepts of bilevel formulations, in this talk numerical bilevel schemes to address imaging and portfolio optimization problems will be anaylsed, combined with different strategies to properly derive the gradient of the upper-level loss, which is one of the major obstacle in bilevel optimization.  Numerical tests will be presented to validate the proposed schemes.",Bilevel learning optimization and applications,[78249],916,"[21, 81]",2954,"Nonsmooth optimization and applications, Part II",84,8,32,Advances in large scale nonlinear optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,41 [building - 303A],"['Convex Optimization', 'Non-smooth Optimization']",TB-32
"Currently, Online Food Delivery [OFD] platforms like DoorDash, Zomato and UberEats dictate the delivery fee charged to customers for orders placed through them. This delivery fee is based on the order value and the distance traveled by the driver to complete the delivery. Pay-As-Asked [PAA] delivery pricing strategy is the name of this tactic. On the contrary, Pay-What-You-Want [PWYW] is an innovative pricing strategy that gives customers the final say over the price they are willing to pay based on how valuable they perceive the service. To investigate how the adoption of pay-what-you-want [PWYW] for delivery fee pricing affects the financial viability of OFD platforms, we present an analytical model. By dividing consumers into three groups—free riders, fair-minded consumers, and generous consumers—we are able to identify their social preferences for things like fairness and reciprocity. This allows us to determine the conditions under which PWYW might be more profitable for the platform than the typical PAA delivery pricing strategy. Our research suggests that to maximize platform profits when implementing a PWYW delivery fee, the OFD platform managers should work to reduce delivery costs as much as they can. Furthermore, we propose that platforms impose a minimum delivery fee that customers may use as a guideline when determining the fee they want to pay to prevent free riders from reducing profits under PWYW. ",The Power of Perceived Value and Transformative Pricing in Online Food Delivery Platforms,"[66983, 54028]",263,"[124, 50, 32]",2955,Pricing and applications 3,11,2,59,Pricing and Revenue Management,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S08 [building - 101],"['Revenue Management and Pricing', 'Game Theory', 'E-Commerce']",MA-59
"As electric vehicle adoption increases worldwide, the growing charging demand necessitates a well-thought-out expansion of public charging infrastructure; insufficient or improperly deployed infrastructure poses the real risk of slowing down the adoption of electric vehicles. Public charging networks are likely to develop into very heterogeneous systems with, for example, fixed and mobile chargers. This paper proposes a multi-period mixed-integer programming formulation for optimally placing both fixed and mobile chargers in stochastic environments with the goal of meeting time-varying charging demands at a minimum cost. To discover EVs' uncertain spatial and temporal behavior, the energy demand scenarios are generated from an existing agent-based simulation of EVs in Frederiksberg and Copenhagen municipalities. Large-scale demand scenarios for Frederiksberg municipality are extracted from the simulation and then fed to a two-stage stochastic optimization to find the optimal expansion of fixed and mobile charging stations. As this formulation leads to an NP-hard problem, we develop a Benders decomposition for solving large-scale examples. In addition, a new algorithm is proposed to get exact sub-problem solutions and their corresponding dual variables without using linear programming solvers. Detailed experimental results confirm the effectiveness of the approach, which can provide exact integer solutions in a short computational time for even large problem instances. ",Dynamic and incremental expansion of large scale fixed and mobile charging infrastructure in stochastic environment - A Benders decomposition based approach,"[78254, 57113, 38073]",624,"[136, 43, 37]",2958,Advancing mobility towards sustainable solutions III,6,12,56,Transportation,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S04 [building - 101],"['Stochastic Optimization', 'Facilities Planning and Design', 'Energy Policy and Planning']",WA-56
"In this paper, we examine the effectiveness with which individuals execute prescriptive psychological heuristics in multi-criteria decision-making contexts.  Despite growing evidence demonstrating the predictive accuracy of simple psychology-inspired decision-rules, there has been limited research exploring how well individuals deploy an “adaptive toolbox” of methods intended for use as prescriptive decision-aids. Contrasting between compensatory and non-compensatory heuristics, our study provides some of the first experimental evidence indicating that compatibility among individual, model and environmental features facilitates the application of heuristic-methods in multi-criteria decision-making settings. We draw on the neuropsychological, behavioral and decision analytic literatures to derive our experimental framework and contextualise our findings within the broader discourse in Behavioural OR [BOR]. ",Executing prescriptive psychological heuristics - Compatibility effects in decision support applications,"[57879, 2267]",113,"[10, 25, 26]",2959,Heuristics in BOR,13,15,11,Behavioural OR,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,12 [building - 116],"['Behavioural OR', 'Decision Analysis', 'Decision Support Systems']",WD-11
"Increasing intermittent renewable generation to meet the climate goals entails a deep transformation of current power systems. The transmission system must adapt to ensure a rapid and flexible response to the changes in the energy flows. Flexibility can be provided by reinforcing the interconnection among all the market agents and/or by installing facilities with fast response to the changes. In this work, we study the investment problem of a central planner that seeks to expand the transmission network and install storage units considering decarbonizing measures. We propose a two-stage stochastic problem with uncertainty on the demand growth and including representative days to characterize the hourly demand and renewable power variability. To obtain better expansion strategies according to a limit on the carbon emissions, second-order stochastic dominance constraints are imposed. Numerical analyses based on the power system of the Canary island [Spain] are provided. The results show that to install storage units is key to efficiently integrate renewable generation. The investments in the transmission system are mostly in lower-voltage lines. The formulation with stochastic dominance constraints results in higher second-stage investments, allowing a better adaptation to the demand growth evolution.",Investments in transmission lines and storage units considering second-order stochastic dominance constraints,"[45493, 18498, 30692]",475,"[136, 37, 126]",2960,Portfolio optimization ,49,9,34,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,43 [building - 303A],"['Stochastic Optimization', 'Energy Policy and Planning', 'Risk Analysis and Management']",TC-34
"Although research on facility location problems is extensive, temporary medical facilities and their application have not received much attention in the domain of preparedness or the immediate response stage after an earthquake where a huge number of casualties will need medical treatment. In this study, we take an applicable step in developing a decision support system [DSS] for local municipalities to assist in positioning emergency treatment sites [ETS] close to destruction sites for minor injuries [over 80% of total casualties], immediately after an event, and thus reducing hospitals’ loads. The DSS is based on characterization of the location network and hybrid simulation that can be applied both as a standalone emergency tool or as a planning tool for preparedness. Using a simulation/optimization model that determines the locations of ETSs, the DSS tool can compare multiple parameters and give recommendations within seconds. The proposed DSS tool includes three main components - database, decision engine and user interface, based on GIS technology [geographic information system]. Preliminary feasibility study was conducted and recommends the best location for ETSs to simulating various destruction site locations with promising results. The development process is done in collaboration with a specific local municipality which will also serve as a beta site for the final DSS product.
Keywords - Decision support system [DSS], Humanitarian application, Facility location",Locating emergency treatment sites [ETS] with decision support system [DSS] for preparedness and response to earthquake event for local municipalities,"[44090, 78255, 11027, 58670]",688,"[26, 58, 64]",2963,Disaster & Emergency Management,80,5,53,Sustainable and Resilient Systems,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,8007 [building - 202],"['Decision Support Systems', 'Humanitarian Applications', 'Location']",MD-53
"Hospital emergency departments are frequently modelled components of healthcare systems, owing to their importance in primary healthcare access and to their scrutiny in politics and the media. Queueing models are routinely used to model emergency departments and it is commonly assumed that service time distributions are independent of system state. This talk will explore a motivating emergency department dataset to discuss the dependence between patient arrivals and service times. A method for detecting the source of the dependence will be presented and the implications for developing accurate queuing models of hospital systems will be discussed.",Dependence between patient arrivals and service times in emergency department data,[78261],594,"[56, 121]",2964,ED logistics,3,12,10,OR in Health Services [ORAHS],"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,11 [building - 116],"['Health Care', 'Queuing Systems']",WA-10
"Support Vector Machines [SVM] constitute a highly effective tool for solving binary clasification problems. On the other hand, Twin Support Vector Machines [TWSVM] are based on the idea of constructing SVMs in pairs through generalized eigenvalues [GEPSVM]. These determine two hyperplanes not necessarily parallel by solving two problems similar to SVM but of smaller size. Consequently, the computational cost during the training phase is significantly reduced compared to traditional SVM. This requires solving two smaller quadratic programming problems [QPPs] that the one solved by the standard SVM.
Moreover, many real-life problems, such as those related to fraud prediction and credit scoring, involve costs of misclassification that can be different for both classes. Although providing accurate values for such misclassification costs can be challenging for the user, identifying acceptable misclassification rates is relatively straightforward. In this work, we propose a new TWSVM in which misclassification costs are considered by incorporating performance constraints into the problem formulation. This model is further enriched by performing variable selection [FS], a fundamental task that makes the method more interpretable and efficient. Numerical results demonstrate the effectiveness of the proposed method.",Enhancing interpretability in Twin Support Vector Machines via variable selection,[56863],123,"[66, 7, 111]",2965,Unraveling the Black Box - Advances in Model Explainability,15,13,27,Mathematical Optimization for XAI,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,047 [building - 208],"['Machine Learning', 'Analytics and Data Science', 'Programming, Mixed-Integer']",WB-27
"This work proposes a universal and adaptive second-order method for minimizing second-order smooth, convex functions. Our algorithm achieves optimal convergence rate when the oracle feedback is stochastic with variance, and improves its speed of convergence when it is run with deterministic oracles, where $T$ is the number of iterations. Our method also interpolates these rates without knowing the nature of the oracle apriori, which is enabled by a parameter-free adaptive step-size that is oblivious to the knowledge of smoothness modulus, variance bounds and the diameter of the constrained set. Building on this machinery, we show that we may be able to extract asymptotically faster rates than the standard Nesterov's accelerated second order methods.",EXTRA-NEWTON - A First Approach to Noise-Adaptive Accelerated Second-Order Methods,[76856],360,"[21, 19, 5]",2967,Adaptive and Polyak step-size methods,84,12,32,Advances in large scale nonlinear optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,41 [building - 303A],"['Convex Optimization', 'Continuous Optimization', 'Algorithms']",WA-32
"This work aims to investigate the blockchain introduction strategies and conditions in a competitive supply chain considering traceability awareness of consumers and the cost of blockchain technology base on game theory. In addition, this work explores supply chain fair coordination issues on the key supply chain decisions for adopting blockchain technology through a cost sharing contract. Firstly, through a proposed Stackelberg game model, a supply chain with two competitive suppliers, one manufacturer and one retailer is investigated under two scenarios, including decision-making without blockchain technology, with blockchain technology. Next, to explore the fair coordination of blockchain technology, the Nash bargaining method is adopted. Theoretical analysis and computational simulation are conducted to show how prices and profits are influenced by traceability awareness of consumers and competitive intensity of suppliers, and the impact of blockchain costs towards blockchain introduction. The results imply that the cost of adopting traceability technology is an important factor to be considered when supply chains adopt the technology. Firstly, this work enriches the literature by extending the relevant theory to considering the traceability awareness of consumers, as well as the cost of adoption the blockchain technology. Secondly, by considering cost sharing coordination, our research contributes to cooperation and fairness among supply chain members.",Stackelberg game approaches for the adoption of blockchain technology in competitive supply chains considering traceability awareness of consumers,"[78244, 36148]",895,"[25, 138, 50]",2968,"Game Theory, Solutions and Structures IX",88,12,36,"Game Theory, Solutions and Structures","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,32 [building - 306],"['Decision Analysis', 'Supply Chain Management', 'Game Theory']",WA-36
"Power-to-Gas [P2G], by converting surplus electricity to hydrogen using electrolysers, will be a crucial component in the future low-carbon energy system. This study investigates the market impacts of large-scale P2G deployment, with a specific focus on electrolysers and hydrogen storage, under a stringent climate scenario. We develop a game-theoretic equilibrium model of coupled day-ahead electricity, natural gas, and hydrogen markets for the Central Western Europe region. One novelty of this model is that we consider multiple hydrogen supply pathways, including national imports, elastic hourly hydrogen demand, and external costs of hydrogen blending in the representation of the [potential] wholesale hydrogen market.
Besides lowering hydrogen market prices and increasing electricity market prices, electrolysers may also increase electricity price volatility due to overlapping peak-load periods for electricity and hydrogen. The analysis of hydrogen storage highlights its role in mitigating general market volatility, albeit with only modest impacts on electricity markets, which indicates its limited potential to provide flexibility to the electricity market. Additionally, hydrogen storage may increase natural gas and hydrogen prices through its interaction with Steam Methane Reforming. This study contributes to the existing literature by providing insights for the effects of P2G on future energy markets in terms of intricate market dynamics and interactions induced by P2G.
",Market effects of Power-to-Gas technologies and their role under future gas supply shocks using a coupled electricity-gas-hydrogen market model,"[75192, 72753, 68618, 29367]",448,"[36, 37, 93]",2969,Decentralized multi-energy markets,22,7,09,Energy Markets,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,10 [building - 116],"['Electricity Markets', 'Energy Policy and Planning', 'OR in Energy']",TA-09
"Evaluating the impact of an activity on a project is essential for risk management, Therefore, over time, various researchers have proposed different risk indicators, including criticality index [Van Slyke, 1963], significance index [Williams, 1992], Crucially index [Williams, 1992], management-oriented index [Madadi & Iranmanesh, 2012], criticality-slack-sensitivity index [Ballesteros-Pérez et al., 2019], and more. However, while each indicator conveys a certain aspect of the activity's behavior, they have faced criticism for sometimes presenting an evaluation that is counter-intuitive to actual practice, and the debate on the choice of risk indicators is ongoing.
In this presentation, we first discuss the desirable properties that risk indicators should possess. These include consistency with an overall risk measure, linkage to a risk-reducing action, and risk decomposition. Based on these properties, we propose a risk assessment framework that uses conditional expectation. In the proposed framework, we measure the overall risk with the expected delay to a reference time point and define the risk indicator by an activity's expected duration conditioned with the delay and criticality of the activity. Finally, we demonstrate the effectiveness of the new indicators by applying them to the cost allocation for stochastic CPM and the buffer sizing for the buffer allocation problem.
",Risk assessment framework for project's activities with conditional expectation,"[58246, 58229]",959,"[118, 0]",2970,"Projects, risk and law",35,8,60,Project Management and Scheduling,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S09 [building - 101],['Project Management and Scheduling'],TB-60
"Data Envelopment Analysis [DEA] allows us to capture the complex relationship between multiple inputs and outputs in firms and organizations. Unfortunately, managers may find it hard to understand a DEA model and this may lead to mistrust in the analyses and to difficulties in deriving actionable information from the model. In this paper, we propose to use the ideas of target setting in DEA and of counterfactual analysis in Machine Learning to overcome these problems. We define DEA counterfactuals or targets as alternative combinations of inputs and outputs that are close to the original inputs and outputs of the firm and lead to desired improvements in its performance. We formulate the problem of finding counterfactuals as a bilevel optimization model. For a rich class of cost functions, reflecting the effort an inefficient firm will need to spend to change to its counterfactual, finding counterfactual explanations boils down to solving Mixed Integer Convex Quadratic Problems with linear
constraints. We illustrate our approach using both a small numerical example and a real-world dataset on banking branches.",Counterfactual Analysis and Target Setting in Benchmarking,"[22145, 62167, 67604]",638,"[66, 35, 72]",2971,Counterfactual Analysis Across Diverse Domains,15,7,27,Mathematical Optimization for XAI,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,047 [building - 208],"['Machine Learning', 'Efficiency Analysis', 'Mathematical Programming']",TA-27
"The multiarmed bandit problem [MAB] is a classic problem in which a finite amount of resources must be allocated among competing choices with the aim of identifying a policy that maximizes the expected total reward. The classical MAB makes the strong assumption that the decision maker is risk-neutral and indifferent to the variability of the outcome. However, in many real life applications, these assumptions are not met and decision makers are risk-averse. Motivated to resolve this, we study risk-averse control of the multiarmed bandit problem in regard to the concept of dynamic coherent risk measures to determine a policy with the best risk-adjusted total discounted return. In respect of this specific setting, we present a theoretical analysis based on Whittle’s retirement problem and propose a priority-index policy that reduces to the Gittins index when the level of risk-aversion converges to zero. We generalize the restart formulation of the Gittins index to effectively compute these risk-averse allocation indices. Numerical results exhibit the excellent performance of this heuristic approach. Our experimental studies suggest that there is no guarantee that an index-based optimal policy exists for the risk-averse problem. Nonetheless, our risk-averse allocation indices can achieve optimal or near-optimal policies which in some instances are easier to interpret compared to the exact optimal policy.",Index policy for multiarmed bandit problem with dynamic risk measures,"[36983, 68392]",535,"[135, 136]",2972,Risk Averse and Contextual Stochastic Optimization,49,8,35,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,44 [building - 303A],"['Stochastic Models', 'Stochastic Optimization']",TB-35
"This research systematically examines the impacts of blockchain and smart contracts on human-machine trust, and thus to understand how these cutting-edge technologies and innovative applications are poised to revolutionize industrial inter-organizational relationships and business collaborations.
The research model reveals that how blockchain and smart contracts enhance the reliability and efficiency of transactions, thereby fostering a new level of trust between humans and machines. We hypothesize that the immutable and transparent nature of blockchain, coupled with the autonomous execution of smart contracts, streamlines processes, reduces disputes, and facilitates seamless inter-organizational collaborations; the token economy fostered by blockchain and consensus mechanism plays a moderating role by incentivizing organizational behaviors and creating value within the ecosystems by redefining business models and revenue streams.
The study adopts a mixed-methods approach. The qualitative interviews from experts and practitioners delve into transformation in trust dynamics and collaborations models enabled by blockchain technology, and the quantitative analysis of data collected from blockchain-implemented systems in industry focuses performance metrics and trust indicators pre & post blockchain implementation.
This research provides a comprehensive understanding of the transformative potential of blockchain and smart contracts in reshaping industrial landscapes.",How Blockchain and Smart Contracts Transform Human-Machine Trust and Inter-Organizational Relationships in Industry,"[78262, 78260]",469,"[10, 32, 148]",2973,Modelling social-behavioural phenomena in creative societies,13,15,07,Behavioural OR,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,1019 [building - 202],"['Behavioural OR', 'E-Commerce', 'Web-based Information Systems']",WD-07
"Nowadays, vehicles are able to drive autonomously in specific contexts. Spoke 6 project, part of the Italian National Centre for Sustainable Mobility [MOST], aims to contribute to developing an ecosystem of research and testing labs to promote the deployment of Connected and Autonomous Vehicles [CAV]. This development will result in a network of permanent facilities for collaborative research, promoting technology transfer, increasing technology readiness in real-world contexts, and facilitating large-scale adoption of automated vehicles. 
In this context, Spoke 6 is extending the Modena Automotive Smart Area [MASA] to create an Urban Living Lab for CAV testing and data collection in a real urban multi-vehicle setting. The MASA facilities have been outfitted with traffic monitoring systems [based on smart cameras, RSUs, and OBUs] to enable the implementation of safety-critical scenarios. In this case, communication between infrastructure and road actors can broaden the electronic horizon and raise awareness of the traffic situation.
In line with the project's pillars, this living lab is producing outcomes on - [i] Simulation, with the development of a Digital Twin of the MASA to reconstruct the environment and visualize real traffic data; [ii] Vehicle Platforms, by equipping an automated vehicle with communication modules and an innovative HMI; [iii] Datasets, with the collection of environmental and driving data to design and test automated driving modules.",Urban multi-vehicle safety-critical environment for Connected and Automated Vehicles - MASA living lab in the context of Spoke 6,"[77832, 78272, 78270, 78274, 78269, 78271, 78268, 78279, 78273, 78277, 78276]",332,"[143, 0]",2975,MOST - MaaS & Innovative Services for Sustainable Mobility,6,12,55,Transportation,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S02 [building - 101],['Transportation'],WA-55
"Given an undirected graph, we study the team orienteering arc routing problem which asks to find a set of maximum profits routes starting and ending at the depot satisfying the vehicles' travelling time constraints. This problem has many important applications in practice, such as waste collection, snow plowing, and security patrolling, etc. In this article, we propose three new integer linear programming [ILP] formulations with only binary variables for this problem. To solve this problem in large instances, we introduce a Logic-based Benders decomposition [LBBD] and incorporate it into a branch-and-cut framework. We design problem-specific techniques to further improve the performance of the algorithm, such as Benders cuts strengthening approaches, repair heuristics, as well as symmetric breaking constraints and valid inequalities. In the computational study, we compare the performance of these formulations and different techniques.  ",Formulations and Exact Solution Methods for the Undirected Team Orienteering Arc Routing Problem,"[77404, 1116, 22042]",725,"[14, 145, 11]",2977,Vehicle Routing II,64,5,29,Combinatorial Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,157 [building - 208],"['Combinatorial Optimization', 'Vehicle Routing', 'Branch and Cut']",MD-29
"One cannot make truly fair decisions using integer linear programs unless one controls the selection probabilities of the [possibly many] optimal solutions. For this purpose, we propose a unified framework when binary decision variables represent agents with dichotomous preferences, who only care about whether they are selected in the final solution. We develop several general-purpose algorithms to fairly select optimal solutions, for example, by maximizing the Nash product or the minimum selection probability, or by using a random ordering of the agents as a selection criterion [Random Serial Dictatorship]. We also discuss in detail how to extend the proposed methods when agents have cardinal preferences. As such, we embed the “black-box” procedure of solving an integer linear program into a framework that is explainable from start to finish. Lastly, we evaluate the proposed methods on two specific applications, namely kidney exchange [dichotomous preferences] and the scheduling problem of minimizing total tardiness on a single machine [cardinal preferences]. We find that while the methods maximizing the Nash product or the minimum selection probability outperform the other methods on the evaluated welfare criteria, methods such as Random Serial Dictatorship perform reasonably well in computation times that are similar to those of finding a single optimal solution.",Fair integer programming under dichotomous preferences,"[69665, 9583, 58371, 10607]",642,"[109, 13, 41]",2978,Market Design 2,87,12,43,Market Design,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,99 [building - 306],"['Programming, Integer', 'Column Generation', 'Ethics']",WA-43
"Given an undirected edge-colored graph with non-negative edge length, the aim is to find a minimum Steiner Tree that uses at most one edge for each color.
The problem, called Rainbow Steiner Tree Problem [RSTP], is known to be NP-hard and can be used to model different real-world applications such as in wireless connectivity.
Given the complexity of the underlying problem, we develop a simple variant of the Kernel Search algorithm based on the multicommodity flow formulation with rainbow constraints.
Comprehensive work has been done on the Kernel search framework to enhance its effectiveness over the discussed problem. We test our approach on benchmark instances and compare its performance with both state-of-the-art algorithms and MIP solvers. Preliminary results are very promising.
",Solving the Rainbow Steiner Tree Problem using Kernel Search,"[78068, 10527, 67738, 80064]",195,"[14, 53, 5]",2979,Combinatorial optimization topics in transportation,64,2,26,Combinatorial Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,012 [building - 208],"['Combinatorial Optimization', 'Graphs and Networks', 'Algorithms']",MA-26
"For optimization models to be used in practice, it's crucial that users trust the results. This is influenced among other factors by the interpretability of the solution process. A previously introduced framework for inherently interpretable optimization models proposes to use decision trees to map instances to solutions of the underlying optimization model. Based on this work, we investigate how we can use more general decision rules to further increase interpretability and at the same time give more freedom to the decision maker. These decision rules do not map to a concrete solution, but to a solution set instead, which is characterized by certain features. A framework for MIP formulations as well as heuristics for generating such decision rules are presented. We outline the challenges and opportunities that these methods can present. In particular, we show the gain in solution quality that our approach can provide by using the conventional framework for interpretable optimization models as a benchmark and discuss the relationship between interpretability and performance. All methods are evaluated by using both artificial and real-world data.",Feature-Based Interpretable Optimization,"[75074, 29733, 50791, 75186]",431,"[7, 8, 111]",2983,Interpretable Optimization Methods and Applications,14,13,03,Data Science Meets Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1005 [building - 202],"['Analytics and Data Science', 'Artificial Intelligence', 'Programming, Mixed-Integer']",WB-03
"Food supply chains play a significant role in carbon emissions, and adopting emerging Industry 4.0 technologies can help mitigate this issue. The study investigates the compatibility of digital technologies at various echelons of the supply chain. Among the components of food supply chains, logistics are also particularly emissions-intensive and therefore require careful attention. This study presents a mixed integer linear programming mathematical model for selecting compatible technologies in food supply chains under a capacitated multi-depot vehicle routing distribution to minimise the supply chain's carbon footprint and to align with global carbon neutrality targets. To address the complexities of real-world scenarios, a matheuristic algorithm is developed empowered by a Reinforcement Learning [RL] approach. The findings indicate that employing suitable technologies with optimal compatibility levels and implementing an optimum routing will lead to achieving a net-zero supply chain and reducing overall costs. The results underscore the importance of both technology selection and compatibility in attaining sustainability objectives.",Technologies Compatibility in a Capacitated Multi-Depot Routing distribution system for Food Sustainable Supply Chain,"[78278, 78296, 78288]",919,"[84, 146, 77]",2984,Sustainable food supply chains,18,4,23,"Circular Economy, Remanufacturing and Recycling ","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,82 [building - 116],"['Optimization Modeling', 'Warehouse Design, Planning, and Control', 'Multi-Objective Decision Making']",MC-23
"Modeling preferences from observed choices is a pivotal theme in mainstream economics and decision theory with expansive applications in policymaking, marketing, and transportation. The standard approach in modeling preferences is to attach utility scores to the choice options consistent with the order defined by the preferences. Nevertheless, the utility scores are merely artifacts or mediums to represent preferences, whereas preferences are the underlying construct generating choices. We aim to develop a framework faithful exactly to this view.
Our model takes preferences as the central primitive. Preferences generate choice data and are represented by analytically convenient mediums called utility functions. We argue that contrary to our approach, defining preferences in terms of utilities and choices can pose severe challenges for nonparametrically inferring the underlying choice process. To address this shortcoming, we model preferences directly by a probability distribution over the set of strict linear orderings of alternatives instead of an underlying utility. We build on the theoretical ground of the random preference literature and use the distributionally robust nonparametric approach to minimize the specification and distributional assumptions required to derive robust conclusions. We address the computational challenges and demonstrate the applicability of our model through an empirical application to a conjoint setting.",Random Preference Model,"[41910, 78286, 19484]",95,"[10, 25, 27]",2985,Choice behavior,13,2,11,Behavioural OR,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,12 [building - 116],"['Behavioural OR', 'Decision Analysis', 'Decision Theory']",MA-11
"We deal with operational fixed interval scheduling problem where start times are given and the actual finishing times can be influenced by random delays. We further consider heterogeneous case, i.e., multiple job and machine types. And we assume that the multivariate distribution of delays follows an Archimedean copula. We consider the highest worst-case probability that the schedule remains feasible, where given proportion of marginal distributions of delays are stressed. This problem has an interesting reformulation containing a commonly used risk measure. We implement a decomposition algorithm. A possible application is gate assignment problem where incoming flights have to be assigned to available gates of an airport. In this problem, heterogeneity is caused by the fact that different aircrafts might need to be assigned to different gates. Moreover, there can be proportion of flights where worse delays occur.",Distributionally robust fixed interval scheduling with heterogenous machines and random delays,[78197],826,"[136, 129]",2988,Robust Optimization - Theory and Applications,49,14,35,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,44 [building - 303A],"['Stochastic Optimization', 'Scheduling']",WC-35
"Wildfires pose significant threats to human life, ecological balance, and the economy. The unpredictable and highly dynamic nature of wildfires emphasizes the need for analyzing wildfire management using non-linear methods. This study explores the causal relationships among the physical nature of fires, resource management, and the costs associated with prevention and suppression under various wildfire management policies via system dynamics approach. Our study extends previous works by incorporating remote sensing technologies in the prevention stages of wildfires. The utilization of such technologies play a significant role in preventing the escalation of potential fires and ensuring the effective use of operational resources. Additionally, calculating the probabilities of errors through Failure Mode and Effect Analysis for potential errors in wildfire management further enrich the system dynamics model. The insights from the model will be provided for policy makers.",Enhancing Wildfire Management - A Comprehensive Approach Integrating System Dynamics and Remote Sensing Technologies,"[76303, 80016, 78295]",621,"[48, 140, 131]",2989,OR in Forestry II,20,8,12,OR in Agriculture and Forestry ,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,13 [building - 116],"['Forestry Management', 'System Dynamics and Theory', 'Simulation']",TB-12
"The share of renewable energy generation capacities will rise substantially in the upcoming decades, which requires the expansion of power grids and storage solutions. Current research therefore often aims at jointly optimizing expansion and operation of generators, storages and grid infrastructure, resulting in complex optimization problems to be solved. Their structure reveals decomposability on the temporal as well as on the spatial scale, which allows the application of decomposition techniques to keep them computationally tractable. While investment decisions spanning over multiple time steps represent complicating variables on the temporal scale, constraints describing inter-regional power exchanges represent complicating constraints on the spatial scale. Previous decomposition approaches in energy system modelling have either been performed on the temporal or on the spatial scale, but combined approaches have not yet been investigated. As energy system models hold complicating variables and constraints, there exist numerous possible decompositions of the original problem, which may be solved by different decomposition techniques. In this work we analyze which techniques are most efficient in optimizing large-scale multi-regional energy systems. We deliver a case study for the comparison of different decomposition strategies for a specific problem type and contribute to the open question of which decompositions are most suitable for which type of optimization problems.",Analyzing decomposition approaches for large-scale energy system models,"[76935, 62168]",841,"[93, 37, 79]",2990,Decomposition techniques applied to energy problems,23,10,19,OR in Energy,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,44 [building - 116],"['OR in Energy', 'Energy Policy and Planning', 'Network Design']",TD-19
"Credit risk in the banking sector is crucial in the current discussions on climate change and environmental sustainability. The transition from non-renewable to renewable energy sources, supported by the Paris Agreement, is the main strategy to mitigate the effects of climate change through the reduction of greenhouse gas emissions. Governments' tailored climate policies are key in this context to promote a greener economic pathway. Climate policy uncertainty places banks and their credit allocation decisions in a significant position, directly influencing investment choices in the energy sector, including the development and adoption of renewable energy technologies. In this context, credit risk emerges as a crucial factor, as banks, the main lenders, guide the directions of
energy investments by granting or withdrawing credit for various energy projects. This study aims to assess the impact of climate policy uncertainty on electricity generation in the U.S. from 1988 to 2015, focusing on non-renewable and renewable sources. The main goal is to investigate the effects that the CPU have on such energy sources. We employ the Fourier-ARDL model and Granger's causality test to explore the complex relationship between CPU and electricity production. The findings have significant implications for policymakers, managers, and banks, with regard to energy infrastructure investment decisions and lending decisions for the acquisition of such infrastructure.","Climate policy uncertainty, electricity production and credit risk - do banks enhance green investments?","[77499, 78307, 72133]",947,"[36, 37, 44]",2993,Pathways to Climate Resilience,80,12,53,Sustainable and Resilient Systems,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,8007 [building - 202],"['Electricity Markets', 'Energy Policy and Planning', 'Finance and Banking']",WA-53
"Integrated planning and scheduling are crucial aspects of airline management, especially when dealing with uncertain passenger demand. A common strategy to tackle this challenging approach is to develop partially integrated optimization models. While existing research deploys these partial models in stages, this study introduces a framework based on a multi-agent system model that combines fleet planning, network planning, and flight scheduling into a single model. Our framework provides a comprehensive solution to the challenges arising from the complex interactions between fleet acquisition decisions, network design, and flight timetables in an airline. The planning and scheduling are carried out through computational agents representing departments and individual decision-makers within an airline. A demand scenario tree is utilized to model demand uncertainties in the future. Nodes in the scenario tree contain similar computational agents and receive demand input associated with that node. The final plan is generated using an innovative collaborative multi-agent planning algorithm that allows information exchange between agents within each node and the nodes themselves. The framework can accommodate detailed operational constraints of the airline, enabling the tailored identification and evaluation of potential operational improvements. Finally, we demonstrate the application and potential improvements of the framework in several cases.",Multi-Agent Airline Integrated Planning and Scheduling Under Demand Uncertainty,"[69094, 55223, 59488]",600,"[4, 3, 26]",2996,Simulation in transportation and logistics,77,9,43,"Agent-based Models in Management, Economic and Organisation Sciences","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,99 [building - 306],"['Airline Applications', 'Agent Systems', 'Decision Support Systems']",TC-43
"In recent years, space exploration has received increasing attention, thus underlining the need for efficient management of space missions. Within this field, Satellite Observation Scheduling Problem [SOSP] concerns the scheduling of observations performed by satellites orbiting around Earth or other celestial bodies. The SOSP has been largely addressed in the last 30 years, dealing with both Earth-centered and outer space missions, with a lot of variants developed to cope with diverse applications and constraints.
Our research focuses on the study of SOSP for the radar MARSIS onboard Mars Express mission, which observes the subsurface of Mars to map the presence of water. Specifically, the Mars Observation Scheduling Problem [MOSP] aims to optimally schedule MARSIS observations to reach a maximum quality coverage of the South Pole of Mars. The MOSP is of high difficulty and has been only manually solved until now.
To solve the MOSP we use an approach combining machine learning and optimization. We first employ a neural network to predict the quality of future observations, starting from a massive historical dataset. We then look for the solution that maximizes the predicted quality by invoking different techniques, including an Integer Linear Program and a set of constructive heuristics and matheuristic algorithms. The resulting algorithm is tested on several real-world instances and scenarios derived from the Mars Express mission, showing good performance.
",Satellite Observation Scheduling Problem - an application on Mars Express mission,"[78009, 62396, 7965, 78304, 78305]",874,"[129, 72, 66]",2997,Applications of combinatorial optimization II,64,8,25,Combinatorial Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,011 [building - 208],"['Scheduling', 'Mathematical Programming', 'Machine Learning']",TB-25
"Transportation services play a pivotal role in international trade, with trucking companies handling approximately 77% of inland freight. Refueling costs constitute a substantial portion of operational expenses, ranging between 20% and 30% in Europe. To address the complexities of refueling decisions in the trucking industry, we propose a data-driven sequential decision-making framework. Our approach captures price fluctuations along trucking routes, incorporating factors such as region, time, and possible price agreements with gas stations. Leveraging the Markov Decision Problem and Reinforcement Learning [RL] methods, we develop a robust framework capable of minimizing refueling costs while accounting for fuel price uncertainties. In a real-world case study conducted with a logistics company in the Netherlands, our framework demonstrated significant savings in refueling costs. Through extensive experimentation, we validate the superiority of our approach compared to in-practice and benchmark policies, showcasing the practical utility and effectiveness of our proposed framework.
",Smart refueling decisions using reinforcement learning approach - a case study from trucking industry,"[71791, 78319, 71792]",859,"[135, 65, 7]",2998,"Discrete, continuous or stochastic optimization and control in networks, transportation and design III",64,4,25,Combinatorial Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,011 [building - 208],"['Stochastic Models', 'Logistics', 'Analytics and Data Science']",MC-25
"Industrial symbiosis [IS] is a concept of circular economy, where one company utilizes waste, residue, or by-products of another company to replace otherwise needed resources. Companies can also engage in energy-based IS. Here they either utilize waste energy streams directly in energy cascades, or they use waste products for fuel replacement and bioenergy production. Examples are the utilization of waste heat from industrial processes in district heating systems or the usage of industrial or urban waste for energy production. The concept of energy-based IS is often applied in so called hubs for circularity [H4C], i.e., industrial parks where co-located companies collaborate to close energy and resource loops. In this work, we present a modelling approach to optimize the operational planning of the energy flows in a H4C utilizing energy-based IS. Since the waste [energy] streams are by-products of other main production processes, they are not plannable and, therefore, uncertain and volatile. To address this uncertainty, we apply stochastic programming. We present results for a case study using industrial and public data and input from literature. In our experiments, we investigate the benefits of energy-based IS for the energy cost and energy mix of the hub. ",Operational optimization of energy flows in energy-based industrial symbiosis,"[45137, 70814, 78306]",471,"[93, 135]",3000,Stochastic models in energy systems planning and operations,21,9,22,Energy Management,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,81 [building - 116],"['OR in Energy', 'Stochastic Models']",TC-22
"Nowadays, public territorial planning is called to properly orient public funding toward interventions that ensure long-term sustainability while contributing to collective well-being. For this reason, resorting to integrated planning tools, opened to public-private partnerships, becomes crucial, together with considering their coherence with the reference planning frame. Based on these premises, the research reflects on the evaluation models’ role in addressing this coherence issue and supporting territorial planning by implementing negotiated planning tools in the Lombardy Region. Indeed, negotiated planning is a means to address public interest toward territorial development by promoting public-private partnerships. It is ruled by the R.L. 19/2019, which provides an evaluation clause to understand the tools’ contribution to achieving regional objectives. Thus, the research aims to define an evaluation model to support the regional administration in ensuring the effective implementation of negotiated planning tools and their compliance with regional strategies. The multi-dimensional nature of the considered issue calls for resorting to Multi-Criteria Analysis. After describing the methodological process to trace the law’s provisions into a multi-criteria frame, the evaluation model is tested on some case studies of implemented Agreements in the Lombardy Region. Such a test better highlights the model’s opportunities, limits, and improvement room.",A MCDA Model to Support Territorial Planning at the Regional Scale - the Case of Negotiated Planning Tools in the Lombardy Region,"[51367, 46433, 67432, 46453]",891,"[26, 55, 77]",3001,MCDA and urban planning 3,44,10,47,Multiple Criteria Decision Analysis,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,50 [building - 324],"['Decision Support Systems', 'Group Decision Making and Negotiation', 'Multi-Objective Decision Making']",TD-47
"We consider a Markovian queueing system with a single server which is subject to breakdowns. When a breakdown occurs, the server continues its operation but at a reduced service speed. The repair process initiates only when the system is empty and during that time, customers continue to accumulate without receiving service. Customers are strategic, and while observing the state of the system, they may choose to renege at any point. To study the impact of reneging behavior, we also consider a similar model where reneging is prohibited, and customers only decide whether to join the system or to balk. In these scenarios, we derive the equilibrium strategy of the customers and we compute the steady state distribution under equilibrium. In particular, we find that in equilibrium, the customers follow a triple threshold strategy which we characterize. To analyze the effect of the customer reneging behavior, we conduct a theoretical comparison of these systems and perform several numerical experiments. We identify cases where reneging is beneficial in terms of social welfare, as well as cases where prohibiting reneging ensures greater welfare.",Reneging behavior of customers in a queueing system subject to breakdowns and repairs,"[78303, 66984]",884,"[121, 50]",3002,Queueing Models with Strategic Customers,47,7,40,Advances in Stochastic Modelling and Learning Methods,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,96 [building - 306],"['Queuing Systems', 'Game Theory']",TA-40
" There are no computationally feasible algorithms that provide solutions to the finite horizon Risk-sensitive Constrained Markov Decision Process [CMDP] problem, even for the problems with moderate horizon. With an aim to design the same, we derive a fixed-point equation such that the optimal policy of the CMDP is also a solution.  We further provide two optimization problems equivalent to the CMDP. These formulations are instrumental in designing a global algorithm that converges to the optimal policy. The proposed algorithm is based on random restarts and a local improvement step. Here the local improvement step involves solving a Linear program, and utilizes the solution of the derived fixed-point equation, while, the random restarts ensure global optimization.  Such MDPs are utilized to model and derive robust inventory control. 
    
Further, one may have constrained stochastic games whose objective and constraints are a combination of linear and risk-sensitive utilities. 
The best response becomes a constrained MDP with a combination of linear and risk-sensitive utilities. We build upon the theory developed for constrained risk-sensitive MDP and develop a solution technique to numerically solve such combined constrained MDPs and eventually the game.

Finally, we suggest an online learning algorithm that can potentially learn the optimal policy, when the data defining the MDP is not known. ","Algorithms to solve Risk Sensitive MDPs, Games and Applications","[62223, 78493, 78316]",14,"[27, 50, 135]",3003,Reinforcement Learning - Methods and Applications ,47,8,40,Advances in Stochastic Modelling and Learning Methods,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,96 [building - 306],"['Decision Theory', 'Game Theory', 'Stochastic Models']",TB-40
"Systemic financial risk refers to the situation in which failures or disturbances in one part of the financial system can spread rapidly and extensively, resulting in significant disruptions to the broader economy. Measuring systemic financial risk is a complex task that typically involves various quantitative and qualitative indicators. Examples include the Systemic Risk Index [SRISK], Conditional Value-at-Risk [CoVaR], and Financial Stress Index [FSI]. Such measures are closely related to tail dependence measures, which are introduced to quantify the extent of dependence between extreme events of a couple of random variables. 
The scope of this research is to provide insight into the possible connection between the risk factors behind climate change and the strength of systemic risk among European companies. To achieve this aim, we first introduce a systemic risk measure based on a recently proposed coefficient of tail dependence which is derived from surface integrals. We then compare the results with existing research. Subsequently, we dynamically relate such measures to the evolution of a set of climate change risk factors.",Surface Measures of Tail Dependence and Climate Change,[56160],412,"[7, 33]",3004,Quantitative methods for systemic and climate risk,9,4,51,Risk management in finance,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,M5 [building - 101],"['Analytics and Data Science', 'Economic Modeling']",MC-51
"This study focuses on the deployment of a fleet of drones required to provide temporary communication during emergencies. It is assumed that drones are initially placed at a depot, while the area is divided into subregions that require continuous communication services. In addition, each drone is subject to battery limitations and thus, needs to return to the depot to recharge its battery. As emergency communication is a continuous activity, drones need to visit the subregions in a sequence. The aim here is to provide continuous coverage of the area by utilizing drones and their limited batteries efficiently. The first aspect of interest is how long a drone should hover over a subregion. Hence, it is aimed to keep each drone as much as possible over every visited subregion, to ensure maximum coverage. Decisions include the selection of visited subregions, the optimal hovering time of each drone over the subregions, and the scheduling of the subregion visit sequence, all aimed at maximizing temporal and spatial coverage. The problem is formulated as a mixed-integer linear program [MILP] resembling the covering tour problem. This mathematical model enables us to model the complex interplay between drone movements, area coverage, and battery constraints. Furthermore, a constraint programming [CP] model is proposed to solve the problem and computational experiments are conducted to assess the performance of the CP and MILP models on randomly generated large-scale instances.",Energy-constrained multi-drone covering tour problem with visit durations,"[77539, 78310, 2435, 75745, 78178]",858,"[111, 14, 72]",3005,"Discrete, continuous or stochastic optimization and control in networks, transportation and design II",64,3,25,Combinatorial Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,011 [building - 208],"['Programming, Mixed-Integer', 'Combinatorial Optimization', 'Mathematical Programming']",MB-25
"The increasing societal emphasis on intellectual property rights and the rampant proliferation of pirated products have led to the gradual significance of corresponding legal frameworks and penalties. This study aims to explore the optimal determination of punitive damages for intellectual property infringement on pirated video platforms under various circumstances, with an empirical study serving as validation. Initially, a utility function is established to determine consumer behavior. The primary objective of this study is to investigate the Stackelberg game model with publishers as leaders and legal platform operators, as well as pirated platform operators, as followers. Subsequently, an analysis of infringement behavior across different scenarios is conducted, assessing how the magnitude of punitive damages impacts publisher revenue. An empirical study is then conducted to validate the relationship between the theoretical model and case study. The overall goal of this research is to provide more concrete and effective methods and strategies for addressing intellectual property infringement issues on online video platforms.",A game theoretical analysis on penalty of online copyright infringement with an empirical study,"[42171, 69414, 78489]",185,"[50, 130, 10]",3006,Experimental economics and game theory 1,73,13,40,Experimental economics and game theory,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,96 [building - 306],"['Game Theory', 'Service Operations', 'Behavioural OR']",WB-40
"Computational tractability remains a significant challenge in modelling large-scale, sector-coupled energy systems. This study investigates the soft-linking of models to broaden the analysis scope while maintaining practical computation times. A case study is defined because soft-linking applications are unique to the specific research question. Resource adequacy assessment is therefore defined as the scope in a bidirectional coupling of Balmorel [which performs investment and dispatch on time slices] and the stochastic dispatch model, Antares, which optimises dispatch in multiple weather years with hourly resolution. The models are fully harmonised in terms of data, geographical granularity and level of sector-coupling in a pan-European context. The bidirectional soft link is analysed by comparing computation times at different temporal aggregation levels with the objective of finding the least-cost system with the lowest loss of electricity and hydrogen load. A higher degree of aggregation leads to lower computation times per iteration but more inadequate systems. Thus, more iterations are required to adjust investments. The opposite is valid for a less aggregated approach. A discussion is carried out of the different methods used to signal the need for more investment in carrier transmission, generation capacity, etc., and a conclusion on the benefits and challenges of soft-linking is derived from these findings.",Benefits and challenges of soft-linking for large-scale energy system modelling,"[75281, 74094, 64035, 58629, 62517]",175,"[63, 37, 135]",3007,"Energy sector coupling, optimization and equilibrium",23,3,21,OR in Energy,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,49 [building - 116],"['Large Scale Optimization', 'Energy Policy and Planning', 'Stochastic Models']",MB-21
"In this paper, we are interested in assessing how changing the market structure interacts with learning spillovers and organizational forgetting to affect incentives to innovate via learning. In particular, we are interested in studying the relationship between market competition and learning-by-doing, a type of innovation, aiming to contribute to the classical debate over the relationship between market structure and innovation. We provide conditions under which a symmetric Closed-Loop Nash Equilibrium exists. Then we conduct a comparative steady state analysis with respect to the parameters of the model. The main conclusion of our analysis is that conditions on the diffusion rate of knowledge and the rate of organizational forgetting exist under which an increase in market competition is not necessarily welfare-enhancing. ",Learning and Forgetting in Oligopoly,"[78300, 78302]",620,"[33, 50, 31]",3008,"Game Theory, Solutions and Structures V",88,7,36,"Game Theory, Solutions and Structures","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,32 [building - 306],"['Economic Modeling', 'Game Theory', 'Dynamical Systems']",TA-36
"Economic and ecological advantages make electric vehicles [EVs] more desirable than internal combustion engine vehicles. The charging need of EVs can have a critical impact on electricity distribution at certain penetration levels. Irregular intraday electricity demand may cause overloads, thus increasing costs and decreasing quality. Appropriate charging pricing policies can be effective in reducing the imbalance in electricity demand. In this study, we examined the effects of fixed and time-of-use [TOU] pricing policies when users have different charging options. We developed a comprehensive simulation model to examine users' behavior in different charging option distributions. A Deep Reinforcement Learning [DRL] model is proposed to determine the pricing policy that offers the best price levels in terms of invariability of the intraday aggregate EV load profile. Our experiments in different charging option availability levels show that the TOU pricing schemes obtained from the proposed DRL model offer prices leading to more balanced load curves compared to fixed-price charging pricing. We also show that peak loads are more likely to occur when EV drivers have similar charging options. In these cases, generated TOU price tables reduce the standard deviation of the load profile more noticeably. This study is supported by TUBITAK under grant number 221M111.",A TOU pricing approach for EV charging via DRL,"[77512, 78760, 78332, 78342, 78331, 78352]",468,"[37, 66, 93]",3009,Optimization for electric vehicles,21,8,22,Energy Management,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,81 [building - 116],"['Energy Policy and Planning', 'Machine Learning', 'OR in Energy']",TB-22
"This paper discusses a  two-stage   bulk transportation problem and develops a polynomial bound iterative algorithm to
find its optimal solution. In this problem, the set of destinations is divided
into two disjoint sets consisting of ‘primary’ and ‘secondary’ destinations,
respectively. Due to limited availability of vehicles, the company prefers
to transport the goods to the primary destinations first [in Stage-I] and
once the demand of the primary destinations is met, the secondary destinations are catered later [in Stage-II] from the left-over availability at
various sources. Like a standard bulk transportation problem, the demand
of a destination must be fulfilled by one source only, however, one source
can cater more than one destinations. The transportation time corresponding to each source-destination link is considered to be known. The
objective of the problem is to find such a transportation schedule that
provides the minimum sum of transportation times of both the stages.
The proposed algorithm, at each iteration, solves a related cost minimizing transportation problem and one of its restricted variants, using a
branch and bound technique and converges systematically, to the optimal
objective value of the problem. The efficiency of the proposed algorithm
is validated through various numerical illustrations. Further, the performance of the algorithm, in terms of CPU time, for various randomly
generated instances, is also provided.",Two-stage Time Minimizing Bulk Transportation Problem,"[66379, 78314]",879,"[14, 5, 11]",3010,Combinatorial optimization issues in transportation [Contributed],64,15,26,Combinatorial Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,012 [building - 208],"['Combinatorial Optimization', 'Algorithms', 'Branch and Cut']",WD-26
"This paper examines the existence of a linear relationship between the consistency ratio and the compatibility index in the AHP/ANP framework. The linear relationship is tested with respect to accuracy principle, using an original development referred as “forward accuracy”. For several years, students in Bucharest University of Economic Studies were asked to pairwise compare the brightness of five colors. Their consistency and compatibility indices were collected and based on these, accuracy curves among these two measures were inferred. While there is hope in the AHP/ANP methodology that a low value of the inconsistency ratio will grant compatibility and the rightness of the decision-making structures, there is rather an insignificant number of practical experiments to challenge this one. This multi-annual experiment shows that in the simple situation when brightness is compared, accuracy curves as linear relationships between consistency and compatibility remain similar, while on the same accuracy curve one could find high consistency with low compatibility and the other way around.  Conclusions are two-folded - is need to perform lots of experiments, to figure out what is the acceptable threshold of inconsistency ratio and alternative  measures like  “forward accuracy” where the impact of a pair [consistency-compatibility] on the shape of the linear relationship is assessed in terms of future impact could prove useful.",Forward Accuracy in AHP/ANP - Unraveling the Consistency-Compatibility Conundrum through Color Brightness Comparisons,[71005],893,"[6, 10, 7]",3011,Pairwise comparisons and preference relations 3,44,12,44,Multiple Criteria Decision Analysis,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,20 [building - 324],"['Analytic Hierarchy Process', 'Behavioural OR', 'Analytics and Data Science']",WA-44
"In this talk, I am going to present my academic profile and introduce myself to the EURO community in order to initiate possible future collaborations. I will then talk about my research interest and my current research work which is on the integration of machine learning into meta-heuristic algorithms to solve combinatorial optimization problems. I will give an overview of the different ways where we can integrate machine learning into meta-heuristic algorithms.

 ",Machine learning at the service of meta-heuristics for solving combinatorial optimization problems,[73435],458,"[66, 74]",3012,YW4OR_3,39,14,12,WISDOM - Women in OR,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,13 [building - 116],"['Machine Learning', 'Metaheuristics']",WC-12
"The paper deals with non-cooperative games in which the payoff function of its players is influenced by exogenous randomness. The main goal is to provide a general concept of stability in those games because the standard notion of Nash equilibrium is no longer satisfactory. One could find a deterministic equivalent to the game with a random payoff by considering a risk measure and defining a new game with a payoff function adjusted by the risk measure. This, however, causes several problems as the Fundamental Theorem of non-cooperative game theory no longer holds for most of such defined equivalents. Our idea is to loosen the standard concept of the best response to an alpha-best response which requires the strategy to be the best response only with a certain high probability. Based on this idea we define the alpha-Nash equilibria and we prove that for every finite game with random payoff non-trivial alpha-Nash equilibria exist. Moreover, those equilibria characterize equilibria in a broad class of deterministic equivalent games. Finally, we extend the idea of a static game with a random payoff to a game with multiple stages and we show that every finite stochastic game may be represented as a sequential game with a random payoff. In the numerical study, this theory is applied to a management problem of competition of hospitals for vaccines during a pandemic.",Stability in games with random payoffs,"[78312, 12024, 13820]",958,"[50, 135, 136]",3013,Stochastic optimization - theory and applications,49,13,35,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,44 [building - 303A],"['Game Theory', 'Stochastic Models', 'Stochastic Optimization']",WB-35
"Real security protects and civilises, unites and reconciles, gives security. Fake security subjects and colonises, divides and rules, takes ‘securities’. One example of fake security is where Israel seeks to secure its future by taking and subjecting two properties for itself, the Gaza Marine Gas Field, and the valuable trading route, from Asia to the Red Sea to the Mediterranean.  In doing so, they are endangering the security of 2.4 million Palestinians in the Gaza Strip, 113 million Egyptians nearby, another 300 million people in the Middle East, and the security of future generations of Israelis. Another example is where, to secure for itself the valuable agricultural land of Ukraine, Russia is destroying the security of 37 million Ukrainian people, and the food security of millions in Africa, that are short of food. The United Nations Organisation could bring real security. A United Nations Protection Force [UNPROFOR] could join the United Nations Relief and Works Agency in the Near East [UNRWA], and bring security to the Gaza Strip, and then to Ukraine. Further, the 2 trillion [tn] dollars wasted on colonisers’ military budgets could then be spent by UNPROFOR on further increasing global security, by reducing international crime - maritime 0.2 tn, cybercrime 6 tn, and organised crime 10 tn. Then to deal with global climate change.  Only real climate security projects and actions provide any prospect of protecting future generations from global climate insecurity.","Real and fake security, and the United Nations",[5121],77,"[41, 25, 15]",3014,"Ethics and OR, societal complexity and public service",28,4,20,OR and Ethics,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,45 [building - 116],"['Ethics', 'Decision Analysis', 'Complex Societal Problems']",MC-20
"Battery Electric Vehicles [BEVs] must connect to a charging point to recharge their batteries. They are often plugged in for longer than they need to gain sufficient charge, so there is scope for smart scheduling of charging within the available window. Moreover, as the maximum charging power increases, this flexibility potential increases, but so does its necessity - collectively, charging vehicles can easily overload the capacity of the local electricity grid. 

Aggregators that control the charging of BEVs use smart charging to optimize the aggregate power demand, for example, to trade on electricity markets. However, even in a fully deterministic setting, direct scheduling of large numbers of BEV charging sessions [tens of thousands] becomes a significant computational burden, because the number of variables increases linearly with the number of vehicles. It would be better to directly represent the flexibility of the aggregate power, but calculating the aggregate flexibility envelope is in general an NP-hard problem. 

I will present a flexibility representation [UL-flexibility] that can efficiently calculate the exact aggregate envelope, for the special case where all vehicles remain connected during a specified interval. This representation involves only 2T parameters and can efficiently be used to construct the polytope that constrains the aggregate power consumption of a fleet – independent on the number of BEVs. ",Efficient representations for the scheduling of aggregate charging power of battery electric vehicles,"[78315, 78322]",344,"[21, 36, 37]",3015,Energy transition and operations,21,5,22,Energy Management,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,81 [building - 116],"['Convex Optimization', 'Electricity Markets', 'Energy Policy and Planning']",MD-22
"Hybrid renewable energy systems [HRES] have emerged as an alternative to single-source systems. With the advantage of combining different energy profiles and exploiting the complementarity of sources, HRES have demonstrated to be a good option for increasing the adoption of renewable energy [RE] technologies, ensuring a stable energy supply, and making the system more cost-effective by sharing operations and maintenance activities, electrical infrastructure, and space leasing. Efficiently designing HRES is essential to ensure stable energy production while maintaining cost-effectiveness. When designing an HRES, the goal is to identify the optimal combination of RE sources and determine their respective sizes and types of equipment. Furthermore, the design process must consider the availability of the system and the operational plan with the most effective maintenance strategies for each unique combination of energy sources. This study aims to analyse the cost competitiveness of an HRES, assessing the economic feasibility of - new HRES or transitioning existing installations into HRES with the addition of new RE sources [e.g. complement an existent wind farm with photovoltaic panels]. A cost model is integrated into the optimization approach to estimate lifelong farm costs and maximize an economic indicator. The farm's availability is modelled using a Markov chain. In addition, this study seeks to identify cost-saving opportunities by integrating different energy sources.",Optimizing hybrid renewable energy systems through availability-based design,"[78313, 46329, 23500]",465,"[93, 0]",3016,Multi-energy systems,23,4,19,OR in Energy,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,44 [building - 116],['OR in Energy'],MC-19
"In this study, we developed an agent-based simulation model for hepatitis C virus [HCV] transmission in the context of Punjab, an Indian state with a high prevalence of the disease. The model incorporated a dynamic cohort, infection spread through all major modes and a previously validated discrete-time Markov chain for disease progression. We experimented the impact of treatment policies varying in frequencies and timings of treatment camps for different uptake rates. For modelling treatment, we used the cure rates and costs of sofosbuvir plus velpatasvir, a type of directly-acting antivirals having high cure rates. In low- and middle-income countries like India, where budget and resources for healthcare are greatly constrained, it is of interest to test whether annual camps are avoidable. Over a ten-year intervention period, we tested the following policies - [1] annual treatment camp [2] a single treatment camp at the beginning of the intervention period [3] a single treatment camp at the end of the intervention period [4] two treatment camps at the end of the fifth and tenth years of the intervention period [5] three treatment camps at the end of the third, sixth and tenth years of the intervention period. For all uptake rates, we found that while treating once at the end is the best to maximize life years, quality-adjusted life years and net monetary benefits, yet the annual treatment camp is the best to minimize costs, HCV spread and active HCV prevalence.","Effect of timings and frequencies of HCV treatment camps on epidemiological, health and cost outcomes in Indian Punjab","[78177, 78354]",971,"[3, 56, 131]",3017,Simulation models in healthcare,3,4,17,OR in Health Services [ORAHS],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,40 [building - 116],"['Agent Systems', 'Health Care', 'Simulation']",MC-17
"At the beginning of each semester, all university students need to plan the courses they want to take this semester. In the process, they need to take into account the rules of their degree program. Those might, for example, regulate that some courses are compulsory, that one needs to complete a certain number of credits from a group of courses, or that one needs to choose a study focus from some given options. Also, there are additional considerations, such as the preferences of the student for certain subjects, the times when the courses are scheduled, or that some courses need to be taken in a certain order. In particular, it is a good idea to not just plan the courses of the upcoming semester, but all courses in all future semesters.

This leads to an optimization problem with multiple objectives that include, for example, minimizing the time until graduation and maximizing the satisfaction of the student with the selected courses. We propose an integer programming [IP] formulation for this problem and apply it to the Bachelor and Master degree in Mathematics at the Technical University of Munich [TUM]. In our computational experiments, we could solve most of the tested instances in about one second using the non-commercial IP solver SCIP, and even faster with Gurobi. We made our planning tool available to the students of the Department of Mathematics at TUM. It is publicly accessible at https://studyplanner.co.cit.tum.de.

",StudyPlanner - Helping students to plan university courses with integer programming,"[75270, 78325, 78326]",117,"[142, 109, 26]",3018,Discrete Multiobjective Optimization,34,13,37,Multiobjective Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,33 [building - 306],"['Timetabling', 'Programming, Integer', 'Decision Support Systems']",WB-37
"Warren Powell recently coined the term parametric cost function approximation [CFA] to describe a practical approach for addressing multistage stochastic decision problems in a deterministic manner. Despite its widespread industrial use, Powell notes a lack of scientific rigor in assessing its effectiveness. In this study, we apply CFA to a real-world multistage lot sizing and inventory management challenge encountered in the automotive sector, where stochastic programming techniques such as SDDiP come with weak convergence guarantees and unacceptable run times. We aim to highlight two key decision points in crafting a CFA algorithm and propose scientific methodologies to tackle them. Firstly, we discuss methods for selecting appropriate parameterization techniques. Second, we explore strategies for identifying optimal parameter values, utilizing Monte Carlo simulation and derivative-free optimization methods. Moreover, we emphasize and explain the importance of evaluating not just the expected value but also the risk associated with resulting policies. Finally, we outline practical strategies for deploying CFA algorithms effectively in production settings marked by varying problem instances and a rolling planning window. Through our investigation, we contribute to enhancing the methodological soundness and practical utility of CFA in real-world decision-making contexts.",Navigating parametric cost function approximation - A practitioner's guide for sequential decision-making with integer constraints in a production setting,[78320],831,"[69, 136, 74]",3019,New problems in logistics under uncertainty,49,15,34,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,43 [building - 303A],"['Manufacturing', 'Stochastic Optimization', 'Metaheuristics']",WD-34
"Not everyone follows the same path in life, and this is no different in the case of academia. There are those who focus on one topic from the beginning, those who explore many and eventually define themselves, or those who keep exploring. I believe I am one of the latter. 

This talk intends to give an overview of my research journey, starting back in 2016 when I first faced Operations Research. I will briefly describe the research I have done so far [vehicle routing, machine learning, cooperative game theory, scheduling, etc.], along with the process and achievements. I will also present my ongoing work and give some insight into the direction I aim to take in the future.
","My [short] research journey - past accomplishments, present projects, and future aspirations",[77054],460,"[151, 0]",3020,YW4OR_4,39,15,12,WISDOM - Women in OR,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,13 [building - 116],['Practice of OR'],WD-12
"We propose a hybrid meta-heuristic for generating feasible course timetables for large-scale problems. A novel instance decomposition technique is introduced. Different types of constraint violations are regarded as objective functions to be minimised. The search targets slots where the most violations are identified. After some iterations without improvements, the most challenging constraint groups are given new weights, guiding the search towards non-dominated solutions, which improve the performance of these objectives, even if the total sum of the violations increases. If this mechanism fails to escape these local optima, a shaking phase is conducted. The decomposition mechanism operates as follows - curricula are iteratively introduced to the problem, and new feasible solutions are found considering the increasing set of lectures. The assignments from each iteration can be modified in subsequent iterations. We tested real-world instances from our university and random sub-divisions. For sub-divisions with 400 curricula, decomposition reduces the solution times to up to 27%. For real-world instances, with 1288 curricula, the reduction is 18%. Clustering curricula with common characteristics when incrementing the instances improved the solution times by 18% more than random increments. Feasible solutions for the real-world instance are found in 21 minutes, whereas the commercial software takes several hours, and can fail to comply with some constraints.",A hybrid meta-heuristic for the generation of feasible large-scale course timetables using instance decomposition,"[78323, 53033, 46526]",300,"[74, 142]",3022,Automated Timetabling,36,2,58,Automated Timetabling,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S07 [building - 101],"['Metaheuristics', 'Timetabling']",MA-58
"This paper applies axiomatic models of decision under ambiguity to the optimal portfolio choice problem with two objectives - firstly to account for the probabilistic model uncertainty, secondly to take into account the information contained in an observed sample of assets' returns.
Especially, a criterion is proposed which is a special case of the Variational Preferences. Restated in the Bayesian statistics notations, it is a regularized optimal problem which explores possible probabilistic models weighted by their likelihood given the observations. Examples and numerical simulations are provided to illustrate its properties, notably its robustness to misspecified models by comparing its performance to the benchmark portfolios of the literature.
More generally, this paper aims to show that axiomatic models which have been developed to accommodate attitudes toward ambiguity provide a behavioral justification for aversion to model uncertainty in optimization problems, and lead to practical criteria which take into account the sample information.",Decision criteria under ambiguity for robust portfolio choice given a sample of assets’ returns,[76352],71,"[27, 45, 47]",3023,Robust decisions in finance and investments,74,7,57,Modern Decision Making in Finance and Insurance,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S06 [building - 101],"['Decision Theory', 'Financial Modelling', 'Forecasting']",TA-57
"The early electric vehicle power batteries have entered the retirement outbreak period in China. Although vehicle manufacturer is responsible for power battery recycling as required by policy, informal recycling channel is disrupting the normal order of recycling market. This work establishes three real situations based recycling modes in closed-loop power battery supply chain, consisting of manufacturer, retailer, cascade utilization enterprise, and informal recycling channel. Using Stackelberg game model, the optimal recycling strategies of closed-loop power battery supply chain under decentralized and centralized decision-making are obtained based on backward induction. Then, fair profit distribution schemes are designed for the three models under centralized decision-making. By validation through computational simulation, the results show that - [1] Under the decentralized decision-making, the profits of retailers and cascade utilization enterprises are severely squeezed by the informal recycling channel; [2] Under the centralized decision-making, informal recycling channel erodes retailers’ profit by eroding theirs recycling volume, while the manufacturers’ profit is not affected by the change in supply chain structure. This work enriches the theoretical literature by considering the informal recycling channel in the closed-loop power battery supply chain, and provides guidance to the formal recycling enterprises to compete with the informal recycling channels.",Optimal recycling strategies of closed-loop power battery supply chain considering informal recycling channels,"[36148, 78327]",926,"[138, 50, 125]",3025,Recycling,18,13,23,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,82 [building - 116],"['Supply Chain Management', 'Game Theory', 'Reverse Logistics / Remanufacturing']",WB-23
"In a global context where competition authorities are investigating and sanctioning Amazon marketplace for practices of self-preferencing at the expense of their business users and consumers, we observe a trend of imposing remedies on dominant players in digital markets.
Therefore, competition authorities and regulators need tools to audit the compliance of these dominant players in the e-commerce sector over the obligations and remedies they are imposing on dynamic, and personalized algorithms. Most of these algorithms embed Machine-Learning components, introducing opacity and potentially biases in the decision-making process.

The aim of our presentation is to explore the benefits of using black-box auditing techniques and counterfactual explanations to provide insights into the behavior of these online algorithms. We anchor our research in the literature of product preeminence from vertically integrated players, of choice ranking, and of the specific literature related to Amazon search ranking, automatic pricing and Buy Box's algorithms [[1];[2]].
Through a longitudinal study of the ranking of several thousand products on Amazon, we will illustrate the potential of surrogate models and the decision-support elements they might provide.

[1] Chen et al An Empirical Analysis of Algorithmic Pricing on Amazon Marketplace 2016
[2] Gómez-Losada et al Automatic Eligibility of Sellers in an Online Marketplace - A Case Study of Amazon Algorithm 2022",Contributions of surrogate models and counterfactual explanations to marketplace blackbox audits,"[47259, 78346, 78582]",638,"[66, 32, 124]",3028,Counterfactual Analysis Across Diverse Domains,15,7,27,Mathematical Optimization for XAI,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,047 [building - 208],"['Machine Learning', 'E-Commerce', 'Revenue Management and Pricing']",TA-27
"Integrating mass transit with demand-responsive service is challenging due to joint optimization of bus routes and multimodal customer trips. State-of-the-art mixed-integer-linear programming [MILP] approaches can solve the problem exactly for less than 10 requests [Posada et al. 2017]. This is due to the cumbersome modeling of partial routes of the mass transit network, where the number of arcs expands rapidly with the network size. Besides, using electric vehicles in this problem leads to additional complexity because of charging scheduling and capacitated charging station constraints. This study proposes a novel MILP formulation of an electric integrated dial-a-ride problem with multiple depots and capacitated recharging stations to minimize overall system costs considering transfer synchronization and customer rejection. To address the abovementioned issues, we use time-dependent shortest paths on transit networks to substantially reduce redundant decision variables. With a four-hour computational time limit, we test the model up to 20 requests with different initial battery levels of vehicles. Results show that this model can solve the problem optimally around 95% faster and to a larger problem size if compared to Posada et al. [2017]. A more compact arc-based formulation is developed concerning capacitated charging stations. The numerical results can reduce up to two-digit computation time compared to the state-of-the-art arc-based method.","The electric dial-a-ride problem with capacitated charging stations, multiple depots, and customer rejection","[69549, 40073, 36484]",352,"[119, 145, 14]",3029,Advancing mobility towards sustainable solutions I,6,9,56,Transportation,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S04 [building - 101],"['Public Local Transportation Systems', 'Vehicle Routing', 'Combinatorial Optimization']",TC-56
"The YOLO series of neural networks is widely recognized within the domain of target detection due to its robust feature extraction capabilities, streamlined network architecture, and efficient detection speed, thus rendering it a cornerstone in contemporary research and practical applications. Ensemble learning, a technique that combines multiple learning algorithms, is employed in the literature to increase predictive accuracy. In this study, ensemble learning is judiciously employed with state-of-the-art object detection algorithms, including YOLOv5, YOLOv7, YOLOv8 and MASK R-CNN. The overarching objective is to refine welding detection outcomes by leveraging an original post-welding dataset comprising images captured from seed-throwing machinery utilized in agricultural contexts. The principal aim is to discern any potential faults within the welding processes applied to the seeding legs of these instruments. The findings of this investigation underscore the superior efficacy of the proposed ensemble approach are compared with individual methodologies documented in extant literature. Although the YOLOv5 model manifests the highest individual performance metrics, the ensemble learning paradigm transcends these benchmarks, culminating in the most promising outcomes relative to the established performance criteria.",Ensemble of YOLO Networks for Welding Detection,"[62593, 78384]",350,"[66, 8, 42]",3030,Hybrid Appraches in Deep Learning and Machine Learning,71,5,04,Recent Advancements in AI ,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,1001 [building - 202],"['Machine Learning', 'Artificial Intelligence', 'Expert Systems and Neural Networks']",MD-04
"It is a well-known result in static oligopoly theory that the Bertrand equilibrium [simultaneous price competition] is more efficient than the Cournot equilibrium [simultaneous quantity competition] - consumers surplus and social welfare are higher in the former than in the latter. In this paper, we reconsider the validity of this efficiency result in a continuous-time duopoly game where production requires exploitation of a common-pool resource. The paper closest to ours is Colombo and Labrecciosa [JET 2015, CL henceforth]. As in CL, we assume that the extracted resource is used in the production of horizontally differentiated products a la Singh and Vives [RAND 1984]. Unlike CL, we assume that the resource is nonrenewable, that the marginal extraction cost is decreasing in the resource stock, and that the goods can be either substitutes or complements. A key assumption in our nonrenewable resource model is that there exists economic rather than physical exhaustion. The equilibrium concept we use is Markov Perfect Equilibrium. We are interested in equilibrium harvesting rates, consumers surplus, profits, and social welfare in the two different market structures. In contrast with static oligopoly literature, we show analytically that price competition does not necessarily lead to larger output, lower profits, larger consumers surplus and social welfare.",Oligopoly Exploitation of Common-Pool Nonrenewable Resources - Price vs Quantity Competition,"[78302, 78300]",620,"[33, 50, 40]",3032,"Game Theory, Solutions and Structures V",88,7,36,"Game Theory, Solutions and Structures","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,32 [building - 306],"['Economic Modeling', 'Game Theory', 'Environmental Management']",TA-36
"Airline passengers usually have their seats selected or assigned before arriving at the gate in the airport. We investigate the impact of the passenger-seat assignment on the boarding completion time, especially when considering a central decision-making process that takes place when passengers are already in line. Different seat assignments may result in different boarding completion times, which directly influence the turn-around time of an airplane. Identifying seat assignments that minimize boarding time can therefore provide airlines with significant cost savings. We introduce the problem of assigning passengers to seats while minimizing boarding time in the context of combinatorial optimization, study the computational complexity of the problem and develop exact, approximation, and heuristic algorithms. In addition to a theoretical analysis, we compare these algorithms in a computational study. Furthermore, we investigate an online variant of assigning passengers to seats—proposed in Jaehn and Neumann’s section on future research—and present results on the competitive ratio.",Minimizing the Airplane Boarding Time by Passenger-Seat Assignments ,[49095],256,"[14, 4, 16]",3034,Airplane Boarding,85,8,54,Public Transport Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S01 [building - 101],"['Combinatorial Optimization', 'Airline Applications', 'Complexity and Approximation']",TB-54
"In Operations Research and specifically for the high-resolution Earth Observation Satellite [EOS] scheduling problem, setwise preferences introduce both a robustness to the automation of decisions, but also a complexity that significantly expands the preference space making the elicitation and utilization of these preferences a formidable challenge. In EOS scheduling, which is crucial for disaster management, sustainability, agriculture, and security, large areas of interest are often subdivided into smaller image strips, where the value of obtaining adjacent image strips in close temporal proximity is significantly increased due to temporal dependencies between images, and consequently the value of obtaining a particular set is of higher value than just the individual images. This study introduces a novel representative approach designed to streamline the elicitation and integration process of setwise preferences by interpolating outranking results from a sampling technique. The adoption of this approach addresses the challenges posed by the large preference space of setwise preferences, offering a pragmatic approach to incorporating these critical considerations into fully automated decision-making systems. Consequently, this study contributes to the broader field of OR by providing a viable solution to the intricate problem of setwise preference elicitation and integration.",Setwise Preferences - A Case Study in High-Resolution EO Satellite Scheduling,[76380],888,"[77, 129, 10]",3035,Preference Learning 2,44,3,44,Multiple Criteria Decision Analysis,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,20 [building - 324],"['Multi-Objective Decision Making', 'Scheduling', 'Behavioural OR']",MB-44
"As global efforts intensify to meet the targets of the Paris Agreement, the necessity to reduce emissions from the power sector becomes increasingly evident. This study explores the role of different power generation technologies in achieving emission reduction targets. For this purpose, the European Model for Power System Investments with Renewable Energy [EMPIRE] is used to optimize power generation pathways across Europe by 2050. The primary focus is on understanding the impact of CCS technology, exploring the balance between different generator types [renewables, biomass, nuclear, and fossil-based generators] and their collective contribution to decarbonizing the power sector. Notably, the study considers high capture rate CCS technology to reveal its potential to reshape the energy mix. The research also develops various pathways associated with CCS and without CCS technology, offering a comprehensive understanding of how these pathways contribute to the decarbonized power system. Moreover, the study  illuminates the impact of these pathways on electricity prices and provides valuable insights into potential changes. Our findings emphasize the crucial role played by CCS in transitioning towards a green power sector and shaping sustainable power generation pathways. In addition to offering practical insights for policymakers, the study illustrates the dynamics of the power system transition and proposes a roadmap for a more sustainable and resilient energy future.",Sustainable power generation pathways - EMPIRE model insights for European countries,"[78267, 78336, 57403, 78335, 10356]",843,"[93, 135, 84]",3036,Towards sustainable development,23,12,19,OR in Energy,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,44 [building - 116],"['OR in Energy', 'Stochastic Models', 'Optimization Modeling']",WA-19
"This work focuses on the aluminium production scheduling problem of a leading European Copper and Aluminium industry. The problem forms an interesting variant of flexible flow-shop scheduling consisting of two successive stages, the preheating and the hot rolling of aluminium slabs i.e., - A set of slabs arrives over time and [in the preheating stage] is assigned to a set of parallel furnaces. Once preheating is completed, each slab is unloaded and immediately transferred to a hot rolling mill.
Each furnace operates in a FIFO manner under slabs' eligibilities and capacity constraints, while the hot rolling mill can handle a single slab every time instant, under predefined quality conditions regarding the minimum and maximum number of slabs per production cycle. Each slab is accompanied with preheating and rolling times and is subject to quality restrictions determined by the sequence of slabs in the same production cycle. Slabs also require specific preheating operations and can only be preheated with slabs of compatible operations in the same furnace. We propose an integrated optimisation framework, comprised by an exact method for each production cycle, which combines a MILP and CP formulation for both stages, and a sequential heuristic approach for planning over multiple cycles. We provide experimentation on real instances, showing the versatility of our approach in terms of supporting different planning approaches thus maximising quality of production and machines usage",Integrated optimisation for aluminium rolling ,"[68903, 72180, 77776, 23864, 79351, 68992]",836,"[69, 111, 129]",3038,Lot-sizing with industrial applications II,32,5,49,"Lot Sizing, Lot Scheduling and Production Planning","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,M1 [building - 101],"['Manufacturing', 'Programming, Mixed-Integer', 'Scheduling']",MD-49
"Antimicrobial resistance [AMR] presents a serious threat to public health by undermining the efficacy of antibiotic treatments, leading to prolonged illness and heightened healthcare costs. Nearly 5 million deaths occur annually due to drug-resistant infections, with the European Union recording over 33,000 fatalities yearly, coupled with an economic impact exceeding €1.5 billion annually in healthcare expenses and productivity loss. The widespread misuse of antibiotics across various sectors, including human medicine, animal husbandry, and agriculture, coupled with inadequate infection prevention and control measures, fuels the proliferation of resistant bacteria, driving the AMR crisis. Addressing AMR in bacteria requires concerted actions to promote judicious antibiotic usage, enhance infection prevention and control protocols, develop new antibiotics, and bolster surveillance systems to monitor resistance trends effectively.
Utilizing molecular detection techniques, this study employs a microfluidic lab-on-a-chip device with DNA and bacteriophage proteins as bio-recognition molecules. The device generates unique signals for different fluid compositions. The aim is to develop a machine learning model capable of analyzing these signals, thereby aiding in the detection and monitoring of bacteria and AMR genes during infections. Ultimately, this will improve antibiotic prescribing decisions in both human and animal healthcare.",AI-powered lab-on-a-chip platform for rapid bacterial detection,"[72308, 46329, 23500]",381,"[8, 7, 17]",3039,Advancements in AI and Genomics - Bridging Technology and Biology for Future Healthcare Solutions,2,10,20,"Computational Biology, Bioinformatics and Medicine","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,45 [building - 116],"['Artificial Intelligence', 'Analytics and Data Science', 'Computational Biology, Bioinformatics and Medicine']",TD-20
"Renewable energy contributes to reducing carbon emissions and attaining the general goal of energy sustainability, security and affordability in a balanced way. However, there exist complex causal relationships between renewable energy development, carbon emissions and economic growth, which makes it extremely difficult to reach consensus on key actions to address climate change across the globe. For example, in recent Climate Change Conferences, one of the key decision problems was how to mobilize climate finance and investment to renewable energy from developed countries in support of climate actions in developing countries.
This work aims to develop data analytics to support the impact evaluation and strategy development of renewable energy, which helps to strike an informed balance between carbon reduction and economic growth in both developed and developing countries. The causal relationships between traditional and renewable energy consumptions, carbon emissions and economic growth in the world’s major economies and carbon emitters are analysed both analytically and visually. The analytical results can be used to support data-driven decision making on net-zero strategies.
",Data analytics for the impact of renewable energy impact on decoupling economic growth from carbon emissions,"[78324, 73895, 50319]",605,"[37, 7, 25]",3040,Renewable Energy Challenges,21,12,22,Energy Management,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,81 [building - 116],"['Energy Policy and Planning', 'Analytics and Data Science', 'Decision Analysis']",WA-22
"The Quality of Working Life assesses the quality of the relationship between employees and their work environment, reflecting the well-being of employees concerning their professional lives. The metaverse, as a new form and location of work, is poised to alter the work environment significantly. Considering these developments, examining individuals’ intentions to work in the metaverse is crucial. This study aims to analyze the intention to work in the metaverse and its effects on the Quality of Working Life. A designed questionnaire will be used to gather empirical data to test the hypotheses. The analysis will use Partial Least Squares Structural Equation Modeling [PLS-SEM] to model and verify relationships between variables. The results are intended to provide insights into employees’ intentions to work in the metaverse against the context of Quality of Working Life. Anticipated influential factors include work-life balance and psychosocial well-being in the metaverse. The PLS-SEM analysis should validate our theoretical model and facilitate the quantification of relationships. This research provides theoretical and practical implications, e.g., for designing digital work environments, and lays the foundation for future studies in this area.",The intention to work in the metaverse concerning the quality of working life - A PLS-SEM approach,[78328],127,"[10, 57, 64]",3041,Behavioural OR meets Information systems,13,9,07,Behavioural OR,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,1019 [building - 202],"['Behavioural OR', 'Human Resources Management', 'Location']",TC-07
"We consider optimal trading strategies in a financial market in which   stock returns depend on a hidden
Gaussian mean reverting drift process.  Investors obtain information on that drift  by observing stock returns. Moreover, expert opinions in the form
of signals about the current state of the drift arriving at fixed and known dates are included in the analysis.		

Drift estimates are based on Kalman filter techniques. They are used to transform a power utility maximization problem under partial information into an optimization problem under full information where the state variable is the filter of the drift. The dynamic programming equation for this problem is studied and closed-form solutions for the value function and the optimal trading strategy of an investor are derived. They allow to quantify the monetary value of information delivered by the expert opinions.

We also study diffusion approximations of the filter processes for high-frequency discrete-time experts. They allow to simplify the problem and to derive more explicit solutions. Finally, we illustrate our theoretical findings by results of numerical experiments.

The talk is based on joint work with  A. Gabih, H. Kondakji, J. Sass and  D. Westphal.",Expert Opinions and Power Utility  Maximization in a Market with Partially Observable  Gaussian Drift,[78145],126,"[83, 136, 45]",3042,"Dynamic portfolio selection - stochastic optimization, filtering, and learning techniques",74,4,57,Modern Decision Making in Finance and Insurance,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S06 [building - 101],"['Optimization in Financial Mathematics', 'Stochastic Optimization', 'Financial Modelling']",MC-57
"The Job Shop Problem [JSP] involves scheduling sequences of operations [jobs] on machines by minimizing the makespan [1]. Our novel approach tackles the JSP as a sequence of decisions, nicely represented by a branch-decision tree. At each decision node, we explore all the job selections for scheduling the next operation.
We use an encoder-decoder architecture [2], where the Graph encoder creates embeddings for operations from the disjunctive graph of JSP instances, and the decoder generates solutions by producing at each decision node a probability of picking each job from these embeddings. Given the high cost and expertise needed for applying Supervised and Reinforcement Learning, we propose a simpler Self-Labeling training strategy, which constructs multiple solutions and uses the best one as a self-generated label to refine the model.
Our model outperforms Constructive Heuristics [1] and Reinforcement Learning approaches [3], with up to 20% reduction in optimality gaps, and also surpasses a disjunctive Mathematical Programming model on benchmark instances.

[1] M. Pinedo, Scheduling - Theory, Algorithms, and Systems, New York Springer, 2016.
[2] O. Vinyals et al. Pointer Networks, Advances in neural information processing systems, 2015.
[3] P. Tassel et al An End-to-End Reinforcement Learning Approach for Job-Shop Scheduling Problems Based ",Self-Labeling the Job Shop Scheduling Problem,"[77483, 2126]",931,"[129, 8, 18]",3043,Job shop scheduling,35,13,60,Project Management and Scheduling,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S09 [building - 101],"['Scheduling', 'Artificial Intelligence', 'Computer Science/Applications']",WB-60
"Supply chains are portrayals of human behavior. Yet, traditional decision-support tools focus on economic goals, neglecting behavioral preferences such as fairness. Although fairness models are mainly grounded in equity or reciprocity concerns, recent efforts to model stakeholders’ preferences within supply chains only consider the former. Given the inherent unfairness in supply chain systems, a move towards the accuracy of decision-support tools would be the modeling of reciprocity. However, based on two renowned models, a steppingstone was made by finding a coordination mechanism between a supplier and a retailer for a wholesale-retail price contract, in which two main concerns are raised - [1] the equity-based model tends to invariably set the disadvantage unto the supplier; [2] the reciprocity-based model fails to provide fully reciprocal outcomes. While these insights remain valid when profit requirements are jointly unattainable, we argue that these dynamics must be explored across a wider spectrum of parameters. To bridge this gap, a numerical analysis of these models is conducted and its implications for supply chains and its members are discussed. It is crucial to highlight that [1] the equity-based coordination ensures stability in the channel’s profit when the retailer increases his demand for profit while; [2] the reciprocity-based model assures equal profits for both members; and [3] the behavior of both models moves closer as the production costs increase.",Dynamics of equity and reciprocity in wholesale-retail price coordination equilibrium,"[67178, 78593, 48740, 78606, 78607]",576,"[10, 0]",3044,Behavioural operations and games ,13,13,07,Behavioural OR,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1019 [building - 202],['Behavioural OR'],WB-07
"Facility Location problems have been studied intensively due to their wide range of practical applications in various sectors such as Humanitarian Aid Logistics, Healthcare, Public Transport. In general, the decisions involved in Facility Location are inherently strategic, leading to the definition of problems characterised by many, potentially conflicting, objectives. The literature on Multi-Objective Facility Location problems is extensive and growing rapidly, yet there is a lack of contributions that comprehensively illustrate the state of the art. For this purpose, we conducted a thorough systematic literature review by analysing a sample of 193 relevant papers published in international peer-reviewed journals between 2011 and 2023. Indeed, we focused on modelling and algorithmic aspects, as well as on the managerial context underlying the arising problems and yielding to their inherent Multi-Objective nature. This study has identified several research gaps at the modelling and methodological level, but also in terms of prominent potential applications not addressed so far from a managerial and practical perspective. Accordingly, we present some findings from our analysis and propose a research agenda outlining some future research directions in Multi-Objective Facility Location for both researchers and practitioners, and signposting how theoretical advancements could enable provision of services and design of systems that are more inclusive, resilient and robust.",A systematic literature review and research agenda on Multi-Objective Facility Location,"[68650, 23578]",767,"[64, 112]",3045,Location with Multiple Actors,29,9,61,Locational Analysis,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S10 [building - 101],"['Location', 'Programming, Multi-Objective']",TC-61
"When a traffic planner plans the offer of different transport modes, she aims at minimizing simultaneously the total travel time, the total operating cost, and the total CO2 emission. We consider the setting of two cities with a fixed travel demand that can be connected by different transport modes such as a highway for cars, a tram line, and regional trains. The contribution of each mode to the three objective functions depends on the number of passengers using that mode and on the service to be set up to serve them. We are interested in Pareto-optimal modal splits, which cannot be improved in one objective without degrading another objective.

Motivated by this application, we study general tri-criteria problems over a simplex [representing possible modal splits of the total demand], where we assume that all objective functions are separable over the variables. In the case that all objectives are linear, the Pareto-optimal set is simply the convex hull of all Pareto-optimal pure modes, which is a face of the simplex. We study the structure of the solution set for more general objective functions, such as convex differentiable or piecewise constant objective functions in order to develop tailored algorithmic approaches.
",Modal split on a single edge to minimize multiple objectives,"[72792, 1601]",824,"[143, 77]",3047,Transit,85,15,51,Public Transport Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,M5 [building - 101],"['Transportation', 'Multi-Objective Decision Making']",WD-51
"In our work, we use system dynamics to investigate the influence of digital solutions aiming to enhance the regional food supply for the hospitality industry and communal catering. A systems thinking approach is applied to identify feedback structures and underlying interdependencies. Therefore, findings from literature and two model regions were collected. With the help of causal loop diagrams, the results get visualized. They indicate that digital solutions can help to overcome key obstacles in such systems, namely insufficient logistics solutions as well as a lack of communication and transparency.",Facilitating System Dynamics to Analyze Digital Logistics Platforms for Regional Food Supply,"[77480, 41085]",852,"[140, 138, 65]",3048,Food Supply Chains,78,12,13,Secure & Sustainable Food Supply,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,15 [building - 116],"['System Dynamics and Theory', 'Supply Chain Management', 'Logistics']",WA-13
"In volume maximization problems, the objective is to extract the largest three-dimensional item[s] with variable scaling from a larger container. To achieve this, values for the continuous translation and rotation variables should be determined so that the items can be maximally scaled up, without violating any containment or non-overlapping constraints. Applications of these problems appear in various real-world contexts, such as gem cutting and circuit manufacturing, where finding better solutions is typically highly valuable. While effective heuristic methods have previously been developed to tackle these problems, the attention to exact methods has been rather limited. One of the reasons for this is most likely the complexity involved with mathematically optimizing rotation in three-dimensional space. Angle-based rotation representations such as Euler angles lead to trigonometric expressions, which are challenging for exact solution methods. Therefore, quaternions are better suited for this purpose. Using quaternions, we can formulate the orientation of an object through quadratic expressions. This allows us to mathematically formulate volume maximization problems with convex containers as quadratically constrained programs, based on quaternions, that can be solved exactly. In this presentation, we will introduce volume maximization problems, give some insights into its applications and heuristic methods, and formulate the aforementioned quadratically constrained models.",A quaternion formulation of the volume maximization problem,"[74411, 25830]",622,"[23, 19, 114]",3049,Cutting and Packing 4 - 3D irregular,81,5,07,Cutting and Packing [ESICUP],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,1019 [building - 202],"['Cutting and Packing', 'Continuous Optimization', 'Programming, Quadratic']",MD-07
"We consider the situation where customers make a finite sequence of purchase decisions and buy at most one item at each stage, where the sets of items on offer at each stage are disjoint and could be drawn from one or more categories. Our aim is to optimize item prices to maximize overall expected revenue. The basis of the optimization is a choice model that describes how customers make purchase decisions at each stage. We consider two choice models - a single-stage bundle-purchase model [known as the Multi-variate Multinomial Logit model] and a multi-stage sequential purchase model.

Under the single-stage model, we assume that customers form a choice ‘bundle’ by selecting at most one item from each product set. We model this behavior similarly to the standard multinomial logit model, where customers choose the utility-maximizing bundle. In the multi-stage sequential choice model, customers make a sequence of choices, where the choice made in one stage depends on the choices made in the previous stages. We include product interactions in both models to account for product compatibility.

Similar problems exist in industries such as tourism [travelers visit different destinations], food [restaurants with a rotating selection of specials], retail [stores with rotating inventory], and entertainment [cinemas with rotating films]. Our empirical results on real data suggest that models with greater purchase history and information attain higher levels of accuracy.",Multi-Purchase Choice Models of a Sequential Purchasing Process,"[77544, 10128, 71153]",934,"[124, 71]",3051,"Online, Omnichannel, and Pricing",30,15,61,Retail Operations,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S10 [building - 101],"['Revenue Management and Pricing', 'Marketing']",WD-61
"Warehouse picking is an essential part of assembly production systems. Robotic systems are often utilized to automate this process to increase efficiency and throughput. In this regard, optimizing the picking sequence of the semi-finished products and other components is significant for overall productivity improvement. In our approach, we model the picking process as a Travelling Salesman Problem [TSP], where a robotic agent needs to select the most efficient path to pick multiple different objects from various locations. We investigate different variants of TSP that arise in this setting based on the specificities of the robotic system utilized [e.g., incorporating a dual gripper for picking]. Further, we explore the potential of Graph Neural Networks [GNNs] to handle the inherent complexities of such warehouse picking tasks. Although traditional methods for solving TSP have been extensively examined, GNNs have been recently proposed for solving various combinatorial optimization problems. Our choice for leveraging GNNs in this context arises from their excellence in capturing spatial relationships in graph structured data. ",Utilizing Graph Neural Networks for autonomous picking sequence optimization ,"[78337, 80284, 78343, 23929, 78344]",876,"[14, 69, 66]",3052,Optimization issues on graphs II [Contributed],64,15,29,Combinatorial Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,157 [building - 208],"['Combinatorial Optimization', 'Manufacturing', 'Machine Learning']",WD-29
"At the beginning of my academic career, I was often unable to answer the question What is the use of mathematics? convincingly , and as a mathematician, I felt rather frustrated. Then, out of pure curiosity, I took a course in Operational Research during my Master's degree and found the first concrete applications of mathematics to real-world problems. Today, thanks to my engagement in technology transfer alongside research, I could go on and on about the location of mathematics in everyday life. In particular, the word location is not accidental - in fact, over the last three years, my research activity has focused on Discrete Facility Location, with the aim of improving decision making in complex scenarios resulting from administrative, managerial and operational needs. Actually, the decisions involved in Facility Location are inherently strategic, and this has led us to define problems characterised by many, potentially conflicting objectives, as this talk will detail. Indeed, we have introduced a novel class of Multi-Objective Covering Location problems and an original Multi-Objective Facility Location problem in Waste Management, arising from a case study for the city of Sheffield. In both cases, we derived ad hoc mathematical formulations and obtained accurate representations or approximations of the arising Pareto Sets. We also conducted a systematic literature review which allowed us to highlight the research gaps in the literature on Multi-Objective Facility Location, and to derive a research agenda to suggest concrete guidelines for both researchers and practitioners interested in this area.",How to “locate” Mathematics in everyday life,[68650],460,"[64, 0]",3053,YW4OR_4,39,15,12,WISDOM - Women in OR,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,13 [building - 116],['Location'],WD-12
"We present a bundle method for the unconstrained minimization of nonsmooth difference-of-convex functions, which is based on the calculation of descent-ascent directions. In fact, we define a descent-ascent direction as a direction which is expected to simultaneously provide a descent for the minuend component function, and an increase for the subtrahend component function. The algorithm only requires evaluations of the minuend component function at each iterate-point, and it can be considered as a parsimonious bundle method, as accumulation of information takes place only in case the descent-ascent direction does not provide a sufficient decrease. Hence, bundle-resets take place every time a sufficient decrease in the objective function is achieved. No line search is performed, while proximity control is pursued independent of whether the decrease in the objective function is achieved. Computational performance of the algorithm, which is proved to terminate at a point satisfying a weak criticality condition, are reported relative to a set of benchmark DC instances, ranging from small to large size.",A bundle-type method based on descent-ascent directions for solving nonsmooth DC programs,"[12727, 78348]",357,"[81, 72, 19]",3054,Subgradient-based methods,70,9,41,Nonsmooth Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,97 [building - 306],"['Non-smooth Optimization', 'Mathematical Programming', 'Continuous Optimization']",TC-41
"ERPsim is a set of computer-based simulation games for the SAP enterprise resource planning [ERP] system in which participants use the system to manage their virtual company in a competitive market. While many students and businesspeople have come to use ERPsim games in learning about the SAP ERP system, little is known about how useful and how easy to use reports available in the system are during ERPsim games. This study attempts to evaluate the usefulness and ease of use of reports and their effects on transactional decisions during ERPsim games, using data from a survey of college students who used the Distribution ERPsim game in a course on ERP systems. Previous research shows that the usefulness and ease of use of information are crucial factors that determine the effectiveness of decision-making. The results of this study would be helpful to those who use ERPsim games as well as those who design ERPsim games and further the SAP ERP system in which ERPsim games run. The results are based upon the perceptions of users in a simulated setting; however, they could provide valuable insights into the effects of usefulness and ease of use of reports on transactional decisions.",Exploring the effects of usefulness and ease of use of reports on transactional decisions - a case of ERPsim game,[67119],713,"[39, 131, 34]",3055,Experimental economics and game theory 3,73,15,40,Experimental economics and game theory,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,96 [building - 306],"['Enterprise Resource Planning Systems', 'Simulation', 'Education and Distance Learning']",WD-40
"Robotic arms are extensively used in production environments to undertake different tasks such as welding, hemming, etc. Minimizing energy consumption and the corresponding carbon emissions of such robotic systems poses a growing concern for manufacturing. We consider an enhanced Digital Twin approach that facilitates modelling energy consumption and carbon emissions for each specific task the robot performs. Based on this, we present how Data Envelopment Analysis and Pareto optimality can be utilized to evaluate alternative settings of speed and acceleration for point-to-point movements under given trajectories with respect to energy consumption and the time required to perform each task. Then, we utilize Integer Programming to optimize its operation for the whole production cycle accordingly, and discuss how this approach can be applied to operational environments with multiple robots that need to coordinate.",Optimizing energy consumption of robotic arm movements based on an enhanced Digital Twin approach,"[23929, 78337, 31329]",548,"[59, 109, 69]",3056,"Digitization in Knolwedge, Technology, and Innovation",54,14,08,"Knowledge, Technology, and Innovation","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,1020 [building - 202],"['Industrial Optimization', 'Programming, Integer', 'Manufacturing']",WC-08
"Over the last two decades, strategizing developed from an exclusive endeavour towards a more decentralized approach that aims to empower stakeholders in strategic decision-making. This new stream in strategizing is named ‘open strategy’ and transforms the traditional exclusive strategic decision-making into a more inclusive and transparent process. Although open strategy is beneficial, e.g., increased diversity or quality of ideas, it also brings new challenges and dilemmas that as of now are unresolved by contemporary open strategy tools. We study open strategy processes in the Dutch energy, road, and drinking water context. From a literature study, we found that operational research [OR] methods can contribute to open strategizing, by resolving some of its dilemmas. Therefore, we apply participatory OR methods to support inter-organizational stakeholders in making strategically aligned decisions to tackle challenges that transcend organisational boundaries. We present findings from an ongoing study that aims to understand the behavioural consequences of OR-supported open strategy processes. We approach this study by investigating model-supported interactions on an individual level, how individuals add, remove, and change model elements, and, on a group level, how these interactions shape the path towards a group decision. Furthermore, we investigate the use of the outcomes from the intervention in the broader multi-organisational context, with evaluative interviews.",Behavioural consequences of supporting open strategizing with OR methods,"[76033, 75744, 78624, 23081]",568,"[133, 10]",3058,Behavior in group decision-making ,13,9,11,Behavioural OR,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,12 [building - 116],"['Soft OR', 'Behavioural OR']",TC-11
"In this talk, we consider the evaluation of periodic screening programme for woman breast cancer and formulate the model as a partially observable Markov decision process [POMDP]. We convert a POMDP with finite state, observation state and action spaces to an equivalent completely observable MDP with continuous state and finite action spaces. By this approach, we have an optimal policy from dynamic programming [DP] equation in an equivalent MDP, but we focus on considering the evaluation in several scenarios of periodic screening for participants with silent condition of breast cancer and seeking an answer which programme is better than others for themselves. The aim of our research is, by using the data sets based on cancer registration and estimated parameters of survival rates and other ratios related to screening and diagnoses in Japan, to evaluate the validity of general recommendation with respect to the consideration of human health in scenarios of breast cancer screening programme in POMDP.",An Application to Markov Decision Models in Healthcare Screening,[27107],502,"[135, 56, 25]",3065,Stochastic Modelling,47,5,40,Advances in Stochastic Modelling and Learning Methods,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,96 [building - 306],"['Stochastic Models', 'Health Care', 'Decision Analysis']",MD-40
"The emergence of social media has transcended the constraints of time and space in communication, facilitating instant interaction among individuals. However, the influx of massive information has led to the widespread dissemination of fake news and deep fakes across online social networks. Identifying the source of fake news promptly and accurately has become a focal point of research. To mitigate the escalation of such situations at an early stage, only information from a limited number of observers can be utilized. Hence, this work first optimizes the deployment of observers in large-scale social networks, achieving maximized coverage and sufficient information collection. Moreover, each observer can provide only limited information, including the time of receiving information and their respective location. In this work, multi-source spatiotemporal data from pairs and a series of observers are integrated by the evidential reasoning algorithm, with weight coefficients determined by the structural characteristics of observers. Through simulation experiments conducted on real-world networks and information propagation paths extracted from Twitter, the proposed method accurately traces the propagation source, demonstrated by superior accuracy and error distance compared to typical algorithms. Such advancements can be further applied in other contexts, such as the management and intervention of infectious diseases within epidemic networks.",Evidential reasoning-based inference of early-stage information propagation source in large-scale social networks,"[50319, 73895]",539,"[7, 132]",3067,"Advancements of OR-Analytics in Statistics, Machine Learning and Data Science 13",16,8,06,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,1013 [building - 202],"['Analytics and Data Science', 'Social Networks']",TB-06
"This paper introduces an innovative model to interpret the dynamics of  banking sector, moving away from the traditional reliance on individual firm variables. Instead, it proposes a network approach to capture information from inter-firm interactions.
The model utilizes Principal Component Analysis [PCA] to extract the first principal component, interpreted as a proxy for each firm's weight within the banking sector. A graph is constructed, with the first principal component acting as the weight for links between firms, and the firm with the highest weight serving as the network's center.
Applied to a dataset of banking sector firms, the model effectively captures sector dynamics. Active nodes, those with an indegree greater than zero, are identified and analyzed over time. Results are compared with the S&P 500 index, revealing a strong correlation [above 0.91].
Additionally, a predictive model is implemented on the probability density function [PDF] of the first principal component to analyze sector evolution. Predictive models demonstrate an error of less than 5%, highlighting the model's robustness.
The proposed model boasts several advantages over existing approaches. Firstly, it provides a more comprehensive view of the banking sector by considering inter-firm interactions. Secondly, it identifies key players in the banking sector and their impact on dynamics. Lastly, it offers predictive capabilities for the banking sector's future evolution.",A network-based model for dynamic analysis of the banking sector,[78345],562,"[33, 16, 79]",3068,"Advancements of OR-analytics in statistics, machine learning and data science 15",16,10,06,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,1013 [building - 202],"['Economic Modeling', 'Complexity and Approximation', 'Network Design']",TD-06
"Supply chain finance [SCF] solutions facilitate access to financing for small and medium-sized suppliers. However, in volatile market conditions, payment defaults among supply chain participants can affect the stability of SCF systems. To effectively mitigate default risks, financial service providers often purchase loan credit insurance [LCI] to transfer potential losses to insurers. When retailers cannot repay or default on their payment obligations, insurers will compensate the lender within the policy terms. Using a game-theoretical approach, we assess the value of LCI for the lender and the capital-constrained supplier. Our analysis reveals that the lender can benefit from LCI when the insurance deductible is relatively high, and the insurer's loading factor is relatively low. The capital-constrained supplier with higher capital investment and lower unit production cost can also benefit from LCI. Furthermore, when the insurance scale and loading factor are low, the supplier is more likely to increase profitability. However, the supplier can achieve superior investment efficiency in scenarios with higher insurance scale and loading factor, suggesting increased capital investment to benefit from LCI better. These findings underscore the value that the lender-initiated LCI can bring to capital-constrained suppliers, offering significant insights for small and medium-sized suppliers and financial service providers engaged in SCF practices.",The Value of Lender-Initiated Loan Credit Insurance in Supplier Financing,"[78229, 65247, 77996, 53603]",280,"[50, 138, 44]",3071,Decision making in Insurance and Pensions,74,3,57,Modern Decision Making in Finance and Insurance,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S06 [building - 101],"['Game Theory', 'Supply Chain Management', 'Finance and Banking']",MB-57
"Electricity markets are essential in facilitating transparent energy transactions, optimizing social welfare, and ensuring system stability. However, their complexity and the sophisticated interactions among participants across various markets make detecting and quantifying market manipulation challenging. The transition to market-based redispatch in Germany has ignited widespread debate among academics and policymakers alike, particularly concerning the risk of market manipulation. While existing studies highlight the vulnerability of market-based redispatch to inc-dec gaming by predictive agents, they predominantly utilize static rule-based strategies, overlooking the potential for dynamic adaptability in response to changing market dynamics, regulatory interventions, or flawed market design.

In addressing these limitations, our study leverages the ASSUME framework alongside a Deep Reinforcement Learning algorithm to assess the impact of adaptable agents on the outcomes of market-based redispatch. We hypothesize that the complexity and risks associated with manipulation in a system populated by multiple adaptable agents may deter such activities, thereby safeguarding the market from manipulation. Specifically, our research compares the effectiveness of reinforcement learning agents versus rule-based agents within wholesale and redispatch markets. By doing so, we aim to shed light on the practical implications of adopting market-based redispatch. 
",Adaptable Agents in Redispatch Markets - A Deep Reinforcement Learning Approach to Assessing Market-Based Redispatch and Manipulation Risks,"[72607, 72653, 72684]",326,"[66, 36, 3]",3076,Machine Learning for Electricity Market Applications,22,4,14,Energy Markets,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,16 [building - 116],"['Machine Learning', 'Electricity Markets', 'Agent Systems']",MC-14
"We model the deposit market, where commercial banks compete using deposit rates, and initially distributed among banks depositors, when switching to another bank, bear the switching cost associated with a lack of information and money transfer fee.
At the first stage, banks choose deposit rate simultaneously. At the second stage, depositors, knowing the rates, can switch to another bank if it is profitable for them, after which banks receive money from depositors and put the money in the central bank, thus making profit. At the first stage, each bank sets such a rate so that its profit at the end of the second stage is maximized, considering the rates of competing banks given, but knowing how depositors could redistribute among banks at the second stage. Equilibrium is such a distribution of depositors among banks and such bank rates that no depositor will change his bank and no bank will want to change its rate unilaterally.
In almost all settings, there are asymmetric equilibria, when banks have depositors with different switching costs and set different interest rates, having a kind of competitive fringe, where for example, one bank without depositors sets the central bank rate, or at least two banks do the same due to Bertrand competition between them, when their depositors have only zero switching costs. Other banks with positive switching costs set the lowest interest rates at which their depositors still remain.",Asymmetric equilibria in the deposit market with the switching costs of depositors,"[68437, 78356, 78361]",847,"[50, 45]",3078,Optimal control in organizations,90,5,33,Optimal Control Theory and Applications,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,42 [building - 303A],"['Game Theory', 'Financial Modelling']",MD-33
"The agri-food sector requires evolutions capable of addressing the ever-present climate crises. Innovations in agri-food are increasingly supported by research theories and methods. Universities may represent the local drivers of rural development, providing training and research to orient small and territorial realities to innovation. Collaboration between agri-food and university can help to improve practices and safeguard the environment. First, this study explores the Strategic Plans of Italian universities, examining their objectives via text analysis techniques and clustering words from strategic plans. Second, the research compares the distribution of keyword clusters with the growth in revenues for agri-food companies, the actions conducted to implement the three Missions of universities, and the spread of agri-food startups and spin-offs on the territory. The Kernel Density Estimation [KDE] permits obtaining a non-parametric density estimator to explain multi-modal variables by better following the pattern of the histogram. These findings may emphasise universities’ external relations on the territory to improve the innovation process and to boost partnerships with nearby agri-food firms. Besides methodological instruments and interpretations of intellectual capital for variable selection, the work may introduce novelty to the literature on the collaboration of the agri-food sector to local development.",Mission [Im]Possible - universities' focus on the agri-food sector,"[72107, 72133]",590,"[89, 92, 137]",3079,OR in Agriculture,20,5,12,OR in Agriculture and Forestry ,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,13 [building - 116],"['OR in Agriculture', 'OR in Education', 'Strategic Planning and Management']",MD-12
"Externalities are the costs that a user of a common resource imposes on others. In a stable M/G/1 system, the externalities created by an arriving customer with known service requirement equal the total waiting time that others will save if she would reduce her service requirement to zero. Naturally, these externalities are stochastic, and the corresponding analysis relies heavily on the underlying service discipline. In this talk, we compare the externalities under the last-come first-served with preemption [LCFS-PR] and first-come first-served [FCFS] service distributions. Specifically, we establish a joint decomposition for the externalities under LCFS-PR and FCFS in terms of a bivariate compound Poisson process. This decomposition can be used to derive several other results regarding the externalities - moments, asymptotic approximations as the service requirement goes to infinity, asymptotics of the tail distribution, and a functional central limit theorem. This is joint work with Royi Jacobovic [Tel-Aviv University] and Onno Boxma [TU/e].",Externalities in the M/G/1 queue - LCFS-PR versus FCFS,"[71996, 78357, 29335]",510,"[121, 135]",3080,Advances in Stochastic Modelling and Applied Probability II,47,4,40,Advances in Stochastic Modelling and Learning Methods,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,96 [building - 306],"['Queuing Systems', 'Stochastic Models']",MC-40
"We define a new multicriteria decision-aiding procedure to manage decision-makers' preferences. The preferences are elicited by conjugating the deck of cards method with the ordinal regression approach. The deck-of-cards method allows the DM to express the ranking order of reference alternatives and the intensity of preferences between reference alternatives. An ordinal regression procedure is then used to define a multicriteria value function that represents the ranking of the reference alternatives as well as the preference intensity. We show how this methodology can be usefully applied to different forms of a value function, such as weighted sum, Choquet integral, and additive value function.",The Deck-of-Cards-Based Ordinal Regression,"[26987, 5550, 15067]",116,"[77, 14, 22]",3087,MCDM for project portfolio problems,44,4,44,Multiple Criteria Decision Analysis,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,20 [building - 324],"['Multi-Objective Decision Making', 'Combinatorial Optimization', 'Critical Decision Making']",MC-44
"Carriers are driven by the dual objectives of minimizing delivery costs and maximizing customer satisfaction. Collaboration can reduce cost, while service consistency enhances customer satisfaction. To address these objectives concurrently, we introduce a multi-period collaborative model which we term “consistent collaborative vehicle utilization” framework. In this framework, carriers can borrow trucks from each other, facilitating deliveries to their respective customers, all while adhering to defined consistency measures. In our framework, the borrowed truck first departs from the lender's depot to the borrowing carrier's and then visits delivery locations based on optimal routing decisions. The consistency measures implemented ensure that customers are served exclusively by their designated carrier and at approximately the same time during periods when demand exists. Carriers select delivery times from a set of options specified by customers which is known as time window assignment problem. We formulate the consistent collaborative vehicle utilization problem as an integer programming problem and develop a branch-and-price algorithm to solve it. Additionally, to address larger instances, we propose two heuristics employing column generation technique. Our experiments shows that the consistent collaborative vehicle utilization framework can lead to a remarkable reduction of up to 43% in the number of vehicles, accompanied by a substantial profit increase of up to 15%.",Consistent Collaborative Vehicle Utilization,[74439],524,"[145, 13, 65]",3088,Freight transportation and logistic III,6,10,55,Transportation,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S02 [building - 101],"['Vehicle Routing', 'Column Generation', 'Logistics']",TD-55
"We propose a simulation-based optimization [SO] framework that integrates a novel mathematical formulation for the storage location assignment problem [SLAP] featuring vertical replenishment and a simulation-guided heuristic for manual order picking. Due to the highly-combinatorial nature of the problem, to solve realistic instances, we design a customized iterated local search [ILS] metaheuristic. Specifically, it determines the “top m” promising moves at each iteration of the ILS and then evaluates the corresponding solutions by discrete-event simulation. This allows measuring the impact of location assignments on warehouse productivity, while also accounting for mutual interferences and waiting phenomena arising during the operations carried out by multiple order pickers. The solution bearing the best [estimated] throughput maximization-based objective function of the order picking process is used to update the global optimum. Hence, simulation guides the search and change of the current feasible SLAP solution. Numerical results are presented for a real distribution center under a picker-to-products rack system following an S-shape policy with a skip-and-go rule to deal with lacking items. After a proper tuning of the ILS parameters and once the ILS has assigned the higher-rotating items as close as possible to the loading gate, the SO framework allows to achieve significant improvements on warehouse productivity.",Simulation-based Optimization for Integrating Storage Location Assignment and Manual Order Picking in Warehousing,"[47314, 913, 47266]",635,"[74, 131, 146]",3089,Warehouse Management,30,5,50,Retail Operations,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,M2 [building - 101],"['Metaheuristics', 'Simulation', 'Warehouse Design, Planning, and Control']",MD-50
"In this work, we present a review of global sensitivity analysis using optimal transport. We describe the properties of the method. The interpretation of the results and the decision analysis implications are thoroughly discussed, also in comparison with other methods developed in the management sciences.",Global Sensitivity Analysis with Optimal Transport,[18483],558,"[25, 66, 131]",3090,Behavioral Decision Analysis I,13,3,11,Behavioural OR,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,12 [building - 116],"['Decision Analysis', 'Machine Learning', 'Simulation']",MB-11
"We introduce a novel methodological framework based on additive value-based efficiency analysis. It considers inputs and outputs organized in a hierarchical structure. Such an approach allows us to decompose the problem into manageable pieces and determine the analyzed units’ strengths and weaknesses. We provide robust outcomes by analyzing all feasible weight vectors at different hierarchy levels. The analysis concerns three complementary points of view - distances to the efficient unit, ranks, and pairwise preference relations. For each of them, we determine the exact extreme results and the distribution of probabilistic results. We apply the proposed method to a case study concerning the performance of healthcare systems in sixteen Polish voivodeships [provinces]. We discuss the results based on the entire set of factors [the root of the hierarchy] and three subcategories. They concern health improvement of inhabitants, efficient financial management, and consumer satisfaction. Finally, we show the practical conclusions that can be derived from the hierarchical decomposition of the problem and robustness analysis.",Robust Additive Value-Based Efficiency Analysis with a Hierarchical Structure of Inputs and Outputs,"[67535, 19484]",388,"[24, 35, 73]",3094,Robustness analysis in MCDA  1,44,5,44,Multiple Criteria Decision Analysis,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,20 [building - 324],"['Data Envelopment Analysis', 'Efficiency Analysis', 'Medical Applications']",MD-44
"Generative AI tools powered by Large Language Models [LLMs] have demonstrated advanced capabilities in generating and articulating coherent textual content closer to the practitioners in domains such as law and finance. Globally, firms have raised ethical concerns regarding LLM's ability to mimic human reasoning, accountability for erroneous outcomes, and the security and privacy of confidential data sent by the prompt of Generative AI. This research aims to find a balance between the responsible application of Generative AI and maintaining human oversight over the generated content by utilizing the inherent immutability and decentralization characteristics of blockchain technology. The proposed blockchain-based auditing system detects unauthorized alterations of data repositories containing ex-ante decisions by an AI decision-support system and automated textual explanations by Generative AI tools. The auditing algorithm compares the unique signature, known as Merkle roots of files stored off-chain [outside blockchain], with their immutable blockchain counterpart. Automated auditing by blockchain promotes the ethical use of AI technologies and minimizes the risk of discrepancies in attributing accountability for adverse decisions. A case study on pre-litigation tort liability legal cases is presented to demonstrate the practical application.",Generative AI Governance by Blockchain Technology,"[70640, 76291, 79469]",351,"[8, 66, 26]",3096,New Trends in Generative Adversarial Networks and Deep Neural Networks ,71,4,04,Recent Advancements in AI ,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,1001 [building - 202],"['Artificial Intelligence', 'Machine Learning', 'Decision Support Systems']",MC-04
"The relationship between locations and their connections can be modeled by using some extensions of the vertex cover problem. In this talk, we revisit some of these extensions and provide new ideas that can be used in solution methods. The applicability of our proposals is tested in a branch-and-cut algorithm on several computational experiments, the results of which will also be discussed.",Some results on an extension of the vertex cover problem,"[47185, 1174, 52056, 61790]",225,"[14, 11, 64]",3098,Integer Programming and Combinatorial Optimization - Complexity Questions and Algorithms,64,10,52,Combinatorial Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,8003 [building - 202],"['Combinatorial Optimization', 'Branch and Cut', 'Location']",TD-52
"Rescheduling problems are understood as strategies for updating existing schedules to deal with unexpected changes in the available data. When the change is given by the arrival of a new set of jobs, which are unknown at the time of scheduling, we speak of rescheduling for new orders. 
In these problems, we consider two sets of jobs, a set of old jobs and a set of new jobs, that must be scheduled together on a single machine. For the set of old jobs, we know the original completion times, that is, the times based on the schedule prior to the insertion of the new jobs. According to these times, additional activities may have been planned and resources made available. Thus, the goal of rescheduling is twofold - to minimize some classical objective function over all jobs, and to minimize the disruption of the original schedule.
When the disruption is measured as the total absolute time deviation from the original completion times, the search for optimal solutions raises the problem of determining the insertion of idle time. We provide timing algorithms for solving this problem when the sequence of jobs is known. We show that the proposed algorithms devise the efficient frontier of solutions in polynomial time, and compute an optimal solution in polynomial time, if a disruption threshold is given.",Polynomial-time algorithms for solving timing problems in single-machine rescheduling,"[67198, 37159, 68433]",933,"[129, 0]",3099,Flow shop and single machine scheduling ,35,15,60,Project Management and Scheduling,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S09 [building - 101],['Scheduling'],WD-60
"Facing the global challenge on climate change, how to maintain economic growth with effective carbon emissions reduction, has become an important and urgent issue. However, the related studies only focus on the unilateral optimization of total supply chain carbon emissions reduction, which will lead to uneven distribution of carbon emissions reduction among the supply chain members and seriously undermine the stability of the supply chain. To fill this research gap, this study establishes a trading path of carbon quota inside and outside the supply chain, and constructs a mixed integer linear programming [MILP] model for carbon emissions reduction based on the carbon cap-and-trade policy considering dual fairness. The production, distribution and capacity planning strategies, as well as carbon trading decisions, of all supply chain members are optimized. Using the Nash bargaining method and the ɛ-constraint method, the carbon emissions reduction and profit of each supply chain member can be fairly distributed. A industrial case based numerical example is introduced to demonstrate the applicability and effectiveness of the proposed model and solution method. The results show that Nash bargaining method could lead to a fair distribution of profit and carbon emissions reduction. In addition, when some members have insufficient carbon emissions allowance, the cap-sharing mechanism plays a better role in fair coordination.",Optimization of supply chain planning considering fair carbon emissions reduction and profit distributions,"[78341, 36148]",11,"[138, 94, 77]",3100,Sustainable Supply Chain Management,19,12,24,Sustainable Supply Chains,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,83 [building - 116],"['Supply Chain Management', 'OR in Environment and Climate change', 'Multi-Objective Decision Making']",WA-24
"This work focuses on the integration of Local Branching into Logic-Based Benders Decomposition [LBBD] schemes. Given a MILP formulation for scheduling on unrelated parallel machines, it is noticed that certain k-OPT neighbourhoods could implicitly be explored by regular local search operators. After enumerating such neighbourhoods and obtaining their local optima, a local branching cut [applied as a Benders cut] eliminates all their solutions at once, thus avoiding an overload of the master problem with thousands of Benders cuts. To guarantee convergence to optimality, the constructed neighbourhood should be exhaustively explored - this procedure is accelerated by domination rules or selectively implemented on nodes which are more likely to reduce the optimality gap.
In this study, we implement internal swaps of jobs, i.e. swaps between jobs on the same machine, over the solution of the master problem, to construct formulation-specific 4-OPT neighbourhoods. The experimentation on two challenging scheduling problems [i.e., the minimisation of total completion times and the minimisation of total tardiness on unrelated machines with sequence-dependent and resource-constrained setups] shows considerable reduction of optimality gaps or acceleration of convergence, in comparison with regular Benders cuts. As our approach is easily transferrable to different optimisation problems, it provides a promising prospect to improve the performance of regular LBBD algorithms.",Local Branching techniques to strengthen Logic-Based Benders Decomposition,"[72180, 23864, 68992, 68903]",865,"[109, 129]",3102,Topics in Integer Programming I,64,12,25,Combinatorial Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,011 [building - 208],"['Programming, Integer', 'Scheduling']",WA-25
"This study deals with the planning and programming production problem of a fruit-based beverage company. This is a two-stage lot sizing and scheduling problem [tank and machine stages] with some specific features, such as a production buffer between the stages and a periodic clean-in-place [CIP] in each stage. Companies spend around 20% of the day cleaning line equipment. These features make this problem different of other beverage problems and includes additional difficulties to solve it. 

This problem with these characteristics is difficult to solve for large real instances and has great applicability. Based on a mathematical model analytical methods are proposed, in order to support decision making in a more efficient way in the planning and lot sizing seeking to minimize the costs associated with production.

Computational tests were performed with instances found in the literature, based on real data, and with generated instances. The proposed methods were tested and the results were compared with the best method found in the literature for the problem under study.

Acknowledgments - This work was supported by the São Paulo Research Foundation [FAPESP] under Grants numbers 2023/08977–5, 2022/05803-3; the Brazilian National Council for Scientific and Technological Development [CNPq] under Grant number 315874/2021–0.
",Fruit-based juice production process - a lot-sizing and scheduling problem,"[78363, 78371]",862,"[111, 72]",3104,Applications of combinatorial optimisation in industry and services II,64,8,29,Combinatorial Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,157 [building - 208],"['Programming, Mixed-Integer', 'Mathematical Programming']",TB-29
"Burnout is a syndrome recognized by the World Health Organization as an occupational phenomenon that costs billions of dollars in healthcare each year. Burnout manifests itself through symptoms of emotional exhaustion, psychological distancing, and a lack of personal accomplishment, which have been related to a variety of negative individual, group, and organizational consequences. More recently research has provided empirical evidence that burnout can be contagious. Explanations behind this contagion suggest that the emotions of an employee transfer via direct contact with other members of the group, a simple within-groups diffusion process. However, if this were the only mechanism behind burnout contagion, burnout levels of work group colleagues should be similar, which is not what is seen in real data. 

Given this contradiction, in this research, I build an agent-based model of burnout contagion considering individual-level variables, both ingroup and outgroup interactions, and different mechanisms as possible paths to the diffusion of the syndrome. Model results are then compared to real burnout data collected using an electronic survey from 1800 employees in an organization. Results shed light on the underlying complexity of burnout contagion and give support to claims for a dynamic understanding of the phenomenon. This will mean moving from more traditional and individualistic research approaches to more complex ones that match the nature of the phenomenon.
",A model of burnout contagion in organizational settings,[78364],851,"[3, 10]",3106,Simulation of organizations II,77,3,43,"Agent-based Models in Management, Economic and Organisation Sciences","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,99 [building - 306],"['Agent Systems', 'Behavioural OR']",MB-43
"Agriculture emerges as a major contributor to global anthropogenic emissions, such as waste production and greenhouse gases, and to resource depletion, such as water and land, requiring a transition from a linear to a circular approach. According to the WWF, the agricultural sector consumes about 69% of the planet’s fresh water and the FAO estimates that the livestock sector alone is producing about 14,5% of all greenhouse gas production. Agri-food is among the most relevant manufacturing sectors in Italy. In this context, a large-scale and multi-disciplinary research program has been funded by the National Recovery and Resilience Plan, named AGRITECH, The National Centre for Agricultural Technologies [https://agritechcenter.it], with the aim to promote innovation and technology transfer. A large research area of this program focuses on circular waste management and multi-dimensional sustainability assessment. This study is part of this research area and focuses on orange peel waste [OPW] management processes. It proposes a critical review and selection of the KPIs to assess alternative technologies of OPW management, and a preliminary application of the Analytic Hierarchy Process [AHP] to rank such alternatives. ",A multi-dimensional assessment of circular technologies in agriculture waste management,"[6297, 56982, 78015]",927,"[6, 139, 89]",3109,New technology for sustainable supply chains,18,14,23,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,82 [building - 116],"['Analytic Hierarchy Process', 'Sustainable Development', 'OR in Agriculture']",WC-23
"Static stability is one of the most important packing requirements, preventing items from collapsing and preserving the safety of operators and cargo during loading/unloading operations. This constraint has often been oversimplified in existing online 3D heuristics, limiting their practicability in real-world settings. Given the diverse impacts of different static stability constraints on solution quality, we embedded four constraints, i.e., full-base, partial-base, and two polygon-based constraints, into online heuristics. The impact of constraints on the heuristics' efficiency regarding bin volume utilization is analyzed on a real dataset. Moreover, the static stabilities of packing layouts in all solutions obtained under four constraints are benchmarked using the static mechanical equilibrium [SME]. The SME approach offers a necessary and sufficient condition for cargo stability, yet its application proves to be very time-consuming and impractical in the online context, leading us to use it as a benchmark. The results showed that a high percentage of items [i.e., over 87%] are statically stable under all constraints. The findings also indicate a trade-off between increasing items' stability and increasing the bins' volume utilization. This study offered valuable insights for practitioners in selecting appropriate static stability based on their preferences in prioritizing item stability or efficient bins' volume utilization. ",Evaluating stability and efficiency - insights from incorporating static stability constraints in online 3D packing heuristics,"[72490, 37579, 663]",544,"[14, 23, 143]",3111,Cutting and Packing 3 - 3D loading,81,4,07,Cutting and Packing [ESICUP],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,1019 [building - 202],"['Combinatorial Optimization', 'Cutting and Packing', 'Transportation']",MC-07
"Conventional vehicles are characterized by long and reliable driving ranges and are furthermore supported by a well-established refueling infrastructure. Therefore, vehicle refueling has traditionally been ignored in vehicle routing problems in the literature. However, with the adoption of electric vehicles [EVs] in distribution fleets, a new extension to the vehicle routing literature has emerged. Today, EVs are limited by short driving ranges and, especially for heavy-duty EVs, the infrastructure of charging stations is still sparse. Hence, much effort has been made in the literature to develop complex routing models that integrate routing and refueling decisions for EVs. However, with a sufficient infrastructure of charging stations, vehicle refueling may yet again be ignored, such that solutions for traditional vehicle routing problems are feasible for EVs. 

In this study, we introduce the Charger Location Problem [CLP], which aims to determine charging infrastructure requirements for supporting charger independent EV routing. We present four variations of the CLP based on how frequently recharging is allowed and the number of trips considered. Through a case study in Norway, we demonstrate the significant impact of strategically locating charging stations along highways. Our findings underscore the importance of proactive infrastructure planning in facilitating the transition to independent EV routing, offering insights for policymakers and fleet operators.",Charging Infrastructure Optimization for Independent Electric Vehicle Routing - A Case Study in Norway,"[71933, 27939]",685,"[64, 109, 65]",3112,Charging Infrastructure toward Sustainable Transport,80,7,53,Sustainable and Resilient Systems,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,8007 [building - 202],"['Location', 'Programming, Integer', 'Logistics']",TA-53
"With the rapid development of the aviation industry, airlines of various countries have to store a large amount of aviation materials at airports and maintenance bases in order to ensure the normal operation of flights. However, this has resulted in a backlog of aircraft inventory, which undoubtedly imposes significant depreciation and administrative costs on airlines. In this paper, we address the aircraft material allocation problem faced by airlines under the multi-base and multi-tier aircraft material inventory distribution model, and mathematically abstract the factors affecting the allocation of aircraft material. By constructing a mathematical optimization model with the lowest total avionics management cost of the whole inventory system as the objective function, and the avionics guarantee rate and downtime as the constraints, we aim to optimize the number of avionics allocation of the avionics base as a whole, in order to reduce the amount of avionics inventory, so as to achieve the purpose of reducing the inventory cost. Taking China Eastern Airlines, one of the three major airlines in China, the high-priced turnover parts of aviation materials as the research object, the mathematical simulation model is prepared using Matlab software. It confirms that the two-level inventory avionics management model proposed in this paper has certain advantages and provides a new solution for airlines in finding the balance between avionics protection and avionics management cost.",Research on Optimization of Air Material Joint Allocation under Multi-level Inventory Model,"[78071, 78366, 78365]",310,"[4, 61, 151]",3113,Industrial Optimization,14,2,03,Data Science Meets Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,1005 [building - 202],"['Airline Applications', 'Inventory', 'Practice of OR']",MA-03
"Complex decision-making problems often have specific configurations such as the need to organize criteria hierarchically and the difficulty in setting importance weights to these criteria. These challenges have not been fully addressed yet when it comes to the multicriteria decision-making method VIKOR. Our first proposal is to introduce the  VIKOR-HS method to bridge this gap. This method integrates a variant of the Multiple Criteria Hierarchy Process [MCHP] into VIKOR, facilitating consideration of multiple levels for decision criteria, and incorporates the Stochastic Multicriteria Acceptability Analysis [SMAA] for eliciting criteria weights across various hierarchy levels. Despite this contribution, a challenge that remains concerns the difficulty in applying such a method in real-life decision-making scenarios due to its demand for mathematical and programming knowledge that is often incompatible with that possessed by typical users. Our second contribution addresses this concern by proposing a user-friendly software tailored for implementing the VIKOR-HS method. To demonstrate the efficacy of the VIKOR-HS method and accompanying software, we applied it to assess the sustainability of different animal production alternatives in Brazil, a problem previously explored in the literature. Notably, the software facilitates robustness evaluation for each alternative while substantially reducing the complexity associated with employing a robust multicriteria approach.
",Addressing Challenges in Multicriteria Decision-Making - Introducing the Method and User-Friendly Software Called VIKOR-HS,"[54647, 78375, 78377, 78380, 78379]",308,"[139, 25, 134]",3114,MCDA applications,44,8,44,Multiple Criteria Decision Analysis,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,20 [building - 324],"['Sustainable Development', 'Decision Analysis', 'Software']",TB-44
"The market for refurbished phones provides significant opportunities for efficiently reusing scarce resources and grows rapidly. In such a circular supply chain system, purchasing, refurbishment, and sales are directly affected by a highly dynamic environment with fluctuating prices and demands. To maximize the long-term average profit in the refurbishment system, determination of the optimal purchasing, processing, and sales strategies is required. We propose a linear programming formulation for a Markov Decision Process that considers the arrival of multiple-raw materials and investigate how the sales price and purchasing price variations, the correlation between sales and purchasing prices, and limited capacities for semi-finished and finished goods inventory and for refurbishment affect the considered supply chain for refurbished mobile phones. Our numerical experiments use industry data and lead to critical managerial insights for our industry partner.",Optimal Processing and Pricing Policies in a Mobile Phone Refurbishment System,"[75474, 909]",924,"[138, 124, 59]",3115,Remanufacturing and refurbishing operations,18,10,23,"Circular Economy, Remanufacturing and Recycling ","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,82 [building - 116],"['Supply Chain Management', 'Revenue Management and Pricing', 'Industrial Optimization']",TD-23
"Millions of pounds are involved in the construction of offshore wind turbines. Therefore, there is a need to optimize the whole process to reduce the cost and duration of the final procedure involved in wind energy generation. Specifically, this project focuses on modeling the procedure for the installation of offshore wind turbines as a mixed integer linear programming and the use of exact methods to solve those models. The objective is to minimize the total cost used in the installation procedure. To obtain an optimal solution, we devise a shortest path-like formulation using strong multicommodity flow models of the network. We show that this new formulation is NP-Hard. We propose a polyhedral analysis of the set of solutions by identifying novel problem-specific valid inequalities required to characterize the convex hull of feasible solutions. Also, we design an efficient cutting plane algorithm that identifies the optimal configuration of vessel schedules that minimizes the installation duration and cost. We present preliminary computational results on the problem specific and the literature instances that are related to the problem formulation. Also, computational instances will be tested by real-world data collected from offshore wind farm installations.",Data-driven integer programming models for installation of offshore wind turbines.,"[78368, 66858, 2630]",197,"[14, 11, 72]",3116,Combinatorial Optimization models and applications in Logistics and Transportation I,64,2,29,Combinatorial Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,157 [building - 208],"['Combinatorial Optimization', 'Branch and Cut', 'Mathematical Programming']",MA-29
"The intersection of artificial intelligence  and problem structuring presents new challenges and opportunities. The paper delves into leveraging AI problem structuring via data gathering, pruning, and strategy dialoguing, emphasizing the importance of moving from treating AI as a search tool to fostering meaningful dialogue. Experiments with generative AI based on large language models in strategy creation reveal valuable insights for ideation and evaluation.",AI and problem structuring,[45621],128,"[8, 149]",3118,Impact of AI on Soft OR - A,26,8,13,Soft OR and Problem Structuring Methods,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,15 [building - 116],"['Artificial Intelligence', 'Problem Structuring']",TB-13
"Social network data sets are often published and used for analysis purposes in many application domains. This may raise privacy issues, as simply hiding the identity of the users does not prevent an adversary from tracing back to the real-world entities associated with certain nodes of the network having sufficiently distinguishable features from others. To protect individuals' privacy, networks are commonly anonymized before publication.
In this context, the k-degree anonymization problem, originally introduced by Liu and Terzi in 2008, aims to identify the smallest set of modifications of the edge set of a network needed to make it k-degree anonymous, meaning that each user of the anonymized network will have the same number of connections as at least other k-1 users. In such a definition, different k values reflect different privacy requirements w.r.t. the vertex degree feature.
This work addresses different settings of the problem allowing edge insertions and/or deletions. We compare different families of integer linear programming formulations of the problem on benchmark social network instances and additionally derive some properties and valid inequalities that we exploit in the solution process.",Anonymizing networks via mathematical programming,"[72278, 3351, 22042]",242,"[14, 53, 72]",3120,Exact Algorithms and Formulations for Network Optimization Problems,64,9,29,Combinatorial Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,157 [building - 208],"['Combinatorial Optimization', 'Graphs and Networks', 'Mathematical Programming']",TC-29
"In this talk we describe our work in scheduling the main professional football league of Bolivia. Following existing work in sports scheduling, we develop an integer programming approach for the season schedule in its two tournaments, the first one organized by groups and the second one, a double round robin scheme. In addition to the traditional problem of scheduling games to rounds, we also schedule the games to specific dates and time slots within each round. Also, besides usual considerations such as league format, game broadcasting, and international tournament calendars, an important aspect of this league is the geographical location of the teams, which are spread around regions with extreme differences in altitude. Our work has been implemented in practice to schedule the 2024 season of the league.
This is the fourth soccer league in South America [Chile, Argentina, Ecuador, Bolivia] programmed by the group of authors of this work.
",Scheduling Bolivia’s First Division football season by integer programming ,"[61565, 32010, 14317, 64776]",664,"[99, 151, 129]",3124,Sports scheduling and optimization,37,7,16,OR in Sports,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,19 [building - 116],"['OR in Sports', 'Practice of OR', 'Scheduling']",TA-16
"Eliciting attribute weights and values are two crucial steps in many multi-attribute decision-making [MADM] methods. Due to their high reliance on judgmental inputs from decision-makers, these steps can be subject to various behavioral factors, including time preference. Given the frequent application of MADM methods in many strategic real-life problems, examining time-related behavioral vulnerabilities and providing systematic approaches to overcome them become imperative. This study investigates the time preference phenomenon in complex, multi-attribute decision-making problems. A special experiment is designed following the Multi-Attribute Value Theory [MAVT] steps, and a health-related intertemporal decision problem is used. In a within-subject design, participants' preferences are used to elicit attribute weights and value functions considering different resolution times of a health-related outcome. Data were collected from six European countries via an online platform. The statistical analyses indicate the significant influence of time preference on elicited importance weights and value functions. Subjects assigned different weights to the same attributes and different values to the same outcomes depending on the time of the realization of outcomes. These findings have significant implications for both practitioners and researchers.",Time preference influences attribute weights and values in multi-attribute decision-making,"[75481, 64114, 40259]",557,"[10, 77]",3126,Behavioral Decision Analysis II,13,4,11,Behavioural OR,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,12 [building - 116],"['Behavioural OR', 'Multi-Objective Decision Making']",MC-11
"Global protein consumption is shifting towards plant protein sources due to population increase, resource depletion, requirements for diversifying diets, and the need to reduce environmental impact. This study aims to redesign the European Union [EU] plant protein processing supply chains for the current and the future state of the plant-protein food market. In this study, this network design problem is formulated as a multi-objective mixed integer linear programming [MILP] model, incorporating decentralized processing operations and the re-use of waste streams across various echelons for side stream valorizations and to form closed loops. The Pareto front is obtained for the minimization of environmental impact and maximization of economic gain. Numerical experiments for technology development and replacement of animal protein with plant protein production and consumption demonstrate that the current supply chain framework can be extended to cover the shift towards plant protein sources, but the network structure depends on the processing technology capabilities and available pathways of value recovery. ",Decentralized and circular plant-based protein supply chain networks for the future  ,"[75747, 71700, 10561]",852,"[89, 100, 138]",3127,Food Supply Chains,78,12,13,Secure & Sustainable Food Supply,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,15 [building - 116],"['OR in Agriculture', 'OR in Sustainability', 'Supply Chain Management']",WA-13
"This article presents an empirical investigation into the influence of Environmental, Social, and Governance [ESG] scores on the solvency ratio of insurance companies, employing a panel regression approach. By analysing an extensive sample of worldwide insurance companies, our study aims to provide a clear and detailed perspective on the dynamic relationships between ESG performance and the financial robustness of firms in the insurance sector. The results obtained through panel regression will offer crucial insights into understanding how ESG variables may affect the solvency ratio, contributing to the growing literature on financial stability and social responsibility within the insurance industry. This research endeavours to provide valuable information for industry stakeholders, regulators, and investors interested in understanding the impact of sustainable practices on the solvency level of insurance companies.",Sustainability risks affecting solvency ratios for insurance companies,"[56890, 27575, 78383, 64952]",189,"[139, 126]",3129,Insurance Risk Management,4,3,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S14 [building - 101],"['Sustainable Development', 'Risk Analysis and Management']",MB-63
"The purpose is to advance a data-driven, day-ahead forecasting model for assessing the probability, direction, and scale [load] of electrical congestions within Germany’s complex electrical power grid. Utilizing state-of-the-art machine learning algorithms, the model is specifically designed to operate on an hourly basis, thereby offering timely insights for grid management. The analysis uncovers compelling evidence that key exogenous variables, such as real-time meteorological conditions, electricity supply-demand indicators, and Brent oil price fluctuations, can be harnessed to make highly reliable predictions concerning grid congestion events. Also, seasonal patterns are uncovered. Our contribution has the potential to serve as a useful resource for transmission system operators [TSOs] and policymakers interested in grid management and cost mitigation efforts.",Day-ahead probability forecasting for redispatch 2.0 measures,[78373],512,"[127, 37, 36]",3130,AI for Energy Finance,17,15,31,Analytics,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,54 [building - 208],"['Robust Optimization', 'Energy Policy and Planning', 'Electricity Markets']",WD-31
"In contemporary discourse, the pursuit of sustainability underscores the profound impact of supply chains [SC] on environmental dynamics. Despite efforts to manage direct greenhouse gas emissions, the less tangible indirect emissions require a comprehensive approach to emissions management. Industrial symbiosis [IS] in a context of  Supply Chain Management that results in Symbiotic Supply Chains [SymSC] is a possible solution that yet remains underexplored. 
This work proposes a novel definition of Symbiotic Supply Chains [SymSC], building upon existing definitions and frameworks. It emphasizes the pivotal role of collaboration in the share of flows under the nexus of material-energy-water in a network involving multiple SCs. Given their complexity, SymSCs require detailed planning and management which can be facilitated with the integration of Operations Research techniques.
We provide the basis for modeling guidelines to optimize SymSC decisions, which build on the existing IS models available in the literature. This study contributes to the understanding of symbiotic relationships across SCs, focusing on key features characterizing optimization models in the context of IS.
Recognizing that SymSCs hold promise in advancing the sustainability agenda, a roadmap for future research and decision-making tools to enhance sustainability performance in the context of SymSC is provided.
",Driving Sustainability through Symbiotic Supply Chains - An Operations Research Modeling Framework,"[62187, 65696, 65869, 47502, 25340, 48740]",546,"[138, 100, 139]",3131,Sustainable Operations,19,5,24,Sustainable Supply Chains,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,83 [building - 116],"['Supply Chain Management', 'OR in Sustainability', 'Sustainable Development']",MD-24
"Outlier detection in functional data presents a critical challenge in contemporary data science research. The inherent complexity of these datasets implies working with an infinite dimensional space. Addressing this challenge demands innovative methodologies that yield robust and interpretable results.

In this study, we introduce novel iterations of the epigraph and hypograph indexes, different from those available in the existent literature. These indices allow to transform the original functional dataset into a multivariate one, enabling the application of available outlier detection methods for multivariate datasets. The inherent visual nature of the epigraph and the hypograph indices facilitates intuitive interpretation of outcomes, enabling users to extract insights into identified outliers. 

This methodology is validated through comprehensive experimentation on both simulated and real datasets, demonstrating competitive results compared to existing methodologies available in the literature.
",Visualizing outliers in functional data with new versions of the epigraph and the hypograph indices,"[70998, 71027, 63237]",123,"[7, 0]",3132,Unraveling the Black Box - Advances in Model Explainability,15,13,27,Mathematical Optimization for XAI,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,047 [building - 208],['Analytics and Data Science'],WB-27
"This paper studies a continuous-review inventory replenishment model with a limited storage capacity S in an uncertain environment. We assume that the demands and returns follow independent Poisson processes. We further assume a random shelf life, a random lead time, and early loss. The storage is managed according to the base-stock [S,s] policy for s0. In case of overstock, each returned item exceeding S is transferred to a foreign facility. If during the lead time a demand reaches zero stock, we consider two alternatives - either allow partial backordering up to L_{B} items, beyond which the unsatisfied demand is lost, or call for an immediate and costly emergency supply up to level 0",An emergency supply policy for an inventory replenishment model with returns and partial backorders,"[47521, 78382]",918,"[61, 135, 151]",3133,Managing product returns,18,3,23,"Circular Economy, Remanufacturing and Recycling ","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,82 [building - 116],"['Inventory', 'Stochastic Models', 'Practice of OR']",MB-23
"Cross-border effects play a significant role in shaping spot market prices. Yet, a thorough meta-study synthesizing their influence and comparing across different European markets remains a gap in relevant literature. Further, a comparison of the relevant literature on the basis of the methodology followed in each study can prove useful.
This meta-study will therefore investigate the relevant existing literature for papers that have established a relation between a cross-border effect and the spot market prices. A number of key cross-border effects of particular interest are first identified, before compiling a comprehensive list of relevant papers that include those effects. Subsequently, the study will rigorously compare the selected papers on a methodological basis. Here the study will classify the papers for modelling technique used, the time period on which influences are defined [short and long-term] and the market under study. These methodologies will then be discussed.
Finally, the conclusions of different papers looking at a particular effect are compared, to assess whether the difference in results drawn in the papers might result from the modelling technique and the methodologies followed in that paper, or was rather due to some other factors. This meta-analysis will thus provide a valuable reference tool for future studies that seek insight into methods to analyze or forecast spot market prices while including cross-border effects.
",Assessing the Impact of Cross-Border effects on Day-Ahead Market Electricity Prices – a Meta-study,[77409],244,"[36, 33]",3134,Modelling European market coupling ,22,5,14,Energy Markets,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,16 [building - 116],"['Electricity Markets', 'Economic Modeling']",MD-14
"In social network analysis, the size of the k-core, representing the largest subgraph with minimum degree k, is often used to measure community cohesion. The Collapsed k-Core Problem aims to identify a subset of crucial users, whose removal results in the smallest k-core. We formulate this problem using mathematical programming for the first time. One approach employs an Integer Linear formulation, modeling deletion rounds. Another approach presents two bilevel programs, differing in their formulation of the k-core identification. We reformulate the first bilevel model as a single-level sparse model using a decomposition method. For the second model, we provide a linear formulation for k-core identification and dualize it, resulting in a compact single-level Mixed-Integer Nonlinear problem. We also outline preprocessing steps and valid inequalities for all formulations. Performance comparison is conducted against a state-of-the-art solver on benchmark instances.",Assessing Network Cohesion Using the Collapsed k-Core,"[72283, 66163, 72278, 23193, 22042]",225,"[14, 53, 132]",3136,Integer Programming and Combinatorial Optimization - Complexity Questions and Algorithms,64,10,52,Combinatorial Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,8003 [building - 202],"['Combinatorial Optimization', 'Graphs and Networks', 'Social Networks']",TD-52
"With the increasing greenhouse gas emissions, the use of electric vehicles [EVs] in logistics has increased. In contrast to conventional vehicles, EVs use batteries to store energy, and they often stop at stations to recharge. The energy consumed per unit distance traveled depends on several factors including vehicle load, speed, and road gradient.
Traversing an arc with a positive gradient requires more energy compared to an arc on a flat network. This is further amplified when the EV carries a heavy load while moving uphill. Conversely, if the driver presses the brake pedal to maintain a constant speed while moving downhill, it can regain energy through regenerative braking technology. The impact of road gradients and regenerative braking may be significant for companies that carry heavy loads in hilly regions. Therefore, an approach that incorporates both cargo weight and road network terrain may improve the route planning of EVs. With this aim, this study addresses an extension of the Electric Vehicle Routing Problem with Time Windows considering a comprehensive energy consumption function that incorporates the cargo load, road gradient, and regenerative braking.
We formulate the problem as a mixed integer linear program, solve small-size instances with Gurobi, and develop a Large Neighborhood Search to tackle the large-size instances. Numerical results show that considering the road gradient along with the cargo weight can significantly influence the routing decisions.",Improving Route Planning of Electric Vehicles by Considering Road Gradients and Regenerative Braking,"[78378, 50804, 58102, 2423]",864,"[145, 72, 65]",3139,Sustainability in Distribution and Transportation,64,9,26,Combinatorial Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,012 [building - 208],"['Vehicle Routing', 'Mathematical Programming', 'Logistics']",TC-26
"Airport ground-handling companies want to improve their operations all around the world want by automating current processes, and finding extra optimizations otherwise unreachable for human dispatchers. We aim to address the catering problem in the crossover between vehicle routing problems and scheduling problem, also known as the simultaneous supply & delivery problem. The MTCVRSPTW-MB [Multi-Trip Capacitated Vehicle Routing and Scheduling Problem with Time Windows and Meal Breaks] is extended to a heterogeneous fleet of trucks and drivers, with qualifications and different load patterns. We introduce a greedy heuristic to solve a relaxed version of the problem, as a baseline. Then, we implement a cooperative rVNS-based metaheuristic. We compare our algorithms on multiple real-life instances at San Francisco International Airport. Our computational study shows the effectiveness of each approach and brings out the strength of our metaheuristic. Our algorithms will help ground handlers specialized in catering operations, to speed up the decision-making process of assigning hundreds of tasks every day. In particular, we show the multiple extra benefits of the metaheuristic approach, including minimizing fuel consumption and the ability to re-assign tasks dynamically.",Assigning airport ground-handling operations - an rVNS metaheuristic,"[78374, 78387, 78388, 78389, 78390, 78391]",331,"[74, 145, 129]",3140,Airline Applications I,6,4,55,Transportation,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S02 [building - 101],"['Metaheuristics', 'Vehicle Routing', 'Scheduling']",MC-55
"The demand for healthcare services provided by Indian private hospitals has increased, which makes it imperative to assess their performances. We also assess the impact of public spending and governance on their performances.
We first estimate output oriented and input oriented technical efficiency scores of 163 private hospitals for the year 2022-23 using Data Envelopment Analysis. Expenditure on compensation of employees of hospitals, on power, fuel & water, on repairs and maintenance and Net Fixed Assets are inputs, with Total Income as output. After this, we estimate Gradient-Boosted Tree Classifier [GBTC] taking output efficiency score as dependent variable and public spending and governance factors as independent variables. Results show, on an average, the output oriented technical efficiency score is 0.66 and the input oriented score is 0.64. Big hospitals outperform small hospitals by 16 percentage points. Extent of misutilisation or underutilization of financial resources is greater than the extent of inadequacy of services delivered by the hospitals. In GBTC analysis, with output oriented scores as dependent variable, the most important features with a Mean Decrease in Impurity greater than 0.1 are Voice and Accountability score, Revenue Expenditure on Education, Revenue Expenditure on Water supply and sanitation and Revenue Expenditure on Urban Development. We recommend better governance structure; increased public spending and periodic audits of private hospitals",Performance of private healthcare sector and the role of public policy - A study of India using Data Envelopment Analysis and Gradient Boosted Trees,[69403],970,"[35, 56, 101]",3141,DEA in healthcare,3,3,17,OR in Health Services [ORAHS],"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,40 [building - 116],"['Efficiency Analysis', 'Health Care', 'OR/MS and the Public Sector']",MB-17
"We present a stochastic Bi-Level approach to support electricity retailers in making investment decisions in sustainable energy systems to supply green electricity. The interaction between the retailer, acting as leader, and the follower, representing a set of residential prosumers, is modelled as a Stackelberg game. Both players solve an optimization problem subject to uncertainty related to fluctuating market prices, weather variables and prosumer load. In particular, the leader solves a strategic problem referring to the optimal sizing of the energy system while conjecturing the possible reaction of the follower to the offered tariffs. The follower reacts to the communicated prices defining the procurement plan in terms of amount of electricity to purchase from the retailer and competitors so to minimize the expected electricity bill. The follower’s reaction affects the retailer’s profit computed as difference between the revenue from electricity selling and the total cost. To account for the retailer's attitude towards risk, the upper level problem includes a safety measure to control the average profit that can be gained in a given percentage of unfavourable situations. A tailored solution approach is developed and tested on a realistic case study calibrated on the Italian electricity market. Numerical results demonstrate the efficiency of the proposed approach and validate the significance of explicitly dealing with uncertainty and integrating safety measures.",Optimizing System Sizing - A Stochastic Bi-Level Approach for Green Electricity Retailers,[2743],405,"[36, 93, 136]",3142,Stochastic Optimization for Energy Transition,49,3,35,"Stochastic, Robust and Distributionally Robust Optimization","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,44 [building - 303A],"['Electricity Markets', 'OR in Energy', 'Stochastic Optimization']",MB-35
"Value functions play a central role in integer programming duality, and they are also used to develop solution methods for stochastic integer programs, bilevel integer programs and robust optimization problems. We propose a column-by-column algorithm for constructing the value functions of integer programs with finite domain over the set of so-called level-set minimal vectors. The proposed algorithm starts with the first column and sequentially adds the rest columns one by one. The advantage is that no integer program needs to be solved in the algorithm and no dominated vectors are generated [only level-set minimal vectors are generated]. Computational results on benchmark instances show that the proposed algorithm can reduce the computation time by up to three orders of magnitude compared with a state-of-the-art algorithm. We also extend the proposed algorithm to build value functions of integer programs with negative elements in the constraint matrix.",Construction of value functions of integer programs with finite domain,[77321],866,"[109, 0]",3143,Topics in Integer Programming II,64,13,25,Combinatorial Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,011 [building - 208],"['Programming, Integer']",WB-25
"While data for policy-making and planning is available for solar and wind resources, this is lacking for renewable hydrogen. Even as hydrogen is seen as an important part of the energy transition away from fossil fuels, it is unclear where, and at what costs hydrogen could be produced at scale and supplied into the global energy system. The lack of available data creates challenges for planning and investment decisions. We present a database that provides techno-economic data on hydrogen production from solar and wind and describes global technological and economic potential. The database starts from 10 years of ERA5 reanalysis data and adds technology performance data to enable optimization of renewable hydrogen supply at the global level. We illustrate potential use of the database by optimising the installed capacity for solar-PV, wind turbines, lithium-ion batteries, and electrolysers to meet future hydrogen demand. The data provides a timely and valuable view of the supply side of renewable hydrogen aiming to underpin renewable energy and hydrogen policy considerations.  ",Renewable Hydrogen Supply  --A Novel Global Dataset on Resource Potential and Hydrogen Production Optimization. ,"[77426, 42085]",640,"[37, 84]",3145,Capacity expansion planning for energy systems,21,13,22,Energy Management,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,81 [building - 116],"['Energy Policy and Planning', 'Optimization Modeling']",WB-22
"We introduce a novel two-phase matheuristic for assignment and packing [2P-MAP] designed to address large-scale assignment and truck loading problems. The scope of our approach involves scenarios with an extensive amount of items requiring assignment to a heterogeneous fleet of trucks and subsequent transportation. The proposed matheuristic strategically tackles the assignment problem in its first phase, followed by a complex truck loading algorithm to validate the assignment solution in the second phase. A distinguishing feature of our heuristic lies in its comprehensive exploration of the solution space, enabling the identification of optimal solutions. Moreover, our approach exhibits seamless adaptability to diverse use cases, given that subproblems are optimized in isolation before being integrated. This approach is motivated by the ROADEF/EURO challenge 2022, where we demonstrate superior performance on select instances, showcasing the potential for significant cost reduction in Renault's Supply Chains.",Two-Phase Matheuristic for Assignment and Packing,"[69741, 61911]",89,"[23, 63, 138]",3146,Cutting and Packing 1 - 2D rectangular,81,2,07,Cutting and Packing [ESICUP],"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,1019 [building - 202],"['Cutting and Packing', 'Large Scale Optimization', 'Supply Chain Management']",MA-07
"Benchmarking is a valuable process for improving strategic and operational decisions in both profit and non-profit organizations. An important sub-process of benchmarking is to measure performance gaps and uncover their causes. While Data Envelopment Analysis [DEA] can handle the evaluation part, Data Mining tasks are suitable to explore the link between computational results and additional knowledge. Therefore, a new Data Mining process called Exploratory Benchmarking is proposed. The process integrates DEA and Multidimensional Scaling [MDS] as a reference model to structure decision support using well-researched methodologies. First, the role of DEA is to evaluate the related data in context. Next, MDS is used to visualize the results of the evaluation. Finally, a systematic analysis is performed to uncover manifest and latent structures using a specially developed exploration procedure. The entire process is exemplified by a real-world application in the field of air navigation service providers at airports. However, the approach can be applied far beyond this as a standard process in business analysis and consulting.","Exploratory benchmarking with DEA and multidimensional scaling – process, reference model and application",[59929],936,"[24, 7, 67]",3147,DEA and Machine Learning,89,3,48,Data Envelopment Analysis and its Application,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,60 [building - 324],"['Data Envelopment Analysis', 'Analytics and Data Science', 'Management Information Systems']",MB-48
"The Internal-Ratings Based [IRB] approach is aimed at providing a measure of the maximum loss that a credit portfolio could generate over a year and with a given confidence level. As most of the standard risk measures, such as the Value-at-Risk [VaR], or the Expected Shortfall [ES], also the IRB measure depends on some parameters that must be estimated. The usual plug-in approach, consisting in substituting in the theoretical formulas the parameters with their estimates, does not consider the effect of the additional uncertainty generated by the estimation error. In this paper, we develop an analytical correction to the IRB formula that enables us to correct for the parameters uncertainty,  using the theoretical setting developed by Gourieroux and Zakoïan [2013]  and, to our knowledge, this is the first application of that approach to credit risk. This approach provides an approximated correction that depends on the variance-covariance matrix of the estimated parameters and does not require specific assumptions on their prior distribution, avoiding also computationally intensive Monte Carlo simulations.  We show the validity of our correction on simulated data and show that our results are consistent with Tarashev [2010] who adopts  Bayesian methods. We argue that our approach is more flexible and suited to be extended to the estimation of other parameters of the IRB formula. We show a practical application of our
approach relying on real data.
",The estimation risk under the IRB approach - an analytic way-out,"[72001, 72002, 71994, 78413]",386,"[44, 126, 131]",3148,New Challenges for Risk Management ,4,10,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S14 [building - 101],"['Finance and Banking', 'Risk Analysis and Management', 'Simulation']",TD-63
"In the construction of composite indicators by the Benefit-of-the-Doubt [BoD] model, proportional virtual weight restrictions [pie-shares] are frequently used to place bounds on the normalised indicators’ relative contribution to the composite for each assessed entity, based either on expert views or simply to prevent the occurrence of zero indicator weights. Two important issues associated with pie-share restrictions are that they cannot accommodate zero values for individual indicators and that they result in evaluating each assessed entity based on a different best-practice frontier, which makes the resulting BoD scores incomparable across entities. In this paper, we introduce a novel type of weight-share restrictions in the BoD model, which involve placing bounds on the share of each nornalised indicator’s weight to the sum of indicator weights rather than the product of an indicator and its weight, which is the case with pie-share restrictions. We show that weight share restrictions can accommodate zero values for individual indicators and that they result in evaluating all entities based on a common best-practice frontier, enabling the ranking of entities based on their BoD scores. We also discuss how, due to the nature of the BoD model, weight-share restrictions are the proper and most intuitive way to place an absolute bound on an indicator’s weight, and we illustrate our findings using data from the European Commission’s Cultural and Creative Cities Monitor.",Restricting Weight Shares in the Benefit-of-the-Doubt Model,"[61899, 72124, 13122]",110,"[24, 35, 110]",3149,"MCDA and Composite Indicators - Issues, Advances and Applications 1",44,14,44,Multiple Criteria Decision Analysis,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,20 [building - 324],"['Data Envelopment Analysis', 'Efficiency Analysis', 'Programming, Linear']",WC-44
"The UK’s NHS is actively looking at how more sustainable and innovative transport modes can be used alongside traditional logistics systems to support their target of reaching net-zero emissions by 2040. Uncrewed Aerial Vehicles [UAVs, or drones] are increasingly being trialled in health care systems outside the UK, though evidence from previous studies and trials is suggesting that they may not be the complete answer to many of the industry’s challenges. Using on-going case study investigations of local NHS same-day logistics networks in the UK region, this paper evaluates the benefits and costs of introducing mixed-mode logistics with UAVs to existing operations. Covering dense urban areas and sparse rural island areas, real-world datasets and realistic operating conditions are assessed using a novel savings algorithm and bin packing algorithm. Implications and recommendations for the wider logistics industry are subsequently given, including suggestions of where UAVs may be most applicable, and the best mode selections for different objectives and environments.
The research is being undertaken as part of the and EPSRC e-Drone project. ",The Value of Speed and Equity – Should Drones Be Used to Deliver Goods or are Better Solutions Available? - Case Studies from the UK,"[78392, 24355, 78429]",485,"[139, 65, 56]",3151,Sustainable Food and Health Care Logistics,19,10,24,Sustainable Supply Chains,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,83 [building - 116],"['Sustainable Development', 'Logistics', 'Health Care']",TD-24
"Efficient urban mobility is paramount in the evolving landscape of Mobility as a Service [MaaS]. This study tackles a shuttle routing problem within this framework, akin to the orienteering problem, aimed at optimizing existing transportation networks. The problem involves optimizing shuttle routes through a directed graph representing an urban network, where nodes correspond to shuttle stops and edges depict road connections with associated travel times. Users at stops need to reach a common destination under specific time constraints, including arrival time and a tolerance window. The primary objective is to maximize passenger count while adhering to these constraints and operating within a predefined service time window. A mixed integer linear program [MILP] has been developed to obtain exact optimal solutions, so far demonstrating efficacy for networks of up to 15 nodes within reasonable computational time. The MILP discretizes shuttle time resources, thus ensuring solutions to be characterized by loop-free paths and optimal time schedules for the picked-up users. Further refinements to this result consider practical limitations, such as the shuttle capacity, and fairness-based incentives for people in order to strategically foster this kind of mobility [e.g., by favoring remote nodes]. This approach lays the groundwork for scalable solutions in terms of both urban networks and number of deployed shuttles, as well as for real-time data integration for dynamic scheduling.",Unleashing Mixed Integer Linear Programming - Optimal Shuttle Routing Solutions for Enhanced Urban Mobility,[78353],332,"[145, 129, 143]",3153,MOST - MaaS & Innovative Services for Sustainable Mobility,6,12,55,Transportation,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S02 [building - 101],"['Vehicle Routing', 'Scheduling', 'Transportation']",WA-55
"Although the environmental policies have pushed for the introduction of electric vehicles [EVs], the development of the EV industry is slowed down by the issues related to short driving range and extended recharging time. These challenges underscore the need for increased investment in the charging infrastructure. We consider strategic and operational planning decisions arising in managing plug-in electric vehicle [PEV] charging stations. At a strategic level, the station owner, makes decisions on the capacity of the station infrastructures [PEV-charging piles, renewable generators, and energy storages] and on the retail prices for PEV-charging energy. These decisions depend on the unknown values for the productivity of the renewable resources, for the number of PEV clients and for the wholesale prices. Based on the worst possible outcome for these scenarios the PEV owner would like to make the best operational decisions regarding the quantity of supplementary energy to be produced. We introduce a robust formulation of the problem and solve it by an iterative method designed for solving linearly constrained optimization problems, whose non-smooth nonconvex objective function is defined as the pointwise maximum of finitely many concave functions. We provide some computational results to compare the adopted solution method with those derived from the literature.",A robust optimization approach for the strategic design of EV charging stations,"[10558, 2743, 12727]",405,"[127, 36, 81]",3154,Stochastic Optimization for Energy Transition,49,3,35,"Stochastic, Robust and Distributionally Robust Optimization","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,44 [building - 303A],"['Robust Optimization', 'Electricity Markets', 'Non-smooth Optimization']",MB-35
"The ecological transition to enhance a sustainable and circular approach to resources is the current global challenge, as well as the complex decision-making process that the global community is called upon to manage. In this context, the agricultural and food production sector is trying to pass from a linear resource consumption to a circular management of resources, through the adoption of different circular technologies. Due to the high level of complexity, and multidimensional effects, this transition process is facing many challenges and issues. One of the main observed challenges concerns stakeholders’ involvement and the identification of the different values engaged within this transition process. This issue is mainly due to the different levels at which the decision problem of implementing the circular economy paradigm in agriculture should be addressed. According to this challenge, this research is interested in providing a methodological framework to apply the Social Multi-Criteria Analysis to address the implementation of circular technologies in agriculture within stakeholders’ perspectives, values, and probable coalition. The main objective is to provide a decision support system able to manage and represent the complexity of the transition to the circular economy paradigm of agriculture. The challenge of the research is using evaluation as a communication, engagement and enabling tool to address innovation according to multidimensional values.",Enabling the Circular Economy in Agriculture - A Methodological Proposal of Social Multi-Criteria Evaluation to Represent Multiple Values,"[77726, 78396, 78398, 78399, 46433]",158,"[77, 26, 15]",3155,"How to support complex decisions. Negotiating the trade-off between Social, Environmental and Economic values 1",44,4,47,Multiple Criteria Decision Analysis,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,50 [building - 324],"['Multi-Objective Decision Making', 'Decision Support Systems', 'Complex Societal Problems']",MC-47
"Accurately predicting financial crises holds immense significance for the national economy and the financial sector. The market's instability has significant implications for decision-making processes, emphasizing the need to accurately predict and anticipate economic fluctuations at the earliest possible stage. Early warning methods can be a valuable tool for financial risk managers in reducing the likelihood of extreme losses. With the advent of modern technology and the availability of real-time data, it has become feasible to create forecasting models that can provide almost instantaneous predictions of financial crises. In this study, we present a technique that combines Artificial Neural Network, Random Forest, and XGBoost to analyze and predict the possibility of a financial crisis based on various performance indicators of pension funds, such as return on investment, risk level, and other related ratios. By employing the rolling window technique and methods of Explainable Artificial Intelligence [XAI], such as SHAP [Shapley Additive explanations] and LIME [Local Interpretable Model-agnostic Explanations], a comprehensive analysis was conducted. According to our results hidden market regime [forthcoming financial crisis] within one to ten days can be predicted with a high degree of accuracy. The risk-adjusted performance metrics, such as the Bernardo and Ledoit ratio, Sterling Ratio, Sortino ratio, and Pain index, turnout to be the most important measures.",Explainable AI methods for early warning system of financial crisis prediction,"[76259, 24791]",387,"[8, 30, 44]",3156,New Tools in Insurance Risk Management ,4,10,02,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,Glassalen [building - 101],"['Artificial Intelligence', 'Disaster and Crisis Management', 'Finance and Banking']",TD-02
"Remote driving is a type of teleoperation that allows a human operator to control a vehicle without direct physical access, typically through wireless communication technologies. Recent advances in 5G wireless cellular networks, sensor technology, high-quality cameras, and machine learning have made remote vehicle operation more feasible and successful for a wider range of applications, both on public roads and inside facilities such as warehouses and factories. The main advantage is that remote drivers do not have to be idle during service operations at customers. Instead, they can switch to other vehicles, which opens up the possibility of operating a fleet more efficiently. In this talk, we introduce the Vehicle Routing Problem with Time Windows and Remote Drivers [VRPTWRD], which extends the traditional vehicle routing problem to include the specificities of remote driving. We model different versions of the VRPTWRD as mixed-integer programming problems, and use Benders decomposition to derive effective branch-and-Benders-cut approaches. Our results indicate that remote driving can effectively reduce the workforce needed to operate a fleet of vehicles.",The Vehicle Routing Problem With Remote Drivers,"[30176, 5934]",749,"[65, 14, 109]",3157,Vehicle Routing Problems With Time Windows,5,13,64,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S16 [building - 101],"['Logistics', 'Combinatorial Optimization', 'Programming, Integer']",WB-64
"The transition towards renewable energy sources presents increasing challenges in operating, maintaining, and planning electrical grids. With the growing complexity of planning tasks, various planning criteria impact the decisions on specific measures for grid reinforcement and expansion.
To address such a growing number of criteria, it becomes necessary to develop suitable models and simulation tools. Therefore, an existing automated power grid expansion algorithm based on pandapower is enhanced to handle multiple objectives. So far, using stochastic local search algorithms like hill climbing or iterated local search, a solution has been found to minimize cost as the sole objective. In addition to cost, other objectives like reliability, security, sociology, or environment are often considered as well. A decision-support approach in grid planning is therefore proposed that considers these additional criteria.
We propose a methodology that aims to optimize grid reinforcement and expansion by considering multiple objectives. The genetic NSGA-II algorithm is employed to find solutions that balance these considerations effectively. The approach aligns with sustainable development principles and fosters resilient and sustainable grid planning. Pareto front solutions, representing trade-offs between different objectives, offer a range of alternatives that can be considered in grid planning processes and therefore address the complex challenges of energy grid planning.",Automated Grid Expansion and Reinforcement Planning Considering Multiple Objectives,"[77829, 78549, 28733]",52,"[77, 93, 37]",3161,Multiobjective Mixed-Integer Nonlinear Optimization,34,9,37,Multiobjective Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,33 [building - 306],"['Multi-Objective Decision Making', 'OR in Energy', 'Energy Policy and Planning']",TC-37
"Nonparametric efficiency analysis methods such as data envelopment analysis [DEA] have been widely employed for evaluating eco-efficiency. However, virtually all published studies in this literature implicitly assume weak sustainability. This is a very strong assumption that allows for substitution among different forms of inputs and/or outputs, treating ecological conservation as merely one among several competing objectives that can be traded off against economic growth. To broaden the scope of nonparametric eco-efficiency evaluation, this paper proposes a novel approach based on a strong sustainability perspective. Drawing on a newly introduced method termed Fixed Proportion Constrained DEA, our eco-efficiency evaluation approach restricts the substitutability among inputs and/or transformability among outputs, highlighting the intrinsic value and irreplaceability of ecosystem services and natural capital. By incorporating this perspective into eco-efficiency measurement, our proposed method offers a more comprehensive understanding of sustainability and underscores the importance of preserving natural ecosystems for future generations.",Extending Eco-Efficiency Evaluation in DEA through a Lens of Strong Sustainability,[77814],940,"[24, 100]",3162,DEA applications in Environment and Sustainability I,89,8,48,Data Envelopment Analysis and its Application,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,60 [building - 324],"['Data Envelopment Analysis', 'OR in Sustainability']",TB-48
"The market introduction of radically new products and their respective innovation diffusion can considerably be affected by consumers’ increased uncertainty due to diverging information received from opposing sources. Such information can originate, for example, from strong general attitudes being prevalent in certain consumer groups [e.g., regarding the pros and cons of integrating AI capabilities in products] or it may even be spread by a competitor employing a negative-word-of-mouth campaign [e.g., when questioning the eco friendliness of electric vehicles]. In such a setting, consumers receive ambiguous signals, which should increase their uncertainty and potentially delay product adoption. However, commonly used belief updating models [such as Bayesian learning], in a counterfactual manner, assume a decrease of uncertainty whenever new information is received even if this information substantially deviates from previous information. In our research, we demonstrate this effect by means of a computational simulation experiment based on a straightforward agent-based model of innovation diffusion and we propose a novel approach that more realistically captures consumer uncertainty with respect to insufficient or ambiguous information.",Innovation diffusion in the presence of opposing information sources - An agent-based simulation approach,"[74888, 4357]",565,"[131, 3]",3163,Simulation in innovation,77,7,43,"Agent-based Models in Management, Economic and Organisation Sciences","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,99 [building - 306],"['Simulation', 'Agent Systems']",TA-43
"LEMON [Library for Efficient Modeling and Optimization in Networks] is a generic open source C++ library of graph and network algorithms and related data structures. It is a package of highly efficient and versatile tools with simple and convenient interface, targeting both computer scientists and the operations research community, both for academic and commercial purposes.

This talk will outline the basic design concepts, features and performance of LEMON, with a focus on the recent additions to the library. Then, two case studies are presented. The first one demonstrates how it may support basic research in Combinatorial Optimization, while the second one shows an example of industrial application.
",LEMON - an Open Source C++ Library for Solving Network Optimization Problems,"[35631, 50858]",192,"[134, 53, 14]",3164,Modern techniques for network optimization,68,9,38,"Conic Optimization - Theory, Algorithms, and Applications","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,34 [building - 306],"['Software', 'Graphs and Networks', 'Combinatorial Optimization']",TC-38
"The vehicle routing problem with simultaneous pickup and delivery [VRPSPD] is a well-known combinatorial optimization problem. It involves finding the least-cost set of routes, starting and ending at the depot, while simultaneously satisfying pickup and delivery demands from multiple locations within a single trip without exceeding the capacity of the vehicle. There are many variants of this problem, each one incorporating additional attributes such as a heterogeneous fleet of vehicles, time constraints, route duration, multiple depots, and decisions regarding depot location. In this work, we propose a unified formulation for ten of these variants, which were considered special cases of a generic problem denoted as heterogeneous location routing problem with simultaneous pickup and delivery and time windows [HLRPSPDTW]. To solve it, we developed a branch-cut-and-price [BCP] algorithm and validated it through extensive computational experiments on more than 530 benchmark instances. The proposed BCP algorithm successfully found optimal solutions for 44.3% of unsolved problems, attained 92.6% of known optima, and improved lower bounds for 69.9% of the instances that remained unsolved. 
",A Unified Exact Approach for a Broad Class of Vehicle Routing Problems With Simultaneous Pickup and Delivery,"[73369, 73832, 67821, 24239]",787,"[145, 13]",3165,MILPs for Vehicle Routing 2,5,12,58,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S07 [building - 101],"['Vehicle Routing', 'Column Generation']",WA-58
"We study a supply chain that is composed of a manufacturer and a retailer. The retailer sells two vertically differentiated and substitutable products to the market and the products are supplied by the manufacturer. The lower-quality product has a stable supply, and thus, its wholesale and retail prices are assumed to be exogenous. The innovative [i.e., ‘high-quality’] product’s price is yet to be determined and its procurement/production is prone to supply disruption risk. We explore the optimal procurement quantities of both products and pricing decisions of the higher-quality product. As for the sequence of events, first, the manufacturer decides the wholesale price of the innovative product. Then, after observing this price, the retailer determines the order quantities of both products. Afterward, the innovative product is supplied or not with a certain probability. Finally, the retailer determines the retail price of the innovative product if no disruption and customers make purchase decisions.

We find that the market potential of the lower-quality product, which is determined by the quality perception and the retail price of that product, plays a crucial role in the optimal policies. When this potential is low, the innovative product is more inclined to be abandoned as its quality gets closer to the lower-quality one. Also, a precautionary stock of the lower-quality product should be kept to make up for a potential disruption of the innovative product.

",Pricing and procurement policies of two substitutable products under supply disruption risk,"[78018, 42083, 1838]",633,"[50, 138]",3167,Retail Cooperation and Competition,30,13,61,Retail Operations,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S10 [building - 101],"['Game Theory', 'Supply Chain Management']",WB-61
"A bankruptcy problem occurs when several agents each claim a portion of an estate that is insufficient to satisfy all the claims, raising the question of how to divide the estate equitably among the agents.
In the traditional bankruptcy problem, there is a single decision made at a single point in time. Looking at the applications of bankruptcy, e.g. water sharing or vaccine allocation, one can see that these are actually problems where allocations have to be made over time.
In this presentation, we introduce the concept of bankruptcy over time in the Multi-period Bankruptcy Problem. We also present three different perspectives on bankruptcy over time.
For each of these perspectives, we present mechanisms to generalize existing bankruptcy rules to the multi-period setting. Moreover, we formulate properties of these multi-period bankruptcy rules, and prove that these properties are inherited from the traditional bankruptcy rules. Finally, we present efficient algorithms to apply the bankruptcy rules using recursive methods.
By introducing the Multi-period Bankruptcy Problem, presenting and computing multi-period bankruptcy rules, we want to extend the usefulness of bankruptcy rules in practice and open avenues for further research on multi-period bankruptcy problems.",Bankruptcy Problems over Time,"[71547, 5554, 72129, 24969]",384,"[50, 0]",3168,"Game Theory, Solutions and Structures II",88,3,36,"Game Theory, Solutions and Structures","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,32 [building - 306],['Game Theory'],MB-36
"Product ratings help customers to evaluate the quality of a product prior to their purchasing decision in online markets. To overcome information overload, the vast amount of reviews is aggregated to a single scalar value, typically the arithmetic mean. However, customers may not base their purchasing decision on the arithmetic mean but on other aggregation functions such as the amount of 1-star and/or 5-star ratings. Our research question therefore asks to what extent the arithmetic mean really reflects how customers aggregate product ratings. In our incentivized laboratory experiment, participants are shown bundles of different products, and subsequently, asked to rank them according to their preferences. Analyzing the rank data with a Placket-Luce model, we find that the arithmetic mean indeed best explains the obtained data. However, our analysis on an individual level reveals more heterogeneous aggregation patterns. In particular, a significant share of participants exhibits a binary bias. That is, moderate 2-star and 4-star categories get over- and the extreme 1-star and 5-star categories under-weighted. We also observe minor clusters who prefer products which have the lowest amount of 1-star ratings. The way our participants aggregate the ratings is neither affected by variation of provided information nor by individual characteristics such as experience, risk attitudes, or demographics. ",Accounting for heuristics in reputation systems. An interdisciplinary approach in aggregation processes ,"[78393, 78403, 78406, 78450, 78455]",187,"[10, 67, 26]",3169,Experimental economics and game theory 2,73,14,40,Experimental economics and game theory,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,96 [building - 306],"['Behavioural OR', 'Management Information Systems', 'Decision Support Systems']",WC-40
"Integrating both short-term flexibility requirements and long-term uncertainty into a unified planning framework results in large-scale multi-stage stochastic mixed-integer programming problems. Consequently, efficient solution methodologies are needed to enable the planning of realistic-size power systems. This work investigates possible ways of combining column generation, lagrangian relaxation, and Benders decomposition in a hybrid decomposition method, aiming to enhance convergence by leveraging the strengths of diverse decomposition techniques. The proposed hybrid decomposition is implemented using distributed computing, allowing parallel initiation of each decomposition approach with the potential for exchanging bounds and solutions. Solutions from Benders decomposition and the bundle method at each iteration can be used to generate new columns for the column generation approach. Likewise, the columns obtained by column generation can be used to derive new cuts for Benders decomposition and the bundle method. The performance of the hybrid decomposition is analyzed through case studies applied to the NREL 118-bus power system.",Hybrid decomposition method for power system planning under uncertainty,[73922],841,"[93, 136, 13]",3170,Decomposition techniques applied to energy problems,23,10,19,OR in Energy,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,44 [building - 116],"['OR in Energy', 'Stochastic Optimization', 'Column Generation']",TD-19
"The growth of social media platforms allowed customers to rely on influencers and past consumers’ experiences for their purchase decisions. In this paper, we address some of the firm's challenges of influencer marketing by considering different types of influencers and partnership schemes with them and we address the effect of social learning. We introduce a two-period model. In the first period, the firm launches a new product and an influencer marketing campaign. The campaign can rely on two types of influencers that differ in popularity and credibility and two potential partnerships; sponsored advertising and public relations. Customers make their purchase decisions using their original beliefs, the information obtained via the influencers they follow, and reviews of early adopters as they become available in the second period. When customers are equally likely to follow the same influencer, the firm should partner with a single influencer type through a single type of partnership. Furthermore, social learning is highly beneficial for an undervalued product when supplemented by advertising. Finally, in a society formed by classes where followers and influencers usually belong to the same class, a diverse mix of influencer advertising methods can be optimal. We suggest the strategic partnerships a firm should establish with social media influencers depending on their credibility and popularity, the market composition, and the characteristics of the social media platform.",Advertising by Recruiting Influencers,"[78401, 78415]",263,"[132, 71, 124]",3173,Pricing and applications 3,11,2,59,Pricing and Revenue Management,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S08 [building - 101],"['Social Networks', 'Marketing', 'Revenue Management and Pricing']",MA-59
"One of the widely adopted practices for e-waste reverse logistics is the sealed bid process. In this process, the seller first invites quotations for selling a set of e-waste. The recyclers then send their bids/ quotations in sealed envelopes which are opened only on the prescribed date. The recycler with the highest bid wins the contract.
In the above process, a seller may sell a particular type of e-waste or a combination of different types of e-waste. However, the recycler may or may not have adequate infrastructure to process each type of e-waste offered by the seller. In such situations, the recycler may collaborate with another recycler[s] to ensure better efficiency and make a higher profit. However, there are various uncertainties faced by the recyclers in terms of the quantity, quality, recovery rate, procurement cost, transportation cost, and processing cost of the e-waste.
This paper tries to address the uncertainty faced by the recycler in terms of the procurement cost during a sealed bid process. While deciding on whether to bid or not in a sealed bid process, the recycler faces a tradeoff between profit margin and the quantity of e-waste collection. A stochastic integer linear programming model is proposed that would help recyclers decide an optimal bidding price for a certain set of e-waste to maximize their profit. Further, this paper analyses the impact of a coalition and the capacity allocation on the profit shares of the partner firms.
",E-waste reverse logistics in India - Collaborative collection and processing,[78385],7,"[125, 136, 50]",3174,"Circular Economy, Remanufacturing and Recycling",18,2,23,"Circular Economy, Remanufacturing and Recycling ","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,82 [building - 116],"['Reverse Logistics / Remanufacturing', 'Stochastic Optimization', 'Game Theory']",MA-23
"Understanding how a car and driver will perform on a given circuit is a central concern of constructors in motorsport. Statistical modelling can be a powerful tool for analysing the interplay between car, driver and circuit features through partial pooling across fine-grained  circuit topographies. Telemetry data provides a rich set of inputs for spatial models, yet, the size of the data requires highly scalable methods. We have developed a framework to approximate a Bayesian hierarchical Gaussian process that allows the estimation of latent circuit topographies and their interaction with car and driver at scale. Our fast approach to modelling the spatial characteristics of circuits is applied to the performance outcomes for all races of the 2023 Formula 1 season. Our results demonstrate the predictive and descriptive utility of the framework as it can predict results for new circuits within season and find topographical features where car and/or driver are expected to have the greatest and least advantage over the field.",Fast Laps and Fast Stats - A Scalable Spatial Model of Circuit Topographies and their Performance Impact for Constructor and Drivers in F1,[78402],949,"[99, 135]",3175,Sports analytics,37,13,16,OR in Sports,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,19 [building - 116],"['OR in Sports', 'Stochastic Models']",WB-16
"Determining optimal replenishment of brick-and-mortar retail stores is a challenging problem. Based on experience from a large north European grocery store, these retailers typically face a non-stationary demand with a recurrent periodic pattern where excess demand is lost. Moreover, replenishment opportunities are limited by a weekly schedule with limited transport capacity allocated to each store.
 
The optimal replenishment decisions for a store in this setting is previously unknown. We show that a periodic modified base-stock policy is optimal, i.e. ordering so that the resulting inventory is as close as possible to the optimal base-stock level of the period by modeling the problem as a Markov Decision Process. It is straightforward to use the model to find the optimal base-stock levels. A numerical study shows the resulting cost increases when using solution methods that disregard some of the setting characteristics, e.g. non-stationary demand, limited capacity, lost sales.
 
Efficiently solving this problem is important not only to minimize the cost at the stores but also to evaluate various planning decisions, e.g. scheduling and routing, in the greater distribution network.",The Optimal Policy for a Cyclic Capacitated Lost Sales Inventory,"[77566, 78948]",480,"[61, 82, 135]",3177,Retail Inventory Management II,30,4,50,Retail Operations,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,M2 [building - 101],"['Inventory', 'Optimal Control', 'Stochastic Models']",MC-50
"Nowadays, the development and practice of personalized medicine become strategically important. It focuses on individual patient needs and conditions, and due to the shelf life of personalized medicine, it has high requirements on temperature in production and transportation. Designing a stable personalized medicine supply chain network to efficiently complete the manufacturing and delivery of drugs is crucial to reduce supply chain costs and improve overall efficiency to promote positive industry development. A multi-period, multiobjective production and distribution network of personalized medical supply chain is established in this paper, considering distribution, manufacturing, logistics, time and logic constraints, to minimize cost and time and maximize demand coverage. With the influence of manufacturing failure rate as an uncertainty factor, the robust optimization method is used to deal with the uncertainties. In this paper, two clustering-based decomposition methods are proposed as the solution approaches, which are shown to solve models more efficiently than the proposed MILP model. Monte Carlo simulation is used to validate the performance the robust optimization approaches. Finally, the proposed model is extended to consider the fairness on waiting time among patients. The research outcomes could help to improve the operational efficiency of the personalized medicine supply chain and promote the sustainable development of the personalized medicine industry.",Multiobjective robust optimization approaches for personalized medicine supply chain network design under failure rate uncertainty,"[78237, 36148, 50318]",952,"[112, 64, 127]",3178,Location planning in healthcare,3,7,15,OR in Health Services [ORAHS],"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,18 [building - 116],"['Programming, Multi-Objective', 'Location', 'Robust Optimization']",TA-15
"Overconfidence, as the most robust behavioral bias, has been confirmed to be a leading cause of human newsvendors' pull-to-center [PTC] ordering bias. Most existing studies have focused solely on inventory decisions, overlooking the complexities that arise from the interface between operations management [OM] and marketing. As marketing and OM become increasingly intertwined and make decision-making more challenging, this paper examines the impacts of overconfidence on newsvendor's joint ordering and advertising decisions. Our study uncovered new decision biases that arise from overconfidence and have not been previously identified. Specifically, we found that overconfidence leads to over-advertising and a novel ordering behavior called the leapfrogging effect, where inventory can jump from underordering to overordering for medium-high margin products.  Furthermore, our study shows that overconfidence can still result in a PTC-like effect for newsvendors who advertise. We also found that overconfidence has a more detrimental effect on profits for newsvendors with advertising. These findings highlight the need for overconfident newsvendors to be aware of choosing advertising because advertising can harm profits for overconfident newsvendors, while consistently improving profits for well-calibrated newsvendors. Overall, our study emphasizes the need to consider the effects of behavioral biases on decision-making in more complex contexts, particularly in the operations-marketing",Newsvendor Overconfidence and Advertising,[62966],576,"[61, 10, 71]",3180,Behavioural operations and games ,13,13,07,Behavioural OR,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1019 [building - 202],"['Inventory', 'Behavioural OR', 'Marketing']",WB-07
"Congestion addressing issue is of utmost importance to ensure the sustainable development of urban conglomerates, particularly in light of governmental initiatives aimed at regulating emissions. Congestion mitigation through resource optimization, not only enhances commuter quality of life, but also alleviates delays and reduces operational cost for carriers. This research introduces a novel bi-level model aimed at addressing congestion and emissions control in transportation networks. At the first level, acting as authority, our model aims to mitigate congestion and reduce emissions. By analysing traffic flows and assessing congestion levels on arcs and routes, we impose a crossing cost proportional to the degree of arc saturation, thereby discouraging congestion buildup while simultaneously penalizing emissions to promote environmental sustainability. At the second level, coalitions make decisions based on cost minimization. Specifically, the follower's problem is presented as a freight forwarding issue, in which the model enables transhipments between vehicles, implicitly allowing coalition formation and promoting a collaborative approach. This model serves as decision support, offering a comprehensive approach to congestion and emissions management in transportation systems. The case study is related to CNMOST PNRR spoke 7 research program. ",A Bi-Level Optimization Model for Congestion and Emission Control in Transportation Networks,"[77386, 46040]",812,"[111, 143, 100]",3181,Carbon Taxing and Emissions Controls,80,15,53,Sustainable and Resilient Systems,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,8007 [building - 202],"['Programming, Mixed-Integer', 'Transportation', 'OR in Sustainability']",WD-53
"Europe still has a strong dependence on fossil fuels, and natural gas in particular, despite the claimed commitments towards carbon neutrality by 2050. The REPowerEU Plan is reducing the huge amount of gas imported from Russia. Nonetheless, the main strategy to guarantee security of supply mainly envisages long-term agreements with more reliable partners for gas and a larger support to the development of renewables. At the same time, a carbon-neutral energy generation technology nuclear fission is undergoing a substantial phase-out, and the use of fossil fuels for energy production is discouraged through the adoption of growing carbon tax levels in the form of excise duties and the Emissions Trading System. This work presents a scenario analysis carried out using the open-software and open-framework energy system optimization model TEMOA-Europe. Taken for granted the absence of Russian gas imports starting from 2027, the examined alternative scenarios envisage - 1] a business-as-usual scenario; 2] a scenario with increasing carbon prices and constraints on nuclear capacity to simulate the current phase-out trend; 3] a scenario with increasing carbon prices and the possibility to increasingly rely on nuclear electricity; 4] a net-zero emissions by 2050 scenario. Energy costs, the profitability of renewable energy projects and the role of gas are evaluated for each of the mentioned scenarios providing useful insights for the development of the European energy system.",A TEMOA-Europe-based scenario analysis to examine crucial strategies for decarbonization,"[78405, 77804]",843,"[37, 26, 93]",3185,Towards sustainable development,23,12,19,OR in Energy,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,44 [building - 116],"['Energy Policy and Planning', 'Decision Support Systems', 'OR in Energy']",WA-19
"The two-echelon inventory-routing problem [2E-IRP] addresses the coordination of vehicle routing and inventory management throughout a two-echelon supply network. The latter consists of intermediate facilities that are located in the city outskirt, supplied from distant depots, and serve a set of geographically widespread customers. The customers' demand is met from either their local inventory or intermediate facilities' inventory. The aim is to minimize the transportation and inventory costs while meeting customers' demands over a finite discrete planning horizon. We introduce an effective tabu search-based matheuristic approach to solve it. 
Computational experiments show that our approach achieves good results regarding solution quality and computational time. For small instances, the matheuristic finds 99 optimal solutions out of 165 known optimal solutions and achieves an average gap of [-2.32%] over 235 instances with a known best upper bound only. Our approach also solves larger instances within a reasonable computational time and provides upper bounds for all 400 large-sized instances for the first time to the literature. 
",The Two-Echelon Inventory-Routing Problem - A Matheuristic Approach,"[70731, 17040, 25632, 55094]",623,"[143, 74, 61]",3186,Vehicle routing I,6,2,60,Transportation,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S09 [building - 101],"['Transportation', 'Metaheuristics', 'Inventory']",MA-60
"Most of scheduling problems consider a deterministic environment for which the
data are known. However, in many real-world problems, the precise values of data in
scheduling models might not know in advance. For example, the precise duration of
surgery is hard to predict because it can vary widely from one case to another, even for identical procedures. The post-surgery duration is also hard to predict because it is influenced by the patient's age, gender, condition, type of surgery and type of anesthesia, etc. We consider a three-stage operating room surgery scheduling problem with uncertain surgery and post-surgery durations. The three successive stages are pre-surgery, surgery, and post-surgery. No-wait constraint is considered among the three stages. Surgeries are known in advance [elective]. Multiple resources are considered throughout the surgical process - PHU [pre-operative holding unit] beds in the first stage; ORs [operating rooms] in the second stage; and PACU [post-anesthesia care unit] beds in the third stage. The objective is to minimize the makespan. We have proposed a genetic algorithm for robust scheduling [GARS]. Randomly generated problem instances are tested to evaluate the performance of the proposed GARS. Computational results show that the robust schedules found by the GARS are relatively insensitive to uncertain surgery durations when compared with the non-robust schedules found by the basic GA [BGA].
",Solving three-stage operating room scheduling problems with  uncertain surgery durations,[78408],965,"[129, 127, 56]",3188,Surgery Scheduling and Operating Room Planning [2],3,15,15,OR in Health Services [ORAHS],"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,18 [building - 116],"['Scheduling', 'Robust Optimization', 'Health Care']",WD-15
"The adoption of non-conventional renewable energy sources [NCRE] is essential to achieve the Paris Agreement's objectives. This transformation does not only involve changing the primary energy source of large-scale electricity generation but, its effective adoption requires changes at the level of the energy transportation and distribution network, including NCRE distributed generation and microgrids systems to provide stability to the grid. Since NRCE are generally variable, energy storage and energy management systems [EMS] has become key allies on this process.
In this talk we present how improvements on mathematical model for battery storage in EMS for microgrid applications can produce better decisions, reducing the microgrid operation costs significantly. For this purpose, a new battery model [MLPP-A] is presented, which incorporates the nonlinearities of the battery charging process through a piecewise linear approximation. Our MLPP-A model is benchmarked against a linear battery charging model [MLB], to get the parameters for both models experimentally, and to compare their adjustment with real system, a microgrid test bed was implemented. To evaluate the EMS performance with both battery models, under same solar irradiance conditions, on different seasons of the year, a computational simulation was implemented, which showed an average economic benefit of 6.33% once MLPP-A model is used instead of MLB model, without significant additional computing time required.",Improving the Battery Model for Energy Management Systems in Microgrid Applications,"[77922, 64248]",472,"[93, 111, 136]",3190,Distributed energy systems,21,10,22,Energy Management,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,81 [building - 116],"['OR in Energy', 'Programming, Mixed-Integer', 'Stochastic Optimization']",TD-22
"Building an operational research model together with the team responsible for the issue at hand helps to elicit knowledge and create commitment to joint action. In these facilitated modelling interventions the facilitator has a central role. How can an analyst perform the role of facilitator? Furthermore, what is effective team facilitation? Understanding the rationale behind tasks helps the facilitator to translate generic advice on what to do, to the specific situation encountered. Our assumption is that the more the facilitator’s behaviour is in line with these attitudes, the more effective they will be in supporting a team decision process. We describe five facilitator attitudes  [helping, neutrality, inquiring, relational engagement, self-reflexivity], as well as the facilitator tasks aligned with each of these attitudes. ",Learning to facilitate team discussions,"[23081, 8371]",568,"[10, 55, 133]",3191,Behavior in group decision-making ,13,9,11,Behavioural OR,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,12 [building - 116],"['Behavioural OR', 'Group Decision Making and Negotiation', 'Soft OR']",TC-11
"This study introduces a new variant of the car sequencing problem to support the operations of a Brazilian automotive assembly plant. We propose an integer linear programming [ILP] formulation to schedule the maximum number of cars without violating the spacing constraints associated with car options, such as air-conditioning or sunroof. In the studied plant, these violations lead to a complete stop of the assembly line, which is the least desirable outcome for the company. We also present valid combinatorial lower and upper bounds, as well as binary and iterative search algorithms to solve the problem when good primal bounds are not readily available. In addition, we develop an effective iterated local search algorithm to quickly obtain high-quality solutions, then used as a warm start to the exact methods. Computational results demonstrate that relatively low gaps are achieved for benchmark instances within a time limit of ten minutes, and we conduct an instance space analysis to identify the features that make the problem more difficult to solve. Moreover, the instances reflecting the company's needs are solved to optimality in less than a second. Finally, simulations with real-world demands, divided into shifts, are performed for a period of four months. In this case, we use the proposed ILP model in all shifts except the last one of each month, for which we employ an alternative model to sequence the unscheduled cars, adjusting the pace of the assembly line.",The maximum length car sequencing problem,"[77906, 74723, 67821, 77178]",243,"[129, 14, 5]",3192,Models and algorithms for real-life combinatorial optimization problems,64,12,52,Combinatorial Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,8003 [building - 202],"['Scheduling', 'Combinatorial Optimization', 'Algorithms']",WA-52
"Surrogate modeling is a well-established field within engineering sciences, where computational simulation and optimization models are becoming increasingly complex. Instead of running costly high-fidelity models, surrogate models can provide accurate estimations , while reducing computation times. Currently, the energy system is becoming increasingly integrated across the energy sectors and vectors, leading to a growing demand for sector-coupled large-scale energy system models. At the same time higher shares of variable renewable energy sources require higher temporal and spatial resolutions when performing the energy system modeling. This development results in an increasing complexity of these models. One possible method to deal with this growing complexity is to build surrogate models representing a specific energy sector [e.g. hydrogen], which can then be coupled to another operational model [e.g. power system model]. However, while surrogate models have been used for various applications in the energy domain, ranging from building energy performance simulation to power grid optimization, no surrogate models have been developed to represent large-scale sectoral energy system models. This study will present a literature review on surrogate modeling for energy systems. It will mainly focus on potential applications of machine learning techniques, such as Bayesian learning and constraint learning, in building surrogate models for large-scale sectoral energy system models.",Machine Learning based surrogate models for large-scale sector-coupled energy systems ,"[77281, 62517, 24214, 58629]",147,"[66, 37]",3193,Learning-assisted Optimization in Energy Problems,23,2,19,OR in Energy,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,44 [building - 116],"['Machine Learning', 'Energy Policy and Planning']",MA-19
"The insurance-linked securities [ILS] market, as a form of alternative risk transfer, has been at the forefront of innovative risk-transfer solutions.  The catastrophe bond [CAT bond] market now stands for almost half of the whole ILS market and is steadily growing. Since CAT bonds are often tied to risks in different regions, we follow this idea by constructing different pricing models that incorporate various scenarios of dependence between catastrophe losses in different areas.  Namely, we consider independent, proportional, and arbitrary two dimensional distributions cases. We also derive normal approximations of the prices and compare them with prices obtained via bootstrapping. We find significant difference between those approaches. We believe that these findings can be helpful in modelling and pricing of other ILS tied to natural disasters. For illustration purposes we analyse Property Claim Services data.
",Modelling and pricing of multi-region catastrophe bonds,[71524],387,"[45, 135, 126]",3195,New Tools in Insurance Risk Management ,4,10,02,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,Glassalen [building - 101],"['Financial Modelling', 'Stochastic Models', 'Risk Analysis and Management']",TD-02
"n this talk we deal with some nonlocal critical growth elliptic problem driven by the fractional Laplacian, with particular emphasis on equations in presence of jumping nonlinearities. 
Using variational and topological methods, we prove the existence of a nontrivial solution for the problem under consideration.
These existence results can be seen as the nonlocal counterpart of the ones obtained in the  context of the Laplacian equations. In the nonlocal framework the arguments used in the classical setting have to be refined. Indeed the presence of the fractional Laplacian operator gives rise 
to some additional difficulties, that we are able to overcome proving new regularity results for weak solutions of nonlocal problems, which are of independent interest.
This is a joint paper with Giovanni Molica Bisci, Kanishka Perera and Caterina Sportelli [Journal des Mathématiques Pures et Appliquées, 2024].",Nonlocal critical growth elliptic problems with jumping nonlinearities,[78411],904,"[84, 0]",3196,	Optimization on Geodesic Metric Spaces I - Smooth case,69,5,41,Optimization on Geodesic Metric Spaces - Smooth and Nonsmooth,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,97 [building - 306],['Optimization Modeling'],MD-41
"In passenger rail operations, train unit and crew scheduling are key processes. Train unit scheduling assigns rolling stock, while crew scheduling allocates personnel like drivers. Both tasks, critical for optimizing vehicle and crew use to save costs, are complex NP-hard problems traditionally solved separately as two stages. This sequential approach limits achieving truly optimal schedules. An integrated approach, considering both simultaneously, promises more optimized solutions and has been successful in bus operations, reducing operational costs and crew usage, but is under-researched in rail planning.
In this study, we introduce an integrated approach for scheduling train units and crews by utilizing pre-generated shifts [duties]. This approach faces unique obstacles in the rail sector, particularly when employing the pre-generation strategy. A key challenge involves allocating staff to auxiliary tasks like shunting and unit coupling/decoupling, which are determined during the integrated process but unknown in advance. Another challenge is the need to pre-generate deadheading trips and incorporate them into pre-generated shifts. This challenge does not exist during the separate procedure. In theory, there is a huge number of deadheading trips, leading to a severe increase on the number of pre-generated shifts. We explore solutions to these issues through both synthetic and real-world examples, demonstrating our approach's effectiveness in solving these challenges.",Integrated railway train unit and crew scheduling using column pre-generation,"[78410, 49632, 79013]",180,"[129, 13]",3198,Crew Planning in Public Transport,85,3,54,Public Transport Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S01 [building - 101],"['Scheduling', 'Column Generation']",MB-54
"Priority setting in healthcare’s financing is a multidimensional problem as there are conflicts among different stakeholders’ groups [patients, scientific specialists, healthcare providers, manufacturers, insurers]. Allocating the limited resources and funding to [new] medical technology requires an evaluation of the multiple types of value medical technology [i.e. tools, tests, devices, drugs, vaccines, processes, etc.] can occur, which extend the traditional economic evaluation which is often limited to the evaluation of costs and a single health related outcome [such as QALYs, DALYs, etc.]. 

Multicriteria Decision Analysis [MCDA] can expand the current more or less standard assessment process by introducing multiple [and often conflicting criteria], the preferences of multiple stakeholders, including the society. 

For this reason, in our paper we are reviewing the relevant literature and through a detailed review and bibliometric analysis of 226 papers we identify the theoretical foundations and trends in the relevant literature. Despite the relevant progress over the last years, MCDA for Health Technology Assessment needs to address several challenges before being widespread. Such challenges could be conceptual, methodological or organizational. Overcoming these challenges can help embedding the multicriteria concepts and methods in healthcare technology evaluations and decision making.
",MCDA for Health Technology Assessment - Trends and Challenges,"[78412, 78416, 78417, 2127]",957,"[25, 56, 73]",3199,MCDA in medicine,44,14,47,Multiple Criteria Decision Analysis,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,50 [building - 324],"['Decision Analysis', 'Health Care', 'Medical Applications']",WC-47
"Being able to explain feature importance when explaining model predictions has been the main focus of Explainable Artificial Intelligence [XAI] methods. However, many of them assign importance values to single variables instead of taking interactions between variables into account, an effect that usually appears in real life problems. In this work we present a comparison study between some extensions of SHAP values [one of the most widely used interpretability methods] to include interactions, and a novel interpretability approach for neural networks named NN2Poly, which in this study is also used in a surrogate manner to explain other kind of models. Extensive simulations are carried out under different settings, both local and global explanations are compared and ways of computing comparable importance order metrics are presented.",Explaining hidden variable interactions inside a model -  a comparison study between different methods.,"[70820, 70860, 63237, 70861]",123,"[8, 42, 66]",3202,Unraveling the Black Box - Advances in Model Explainability,15,13,27,Mathematical Optimization for XAI,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,047 [building - 208],"['Artificial Intelligence', 'Expert Systems and Neural Networks', 'Machine Learning']",WB-27
"As one of the most extensively studied problems in operations research, the Vehicle Routing Problem [VRP] attracts considerable academic and practical interest. The state-of-the-art exact approaches for VRPs rely on the integration of cut and column generation within Branch-Cut-and-Price [BCP] algorithms. A critical component of modern BCP algorithms is the separation of non-robust cuts, such as rank-1 cuts with limited memory, which are considered strong but affect the structure of the pricing subproblem. In this research, we investigate algorithms for separating rank-1 cuts aimed at being both effective and efficient. The study involved implementing these algorithms within the VRPSolver BCP framework and comparing its performance against the existing local search-based separation method. Preliminary results of this study for the classic Capacitated Vehicle Routing Problem [CVRP] will be presented.",On the Separation of Rank-1 Cuts in the VRPSolver Framework,"[78347, 73369, 67821, 13622]",755,"[145, 14, 109]",3205,Column Generation for Vehicle Routing,5,4,58,VeRoLog - Vehicle Routing and Logistics,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S07 [building - 101],"['Vehicle Routing', 'Combinatorial Optimization', 'Programming, Integer']",MC-58
"In Supply Chain Network Design [SCND], the decision maker’s task is to decide upon locations for production or distribution facilities, product allocations to warehouses and factories, customer allocations and the choice of the transport modes. These decisions do not only have an impact on the economic performance of the supply chain but also on environmental indicators like the emission of CO2. To account for trade-offs, multi criteria SCND-modelling approaches are discussed increasingly.
In this paper, we will present a bi-objective SCND modelling approach which explicitly considers the selection of the mode of transport, as one possible leaver in the reduction of CO2. We model specific characteristics of transport modes, like the lead time and the shipment size and their influence on safety stock and cycle stocks. Through this, we are able to model trade-offs, which so far are not considered in the literature on multi criteria SCND-models. We describe a solution procedure, which includes the conversion into a mixed integer conic quadratic optimisation model, demonstrate the applicability within a case study and discuss computational and managerial insights. Further we give an outlook on the possible error supply chain decision makers can make, when ignoring those effects in the application of multi criteria SCND models.
",Bi-Objective Supply Chain Network Design with Transport Mode Selection,[70823],484,"[138, 40, 79]",3206,Sustainable Supply Chain Design,19,2,24,Sustainable Supply Chains,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,83 [building - 116],"['Supply Chain Management', 'Environmental Management', 'Network Design']",MA-24
"This research addresses the production planning challenges inherent in the bicycle manufacturing industry, focusing on the optimization of bicycle production with demand deadlines. Our approach uses a time-dependent bill of material [BoM] structure, considering the availability of components to meet daily production capacities, guided by forecasted monthly demand objectives. The planning horizon spans six months, requiring solutions that balance quality and computational complexity. Leveraging techniques from integer linear programming and heuristics, we aim to efficiently generate production plans for different types of bicycles composed of many different components. Our methodology uses a multi-ranking system to select bicycles to produce so that we balance demand, producibility, and planning complexity. This strategy fits with the unpredictable nature of the bicycle market, enabling quick responses to updated demand forecasts while maintaining planning robustness.",Flexible production planning using BoM structure and time-varying demand objectives,[77585],805,"[84, 111, 138]",3207,Lot-sizing with industrial applications I,32,4,49,"Lot Sizing, Lot Scheduling and Production Planning","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,M1 [building - 101],"['Optimization Modeling', 'Programming, Mixed-Integer', 'Supply Chain Management']",MC-49
"Dynamic pricing is a widely used strategy for adjusting the price of goods and services in response to changing and demanding consumer behavior. In grocery retailing, this approach is often applied to perishable products to encourage consumers to purchase items with limited remaining shelf life. However, existing pricing strategies often overlook the impact of sales cannibalization among similar products and lack differentiation in discounts across the product spectrum. 
In this study, we develop a dynamic pricing approach for a variety of perishable items with a focus on optimizing retail profits and reducing food waste. We first test assumptions about substitution effects among similar products using historical data from a European retailer. We then develop a demand forecasting model for a subset of the products and incorporate it into a reinforcement learning algorithm. This algorithm determines the pricing strategy for the selected set of products, establishing continuous discounts throughout their shelf life. Finally, we analyze the impact of considering multiple products rather than a single product at a time to develop the pricing policy. 
This research provides insights into a methodology for constructing dynamic pricing strategies and uncovers practical implications.",Dynamic pricing strategy for substitute perishable products,"[71584, 43398, 73154, 23114]",424,"[124, 137, 66]",3208,Food Waste,30,2,50,Retail Operations,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,M2 [building - 101],"['Revenue Management and Pricing', 'Strategic Planning and Management', 'Machine Learning']",MA-50
"This study tackles an inventory control challenge in a complex multi-echelon distribution system, drawing insights from Air Liquide's closed-loop supply chain with product returns. The inherent complexity arises from the distribution structure and the handling of returned cylinders. The goal is to frame this as a multi-echelon periodic review inventory control problem and identify the optimal control parameter.

Conventional stochastic programming approaches prove impractical due to the curse of dimensionality resulting from unmet customer demand and potential lost sales. In response, we introduce a heuristic approach, validated through simulation-optimization. This heuristic involves breaking down the multi-echelon distribution system into serial systems, optimizing target stock levels for each echelon, and aligning cumulative backorder quantities by aggregating shared echelon stock levels. The heuristic achieves a 0.74% average reduction in total inventory cost, demonstrating comparable results to simulation-optimization, with the notable advantage of rapid computation in seconds for practical real-world applications.

Applying the heuristic to real-world data enhances inventory control in the multi-echelon distribution system, leading to a 12.63% reduction in stock costs for regularly demanded products and a 21.89% reduction for sporadic demand. These findings highlight the benefits of a global inventory control approach in a multi-echelon distribution system.",A heuristic for an inventory control policy in a closed-loop distribution multi-echelon supply-chain with returns,[49338],833,"[138, 61, 136]",3209,Stochastic inventory systems,32,12,49,"Lot Sizing, Lot Scheduling and Production Planning","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,M1 [building - 101],"['Supply Chain Management', 'Inventory', 'Stochastic Optimization']",WA-49
"In this work, we investigate the problem of reusing dug materials in a large-scale railway building construction project. The project is inspired by a real-world scenario from HS2, the company responsible for building a 225-kilometer fast train line in the UK. The company estimates that the dug material will amount to around 132 million tons, and material imported for construction will be around 20 million tons. The dug material can often be repurposed for fill operations to minimize environmental impact. In this research, we model the problem of allocating dug materials to fill operations, considering operational and financial constraints. We propose two approaches to solve the problem. The first one consists of an integrated MILP model, while the second one is a heuristic algorithm that initially solves a simplified version of the integrated model for each month of the time horizon, and then calls another MILP that tries to find a feasible solution. Both approaches were tested on instances generated based on real data provided by the company. Preliminary results show that the first approach is only capable of finding optimal solutions for small and medium sized instances. On the other hand, the second one is significantly more efficient in terms of runtime required to find feasible solutions, especially for larger instances, but it struggled to find the optimal solutions obtained by the first approach.",Optimizing the usage of cut and fill materials in a large-scale railway construction project,"[77925, 73329, 77178, 67821]",243,"[111, 122, 14]",3211,Models and algorithms for real-life combinatorial optimization problems,64,12,52,Combinatorial Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,8003 [building - 202],"['Programming, Mixed-Integer', 'Railway Applications', 'Combinatorial Optimization']",WA-52
"We study the problem of allocating compensation for a collective effort of competing market participants, such as rewarding market-makers tasked to maintain active price quotations and low bid-ask spreads. Several countries regularly issue government bonds, typically via an auction, and the secondary market stability is important for success of future auctions. In order to achieve that, participation in auctions is restricted to selected bidders who are in turn required to be active in the secondary market. Furthermore, some debt management offices provide annual monetary compensation to incentivize the secondary market performance. We develop a framework to design optimal allocation of the annual incentive amount for individual secondary market performance over time and multiple markets. The proposed scheme is compared to the one currently used by the Danish Debt Management Office.",Rewarding Primary Dealer Performance,"[72196, 75468, 57023, 62167]",639,"[33, 50]",3212,Market Design 1 - Auctions,87,10,43,Market Design,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,99 [building - 306],"['Economic Modeling', 'Game Theory']",TD-43
"The “Subject to” [s.t.] podcast offers a series of informal conversations with relevant figures in the fields of Operational Research, Combinatorial Optimization and Logistics, and they are hosted by Anand Subramanian, an Associate Professor at Universidade Federal da Paraíba [UFPB], Brazil. The episodes can be accessed on YouTube [https://www.youtube.com/@ Subjectto_], as well as various podcast platforms such as Spotify, Google, Apple, etc. The goal of the s.t. podcast is to inspire the next generation of operation researchers by means of informal yet in-depth conversations with great names in the field of OR in the form of oral history. Moreover, in addition to providing technical discussions, the podcast attempts to portray the human side of the guests, otherwise generally known only for their work, by walking through their professional and personal life stories, and presenting a snapshot of their career within the relevant historical OR context. The episodes often touch on societal elements such as gender and diversity issues, and other pertinent nontechnical topics like the best strategies to better promote the field. So far, there have been more than 85 episodes with many important names from 28 different nationalities spread across 22 countries all over the world. This contribution aims to share some insights gained from experiences of distinguished personalities, predominantly from academia, but also from the industry.","The “Subject to” [s.t.] podcast - sharing experiences, gaining Insights",[67821],19,"[88, 106, 92]",3213,Moments in the history of OR  1,27,13,20,Moments in the history of OR,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,45 [building - 116],"['OR History', 'Profession of OR', 'OR in Education']",WB-20
"Our purpose in this talk is to present some results related to the study of solitons of the spacelike mean curvature fow in a generalized Robertson-Walker [GRW] spacetime. Under suitable constraints on the warping function  and on thecurvatures of the Riemannian fiber, we apply suitable maximum principles in order to obtain nonexistence and uniqueness results concerning these solitons. Applications to standard models of GRW spacetimes, namely, the Einstein-de Sitter spacetime, steady state type spacetimes, de Sitter and antide Sitter spaces, are given. Furthermore, we establish new Calabi-Bernstein type results related to entire spacelike mean curvature flow graphs constructed over the Riemannian fiber of the ambient spacetime [Joint work with Márcio Batista, Henrique F. de Lima and Wallace F. Gomes New York J. Math. 29 [2023] 554–579].",Solitons of the spacelike mean curvature flow in generalized Robertson-Walker spacetimes,[78419],904,"[31, 0]",3214,	Optimization on Geodesic Metric Spaces I - Smooth case,69,5,41,Optimization on Geodesic Metric Spaces - Smooth and Nonsmooth,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,97 [building - 306],['Dynamical Systems'],MD-41
"Unconstrained optimization problems such as integer linear programs can be relaxed
to the task of finding the ground state of a physical system described by a quantum
state. In the realm of quantum devices having comparably low numbers in qubits
prone to noise, quantum circuits consisting of a sequence of parameterised unitary
gates controlled by classical optimization methods are the prevalent technique of
near-term quantum algorithms. However, the omnipresent phenomenon of barren
plateaus - parameter regions with vanishing gradients - sets a persistent hurdle that
drastically diminishes their success in practice.
In this work, we introduce an approach -based on non-unitary operations - that
favours jumps out of a barren plateau into a fertile valley. These operations are con-
structed from conic extensions of parameterised unitary quantum circuits, relying on
mid-circuit measurements and a small ancilla system. We further reduce the prob-
lem of finding optimal jump directions to a low-dimensional generalised eigenvalue
problem.
As a proof of concept we incorporate jumps within state-of-the-art implementa-
tions of the Quantum Approximate Optimisation Algorithm [QAOA] - a prominent
descendant of the class of variational quantum algorithms. We demonstrate the ex-
tensions' effectiveness on QAOA through extensive simulations, showcasing robust-
ness against barren plateaus and highly improved sampling probabilities of optimal
solutions.",From barren plateaus through fertile valleys - Conic extensions of parameterised quantum circuits,"[78422, 75302]",383,"[5, 52, 53]",3216,Optimization in Quantum Information,83,5,42,Quantum Computing Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,98 [building - 306],"['Algorithms', 'Global Optimization', 'Graphs and Networks']",MD-42
"Effective Operating Room [OR] scheduling holds significant financial implications for hospitals, particularly when dealing with inpatient procedures at the operational level. However, the duration of a surgery is often uncertain and it is rather challenging to accurately predict. Estimates can vary significantly across different departments, leading to various extents of underestimations. As a result, it exacerbates nurse shortage issue in hospitals. To address this, we propose an approach to build robust surgery schedules in an OR theatre made of several rooms with different specialties and sharing nurses. Our method utilizes a two-stage stochastic optimization model. In the first stage, we determine the number of nurses working each day, while the second stage focuses on creating the detailed schedule. Additionally, we adopt the sampling average approximation [SAA] technique to address uncertainties in surgery duration. Based on the samples generated from real-life data, the simulation results imply that i] the surgery schedule could be fully scheduled with fewer number of nurses; ii] the nurse shortage/absence issue can be potentially avoided as we can accommodate uncertain duration better.",Operating Room Scheduling under Human Resource,"[77873, 41899]",965,"[56, 136, 129]",3218,Surgery Scheduling and Operating Room Planning [2],3,15,15,OR in Health Services [ORAHS],"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,18 [building - 116],"['Health Care', 'Stochastic Optimization', 'Scheduling']",WD-15
"We propose a framework that allows to quantitatively analyze the interplay of the different agents involved in gas trade and transport in the context of the European entry-exit system. While previous contributions focus on the case of perfectly competitive buyers and sellers of gas, our novel framework considers the mathematically more challenging case of a strategic and monopolistic gas seller. We present a multilevel framework that is suitable to capture the sequential nature of the decisions taken. We then derive sufficient conditions that allow for reformulating the challenging four-level model as a computationally tractable single-level reformulation. We prove the correctness of this reformulation and use it for solving several test instances to illustrate the applicability of our approach.",On a Tractable Single-Level Reformulation of a Multilevel Model of the European Entry-Exit Gas Market with Market Power,"[51418, 48995, 23956, 44863, 71656, 47006]",838,"[93, 37, 50]",3219,OR in Gas Networks ,23,7,19,OR in Energy,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,44 [building - 116],"['OR in Energy', 'Energy Policy and Planning', 'Game Theory']",TA-19
"Michelin, founded in 1889 in France, has evolved into one of the world's leading tire manufacturers. The company's commitment to research and development has propelled it to the forefront of tire technology, ensuring that its products meet the evolving needs of its various clients. This presentation is motivated by a real tire production line which, even though it is highly automated,  still requires more than one operator to run. Operators play a crucial role in verification processes and are instrumental in resolving unforeseen issues. 
Our goal is to schedule the operations on the production line more efficiently, but also to guide the operators when random event occur. Actually, some of these random events should be considered when determining the schedules.
The problem at hand can be defined as a flexible job-shop scheduling problem, where the routing of the operations of each job includes automated and manual operations. As all the automated operations are performed by the same machine, we are interested in scheduling the manual operations that should be assigned to and scheduled on one operator among the eligible operators. The objective to minimize is the flow time of the tires for a given team of operators.
The presentation will detail how the problem is modeled and introduce the solution approaches for the deterministic case and for the stochastic case where random events are taken into account. Computational experiments on industrial data will be discussed.
",Scheduling manual operations on tire manufacturing machines,"[69921, 16259, 16596]",409,"[129, 0]",3220,Manufacturing scheduling with sustainability considerations,35,12,60,Project Management and Scheduling,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S09 [building - 101],['Scheduling'],WA-60
"This research explores the impact of weather uncertainties on the ability of drones to improve pathology collections over the business-as-usual road-based logistics system in the Solent area of the UK. Weather uncertainties impact on the manoeuvrability and speed of drones, which can lead to delays and incidents. In this paper, we present a novel framework that combines a machine learning drone flight duration prediction model with a meta-heuristic Bees optimisation algorithm to solve the drone-cargo bike-van multimodal medical logistics problem in the presence of variable weather conditions. 

Experiments have been conducted to evaluate how varying wind conditions affect drone schedule efficiency when used in combination with other transportation modes [cargo bikes and vans], and quantify the logistics system's overall economic and environmental impacts as a result. The case studies used concern the routine collection of pathology samples from doctors surgeries across the Southampton and Portsmouth area of the UK for delivery to pathology laboratories.

This study is part of the Solent Future Transport Zone project [FTZ] funded by the UK’s Department for Transport and led by Solent Transport.","Multi-modal routing and scheduling of drones, cargo bikes and vans in the presence of weather uncertainties for pathology collections in the Solent Area","[78330, 22418, 18962, 78392, 24355, 78429]",953,"[56, 65]",3222,Healthcare logistics and routing,3,5,10,OR in Health Services [ORAHS],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,11 [building - 116],"['Health Care', 'Logistics']",MD-10
"The transition from diesel vehicles to tailpipe emission free technology such as battery electric trucks [BETs] in surface transportation is crucial to meet the ambitious decarbonization goal set by the European Union. While the first models of electric heavy goods vehicles have become available, the charging infrastructure necessary to operate these vehicles is still largely absent. At the same time, current BET models exhibit limited range and increased refueling times, which creates a number of challenges in both the operational and strategic planning domain. We present work on mathematical models and algorithms to support decision making for the placement of BETs and corresponding charging infrastructure. Our efforts are directed towards internal strategic planning as well as collaborations with industry, academic partners and public policy. The goal of our external collaborations is to foster the coordination of charging infrastructure investments across multiple parties, which will help to accelerate the transition to carbon free transportation.",Optimized demand-based charging networks for long-haul trucking in Europe,"[77791, 80043, 80044]",290,"[100, 143, 65]",3224,Scheduling and sustainability,92,13,57,Optimization at Amazon,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S06 [building - 101],"['OR in Sustainability', 'Transportation', 'Logistics']",WB-57
"Digital transformation has become a worldwide phenomenon and has a significant impact on the functioning of organisations in many areas. In our research, we focus on the application of digitisation in the field of human resource [HR] management. There are many possibilities for utilising digitisation in HR, e.g., employee recruitment, more efficient planning of interviews, productivity measurement, psychodiagnostics tests, adaptation of new employees, and other areas. A questionnaire has been developed to determine i] the extent to which digitisation is implemented in organisations in the Czech Republic, ii] which specific elements of digitisation are used, iii] what consequences organisations observe in this context, and iv] whether barriers to the introduction and implementation of digitisation are observed in relation to the age of employees. Statistical methods are used for analysis in which we focus on whether the implementation of digitisation varies depending on the size of the organisation or the economic sector. Attention is also paid to whether it can be argued that the benefits of digitisation increase disproportionately when implemented in multiple areas. We employ the GUHA method to discover the knowledge from the data and generate statistical hypotheses. Hypotheses are tested by means of the chi-square test. Additionally, selected multidimensional statistical methods, such as factor or cluster analysis, are also applied to evaluate the obtained results.",Analysis of the implementation of digitisation in human resource management depending on the size of the organisation or economic sector,[48733],210,"[57, 0]",3226,Analytics and the link with stochastic dynamics II,17,8,31,Analytics,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,54 [building - 208],['Human Resources Management'],TB-31
"We explore a consistent vehicle routing problem for home health care under uncertain travel and service times. In this problem, the planner must schedule and route a group of care workers to visit patients multiple times over a planning horizon while ensuring service consistency so that each patient will be serviced by the same care worker at the same time slot every time the service is provided. This problem variant extends the home health care routing and scheduling problem [HHCRSP] to include service level constraints under stochastic travel and service times. We propose a stochastic solution framework where the uncertainty is represented by [i] a discrete scenario set and [ii] an extreme value theory-based [EVT-based] approximation. A unified branch-and-check algorithm that leverages constraint programming [CP] to handle the subproblem is developed to efficiently solve the stochastic models with these uncertainty representations.  Results show that the stochastic optimization framework can efficiently handle practical benchmark instances and produce schedules with significantly improved service levels than the deterministic model.",Models and solution approaches for the consistent vehicle routing problem under stochastic travel and service times,"[57787, 78436, 53667]",281,"[136, 145, 129]",3227,Robust and Stochastic Routing Problems,49,2,35,"Stochastic, Robust and Distributionally Robust Optimization","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,44 [building - 303A],"['Stochastic Optimization', 'Vehicle Routing', 'Scheduling']",MA-35
"We proposed a mixed-integer linear programming [MILP] for the consistent vehicle routing problem with time windows [ConVRPTW], which is solved with a Tabu Search metaheuristic. This study is motivated by the necessity of increasing the service level of companies in the logistics area. The study considers a schedule of routes in multiple periods, where the set of clients and their demand fluctuates in the planning horizon. The objective is to minimize the length of routes fulfilling the requirements of service consistency. The solution is evaluated through Key Performance Indicators[KPIs] to analyze the balance between routes, design strategies to perform routing plans, and improve future benchmarks in the literature.",Key performance indicators for consistent vehicle routing problem,"[74280, 71223]",978,"[145, 65, 74]",3228,Vehicle routing II,6,3,60,Transportation,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S09 [building - 101],"['Vehicle Routing', 'Logistics', 'Metaheuristics']",MB-60
"The need for a socio-ecological transformation of society is being emphasised in different contexts. Tomorrow's business leaders will play a key role in this transformation. Therefore the call for more Responsible Management Education [RME] is becoming steadily louder. In response, the United Nations-backed Principles for Responsible Management Education [PRME] initiative was founded, aiming to provide future leaders with the skills they need to contribute to a sustainable development. 
Three curricular dimensions influence, whether students gain these skills - the formal, the informal and the hidden curriculum. Aims of RME can only be reached if all three dimensions are aligned. However, currently scholars pay little attention to the hidden and informal curriculum.
This research aims to generate knowledge about the hidden curriculum. Moreover, connections between formal and hidden curricula shall be researched. 
The hidden curriculum will be examined through questionnaires and group discussions, including stimulated recalls. The formal curriculum will be raised through a documentary analysis. Discussions and documents will be evaluated with qualitative content analysis. 
The results will presumably provide profound knowledge concerning the interaction mechanisms of formal and hidden curricula. The outcome can help researchers develop RME implementation models. Moreover, this knowledge can support lecturers in questioning and improving RME in their institutions.
",The Role of the Multi-dimensional Learning Environment in the Context of Responsible Management Education,[78428],791,"[41, 92, 15]",3229,OR Education I,48,2,16,OR Education,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,19 [building - 116],"['Ethics', 'OR in Education', 'Complex Societal Problems']",MA-16
"The Malmquist Productivity Index [MPI], a major DEA advancement, evaluates productivity changes over time in decision-making unit [DMU] with multiple inputs and outputs. This paper addresses the underexplored scenario of a single input with multiple outputs or vice versa. Existing MPI models often overlook non-Archimedean epsilon, but our novel approach incorporates it in scenarios featuring a single input or output. An efficient procedure is proposed for problem-solving in both input- and output-oriented cases. Theoretical validation through theorems is complemented by a practical analysis of productivity growth across 18 OECD countries from 2005 to 2021, demonstrating the methodology's effectiveness.",Exploring Productivity Growth - A Data Envelopment Analysis in Single Input/Output Scenarios,"[78433, 78186, 77849]",945,"[24, 35, 110]",3230,DEA methodological developments I,89,14,48,Data Envelopment Analysis and its Application,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,60 [building - 324],"['Data Envelopment Analysis', 'Efficiency Analysis', 'Programming, Linear']",WC-48
"Kidney transplant is crucial for end-stage renal disease patients, yet demand for kidneys surpasses supply. Organs can be obtained from both deceased and living donors, with compatibility between patient and donor being a pivotal factor in the matching process. Kidney Exchange Programs [KEPs] aim to address the kidney shortage issue by facilitating exchanges between living incompatible donor-recipient pairs, either through cycles or through chains initiated by a living donor. To boost transplant rates, the use of deceased donor kidneys to trigger transplant chains has been proposed, which involves the last donor in the chain donating back to the deceased donor waiting list. This study investigates when to use deceased donors to initiate chains, aiming to maximize the total number of transplants, while ensuring patient types on the deceased donor waiting list are not disadvantaged. First, we use an offline Integer Linear Programming framework to assess the efficacy of using deceased chain-initiating kidneys [CIKs] within the existing practice. Building on this foundation, we introduce a stochastic model using Sample Average Approximation to address the dynamic and uncertain nature of the problem. Through simulation, using secondary data from the National Kidney Registry between 2012 and 2016, we show that deceased CIKs can potentially enhance patient outcomes in terms of the number of transplants and wait times, but the results are sensitive to the characteristics of the KEP.","Wanted, Dead or Alive - Leveraging Deceased Donors for Enhanced Patient Outcomes in Kidney Exchange Programs","[78432, 22950, 59035, 61302]",951,"[56, 0]",3233,Kidney Exchange II,3,10,10,OR in Health Services [ORAHS],"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,11 [building - 116],['Health Care'],TD-10
"The transition towards more sustainable food systems is essential for achieving climate targets and the Sustainable Development Goals [SDGs]. Achieving goal 12, responsible consumption and production, in particular, will require changes in consumer behavior,  the adoption of sustainable practices by suppliers, and, most importantly, the implementation of policies that foster such sustainable behaviors. Whether it is the availability of supply or the increasing demand that drives sustainability transient consumption is often discussed in economic circles. From the complex systems perspective, it is their interaction that matters. This article explores how consumer preferences and restaurant menus co-evolve to contribute to a more sustainable food system and how this co-evolution is affected by different policy instruments. We use a spatially explicit agent-based model of the catering industry in Amsterdam as a case study. The model is built using spatial microsimulation to expand on survey data from a discrete choice experiment about restaurants in Amsterdam. We observe that in the absence of policy interventions, it takes quite a lot of time for changes in diet patterns to occur. Multiple cities are working on their sustainability. Based on the generated insights, we expect our research to contribute to the current debate on the policy interventions to achieve sustainable urban food systems and the SDGs, and to be of consequence across multiple cities worldwide.  ","Steering sustainable food systems - the complex co-evolution of consumer preferences, sustainable restaurants, and policymaking ",[78431],601,"[10, 33, 100]",3234,Simulation in sustainability,77,8,43,"Agent-based Models in Management, Economic and Organisation Sciences","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,99 [building - 306],"['Behavioural OR', 'Economic Modeling', 'OR in Sustainability']",TB-43
"Copper, an essential mineral for renewable energy technologies, faces significant supply chain disruption risks due to its uneven global distribution. This research focuses on the vulnerabilities within the copper industry's supply chain, which is extensively interconnected from the extraction of raw materials to the production of finished goods.

This study leverages complex network theory to develop a weighted, multi-layer supply chain network model at the firm level within the copper industry, utilizing real-world data from the FactSet Supply Chain Relationship database. It examines the network's structural characteristics and employs degree-based removal strategies to simulate targeted attacks on the network, offering insights into its robustness.

This study enriches the literature by providing an in-depth analysis of the copper supply chain at the firm level, employing dynamic risk transmission modeling to explore supply risks across firm, industry, and regional levels. The study not only identifies vulnerabilities but also proposes strategies to enhance network resilience. Through this comprehensive framework, we offer new insights into mitigating supply chain vulnerabilities in the copper industry, highlighting the critical role of network structure and interdependencies in supply chain resilience.
",Multi-layer Supply Network Dynamics and Resilience in the Copper Industry - A Complex Network Theory Analysis,"[77623, 79756]",513,"[126, 97, 78]",3235,Raw Material Supply Chains,19,3,24,Sustainable Supply Chains,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,83 [building - 116],"['Risk Analysis and Management', 'OR in Mining', 'Natural Resources']",MB-24
"This work tackles the issue of waste collection for a company situated in the Northen Italy. The project aimed to streamline waste collection planning through the implementation of a dedicated optimization engine. This problem involves collecting various types of waste from multiple customers, which then need to be transported to specific landfills. Compatibility constraints arise from the nature of the waste, the vehicle type, or the container loaded onto the vehicles. The collection process can unfold in various ways - containers may be loaded directly onto the waste management company’s trucks and then returned empty to the original customer, or the loads [always equivalent to full containers] can be transferred onto the containers attached to the trucks and transported to the landfill. Moreover, the service must adhere to specific time windows associated with depot, customers, and landfills. The problem is defined as a roll-on roll-off pick-up and delivery problem with time windows. Given its complexity, a three-phase covering heuristic is developed. In the first phase, the algorithm clusters customers into subgroups; in the second phase, route generation takes place, while in the third phase, a modified version of the Chvátal heuristic is implemented to determine a feasible solution, with the aim of minimizing costs and ensuring the maximum coverage of served customers. The approach was tested using real data instances provided by the partner company.",A Roll-On Roll-Off Vehicle Routing Problem for Industrial Waste Collection - A Case Study in Northern Italy,"[54545, 13546, 78438, 78437]",777,"[145, 59, 125]",3236,Waste Collection,5,9,58,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S07 [building - 101],"['Vehicle Routing', 'Industrial Optimization', 'Reverse Logistics / Remanufacturing']",TC-58
"Using systems theory and the tools of systems dynamics the worldwide spread of the covid19 pandemic can be analyzed as an effect of intrusions into ecosystems. The spreading of the covid19 virus started in 2019 in China as a local intrusion into an ecosystem. Subsequently the spreading of the virus in the word in 2019, 2020 and 2021 can be explained from this perspective. Thereby taking into account the globalizing of our world and the increasing influence of social media in the communication about and handling of the pandemics. The variables that explained the expanse of the pandemic are summarized in a global systems dynamic map. In this context the effect and efficacy of the  governance policies to handle the pandemic is analyzed. Thereby taking into account economic inequality in and between countries and associated social unwanted effects on the present and next generation. Also the unexpected and disturbing outcomes of linear and nonlinear feedback loops on the way people reacted on government policies and the role social media played is studied. Thereby at last focusing on the ethical questions concerning wanted and unwanted effects of those policies.  As a case study, with the aid of the system dynamics map a simulation model is developed to evaluate the policy of the government in the Netherlands and develop for this case alternative scenario’s, with a better balance between wanted and unwanted effects on the population, thereby addressing ethical questions.  ",The ethics of wanted and unwanted effects of government policies to handle the worldwide covid19 pandemic,[78434],77,"[15, 140, 41]",3239,"Ethics and OR, societal complexity and public service",28,4,20,OR and Ethics,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,45 [building - 116],"['Complex Societal Problems', 'System Dynamics and Theory', 'Ethics']",MC-20
"Cartels in Latin America pose a significant threat to societal stability and public safety, perpetuating extensive violence and socio-economic disruption. Despite substantial investments in security measures by states like Mexico and the U.S., violence continues to escalate, and the power of cartels remains resilient. In this presentation we examine the effectiveness of current strategies and propose a novel approach using optimal control theory to analyze cartel dynamics. By modelling the allocation of resources between security initiatives and social programs, this study aims to identify an optimal strategy for reducing cartel violence and mitigating its impacts on society. Through this analytical lens, we offer insights into potential avenues for more effective policymaking and resource allocation in the ongoing fight against organized crime.",The optimal mix of incapacitations and social programs for fighting cartel violence in Mexico,"[4861, 39239, 23372, 4860, 78441, 19666, 78443]",847,"[20, 15, 101]",3240,Optimal control in organizations,90,5,33,Optimal Control Theory and Applications,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,42 [building - 303A],"['Control Theory', 'Complex Societal Problems', 'OR/MS and the Public Sector']",MD-33
"We study how a firm's pricing flexibility to potentially increase its prices during a disruption impacts its decisions on the level of reserve inventory or reserve capacity to carry in anticipation of disruptions.  While the reserve inventory provides a certain quantity of inventory that the firm can continue to sell during a disruption, reserve capacity allows the firm to maintain production during a disruption at a certain production rate.  Specifically, we consider a firm producing a single product and that is exposed to random disruptions.  The firm first decides on the level of reserve inventory or reserve capacity to carry in anticipation of disruptions.  When a disruption does occur, it may then choose to adjust its price to better align demand with the supply restrictions during the disruption.  We characterize the firm's optimal pricing decisions during disruptions, and the level of reserve inventory or reserve capacity to hold in anticipation of disruptions.  We find that a certain level of pricing flexibility may be required to justify holding reserve inventory or reserve capacity.  Further, we also show that while a firm's pricing flexibility during a disruption and reserve capacity decisions are substitutes, pricing flexibility and reserve inventory may act either as substitutes or complements, that is, a firm may be incentivized to carry a lower or higher amount of reserve inventory if it possesses further flexibility to increase its prices during a disruption.",Disruption Mitigation and Pricing Flexibility,"[78435, 78485]",263,"[124, 138, 61]",3241,Pricing and applications 3,11,2,59,Pricing and Revenue Management,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S08 [building - 101],"['Revenue Management and Pricing', 'Supply Chain Management', 'Inventory']",MA-59
"The International Energy Agency [IEA] recently revealed that - “Strong GDP growth and a weak monsoon drove up India's energy-related carbon emissions by around 190 million tonnes in 2023, though the country's per capita emissions remain far below the global average”. Given its target growth rate of 9% of reaching an economy of $5 trillion by 2024-25, it is an immense challenge for India to meet the growth target while keeping its GHG emissions under control. While SDGs 7 & 13 call for affordable and clean energy & climate action respectively by 2030, improving energy efficiency while keeping the GHG emissions under control could be one of the key paths to address the issues of energy security, climate change and economic stability for India. Further, India is characterized by considerable differences in energy efficiency in different regions resulting from differences in their economic and demographic trends, resource availability and industrial profiles. Given this background, this paper aims to - 1] identify the potential sources which influence India’s GHG emissions at regional level 2] explore various methods of decomposition analysis viz. IDA, SDA and PDA and apply the most appropriate amongst them to investigate potential sources on GHG emission changes 3] to extend the authors’ research presented at IFORS-2023 by performing decomposition analysis with GDP as the desirable output and GHG emissions as the undesirable output respectively.",Energy consumption decomposition analysis in developing countries with specific reference to India,"[57516, 3524]",55,"[28, 29, 94]",3242,Sustainable Economics in Developing Countries  ,67,13,18,OR for Development and Developing Countries,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,42 [building - 116],"['Developing Countries', 'Development', 'OR in Environment and Climate change']",WB-18
"Fantasy Premier League is a “play along” fantasy game that follows the real-world events of Premier League games. It allows Premier League fans to compete against each other in their knowledge of players and teams. Players can set their own teams of players in the Premier League, who then earn points through their real-world performance in the Premier League. There is a substantial body of research on American Football Fantasy Leagues. However, possibly during to the low scoring nature of soccer as compared to American football, the Fantasy Premier League has not drawn as much attention. We show how to incorporate publicly available information into frequentist and Bayesian statistical models that will be used in optimization model to select teams for each week. ",Optimisation for the Fantasy Football League,"[59052, 23431, 78459]",664,"[99, 7]",3243,Sports scheduling and optimization,37,7,16,OR in Sports,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,19 [building - 116],"['OR in Sports', 'Analytics and Data Science']",TA-16
"The increasing integration of solar photovoltaic [PV] power poses challenges for power system operation. Accurate forecasts of PV power are both financially beneficial for electricity suppliers and necessary for grid operators to optimize operation and avoid grid imbalances. This talk will explore a framework utilizing conformal prediction [CP], an emerging probabilistic forecasting methodology, to assist decision-making for PV power market participants on the day-ahead market [DAM]. It will begin by demonstrating how machine learning models can be employed to construct the point predictions based on weather forecasts. Subsequently, various variants of conformal prediction are introduced to quantify the uncertainty of these predictions. The talk will then explore the application of several market bidding strategies, including trust-the-forecast, worst-case, Newsvendor and expected utility maximization, to facilitate decision-making for market participants on the DAM using CP methods. Through a case study in the Netherlands, it will highlight how CP when combined with certain bidding strategies can lead to increased profit with minimal energy imbalance, outperforming classical probabilistic forecasting methods, such as linear quantile regression. The talk will conclude by highlighting the need to expand the framework to cover other short-term markets, namely intra-day markets.",Conformal Prediction for Stochastic Decision-Making of PV Power in Electricity Markets,[76656],344,"[36, 47, 136]",3246,Energy transition and operations,21,5,22,Energy Management,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,81 [building - 116],"['Electricity Markets', 'Forecasting', 'Stochastic Optimization']",MD-22
"Access to clean drinking water can be a luxury in impoverished communities in the United States. Nationally, more than 2 million people lack access to safe drinking water and indoor plumbing, and this figure does not include homes with a working tap of unsafe drinking water. Therefore, a far larger number of Americans face water insecurity. A recent examination of US Census and Safe Drinking Water Act violation data reveals that rural, low-income and minority communities are significantly more likely to be burdened with unavailable or unsafe in-home drinking water. Among community advocates, practitioners, and researchers, a consensus has emerged - alternatives to traditional infrastructure must be explored in the U.S. One promising alternative is delivering water to residences without access to reliable, clean water sources. This research explores how to optimally deliver drinking water to households by adapting the inventory routing problem [IRP] and creating heuristic algorithms to schedule efficient water delivery. The IRP and heuristics inform three key decisions - when to serve a customer, how much to deliver, and which delivery routes to use. These algorithms will be verified through implementation in central Appalachia and the Navajo Nation with a nonprofit organization dedicated to combating the water crisis in the US.",Adapting the inventory routing problem to optimize water deliveries in water insecure areas,[78440],543,"[145, 147, 65]",3247,Facilities Routing and Planning in Developing Countries,67,14,18,OR for Development and Developing Countries,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,42 [building - 116],"['Vehicle Routing', 'Water Management', 'Logistics']",WC-18
"In this work, we address the Min Max Multi-Trip Location Arc Routing Problem [MM-MT-LARP]. We consider a depot from which a set of P trucks, each one carrying a drone, must travel to P out of D available points, where the drone is launched. Each drone has a limited autonomy which allows it to fly a maximum time L before having to get back to the launching point to change its battery so that it can start another route. Once the drone has completed all its routes, the truck goes back to the depot. The goal of the MM-MT-LARP is to determine the launching point for each truck and find a set of drone routes for each truck, each one starting and ending at its launching point and with flight time not greater than L, in such a way that all the drones’ routes jointly traverse all the given edges and the largest total time of all the trucks [time of traveling to the launching point, flight time of the drone and time of traveling back to the depot] is minimized. In this talk, we present a route-based formulation for the problem and a branch-and-price algorithm to solve it, as well as some preliminary computational results on a set of instances with different characteristics.",A Branch and Price for the Min Max Multi-Trip Location Arc Routing Problem,"[77417, 9794, 60281]",743,"[145, 14, 143]",3249,Arc Routing,5,8,64,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S16 [building - 101],"['Vehicle Routing', 'Combinatorial Optimization', 'Transportation']",TB-64
"This study focuses on solving routing problem of industrial inspection robots by using mathematical model. Inspection robots are generally used to control. Especially in the production of bus, inspection of body in white of a bus has approximately 3000 subparts and all of them must be controlled. Since inspection time is important, routing of robot[s] is crucial. Robot arm starts from a starting point and moves another check point. When it finishes the control, traverses another check point until ensures all sub parts are controlled. However, in this process, determining the control points and calculating the shortest path distance between all pairs of nodes is important. Distance between check points should not be greater than sensor observation capabilities. Additionally, robot's movements are limited because of obstacles of body in white. Finally, a good route should be determined by visiting some check points.
In this study the shortest path distances was calculated by using Floyd-Warshall algorithm and an integer linear mathematical model so as to minimize total traversing and inspection time is developed based on TSP. However, in this model, robot does not need to visit all check points in the network. The developed model was successfully validated on various test problems and the optimal solution could be obtained for small size problems. It was shown that a good solution can be obtained within 5 minutes for the problem having more than 1000 sub-parts with 170 nodes. ",A Mathematical model for routing of Industrial Inspection Robots,"[75791, 3614, 20156]",866,"[14, 145]",3250,Topics in Integer Programming II,64,13,25,Combinatorial Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,011 [building - 208],"['Combinatorial Optimization', 'Vehicle Routing']",WB-25
"We perform a cluster analysis on treatment plans of rehabilitation patients. Rehabilitation patients receive multidisciplinary care during treatment, which allows treatment plans to be customised to each patient. We cluster patients with similar treatment plans with the goal of standardising these plans and to make predictions for new patients. We aim to use the clustering model as an input for capacity and planning models.
We partition each treatment plan into patterns based on decision moments where the doctor decides how to continue the patient’s care. The patterns are multivariate count data, as we count the number of appointments per discipline over the duration of the pattern.
We use finite mixture methods to cluster the patterns. Each cluster is modelled as a multivariate Poisson distribution. Given a number of clusters, we use Expectation-Maximization to find the optimal parameters for the model. We use the Bayesian Information Criterion to optimise the number of clusters.
We analyse the treatment plans, now sequences of patterns, using a non-stationary Markov chain. The transition probabilities are estimated from the clustered data. Subsequently, we calculate the probability of a treatment plan to identify common treatment groups.
Finally, we show the validity of the clustering model by simulating new patients and comparing to the actual data. Our method is used by Reade rehabilitation centre in Amsterdam to support the development of standard treatment plans.",Cluster analysis of treatment plans of rehabilitation patients,[77522],973,"[56, 0]",3251,Capacity and treatment planning in healthcare,3,14,10,OR in Health Services [ORAHS],"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,11 [building - 116],['Health Care'],WC-10
"In recent years, an increasing number of companies have been introducing drones into the transport sector, mainly to cope with last-mile distribution issues, being these means of transport more cost effective, sustainable and efficient than traditional modes. Notably, their use in remoted areas can increase accessibility and provide support in case of emergency.
Existing research studies mainly focus on drone components and on routing optimization problems, but there seems to be a lack of papers dealing with the design of distribution chains implementing drones for last-mile logistics services.
This research proposes an application of genetic network programming to optimize the logistics network configuration, with the aim of defining the optimal combination of transport modes serving the implementation of drones in the last-mile distribution. In particular, a multi-criteria method is used to evaluate the fitness of the candidate solutions by combining environmental impacts, transportation performances and costs. Moreover, an ad-hoc crossover operator is developed to preserve the most promising structures during the evolution process. 
The methodology has been tested through an application to a case study in a mountain area in the North-East of Italy.
",Application of Genetic Network Programming to optimize logistics services in mountain areas using drones,"[79799, 78444, 79822]",977,"[143, 53, 26]",3252,Last mile delivery with drones,6,15,56,Transportation,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S04 [building - 101],"['Transportation', 'Graphs and Networks', 'Decision Support Systems']",WD-56
"The development of Electric Vehicles [EVs] has exceeded all expectations. 
At the current pace, opportunities will open up for electric freight transport and aviation. For these industries with tight schedules, however, long battery recharge times will be problematic. The ability to swap depleted batteries may provide an alternative. Swaps can be done quickly, and the EV can be deployed while the used battery recharges at a depot. 

This paper studies scheduling operations of battery swapping and charging for a EV fleet that needs to perform a tight schedule. We propose a stochastic recourse optimization model to size the infrastructure - the charging power and the number of spare batteries. This aims to minimize the infrastructure and charging costs, given a stochastic amount of traffic to be fulfilled. The charging costs are determined in the second stage of the model, which optimizes the charging operations of the vehicles, given a day's traffic schedule. We expand on current studies by leveraging the knowledge of the traffic schedule, and by allowing for preemptive charging.  

We show our method for an airline operating small electric aircraft, where swaps are done at hub airports. We use the domestic network of Norway's Widerøe Airlines of 43 airports and 56000 annual flights. We show that 4400 kW in power supply and 23 spare batteries are required for electrification.  Furthermore, we show that this grants high scheduling flexibility with a smaller infrastructure.",A two-phase stochastic approach for sizing and scheduling of battery swapping and charging of electric vehicles,"[78446, 78452]",650,"[4, 100, 129]",3253,Airline Applications II,6,5,55,Transportation,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S02 [building - 101],"['Airline Applications', 'OR in Sustainability', 'Scheduling']",MD-55
"Assessing responsibility and irresponsibility in anticipation practices, this research questions future-oriented organizational and personal behaviours over time from both factual and symbolic perspectives [Marchais-Roubelat, 2012]. Together with the concepts of responsible futures [Arnaldi et al, 2020, Fuller and Roubelat, 2021] and of responsible foresight [Tonn, 2018, Van der Duin, 2019], the one of responsible anticipation introduces the stance of responsibility in future-making activities.
As a commitment to action [Fuller, 2017, Marchais-Roubelat, 2021], anticipation offers critical viewpoints to assess the responsibility of future-oriented practices in a principle of action for situating acts, their effects and their assessments from multiple moving rules over time. Supplementing axiological and deontological dimensions of anticipation [Danaher, 2021], the principle of action stresses the factual and the symbolic sides of commitments, of their effects, as well as of their assessment.
From the longitudinal study of the sustainability of nuclear energy in anticipation practices from 1950s to present, we suggest that this principle of action be part of a critical approach to assess the responsibility of anticipation practices. Such a responsible anticipation faces the gaps between future and past or present assessments and invite to introduce the risk of anachronism to question responsible or irresponsible behaviours.",Assessing responsible anticipation over time. The factual and symbolic sides of future-oriented practices,"[77894, 77886]",677,"[10, 0]",3254,Scenarios and foresight practices - Behavioural issues III,13,14,11,Behavioural OR,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,12 [building - 116],['Behavioural OR'],WC-11
"First order stochastic optimization methods are effective tools for the minimization of problems arising in deep learning applications. In this work we study a stochastic first order algorithm which rules the increase of the mini-batch size in a predefined fashion and automatically adjusts the learning rate by means of a monotone or non-monotone line search procedure. The mini-batch size is incremented at a suitable a priori rate throughout the iterative process in order that the variance of the stochastic gradients is progressively reduced. The a priori rate is not subject to restrictive assumptions, allowing for the possibility of a slow increase in the mini-batch size. On the other hand, the learning rate can non-monotonically vary along the iterations, as long as it is appropriately bounded. Convergence results for the proposed method are provided for both convex and non convex objective function. The low per-iteration cost, the limited memory requirements and the robustness against the hyperparameters setting make the suggested approach well-suited for implementation within the deep learning framework, also for GPGPU-equipped architectures. Numerical results on training deep neural networks for multi-class image classification show a promising behaviour of the proposed scheme with respect to similar state of the art competitors.",On stochastic first order optimization methods for deep learning applications ,"[76611, 71816, 78456, 78457, 69872]",423,"[81, 66]",3256,Optimization and learning for data science and imaging [Part III],84,4,34,Advances in large scale nonlinear optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,43 [building - 303A],"['Non-smooth Optimization', 'Machine Learning']",MC-34
"Anchoring bias, a well-known cognitive bias, occurs when people heavily rely on the initial information they receive, influencing subsequent judgments or decisions. This study examines anchoring bias in multi-attribute value theory [MAVT], focusing on its impact during value function and weight elicitation process. Hypotheses were formulated to investigate the presence of anchoring bias in MAVT and to explore potential strategies for mitigating its effects. These strategies were derived from the consider-the-opposite concept, a recognized de-biasing approach for anchoring bias and were customized to fit the MAVT process. To test the hypotheses, an experimental study was conducted based on a real-life decision-making problem. We created a questionnaire in alignment with the MAVT methodology and collected data via an online platform. The findings indicate that anchoring bias has an impact on both the value function and weight elicitation steps. Additionally, suggested mitigation strategies were observed to be somewhat effective, offering decision-makers practical tools to partially address anchoring bias. In conclusion, this research contributes to enhancing decision-makers' awareness of anchoring bias as well as offering actionable insights for decision-makers aiming for more accurate and unbiased decisions.",Anchoring bias in Multi-Attribute Value Theory,"[76894, 78621, 40259]",557,"[10, 25, 27]",3257,Behavioral Decision Analysis II,13,4,11,Behavioural OR,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,12 [building - 116],"['Behavioural OR', 'Decision Analysis', 'Decision Theory']",MC-11
"The upcoming climate crisis is a consequential driver for displacement of communities, a little-studied problem in operations research. Its effects would strike people worldwide, causing displacement in great quantities. The unique characteristics of this problem - including consideration of a long-term planning horizon, future uncertainties, and heterogeneous populations and numerous locations under different climate change impacts at varying levels – require a different approach to handle this problem. For these reasons, our approach can assist in developing relocation plans to manage climate-change-induced movements with a long-term outlook while using the resources effectively and protecting the well-being and dignity of both displaced and receiving communities. The first step of these plans is the high-level planning of where the displaced people should go and when. The second step reflects the mixed decision-making structure resulting from the involvement of many local authorities and the necessity of looking after global humanitarian interests. We suggest a consensus structure that compromises the global outcomes and local interests using a global optimal model, self-optimal local models, and a final global model that uses the outputs of the global and self-optimal models and iterates by adjusting the decisions until fairness among all participating hosts is achieved.",Decentralization Approach for Global Relocation Planning under Climate Change ,"[61143, 71173, 61145, 2268]",556,"[58, 94]",3258,Complex societal problems,38,10,21,OR in Humanitarian Operations [HOpe],"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,49 [building - 116],"['Humanitarian Applications', 'OR in Environment and Climate change']",TD-21
"We introduce an equitable crowdsourced last-mile delivery model using the Nash Social Welfare [NSW] solution to ensure fairness among drivers. In the private logistics sector, fairness considerations are growing due to pressures to improve workload equity. Our model aims to maximize driver equity and efficiency while ensuring company efficiency by putting a cap on the deviation of the company's cost from the least-cost solution value. We formulate the problem as a variant of the vehicle routing problem with a nonlinear objective function inspired by NSW's approach. A column generation method is developed to solve the problem, and a computational study evaluates the model's behavior in terms of company cost, drivers' profit, and equity. ",Equitable Workload Allocation in Vehicle Routing Problem with Heterogeneous Drivers,[70769],588,"[143, 145, 13]",3260,Crowdsourcing Logistics,6,8,56,Transportation,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S04 [building - 101],"['Transportation', 'Vehicle Routing', 'Column Generation']",TB-56
"We investigate machine learning based preference elicitation strategies in the context of Interactive Multiple Criteria Decision Making. Specifically, such methods employ popular machine learning techniques such as neural networks among others. We study how to make the training phase efficient in such an approach by employing certain preference representation structures that provide for augmentation of the training set. Such augmentations help accelerate the preference learning phase by making maximum use of the elicited preferences in eventually estimating the decision maker's preference profile based on partial preference information. We study different elicitation strategies in combination with different levels of augmentation and provide some empirical results illustrating their effectiveness.",Preference Learning Approaches in Interactive Multiple Criteria Decision Making Using Augmented Training Sets,"[58847, 78460, 57967]",888,"[77, 66, 8]",3262,Preference Learning 2,44,3,44,Multiple Criteria Decision Analysis,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,20 [building - 324],"['Multi-Objective Decision Making', 'Machine Learning', 'Artificial Intelligence']",MB-44
"We propose a new linear programming algorithm that arises from the generalization of the weight reduction algorithm, which, in its turn, is based on the von Neumann algorithm. The algorithm developed by von Neumann has important features, such as fast initial convergence and low computational cost to obtain the search direction, which are inherited by both - the weight reduction algorithm and by the present generalization. However, its usefulness is limited due to considerably slow convergence. Thus, with the objective of improving the performance of the weight reduction algorithm, the idea is to consider more than two columns to compute the direction. It is computed considering p coordinates, choosing p/2 columns that form the smallest angles with the residue and p/2 columns for the largest angles. The advantage of this new algorithm is that by using more columns it is possible to find a better direction. Computational experiments are performed and the results show that with the new algorithm there is a faster decrease in the residual value as the p value increases. Also, the number of iterations and processing time necessary to determine a good solution for the problem, reduces with the value of p. The generalized algorithm can also be used together with other methods, such as interior point methods. It can be used, for instance, to obtain an advanced starting point and, thus, improving its efficiency.",A generalization of the Weight Reduction algorithm,"[1795, 69478, 78458]",702,"[110, 60, 134]",3263,Specialized Optimization Algorithms,76,13,30,Software for Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,53 [building - 208],"['Programming, Linear', 'Interior Point Methods', 'Software']",WB-30
"This paper focuses on the efficient distribution of goods in urban areas by using the time-dependent vehicle routing problem with fleet and crew assignment [TD-VRPFCA]. The objective is to find the best routes for a group of vehicles to deliver goods to a set of destinations, considering various factors that are specific to city logistics, such as traffic congestion, the locations of different buildings, and the difficulties that delivery workers may face in accessing these places. Unlike the traditional vehicle routing problem, our focus is on the efficient crew assignment for delivery vehicles. The crew composition can consist of different roles, such as drivers, delivery workers, or a multifunctional worker who perform both driving and delivery tasks. The crew assignment aims to minimize the total distribution time, considering that service and travel time depend on the time of day and the location of each vehicle due to the traffic flow of people in the buildings and congestion on the streets. 

To solve this problem, we propose a new algorithm called Black Widow Optimization [BWO] for Vehicle Routing Problems. BWO is an efficient metaheuristics algorithm proposed for continuous variables, but in this paper, we propose an algorithm for mixed integer variables. The resulting set of efficient routes and crew vehicle assignments can support decision-making and improve customer service.
",A Black Widow Optimization for The Time-dependent Vehicle Routing Problem with Fleet and Crew Assignment,"[74327, 74340]",623,"[145, 74, 65]",3264,Vehicle routing I,6,2,60,Transportation,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S09 [building - 101],"['Vehicle Routing', 'Metaheuristics', 'Logistics']",MA-60
"We introduce a dynamic game framework that does not rely on linearity as-
sumptions, while at the same time retaining tractability. As an illustration, we employ this framework to derive new insights on climate-related policy. We build a game-theoretic version of the dynamic integrated assessment model in Golosov et al. [2014]. Our framework allows us to characterize emissions, consumption, and welfare across different equilibrium outcomes by computing the Nash and Stackelberg equilibria of a dynamic game capturing aspects of climate negotiations between the “global north” and “global south.” We use these to draw various comparisons between such outcomes and the efficient frontier. We find that both the global north and the global south over-consume/pollute in the non-cooperative equilibria and the global south underinvests in abatement. Stackelberg leadership by the global north in climate negotiations reduces global emissions and increases welfare over the Nash outcome, although the emissions remain far from the optimum.
Asymmetric vulnerability to climate change leads to a reversal in emissions, where the less vulnerable country emits more. We employ robust control to study how deep uncertainty affects outcomes.",Climate policy in a heterogenous world,"[67583, 78461, 22952, 67586, 78462]",846,"[20, 50, 40]",3265,Heterogeneity in optimal control problems,90,7,33,Optimal Control Theory and Applications,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,42 [building - 303A],"['Control Theory', 'Game Theory', 'Environmental Management']",TA-33
"Disruption on commuter railway systems with significant capacity restrictions [e.g. line closures] lead to substantial effects on the planned operations.
One strategy to manage occurred disruptions is based on the use of pre-structured operating concepts, prepared to deal with generic disruption scenarios, so-called disruption programs [DRP]. Since DRPs are planned offline for specific scenarios [e.g. failure of a certain group of infrastructure elements, like signal boxes], they allow dispatchers to react more effectively to the occurred event.
If a disruption affects a combination of infrastructure elements that are not explicitly considered within an existing DRP, the full potential of the DRP cannot be utilized. To support dispatchers in responding to disruptions, an ad-hoc adjustment of the DRPs to the availability of the infrastructure has been developed.
The approach determines the need, develops and introduces necessary adjustments to existing DRPs. At its core, the adjustment is a problem of combinatorial optimization, which is solved using a heuristic method. By combining components of different metaheuristics, a problem-specific solution approach based on a local search was developed.
The approach assesses the adjustments based on their impact on the resistance of the passengers' connections and ensures the operational feasibility of the adjusted DRP. To illustrate the approach, it is applied to an example of a disruption in a commuter railway system",A Heuristic Approach for Real-Time Adjustment of Disruption Programs - Applied to the Context of Commuter Railway Systems,[69957],179,"[122, 129, 26]",3266,Disruption management in passenger railways,85,2,54,Public Transport Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S01 [building - 101],"['Railway Applications', 'Scheduling', 'Decision Support Systems']",MA-54
"Player recruitment and selection is one of the most important factors in determining the success of a team. The recruitment process is a complex task that involves considering various factors [form, fitness, etc] whilst being constrained by the team's budget and the availability of players in the market. The recruitment process is further complicated by the fact that the performance level of a player is not static and can change over time. Therefore, it is important to consider both the players current form and the player's future form in recruitment and selection decisions. In this paper, we propose a new approach for player selection and recruitment in team sports. TOPSIS++ is an extension of the Technique for Order Preference by Similarity to Ideal Solution [TOPSIS] method that is widely used in decision-making problems. TOPSIS++ is designed to address the limitations of the original TOPSIS method by considering not just a player’s current form but also their previous and future predicted form. We demonstrate the effectiveness of TOPSIS++ by applying it to the problem of player selection and recruitment in football. We compare the performance of TOPSIS++ with the original TOPSIS method and show that TOPSIS++ performs better in terms of identifying the best player to recruit when evaluated against the actual performance of the players in proceeding seasons. Our results show that TOPSIS++ is a promising approach for player selection and recruitment in team sports.",TOPSIS++ - A New Approach for Player Recruitment and Selection in Team Sports,[77634],668,"[99, 77, 25]",3267,Performance and scouting in football,37,10,16,OR in Sports,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,19 [building - 116],"['OR in Sports', 'Multi-Objective Decision Making', 'Decision Analysis']",TD-16
"We introduce a new bilevel version of the classical shortest path problem and completely characterize its computational complexity with respect to several problem variants. In our problem, the leader and the follower each control a subset of the edges of a graph and together aim at building a path between two given vertices, while each of the two players minimizes the length of the resulting path according to their own edge lengths. We investigate both directed and undirected graphs, as well as the special case of acyclic directed graphs. Moreover, we distinguish two versions of the follower's problem - Either he has to complete the edge set selected by the leader such that the joint solution is exactly a path, without any additional edges, or he is allowed to include only a subset of the leader's selection into the final path. In general, the bilevel problem turns out to be much harder in the former case - We show that the follower's problem is already NP-hard here and the leader's problem is even hard for the second level of the polynomial hierarchy, while both problems are one level easier in the latter case. Interestingly, for acyclic directed graphs, this difference turns around, as we give a polynomial-time algorithm for the first version of the bilevel problem, but it stays NP-hard in the second case.",On the Complexity of the Bilevel Shortest Path Problem,"[75184, 69660]",466,"[14, 16, 53]",3268,Robust and Multi-Level Optimization,86,10,04,MINLP,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,1001 [building - 202],"['Combinatorial Optimization', 'Complexity and Approximation', 'Graphs and Networks']",TD-04
"Due to the required reduction of emissions, modern mobility concepts are rapidly evolving. Ridepooling is one of these concepts. Beside the reduction of emissions due to electric vehicles, ridepooling services promise to reduce traffic due to pooling and to increase mobility access especially in suburban areas. In practice, ridepooling services receive customer orders dynamically and thus have to integrate them in the vehicles’ tours. In this talk, we discuss an efficient procedure to insert new customer requests into given tours while incorporating possible future customers with the objective to serve as many customer requests as possible over the time horizon.",Insertions with lookahead for dynamic ridepooling services,"[63599, 63590]",822,"[65, 0]",3269,Demand-responsive public transport 3,85,10,54,Public Transport Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S01 [building - 101],['Logistics'],TD-54
"Unexpected failures of operating systems can result in severe consequences and huge economic losses. To prevent them, preventive maintenance based on condition data can be performed. Existing studies either rely on the assumption of a known deterioration process or an abundance of data. However, in practice, it is unlikely that the deterioration process is known, and data is often limited [to a few runs-to-failure], especially for new systems. This paper presents a fully data-driven approach for condition-based maintenance [CBM] optimization that is especially useful in situations with limited data. The approach uses penalized logistic regression to estimate the failure probability as a function of the deterioration level and allows any deterioration level to be selected as the preventive maintenance threshold, also those that have not been observed in the past. Numerical results indicate that the preventive maintenance thresholds resulting from our proposed approach closely approach the optimal values.",Data-Driven Condition-Based Maintenance Optimization Given Limited Data,"[78339, 37356, 35181]",797,"[123, 135, 69]",3270,Reliability Models,50,13,39,Stochastic Modelling,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,35 [building - 306],"['Reliability', 'Stochastic Models', 'Manufacturing']",WB-39
"In this paper, we propose the first Quasi-Newton method with a global convergence rate of $O[\frac{L_1R^2}{k}]$ for general convex functions. Quasi-Newton methods, such as BFGS, SR-1, are well-known for their impressive practical performance. However, they are theoretically slower than gradient descent for general convex functions. This gap between impressive practical performance and poor theoretical guarantees was an open question for a long period of time. In this paper, we make a significant step to close this gap. We improve upon the existing rate and propose the Cubic Regularized Quasi-Newton Method with a convergence rate of $O[\frac{L_1R^2}{k}]$. The key to achieving this improvement is to use the Cubic Regularized Newton Method over the Damped Newton Method as an outer method, where the Quasi-Newton update is an inexact Hessian approximation. Using this approach, we propose the first Accelerated Quasi-Newton method with a global convergence rate of $O[\frac{L_1R^2}{k^2}]$ for general convex functions. In special cases where we have access to additional computations, for example Hessian-vector products, we can improve the inexact Hessian approximation and achieve a global convergence rate of $O[\frac{L_2R^3}{k^{3}}]$, which make it intermediate second-order method. To make these methods practical, we introduce the Adaptive Inexact Cubic Regularized Newton Method and its accelerated version, which provide real-time control of the approximation error. We show that the p",Accelerated Adaptive Cubic Regularized Quasi-Newton Methods,[76160],363,"[63, 19, 21]",3271,Beyond First-Order Optimization Methods,84,13,32,Advances in large scale nonlinear optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,41 [building - 303A],"['Large Scale Optimization', 'Continuous Optimization', 'Convex Optimization']",WB-32
"The promotion of sustainable transportation modes, such as cycling, has emerged as a crucial aspect of urban planning and design. The design of an efficient bicycle network is a complex optimization problem that requires analysis of different factors including infrastructure layout, connectivity, safety, and accessibility. However, before building the network, it is fundamental to understand the behavior of its users. The fastest routes are not always the preferred options for cyclists, who consider different criteria when choosing the best routes from their origin to their destination. Therefore, the aim of this study is to analyze the cyclists' choice behavior in the city of Parma, Italy. This analysis consists of defining a set of user profiles, where each profile corresponds to a combination of criteria used by the cyclist to choose a route, such as shortest time, safety, and connectivity. We proposed a bilevel formulation and developed a hybrid method to address this problem, which combines the solution of multiple shortest path problems with a heuristic method to determine the profiles. This approach takes advantage of solving the problem in a closed form by computing beforehand as many shortest path solutions as the number of profiles and then solving a simple quadratic programming problem. We validated our method using both randomly generated and real data collected from bicycle sharing companies in the city of Parma.",Optimizing the Determination of Criteria for Cyclists' Route Selection,"[73832, 78474, 78476, 78479, 78481, 78483, 67821]",612,"[79, 19]",3272,Optimization of sustainable urban mobility,79,7,18,Sustainable Cities,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,42 [building - 116],"['Network Design', 'Continuous Optimization']",TA-18
"Climate change impacts energy systems reliability. Extreme weather events, such as droughts and heat waves, can undermine the ability of technologies to convert energy and can exacerbate the demand for electricity, heating, and cooling. This causes unexpectedly high operative costs and, in the worst cases, inability to supply demand.
Hence, it becomes crucial to consider the impact of more frequent and severe extreme weather events during the planning of a cost-optimal low-carbon energy system to ensure climate resilience. This work introduces a method to convert the output of the global climate model CESM2 into hourly-resolved energy-related metrics, e.g., capacity factor, by capturing the impact of weather on conventional, renewable technologies, and on energy demand. Such metrics are used to parametrize a multi-year transition pathway optimization problem of the European energy system until 2050.
The resilience of the system with respect to extreme events is investigated through a scenario analysis. First, the system is optimized in a base climatic case; then, alternative extreme scenarios are generated via a boosting method that aggravates those short-term weather events that are most critical to the operation of the energy system. The new climatic time series are converted again as input of the optimization problem.
The resulting climate-resilient systems trade-off investment and operative cost to guarantee reliable and clean energy in a climatically unstable future.",Identification of extreme weather events for the design of a climate resilient European energy system,"[77711, 69248]",947,"[37, 40, 38]",3273,Pathways to Climate Resilience,80,12,53,Sustainable and Resilient Systems,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,8007 [building - 202],"['Energy Policy and Planning', 'Environmental Management', 'Engineering Optimization']",WA-53
"Agent-based modeling [ABM] presents an effective framework for analyzing complex and highly interconnected systems such as the electricity market. With the ongoing energy transformation, understanding the individual decisions of actors such as households with PV-storage systems [PVS], heat pumps [HP], and electric vehicles [EV] is crucial. However, ABMs for the electricity market on a national level with complex interconnected actors face scalability limitations with regard to their individual decision making. Though, aggregating actors into representative entities and applying a single decision strategy fails to consider the diversity of individual decision-making processes. Moreover, this oversimplified approach neglects the influence of the varying attributes of the individual actors. 
To solve this problem, we propose leveraging machine learning [ML] techniques to address these requirements while controlling the scalability challenge. The main idea is to utilize ML methods to learn and predict the aggregation of individual actor decisions. Central to our approach is a uniform forecasting of aggregated demand time series for all actor types, i.e., PVS, HP, and EV. The underlying demand time series result from applying optimization models specific to each actor type. The results show similarly good predictions for PVS, HP, and EV. This method enables a more nuanced understanding the individual impact of decision-making processes of households on a national level.",Enhancing Household-Level operational decision making with Machine Learning,"[72774, 72793, 78473]",564,"[3, 47, 66]",3274,Simulation in economics I,77,4,43,"Agent-based Models in Management, Economic and Organisation Sciences","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,99 [building - 306],"['Agent Systems', 'Forecasting', 'Machine Learning']",MC-43
"The daily notional volume of 0-DTE SPY options is over 1 billion USD. We created an Integer LP model to take advantage of this market every day. Our out-of-sample backtests produce Sharpe ratios that exceed 3. We include commissions, slippage, and the margin requirement, without which the results would have been unrealistic.

Our model, similar to Papahristodoulou’s [2003], is formulated so that the model’s optimal solution will determine the optimal speculative portfolio. The integer decision variables are the quantities of available 1-DTE SPY options contracts. We assume we can trade these quantities a very short time before the closing bell, the day before the expiration date. So, technically, we trade 0-DTE SPY options contracts. In the future, we shall experiment with SPX options, which are cash-settled, unlike SPY options.

We use constraints that limit all significant portfolio Greeks, delta, gamma, theta, rho, and vega. Since there is only one underlying instrument, SPY, and one specific expiration date, all Greeks are additive in each portfolio. Our objective function maximizes the sum of differences between theoretical prices and market prices of available options, net of commissions and slippage. We compute theoretical prices as in Papahristodoulou [2003]. The gist of our paper is the way we calculate the theoretical prices of options and the validity of this computation.",Creating Optimal Speculative Portfolios for 0-DTE [Zero-Days-To-Expire] SPY Options,"[78464, 76169]",372,"[45, 83, 109]",3275,Advanced Options Strategies Using O.R. and Machine Learning,4,9,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S14 [building - 101],"['Financial Modelling', 'Optimization in Financial Mathematics', 'Programming, Integer']",TC-63
"Urban transportation network design is a crucial area of research. However, there is a lack of comparative studies between different optimization techniques, primarily due to the unique nature of each method, adapted to specific problem structures. This study aims to address this gap by evaluating two models—Stackelberg and multi-objective—for urban transportation network design. In the Stackelberg model, users lead transport system usage decisions, while the transportation provider follows. Conversely, the multi-objective model considers both user and provider objectives simultaneously. These models are represented as multi-layer network flow problems, and they aim to optimize network expansion and flow distribution, balancing the provider's cost minimization objective with user objectives of minimizing travel time and network accessibility time. The evaluation of these models involves measures such as betweenness, edge flow, accessibility, and demand compliance. The models are applied to four types of topological networks representing city transportation systems - mesh, hub and spoke, linear, and tree structures. The results indicate that the mesh network modeled by the Stackelberg model achieves the highest accessibility and cost efficiency. Thus, the study contributes to the understanding of urban transportation network design by providing insights into the performance of different optimization models and their applicability in various network structures.",Comparative analysis of a Stackelberg model and a multi-objective model for the design of urban transportation networks,"[69348, 69505, 61145, 78486]",817,"[119, 79, 150]",3276,Network Design for Public Transport,85,10,51,Public Transport Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,M5 [building - 101],"['Public Local Transportation Systems', 'Network Design', 'Network Flows']",TD-51
"The Open Vehicle Routing Problem [OVRP] is a complex combinatorial optimization challenge with numerous real-world applications in transportation, logistics, and urban planning. 
The OVRP differs from the Vehicle Routing Problem [VRP] because the vehicles either are not required to return to the depot, or if they are to so do by revisiting the customers in the reverse order. 
The OVRP is faced by a company which either does not own a vehicle fleet at all, or its vehicle fleet is inappropriate or inadequate to satisfy the demand of its customers. 
Our approach is based on combination of the Iterated Greedy [IG] with biased randomization technique.
The IG has been developed by to solve the Permutation Flowshop Scheduling Problem [PFSP] in order to minimise makespan. It is reasonably simple to implement and parameter free. 
One of our aim of choosing this method is that, the IG has been applied successfully for some Combinatorial Problems specifically and has been proven to generate high quality solutions. Despite the proven impressive performance of IG, it has not been applied in the OVRP. 
Biased randomisation technique refers to the utilisation of pseudo-random numbers to generate random outcomes throughout the solution search. By randomizing some steps in a deterministic heuristic, it is transformed into a probabilistic procedure. Then, it can be run multiple times – either in sequential or parallel mode – in order to obtain different outcomes or solutions. 
",Hybrid Randomised and Greedy Technique as a Novel Approach to Solve the Open Vehicle Routing Problem [OVRP],"[54957, 74447, 74451]",785,"[145, 74, 14]",3278,Heuristics for Vehicle Routing 2,5,15,64,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S16 [building - 101],"['Vehicle Routing', 'Metaheuristics', 'Combinatorial Optimization']",WD-64
"Maritime transportation, crucial for global trade, moved 165 million twenty-foot equivalent units in 2022, with a stable annual growth of 3.5% over the past two decades [1]. Despite being the most eco-friendly mode of cargo transport [2], container stowage optimization can achieve further efficiencies and CO2 reductions. 
Despite its economic and environmental importance, the Container Stowage Planning Problem [CSPP] has seen limited research attention, with only 200 related publications since the 1970s [3]. The main challenges come from the industry's insularity and the scarcity of publicly available data, leading to oversimplified and insufficient models.
In partnership with Sealytix, we have created the Representative Container Stowage Planning Problem [RCSPP] to make the CSPP more accessible. Sealytix's industry knowledge has helped capture essential aspects without unnecessary complexity. The RCSPP is partnered with a comprehensive benchmark suite using real-world data, evaluated with a Large Neighborhood Search algorithm.

[1] UNCTAD, 2022. Review of maritime transport 2022, United Nations Publications, New York, NY.
[2] IMO, I.M.O., 2021. Fourth imo greenhouse study 2020, International Maritime Organization, London, England.
[3] van Twiller, J., Sivertsen, A., Pacino, D., Jensen, R.M., 2023. Literature survey on the container stowage planning problem. European Journal of Operational Research
",A novel formulation and benchmark suite for the container stowage planning problem,"[78350, 67073, 32277]",670,"[74, 143, 70]",3279,Seaside Planning II,52,5,62,OR in Port Operations,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S12 [building - 101],"['Metaheuristics', 'Transportation', 'Maritime applications']",MD-62
"In the field of natural language processing [NLP], identifying cheap talk remains a challenge. Boiling down the sole content from stylized text is one approach to tackle this problem. Therefore, this study translates a recent approach known as PISCO from the computer vision [CV] field into the NLP field. This method post-processes and thereby disentangles embeddings into distinct style and content latent vector coordinates. We then isolating the chunk of neurons that only encodes the content of a text and further use this information for a classification head. Thereby, we improve the objectivity of NLP applications by removing the confounding influence of stylistic variations. With a focus on financial disclosure analysis, we aim to improve the predictive quality of sentiment and specificity downstream classification tasks. We demonstrate the technique's applicability by applying this method upfront these final classification tasks. Our initial findings suggest a promising enhancement in predictive performance for both downstream tasks. This demonstrates the potential of disentangled embeddings as a novel intermediary step towards improving NLP classification tasks. Our advances enable more accurate and evidence-based decision-making and insight generation, contributing to a more reliable and, to a certain extent, interpretable use of recent rapid advances in NLP models.",Advancing NLP classification with disentangled embeddings - a post-processing approach to separate style from content,"[77727, 80026, 78571, 78573]",211,"[7, 66, 45]",3280,Analytics and the link with stochastic dynamics III,17,9,31,Analytics,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,54 [building - 208],"['Analytics and Data Science', 'Machine Learning', 'Financial Modelling']",TC-31
"In this talk, I present the non-asymptotic convergence rate of the DCA [difference-of-convex algorithm], also known as the convex–concave procedure, with two different termination criteria that are suitable for smooth and non-smooth decompositions, respectively. The DCA is a popular algorithm for difference-of-convex [DC] problems and known to converge to a stationary point of the objective under some assumptions. I derive a worst-case convergence rate of O[1/sqrt[N]] after N iterations of the objective gradient norm for certain classes of DC problems, without assuming strong convexity in the DC decomposition and give an example which shows the convergence rate is exact. I also provide a new convergence rate of O[1/N] for the DCA with the second termination criterion. Moreover, I present a new linear convergence rate result for the DCA under the assumption of the Polyak–Łojasiewicz inequality. The novel aspect of this analysis is that it employs semidefinite programming performance estimation.",On the convergence rate of the difference-of-convex algorithm [DCA],[74865],902,"[5, 16, 115]",3281,Advances in algorithms and applications for linear and convex optimization,68,14,38,"Conic Optimization - Theory, Algorithms, and Applications","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,34 [building - 306],"['Algorithms', 'Complexity and Approximation', 'Programming, Semidefinite']",WC-38
"Predictive Process Monitoring [PPM] enhances the field of Process Mining [PM] by applying predictive analytics to forecast the future course of ongoing business processes. The prediction of suffixes, which involves predicting the sequence of forthcoming events, including their activity labels, timestamps, and remaining runtime, is particularly intricate. Present methodologies predominantly encounter two issues - firstly, they are predominantly trained to anticipate the next event, employing iterative feedback mechanisms for generating suffixes upon deployment—a strategy that not only introduces inaccuracies but also limits the exploitation of informative event attributes; secondly, the prevalent reliance on Long Short-Term Memory [LSTM] networks impedes their performance due to their inherent challenges with long-range dependencies and information loss over lengthy sequences. In response, this study unveils the Suffix Transformer Network [SuTraN], pioneering the use of an encoder-decoder Transformer framework for comprehensive suffix prediction in PPM. Tailored for this purpose, SuTraN innovatively forecasts complete event suffixes through a singular forward pass, eliminating the need for iterative loops and leveraging the full spectrum of event data without the constraints found in LSTM-dependent methods. Our evaluations, conducted on several real-world event logs, confirm that SuTraN delivers improved performance in suffix prediction relative to established methods.",Introducing SuTraN - A Transformer-Based Encoder-Decoder Model for Enhanced Suffix Prediction in Business Process Monitoring,[74742],935,"[104, 8, 7]",3282,Advances in Process Mining,15,8,27,Mathematical Optimization for XAI,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,047 [building - 208],"['Process Systems Engineering', 'Artificial Intelligence', 'Analytics and Data Science']",TB-27
"A scheduling problem for a two-stage flexible flow shop with parallel batch processing machines [BPMs] is considered. Here, a batch is a group of jobs that are processed at the same time on a single machine. This problem is motivated by process conditions found in semiconductor wafer fabrication facilities [wafer fabs]. Jobs having unequal ready times are assumed. Moreover, critical queue time constraints, i.e. maximum time lags, are taken into account. A blended objective function combining the total weighted tardiness [TWT] and the total electricity cost [TEC] under a time-of-use [TOU] tariff is considered. A genetic programming [GP] procedure is designed to automatically discover priority indices for a heuristic scheduling framework. Results of computational experiments based on randomly generated problem instances are reported that demonstrate that the learned priority indices lead to high-quality schedules in a short amount of computing time.",Learning Priority Indices for Energy-Aware Scheduling of Jobs in Flow Shops with Batch Processing Machines,"[78469, 14225]",546,"[129, 0]",3283,Sustainable Operations,19,5,24,Sustainable Supply Chains,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,83 [building - 116],['Scheduling'],MD-24
"Social impact assessment [SIA] of re-naturing measures in urban areas offers an exclusive perspective on complex interactions and trade-offs between humans and nature. Despite advancements in methodologies for short-term assessment, long-term assessments, and evaluations still require a concrete framework. The main challenges in long-term SIA procedures are the high volume of complexities and interconnectivity of interaction components and difficulties in citizens' engagements. Focusing on these issues by utilizing digital platforms, this research proposes a long-term SIA procedure based on the system dynamics [SD] framework and co-monitored parameters to understand the value of re-naturing measures and biodiversity in urban areas. A systematic review collects direct and indirect social indicators, impacts, and factors of re-naturing through ecosystem services [ES]. After contextualization at Giardino San Faustino, Milan, through consultation with actors, the results shape causal loop diagrams in the system dynamics framework. To comprehensively cover the extensivity of interactions and variable relations, the simulation tool of AnyLogic is used for full coverage of relations. This digital tool enables a complete construction of relations and simulates interactions for SIA in defined timelines reflecting limitations and advantages in each scenario. The results render a spectrum of interaction dynamics aiding decision-makers in better evaluating human and nature interactions.",Social impact assessment using ES through socio-ecological system dynamics digital simulation,"[76236, 78475]",76,"[140, 131]",3284,"Assessment Methods for Shaping the Green, Inclusive, and Digital Cities  I",79,2,18,Sustainable Cities,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,42 [building - 116],"['System Dynamics and Theory', 'Simulation']",MA-18
"In this work, we consider an assignment problem with decision-dependent completion times which is motivated by a real-life application in trademark evaluations. We formulate the problem of assigning evaluations to agents as a Mixed Integer Programming [MIP] model to help decision-makers better assign workers to evaluation tasks, considering their expertise and current workload. The aim is to minimize the backlog level of the office, which is quantified by the total number of tardy jobs. We provide two alternative formulations and explore their computational performance on a large number of test instances. We integrate both linear and neural network prediction models by using machine learning algorithms and evaluate the improvements in the total number of tardy jobs.",Assignment optimization with decision-dependent completion times for trademark application evaluation,"[78467, 56337, 55115]",561,"[66, 7, 42]",3285,"Advancements of OR-analytics in statistics, machine learning and data science 14",16,9,06,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,1013 [building - 202],"['Machine Learning', 'Analytics and Data Science', 'Expert Systems and Neural Networks']",TC-06
"In the last decade, the emerging technology of unmanned aerial vehicles, commonly known as drones, has provided new opportunities for practitioners in urban logistics. Drones present attractive advantages compared with standard ground vehicles, such as avoiding the congestion on road networks, eliminating the risk of personnel in difficult access operations or getting higher measurement accuracy in infrastructure inspection. 
The use of drones to perform the service in Arc Routing Problems [ARPs] involves significant changes in the traditional way of modeling and solving them. While in traditional ARPs the ground vehicles are limited to travel through the edges of a network [following the local infrastructure], drones have the flexibility to travel off the network as well. These aerial vehicles may service only part of an edge and then travel in a straight line to any point of another edge, without following the links of the network. This consideration makes Drone ARPs continuous optimization problems with an infinite and uncountable number of feasible solutions. In this talk, I summarize these ideas and review some variants of ARPs with drones that we have addressed and studied during my doctoral thesis.",An overview of Arc Routing Problems with drones,[77402],457,"[145, 0]",3286,YW4OR_1,39,12,12,WISDOM - Women in OR,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,13 [building - 116],['Vehicle Routing'],WA-12
"Reducing pollution is something desireable for a firm, however, sometimes this is only achieveable by means of reducing production which in turn has a negative impact on revenues. In the present talk, we consider a simple two-objective optimal control problem. We discuss the advantage of using the epsilon-constraint method to determine the Pareto front as opposed to the conventional weighted-sum approach. We are able to derive analytic results for our optimal control problem and discuss their implications.",Optimal Sustainable Investments in a Multi-Objective Dynamic Model of the Firm,"[23371, 10538, 61089]",373,"[82, 0]",3287,Optimal control in environmental economics,90,8,33,Optimal Control Theory and Applications,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,42 [building - 303A],['Optimal Control'],TB-33
"We examine European banks’ financial performance and risk management and their trends over the years and across banking characteristics around the financial crisis of 2007-2008. By testing for the linear and nonlinear variations using an exogenous quasi-experiment design, we demonstrate the usefulness of understanding the cause of variations in banks’ operating efficiency and riskiness. We adopt the positive theory of capital allocation decisions and risk-taking activities to assess the relationship between banks’ efficiency and risk with value creation from intermediation services. 
First, we peer-benchmark banks’ efficiency, using data envelopment analysis, and obtain Fitch risk ratings to test for their yearly and geographical resilience, including linear and nonlinear variations before and after the crisis. Second, we determine bank-specific and industry-specific operational and financial factors, including longitudinal and cross-sectional heterogeneities, that affect European banks’ operating efficiency and riskiness.
We find that banking efficiency can provide intuition on mitigating excessive investments and risk-taking around the financial crisis. Further, we capture the informational relevance of banking efficiency to third-party agencies in assessing financial strengths and credit risks. Our study provides a foundation to examine variations in European banks’ performance and risk realizations, including financial decisions and operating saliency.",Nonlinear Inferences on European Banking Resilience in Operating Efficiency and Riskiness Around the Financial Crisis,"[78236, 17164, 78480, 76204]",661,"[35, 126, 30]",3289,OR in Accounting - Wealth and Risk,7,15,59,OR in Financial and Management Accounting,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S08 [building - 101],"['Efficiency Analysis', 'Risk Analysis and Management', 'Disaster and Crisis Management']",WD-59
"A recently introduced approach is extended to probabilistic electricity price forecasting [EPF] utilizing distributional artificial neural networks, based on a regularized distributional multilayer perceptron [DMLP]. We develop this technique for a multivariate case EPF with incorporated dependence. The performance of a fully connected architecture and a LSTM architecture of neural networks are tested. The empirical data application analyzes two day-ahead electricity auctions for the United Kingdom market. This creates the opportunity to buy in the first auction for lower price and sell in the second for higher price [or vice versa]. Utilizing forecasting results, we develop trading strategies with various investors’ objectives. We find that, while DMLP shows similar performance compared to the benchmarks, the algorithm is considerably less computationally costly.",Multivariate Probabilistic Forecasting of Electricity Prices With Trading Applications,[78471],246,"[36, 66]",3291,"AI in Eco-Finance - Time, Space, and Networks",53,3,08,AI & Innovation in Sustainable Finance,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,1020 [building - 202],"['Electricity Markets', 'Machine Learning']",MB-08
"Chile is actively pursuing an ambitious agenda for the widespread electrification of the transportation sector, stablishing that by 2035, 100% of sales for all light and medium-duty passenger vehicles must be zero-emission. The large-scale integration of electric vehicles, while posing substantial challenges in managing the power grid, concurrently presents opportunities to significantly enhance system flexibility. This work analyzes the potential of electric vehicles to augment the flexibility of the Chilean power system. We introduce an operational model for electric vehicles, encompassing both personal vehicles and ride-hailing services, while integrating intelligent charging and discharging modes. The EV operational model is integrated into a unit commitment model, with the objective of assessing the impact of various electric vehicle operation modes on the power system operation. Applying this model to a realistic representation of the Chilean power system towards 2050, we evaluate the potential of electric vehicles in enhancing flexibility metrics. Furthermore, we investigate the influence of the adoption of ride-hailing services on the flexibility provided by the vehicle fleet. This research contributes valuable insights within the context of the Chilean power system, utilizing a realistic representation and considering the evolving landscape of electric transportation.",Assessing the Capability of Electric Vehicles to Enhance Flexibility of the Chilean Power System,"[78468, 73922]",802,"[37, 93, 143]",3293,Electric Vehicles within Electric Power Systems,23,5,19,OR in Energy,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,44 [building - 116],"['Energy Policy and Planning', 'OR in Energy', 'Transportation']",MD-19
"Electromobility plays a pivotal role in achieving Net Zero emissions targets globally, with transportation accounting for 15% of emissions, 72% of which stem from road transportation. However, the widespread adoption of electric vehicles [EVs] poses significant challenges. EV penetration and CS network establishment are intertwined, resembling a chicken-and-egg problem. If EV demand stagnates, CS utilization remains low, discouraging investment from charging point operators [CPOs]. Conversely, inadequate CS infrastructure worsens user concerns like range anxiety, hindering electrification. While existing studies often focus on one-time CS location decisions, developing a well-distributed CS infrastructure requires a holistic approach in decision-making. To address this, our objective is to formulate a multi-period CS network deployment plan focused on the most frequented highways. We propose a multi-objective stochastic programming approach to optimize sustainability dimensions and identify new CS locations to augment the existing network. This model embraces the Triple Bottom Line [TBL] approach, considering economic, ecological, and social aspects from the perspectives of charging point operators [CPOs], government entities, and EV users, respectively. Factors such as location, charger quantity, load capacity, grid expansion needs, and energy source types—be they transformer upgrades or renewable installations—are assessed across various EV penetration scenarios.",Optimizing Charging Station Infrastructure Deployment - A Multi-Objective Stochastic Programming Approach,"[76765, 52248]",603,"[72, 64, 84]",3294,Applications of Multiobjective Optimization,34,10,37,Multiobjective Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,33 [building - 306],"['Mathematical Programming', 'Location', 'Optimization Modeling']",TD-37
"Pencil beam scanning [PBS] is an advanced technique for treating cancer with proton radiation. It can deliver radiation with great precision, allowing the tumor to be targeted with less harm to healthy tissue. However, the precision comes with a large sensitivity to uncertainty of the patient’s position and anatomy. Whilst robust optimization has proven successful in managing positional uncertainty in PBS treatments, treatment of motion-subjected tumors due to variational anatomy, such as those in the lungs, may benefit from a motion-adaptive treatment planning approach.

In this work, we build on previous attempts to explicitly plan with respect to the respiratory patterns of lung patients, as well as motion-adaptive optimization proposed for photon treatments, to propose a motion-adaptive control strategy for PBS. During the delivery of the treatment, the patient’s breathing motion is tracked and predicted into the future. The predicted motion is then used to re-optimize the treatment over a receding horizon.

We show that the motion-adaptive strategy is superior to robust optimization in terms of both consistently treating the target with sufficient radiation and sparing nearby healthy tissue. Although much work remains to demonstrate the feasibility of implementing the proposed strategy in real-time, our results demonstrate the benefits of successfully implementing a motion-adaptive strategy, further realizing the precision advantage of proton therapy with PBS.
",Motion-adaptive optimization for proton therapy with pencil beam scanning,"[78289, 78508, 78509]",609,"[82, 73, 56]",3295,Radiotherapy and chemotherapy planning,3,7,10,OR in Health Services [ORAHS],"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,11 [building - 116],"['Optimal Control', 'Medical Applications', 'Health Care']",TA-10
"Refugees need periodic relief aid along their routes to safe places. To this end, continuation of service is crucial to alleviate their mental and physical burden. It has been shown that employing mobile facilities over fixed facilities to provide relief aid can reduce costs. The challenge in coordinating the number and schedules of mobile facilities has been formulated in the multi-period mobile facility location problem with mobile demand [MM-FLP-MD]. Our research aims to determine the impact of using a fleet of different facility types and introducing capacity and duration constraints for facilities. We compare solutions from a Mixed Integer Linear Programming model and an Adaptive Large Neighborhood Search [ALNS]. A prior proposed ALNS is enriched with additional operators and we exploit the problem’s structure to permit a two-step repair procedure. Moreover, we discuss the application of an ALNS for decisions on the intensive margin, i.e. to make decisions from a non-binary decision space. This methodology is applied in a case study of the Western Balkan route during the 2015 European refugee crisis. We find that some of our operators provide similar improvements at a lower computational cost compared to the best operators in previous literature and some decision variables lie indeed on the intensive margin. An insight for humanitarian organizations is to use more smaller facilities in optimistic cost scenarios and favor larger facilities in more pessimistic scenarios.",Fleet Composition and Scheduling for Provision of En-Route Relief Aid Using an Adaptive Large Neighborhood Search with Decisions on the Intensive Margin,"[75521, 80051, 77345]",773,"[30, 58, 74]",3296,Post-Disaster Relief Distribution,38,12,21,OR in Humanitarian Operations [HOpe],"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,49 [building - 116],"['Disaster and Crisis Management', 'Humanitarian Applications', 'Metaheuristics']",WA-21
"This research explores an approach to addressing mathematical optimization problems characterized by nonlinear objective functions and constraints, particularly in situations where derivatives are unavailable. A Sequential Penalty Approach is introduced, incorporating a Logarithmic Barrier for managing inequality constraints and an Exterior Penalty for equality constraints. Our proposed method aims to providing a comprehensive solution for a wide range of nonlinear optimization challenges.
This strategy, proposed for the first time in a derivative-free setting in the framework of linesearch methods, is adapted and incorporated into SID-PSM algorithm, a generalized pattern search method, allowing to effectively handle general nonlinear constraints. Under reasonable assumptions regarding the smoothness of the functions, convergence is established, without any convexity assumption. 
Empirical validation is conducted through experimentation on a diverse set of optimization problems. The results corroborate not only the efficacy of the proposed approach but also its practical utility. The direct search algorithm, when complemented by the Logarithmic Barrier and Exterior Penalty, consistently exhibits promising numerical results, demonstrating the potential of the techinque.",Combining interior and exterior penalty for nonlinear costrained black-box optimization problems,"[68107, 72514, 25723, 67015]",285,"[19, 60, 113]",3297,Large Scale Constrained Optimization - Algorithms and Applications,84,2,32,Advances in large scale nonlinear optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,41 [building - 303A],"['Continuous Optimization', 'Interior Point Methods', 'Programming, Nonlinear']",MA-32
"In this work, we consider a bivariate risk process describing the capitals over time of two insurance companies that share premiums and claims with fixed proportion. Such a model can be used in modeling of the capital for the insurer-reinsurer system with proportional reinsurance [1-5]. Our main results include a formula for the ruin probability of insurer or reinsurer in the infinite time for phase-type distributed claims [5].  Additionally, we will introduce two alternative methods for approximating the aforementioned ruin probability - De Vylder-type and diffusion approximations [2,4]. The accuracy of our results is validated using both synthetic and empirical data [2-5].

[1] F. Avram, Z. Palmowski, M. Pistorius [2008]. A two-dimensional ruin problem on the positive quadrant. Insurance - Mathematics and Economics 42[1], 227-234
[2] K.Burnecki, M.Teuerle, A.Wilkowska [2019]. De Vylder type approximation of the ruin probability for the insurer-reinsurer model, Mathematica Applicanda 47[1], 5-24
[3] K.Burnecki, M.Teuerle, A.Wilkowska [2021] Ruin probability for the insurer-reinsurer model for exponential claims, Risks 9[5], 86
[4] K.Burnecki, M.Teuerle, A.Wilkowska [2022]. Diffusion approximations of the ruin probability for the insurer–reinsurer model driven by a renewal process. Risks 10[6], 129
[5] K.Burnecki, Z. Palmowski, M.Teuerle, A.Wilkowska [2023]. Ruin probability for the quota share model with phase-type distributed claims.https://arxiv.org/abs/2303.07705",Ruin probability of the insurer-reinsurer quota-share model with phase-type distributed claims,[78478],387,"[126, 45, 135]",3298,New Tools in Insurance Risk Management ,4,10,02,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,Glassalen [building - 101],"['Risk Analysis and Management', 'Financial Modelling', 'Stochastic Models']",TD-02
"This study presents the development of an innovative approach to Stochastic Data Envelopment Analysis [SDEA], considering Chance-Constrained Programming [CCP], Window Analysis [WA], and Optimal Control Theory [OCT] principles in a framework with time and probabilistic constraints. Besids, it involves the average values, covariance matrices and probabilities of each window over time. By incorporating dynamic elements from recent advancements, such as intermediate variables [inventory costs], this model examines the total cost efficiency based on the relationship among variables, such as costs of goods sold [related to the demand], costs of purchased [or produced] products [related to the production or ordered quantities], and inventory costs [related to the initial and final inventories of the period], along with their uncertainties. This enables a dynamic and stochastic evaluation of the cost efficiency of inventory control systems. OCT principles are applied to link variables and introduce dynamics into the system, whereas CCP and WA are utilized to construct temporal windows and integrate their stochastic properties. The result is an SDEA model grounded in OCT principles to assess the total cost efficiency of the control systems from Decision-Making Units [DMUs], as in the retail sector, over time and considering uncertainties. This approach considers real-world conditions, thereby improving analysis and contributing to the development of more efficient control systems.",Stochastic Dynamic Cost-Efficiency - Development of a Chance-Constrained Data Envelopment Analysis [CCDEA] model with Window Analysis and Optimal Control Principles,"[62199, 5889, 71133, 65176, 61803]",937,"[24, 117, 82]",3299,DEA and stochastic models,89,4,48,Data Envelopment Analysis and its Application,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,60 [building - 324],"['Data Envelopment Analysis', 'Programming, Stochastic', 'Optimal Control']",MC-48
"We introduce a mathematical framework for modeling the dissemination of an undesirable agent, encompassing both biological pathogens and misinformation. This framework is articulated as a heterogeneous, partially observable vector auto-regression [VAR] model. In epidemiological terminology, it can be described as a stochastic SEIR-like model.

The utility of a closely related model has been previously validated through its application in analyzing and formulating policy recommendations during the COVID-19 pandemic. In this study, we elaborate on the modifications necessary for adapting this model to accurately represent the dynamics of undesirable content propagation, with empirical validation provided through analysis of data derived from social networks.",Epidemic model of a misinformation spread?,[54263],958,"[56, 135]",3301,Stochastic optimization - theory and applications,49,13,35,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,44 [building - 303A],"['Health Care', 'Stochastic Models']",WB-35
"For fully meeting the transportation demand, railway network has gradually become density and busy, making a close coupling relationship between trains. However, this leads to the severe delay propagation between trains, when disturbance occurs. And train timetable resilience gives timetable the ability to resist disturbances. The key to improve the resilience is to reduce the propagation of train delay, and the structure of train timetable plays a decisive role in this regard. And heterogeneity of train timetable is a key factor. When the heterogeneity is high, the timetable struc-ture is complicated, and once the disturbance occurs, it will constitute a complex delay propagation chain between trains. So, it is necessary to improve the resilience by optimization of timetable structure.
In this paper, we study the optimization of timetable structure towards the im-provement of resilience. And we take the maximum train timetable resilience as the objective function, construct an optimization model of train timetable structure and design a two-stage solution algorithm. Besides, the results of case studies show that the train timetable structure optimization model and algorithm we proposed is effective. And we further verify by simulation that timetable structure optimization method has a significant effect on improving the resilience.",Optimization model and algorithm of train timetable structure for train timetable resilience enhancement,"[78482, 40486, 75779, 68076]",193,"[142, 112, 143]",3302,Resilience in Public Transport Planning,85,7,54,Public Transport Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S01 [building - 101],"['Timetabling', 'Programming, Multi-Objective', 'Transportation']",TA-54
"In 2019, Fédération Internationale de Volleyball [FIVB] introduced a new power ranking, which relies on the probabilistic model relating the teams' strengths to the obtained outcomes. While there are many rankings which already use such models, the FIVB ranking is the first official one which uses the explicit probabilistic model in multilevel games [that is, games with more than three results].

In this work, we explain/assess the  model and the on-line ranking used by the FIVB, placing it in a general statistical framework. In particular, we describe the ordinal model used in the ranking, show why it cannot be used directly for on-line ranking, and explain the approximations adopted by the FIVB. We introduce analytical perspective which explain why the approximations are suboptimal and propose an alternative formulation which significantly simplifies the ranking. Our analysis is backed up by the numerical optimization, where we use the official FIVB games to fit and to optimize the models we consider. 

We conclude that, while the use of the explicit probabilistic model for multilevel ordinal data is an interesting and refreshing change in the world of rankings, and should be a preferred ranking strategy, the resulting algorithm is too complex and suboptimal. We thus formulate a number of recommendations, which vary in the amount of required changes and the performance/complexity trade-offs of the resulting rankings.",FIVB ranking - a misstep in the right direction,"[78484, 78488, 78490]",440,"[99, 7, 5]",3303,Ranking in sports,37,8,16,OR in Sports,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,19 [building - 116],"['OR in Sports', 'Analytics and Data Science', 'Algorithms']",TB-16
"Multi-Criteria Decision Making [MCDM] and Multi-Criteria Decision Analysis [MCDA] are widely used with problem structuring methods to support thinking and decision-making in real-world applications. Within MCDM, Multi-Objective Optimization [MOO] is a commonly used method and yet, review of the literature highlights lack of studies employing a structured method to develop MOO problem formulation from a real-world application and to incorporate stakeholder involvement. Traditionally, research related to MOO focuses on development of solution algorithms, working with a pre-developed mathematical problem formulation. As part of an ongoing research consortium - SIPHER [Systems Science in Public Health and Health Economics Research], we are working with policy partners to develop a Decision Support Tool [DST] that can be used by budget holders in identifying where to target policy action while considering multiple and potentially conflicting outcomes and performance metrics at the same time. We have employed a mixed-method approach in the DST which combines participatory systems mapping with subject-matter experts, developing an evaluation model for the potential policy options and co-working with policy partners to elicit MOO problem formulations. In this presentation, we give rationale for using this approach, our experience of using it along with policy partners, lessons learned and future directions for research.",A Structured Decision Making approach to Improve Population Health and Reduce Health Inequalities,"[78394, 49462, 56920, 78494, 78495, 78496, 49314]",130,"[26, 149, 77]",3304,Methodological Developments in Soft OR and PSMs,26,5,13,Soft OR and Problem Structuring Methods,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,15 [building - 116],"['Decision Support Systems', 'Problem Structuring', 'Multi-Objective Decision Making']",MD-13
"Multimorbidity of malaria, anaemia, and malnutrition [MAMM] is a condition an individual cohabits with two or more of these health outcomes and is becoming an emergent public health concern in sub-Saharan African countries. In addition, the independent associations of child’s sex, age, and socioeconomic disparities with child’s health outcomes and MAMM have been established in literature. But, the effects of the intersection of these factors, while accounting for other covariates have not been studied. Therefore, this study aimed to determine how child’s sex, age and socioeconomic status interact to explain the variations in MAMM among children age 6-59 months in Nigeria. Data from the 2018 Nigeria Demographic and Health Survey was used. A multilevel mixed effect ordinal logistic regression model was used. Five models were created in this scenario. Model 1 is the interaction between child's sex and wealth status; model 2, child's sex and age; model 3 is between child's age and wealth status; model 4 has the three 2-way interactions of child's sex, age, and household wealth status, and model 5 includes model 4 and the 3-way interactions between a child's sex, age, and wealth quintiles. Model 3 was however the model of best fit compared with other models, So, the two-way interaction effects of a child's age and wealth status are relevant in the model prediction. Thus, the variation in MAMM over a child's age differs depending on the household wealth quintile. ","The intersection of sex, age, and household socioeconomic status in the multimorbidity of malaria, anaemia, and malnutrition among children 6-59 months of age in Nigeria",[53751],966,"[56, 65, 139]",3305,Machine learning and analytics in healthcare,3,3,15,OR in Health Services [ORAHS],"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,18 [building - 116],"['Health Care', 'Logistics', 'Sustainable Development']",MB-15
"In the European rail network, infrastructural limitations of terminals pose critical bottlenecks for the scheduling of rail freight services. In addition, passenger traffic on mainlines leading up to the terminals puts additional constraints on efficient rail freight scheduling possibilities. Thus, integrating terminal activities and mainline operations in a rail freight scheduling approach could significantly improve infrastructure usage. However, interactions between freight and passenger traffic in freight terminal scheduling have received little attention in scientific literature so far.
	We propose a two-stage approach to schedule rail freight services in a freight network with interacting mainline passenger services. The focus is on short-term scheduling, and thus freight trains are considered and included in the schedule as requests arrive. In the first stage, we use a dynamic infrastructure cost structure to schedule successive trains. In the second stage, we apply the initial solution from the first stage for a subsequent global optimization of the requested freight train services.
	The applied two-stage optimisation approach minimises the deviation from requested train paths and allows determination of critical infrastructure elements for each included subset of trains. This model improves the quality of freight train scheduling and offers a better inclusion of yards leading up to the terminals into the train scheduling.
",Two-stage approach for rail freight terminal scheduling with mainline passenger traffic interaction,"[62098, 41723]",329,"[122, 143, 129]",3306,ML & OR Applications in Transport Modelling,6,7,55,Transportation,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S02 [building - 101],"['Railway Applications', 'Transportation', 'Scheduling']",TA-55
"Textile waste is a crucial problem for today’s society. The majority of it is generated by the consumer and does not end up as separate waste, but is mixed with residual waste and then burned for energy recovery. To obtain recycled textiles, the operational processes required [e.g., collection and sorting] are still costly compared to using virgin materials, resulting in higher production costs for greener products.
The increase in recycled materials for textiles therefore depends on consumers' willingness to pay a premium for more environmentally friendly products. While some customers may be willing to pay such a premium, others may focus solely on the price of a textile. Understanding these customer segments, particularly a customer's individual willingness to pay for a greener product, will benefit a retailer that offers multiple substitutable products when making inventory decisions.
In this work, we investigate the influence of consumers’ environmental awareness when purchasing textiles and its impact on inventory decisions of a retailer. Therefore, an empirical study including a choice-based conjoint analysis is carried out to reveal consumers’ preferences and willingness-to-pay for ‘greener’ textiles in Austria. These empirical findings are used by a newsvendor model to derive inventory decisions of a retailer, which are evaluated both economically and ecologically.
",Impact of consumers’ environmental awareness on inventory decisions for a retailer in the textile industry,"[43640, 46997, 77815]",427,"[61, 0]",3309,Retail Operations and Marketing,30,8,50,Retail Operations,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,M2 [building - 101],['Inventory'],TB-50
"Airlines try to reduce the boarding time by letting certain groups of passengers enter the queue at the gate in a specific order. Slow passengers use long time to clear the aisle and block other passengers from reaching their seats. We show that a significant proportion of slow passengers can be permitted without increasing the boarding time, by using a simple Slow-in-the-Back-First [SBF] boarding policy that consists of only two groups. The policy is superior to several other group policies that are based on expected differences in aisle-clearing times, e.g., due to hand luggage.

The practical SBF policy allows for only slightly fewer slow passengers than a corresponding optimal “lens” policy, which on the other hand is hard to implement. The lens policy is based on the same principle as an optical lens and is optimal in the sense that no fast passenger can be replaced by a slow passenger without increasing the boarding time. A geometric representation of the queue position and row designation of each passenger and a Lorentzian metric enable computation of the asymptotic boarding time when the number of passengers is large. The analytical asymptotic results are corroborated by simulations with a realistic number of passengers.

We also show how the SBF-policy can be adjusted to include a specified proportion of slow passengers that exceeds the critical upper limit, with limited increase in total boarding time. 

",Geometrical optics inspires fast boarding with slow passengers,[78359],256,"[4, 121, 135]",3310,Airplane Boarding,85,8,54,Public Transport Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S01 [building - 101],"['Airline Applications', 'Queuing Systems', 'Stochastic Models']",TB-54
"Data-driven decisions often raise concerns about fairness and justice, which are frequently confused and used interchangeably. This study investigates the relationship between these two definitions - Fairness is generally considered during the prediction phase of decision-making, yet justice mostly relates to the equality or equity of the resulting decisions. Thus fairness in predictions may not automatically yield just decisions. We propose metrics for quantifying distributive justice in decision-making and investigate how fair predictions relate to just decisions. Two main questions guide our research - Do fair predictions translate into just decisions? Secondly, what is the financial impact of making just decisions? We propose justice metrics and developed mathematical models to improve the quality of the decisions from this viewpoint. We focus on a credit lending application. Our results underscore the distinction between fairness and justice, demonstrating that fair predictions do not guarantee just decisions and may even reduce the overall profit for the lending firm. ",From fair predictions to just decisions,"[65615, 43480]",121,"[66, 0]",3311,On Mathematical Optimization for Explainable and Fair Machine Learning,15,3,27,Mathematical Optimization for XAI,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,047 [building - 208],['Machine Learning'],MB-27
"We consider a lot-sizing problem motivated by liquefied natural gas [LNG]. LNG is an alternative transportation fuel. It can be supplied from alternative sources with different quality and price, at different costs. LNG is subject to quality deterioration. It is yet possible to upgrade its quality by mixing different loads with different levels of quality. LNG is provided to customers via special-purpose facilities. We address the lot-sizing problem in such a facility. The problem entails finding a minimum-cost replenishment plan satisfying demands over a finite planning horizon, while meeting a minimum quality level. We initially formulate the problem as a mixed integer non-linear program, which we then linearize by means of standard linearization techniques. This model, however, is computationally too expensive to be used in practice. To overcome the computational challenge, we identify a class of feasible solutions and develop an efficient mixed-integer linear program by which we can find a minimum-cost solution within this specified class of solutions. We numerically illustrate that our approach provides optimal or close-to-optimal solutions for practical instances within very reasonable computational times.",Dynamic lot-sizing for LNG inventories,[12905],809,"[84, 0]",3314,Lot-sizing with energy aspects,32,8,49,"Lot Sizing, Lot Scheduling and Production Planning","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,M1 [building - 101],['Optimization Modeling'],TB-49
"According to the United Nations, while housing is an integral part of 7 SDGs, it makes direct and indirect contributions to all Agenda 2030 goals. In particular, increasing access to affordable housing remains essential for SDG11, addressing the requirements of low-income, vulnerable, and marginalized households while advancing housing affordability for progress towards sustainable communities. The multi-attribute nature of housing is reflected in its market value, determined not only by economic variables [e.g., price, income, GDP] but also by non-economic factors influencing real estate dynamics, specifically in tourism contexts. Establishing a monitoring framework for residential real estate urban dynamics at the neighborhood level is useful for decision-makers in addressing planning uncertainty associated with uneven spatial development, housing affordability, and gentrification.
This contribution aims to experiment with spatial composite indicators produced through MCDA aggregating methods to rank urban districts of Rotterdam, the Netherlands, concerning the three above-mentioned phenomena. A set of monetary and non-monetary indicators has been implemented as proxy variables to assess the characteristics of housing stock. Assessing housing values using GIS-MCDA aggregation procedures is still a low-explored field which the authors aim to stress to advance research in the assessment of real estate spatial dynamics at the urban scale.","A GIS-MCDA Approach for Spatial Assessment of Real Estate Dynamics - a trial in Rotterdam, the Netherlands","[78111, 51402, 78131, 48194]",891,"[26, 137, 139]",3316,MCDA and urban planning 3,44,10,47,Multiple Criteria Decision Analysis,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,50 [building - 324],"['Decision Support Systems', 'Strategic Planning and Management', 'Sustainable Development']",TD-47
"Investment projects typically involve flexibility of action, also known as managerial flexibility. Managerial flexibility has a value that can be quantified using Real Options Analysis. In order to exercise managerial flexibility, information is required. Unlike financial options, there is typically no objective market information available for real options. Instead, the decision maker usually has noisy information. This means that the current value and relevant factors affecting the project have to be estimated and therefore decisions have to be made on the basis of imperfect information. These estimates are subjective and can be affected by cognitive biases. Cognitive biases are systematic errors in judgement and decision making. As a result, cognitive biases can lead to poor decisions, which can reduce the value of managerial flexibility. In this context, the question arises as to how wrong decisions due to cognitive biases can be avoided. Since cognitive biases can be interpreted as a deviation from rational action, it would be helpful to be able to identify an optimal policy as a benchmark. However, this can be challenging, especially with noisy information. Machine learning algorithms could be useful in deriving an optimal policy. The basic idea is that wrong decisions lead to opportunity costs that can be minimised by an algorithm based on a simulation model. Neural Networks and Reinforcement Learning could be used to make optimal decisions.",Behavioural Real Options Analysis of Projects using Machine Learning,[78497],127,"[131, 66, 10]",3318,Behavioural OR meets Information systems,13,9,07,Behavioural OR,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,1019 [building - 202],"['Simulation', 'Machine Learning', 'Behavioural OR']",TC-07
"At a conference like EURO, it is impossible to catch up with everything; and all too easy to stick to a single stream within your usual professional specialism. This session showcases some of the breadth of OR in action, with a set of twelve 5-minute presentations handpicked to inspire, interest and extend your knowledge. The Fast and Furious session is always intense, enlightening and fun for audience and speakers alike, so a great way to start the conference.

Some of the presentations will be designed for this session only, others will be abbreviated versions of full talks appearing elsewhere in the conference. We are holding this session early on in the conference to give the audience the best chance of catching up with the speaker later on.

The talks cover a wide range of OR methodology across a great variety of practical application areas. Speakers include academics across the range from eminent to early career, as well as practitioners from a variety of industries.

The latest details of speakers can be found on the Making an Impact page on the main conference website [euro2024cph.dk/programme/making-an-impact-2024].",Fast And Furious - Lightning Talks,"[59535, 76066, 61471, 77548]",492,"[151, 0]",3320,Fast And Furious - Lightning Talks,40,2,46,Making an Impact,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,40 [building - 324],['Practice of OR'],MA-46
"An essential aspect of Emergency Medical Services [EMS] is the effective allocation of ambulance resources, which can greatly influence patient outcomes. Key to this is the dispatch policy for ambulance deployment and the question of whether specialised ambulances, specifically designed for certain types of incidents, should be assigned to other types of emergencies when available. This research uses queuing models to represent the different dispatching policies and implements simulations to identify the most efficient policy for ambulance resource allocation that would yield optimal patient outcomes when using specialised vehicles. This study proposes a simulation framework and methodology that could be adapted to other geographical regions, thus offering a versatile tool for evaluating the potential benefits of deploying specialised emergency vehicles. The model considers multiple types of emergency vehicles, the distribution of patient demand, and the EMS response process. The experimental results demonstrate that the deployment of advanced specialised ambulance vehicles can enhance the survival rates of emergency incidents, particularly those corresponding to the vehicles' intended function.",Rendezvous Strategy for Emergency Logistics,"[71865, 72430, 47364]",593,"[143, 56, 84]",3321,EMS logistics,3,2,10,OR in Health Services [ORAHS],"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,11 [building - 116],"['Transportation', 'Health Care', 'Optimization Modeling']",MA-10
"Local energy management [LEM] addresses local stakes and thus a LEM strategy does not necessarily align with the aim of a centralized electric system. Yet, if spread widely, it can affect the global load curve and therefore the global electric system [GES] and the price associated. In addition, the investment decision into a LEM solution and oftentimes the management strategy itself are highly dependent on the global electricity price and the accuracy of its prediction. Hence, what would be the impact of a LEM solution adoption – by a significant number of consumers, referred to as “local consumers” – on its own profitability through the effect it can have on the global price? This paper aims at modeling the interaction between LEM and a GES. To do so, we propose a two-level equilibrium optimization model. One level models the impact of implementing a LEM solution for a “local consumers” on their aggregated load curve. The second level models the variation in the global electricity price resulting from the shift of the global load curve induced by those local consumers. The price being both a decision parameter for the LEM in the first level and the output of the second, the optimization purpose is to find the equilibrium. This paper then presents the results of the model for several LEM solutions such as demand response, residential photovoltaic self-consumption and management of electric vehicle charging, using aggregated simulated data for the consumption.",Modeling the interaction between local energy management and a global electric system,"[78498, 31541]",842,"[36, 93, 33]",3323,OR in Energy,23,13,19,OR in Energy,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,44 [building - 116],"['Electricity Markets', 'OR in Energy', 'Economic Modeling']",WB-19
"In this research, we study a variant of the project scheduling problem with
resource constraints [RCPSP] under uncertainty. In reality, in fact, project parameters are subject to considerable uncertainty, which generally leads to numerous schedule disruptions. Most optimization approaches focus on the quality robustness and only evaluate the relevance of the solutions through the objective function value. However, it can be more important to optimize the solution robustness, concerning the structural similarities between the nominal solution and the disrupted solutions.
This translates into the need of developing robust schedules that can absorb disruptions as smoothly as possible and, once the uncertainty is revealed, find an alternative solution which is as similar as possible to the nominal solution.
Following this stream, in this talk, we present a two-stage stochastic
programming formulation for the RCPSP where in the first stage, scheduling
decisions are taken considering the information available on the project
parameters. In the second stage, the schedule is completely re-optimized
leading to a new set of starting times, induced precedence relations and
resource flows. We will incorporate solution robustness through stability
constraints added into the model.",The resource constrained project scheduling problem with stability constraints.,"[11581, 13330]",348,"[118, 136]",3325,Project scheduling under uncertainty,35,4,60,Project Management and Scheduling,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S09 [building - 101],"['Project Management and Scheduling', 'Stochastic Optimization']",MC-60
"The Minimum Life On Receipt [MLOR] rule is established by retailers to impose the maximum age a perishable product can be accepted upon reception. Such a rule tends to be rigid but allowing the rule’s flexibility may bring benefits for both producer and retailer. The objective of this work is to investigate such benefits.
The problem will be represented by a bilevel optimization model where the retailer is the leader, and the producer is the follower. In this problem, the rational response of the producer is to either comply with the fixed MLOR rule imposed by the retailer or offer a discount to the retailer’s orders whenever he/she is willing to adopt a flexible rule.
An instance composed of a perishable product with 3 days of shelf life and a time horizon of 3 periods is used to enumerate all possible solutions for the problem. The solutions are examined in terms of profits and waste generated.
Afterwards, an exact solution method is developed through single-level reformulation aiming to solve the general problem.
",Definition of the MLOR rule through a bilevel optimization model,"[56864, 43398, 23114]",425,"[72, 50]",3326,Retail Inventory Management I,30,3,50,Retail Operations,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,M2 [building - 101],"['Mathematical Programming', 'Game Theory']",MB-50
"The transition to the circular economy goes hand in hand with major redesigning challenges in manufacturing and logistics. Reverse supply chains form an important area of study within this context. The study of reverse supply chains touches upon many different disciplines, and from an Operations Research perspective provides interesting optimization problems. Our study concerns the reverse supply chain of insulation panels in the construction sector that contain rigid polyurethane foam [PUR]. We examine a stochastic facility location model for locating treatment plants. Context specific constraints incorporating a cap on emissions and the economically rational behaviour of waste sorting companies are embedded into the model. Uncertainty affects the model, as the exact location and quantities of different types of insulation panels are not known exactly. Furthermore, some of the model parameters depend on future policy and advances in technology. For large buildings, we assume that the panels can be collected directly from the demolition site. As each of these buildings thus has an individual demand for collecting the material, both the model and the number of uncertain parameters to become very large. We investigate the performance of different methods of reducing the problem size for our model. We address methods to reduce the number of scenarios, and methods to reduce the number of demand points individually, and in synthesis with one another.",Large stochastic facility location problems for insulation waste treatment - Reduction-based solution techniques,"[73761, 39439, 71721, 47048]",922,"[100, 136, 138]",3328,Supply chain design in the circular economy,18,8,23,"Circular Economy, Remanufacturing and Recycling ","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,82 [building - 116],"['OR in Sustainability', 'Stochastic Optimization', 'Supply Chain Management']",TB-23
" In this 90' panel session, recruiters of OR professionals, data scientists or analytics people will discuss and compare how they go about it, and address burning questions relevant to both those recruiting professionals, as well as those looking to secure a new industry position.   

How do companies and teams come to define the profile to hire, crafting the job ad? What skillset and traits do you look for, and how do you define that? How do you design a hiring process covering those aspects, and what are the challenges and best practices for the recruiting team themselves? What advice would the hiring team give their counterparts. 

Join us to learn about each other’s challenges, best-practices, and ask your own questions during a Q&A. 

For the latest details of speakers, please see the Making an Impact page on the main conference website https://euro2024cph.dk/programme/making-an-impact-2024",The Quest for the Right OR Professionals – a Hiring Perspective [90' panel discussion],"[46961, 71364, 49440, 21574, 80287]",493,"[151, 106]",3329,The Quest For The Right OR Professionals,40,5,46,Making an Impact,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,40 [building - 324],"['Practice of OR', 'Profession of OR']",MD-46
"In this work we study how carbon tax induces a transition to cleaner production under different types of technologies and different demands over a planning horizon. To determine the effectiveness of carbon tax we propose a model based on strategic capacity production planning under carbon taxes. We also present different performance measures to quantify the effectiveness of carbon taxes. The model used is formulated as a mixed integer linear problem [MILP] and consider elements that previous works have not jointly studied, such as machine replacement, workforce planning, and maintenance; elements that are relevant when studying the technological transition. The effectiveness measures proposed in this works, consider levels of clean production and periods to reach a full technological transition. Our computational experiments, based on a real case, have shown that in the absence of carbon taxes, a firm has no incentive to transition to clean technology. Still, the effectiveness of carbon taxes depends on the characteristics of the technology available for the production process and the magnitude of the demand. Finally, we present managerial insights aimed at both companies and the environmental authority to help the planning for a transition to clean technology.",On Carbon Tax Effectiveness in Inducing a Clean Technology Transition - An Evaluation Based on Optimal Strategic Capacity Planning,[73581],812,"[100, 12, 137]",3330,Carbon Taxing and Emissions Controls,80,15,53,Sustainable and Resilient Systems,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,8007 [building - 202],"['OR in Sustainability', 'Capacity Planning', 'Strategic Planning and Management']",WD-53
"Expand your skills with these two 45' practical sessions - Building And Sharing Apps - An Introduction To Streamlit, and OR Goes Kubernetes - How To Run Long Running Optimization Jobs In The Cloud 

[i] Building And Sharing Apps - An Introduction To Streamlit [Sander van Aken, Flixbus]
Making impact with OR often requires an end-user or other stakeholders to interact with the models and algorithms you build, or with its outcomes. With interactive applications, we can leverage the true power of human-machine collaboration, e.g. by letting users guide your optimization model with their knowledge, or enabling them to explore multiple solutions. Developing fully-fledged front-end applications - or even discovering what is the right thing to develop - can however take up a tremendous amount of time. For some analyses or projects, it is not even worth the effort. 
The Python-based framework Streamlit could be your new companion in these endeavours. In this tutorial, we demonstrate how we can use it to quickly develop a first version and iterate on that. 


[ii] OR Goes Kubernetes - How To Run Long Running Optimization Jobs In The Cloud  [Jonas Witt DHL]
While response time often matters in OR applications, in many cases it does not [think network design, tactical capacity planning, …]. In these use cases, model usage will be dispersed over time and not warrant the constant provisioning of a high-end compute resource at all times. In our OR applications at DHL Group, we experienced that this often led to some boilerplate functionality that had to set up across use cases - Sequencing of requests, automatic deployment of workloads, status monitoring, result retrieval, etc. In this tutorial we will share what we are currently building to replace boilerplate code with a scalable, modern tech-stack to support the deployment of tactical decision support tools.
",Practical skills - Knowledge-sharing tutorials -1- ,"[46961, 42315, 71364]",494,"[151, 84]",3331,Practical Skills - Knowledge-Sharing Tutorials -1-,40,4,46,Making an Impact,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,40 [building - 324],"['Practice of OR', 'Optimization Modeling']",MC-46
"In the wake of an increasing number of natural disasters such as storms and cyclones, overhead electricity and telecommunications infrastructures are exposed to significant risks, with damage to pylons, antennas and poles disrupting essential services.

This study introduces a hybrid model that integrates machine learning and discrete optimization to optimize emergency response. By analyzing storm damage to electricity and telecoms networks, we apply machine learning to cluster affected areas based on storm severity, customer reports and wind speed. This data-driven insight informs a unique mixed-integer programming model, treating the challenge as a variant of the capacity expert routing problem. Our solution employs computational experiments and the Branch and Price algorithm, with a detailed case study of a storm event in France serving as a practical illustration.

The results demonstrate the effectiveness of our hybrid approach in optimizing emergency team deployment strategies, adapted to the varying requirements of different storm severities. This contribution enriches the emergency operations management and disaster relief domain, presenting a novel perspective on combinatorial optimization for crisis mitigation.
",Optimizing Post-Disaster Recovery Strategies in Telecommunication Network Infrastructure,"[59749, 66471]",317,"[141, 8, 145]",3332,Data Science and Optimization,14,12,03,Data Science Meets Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1005 [building - 202],"['Telecommunications', 'Artificial Intelligence', 'Vehicle Routing']",WA-03
"At a conference like this, we want to interact with, and learn from, each other. Unfortunately, it’s sometimes just not so easy to bump into and connect with the right people to discuss the topics you want to learn about. 

In this open space session, it’s the participants who bring in, and discuss about, topics which are on their mind. So you want to learn from others about a particular topic related to making an impact with OR in practice? Join us in this session where you set the agenda. At the start, we will collect the topics which are on your and others’ mind. Using this, we split in smaller groups, and you join the one[s] in which you are interested. As such, it’s you who shape the discussions, and can get a lot out of it. 

No topic, just curious what’s on others’ minds? There will already some kick-starter points available. ",What’s On Your Mind? Getting What You Want From This Conference ,[71364],496,"[106, 151]",3333,Open Space,40,8,46,Making an Impact,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,40 [building - 324],"['Profession of OR', 'Practice of OR']",TB-46
"Large Language Models [LLMs] have taken the world by storm since the release of ChatGPT. These models can work with large amounts of unstructured data, such as text and images. Their output is easy to understand, but they cannot reason and apply logic.
On the other hand, Mathematical Optimization Models require structured and complete data but provide optimal output. However, it can often be a challenge to collect the necessary data in an organization and ensure that users can interact with the final application and understand the output. 
Can these very different models complement each other when we build OR applications?
This 90' session will be led by Michael Lindahl. Michael has been building OR-powered decision applications across different industries for more than 10 years. Since the release of ChatGPT, he has been exploring the synergies between simple optimization and LLMs by building the application www.findgaven.ai on top of their API's.
Michael will give an introduction to large language models, explaining their capabilities, the pitfalls, and how they can fit into the OR toolbox. 
This will lead to discussion, with the audience invited to share not only their questions but also their own experiences, aspirations, and concerns.
","Generative AI For OR People - Disruptor,  Enabler Or Distraction?","[46961, 49440]",495,"[151, 8, 106]",3334,"Generative AI For OR People - Disruptor, Enabler Or Distraction? ",40,7,46,Making an Impact,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,40 [building - 324],"['Practice of OR', 'Artificial Intelligence', 'Profession of OR']",TA-46
"The evolving retail landscape, marked by increasing production costs and resource constraints, highlights the need for assortment planning decisions made with supply chain considerations. Retail product manufacturers typically have their assortment decisions led by the marketing department, yet these are intricately linked to operational decisions across procurement, production, and inventory planning. We propose an integrative optimization model to address assortment planning problem in a make-to-stock environment, leveraging customer substitution behaviors to mitigate supply and demand uncertainties. Our framework addresses the challenge of aligning product offerings with supply chain capabilities and customer preferences to maximize expected profit, while considering procurement and production capacity constraints. We formulate the problem as a multi-stage stochastic program with uncertain and time-varying demand, represented by multi-period look-ahead forecasts, as commonly observed in practice. Results are presented using data from an e-commerce furniture manufacturer and retailer. ",Assortment Planning Optimization under Uncertainty with Customer-driven Product Substitution,"[66036, 32566, 62766]",481,"[138, 105]",3335,Assortment Management,30,9,50,Retail Operations,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,M2 [building - 101],"['Supply Chain Management', 'Production and Inventory Systems']",TC-50
"Fast fashion brands seamlessly integrate various sales channels to create a cohesive shopping experience. This seamless integration extends to the handling of product returns, ensuring that customers experience consistent policies across all channels, thereby enhancing overall satisfaction and retention. Particularly during peak shopping seasons and special occasions, consumers value the convenience of returning products to any store within the brand's network. However, the growth of applying omnichannel retailing has also led to an increase in product returns, posing challenges for inventory management and environmental sustainability due to unsold merchandise. This paper presents a solution for inventory management, considering the complex dynamics of omnichannel systems and the unique challenges generated by high return rates. Using dynamic programming, we formulate a model that optimizes pre-season order quantities for online warehouses and transshipment quantities to brick-and-mortar stores during the selling season. Our sensitivity analysis offers valuable insights for retailers seeking to balance profitability with environmental responsibility in the face of evolving consumer preferences and market dynamics. ",Inventory Management for an Omnichannel Retailer under Product Returns,"[67560, 78506, 43480, 15094]",918,"[61, 138, 125]",3337,Managing product returns,18,3,23,"Circular Economy, Remanufacturing and Recycling ","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,82 [building - 116],"['Inventory', 'Supply Chain Management', 'Reverse Logistics / Remanufacturing']",MB-23
"Just how different is it to work within an industry or a commercial consultancy, if you have been used to working from within academia? In this 90' session, a panel of people who have made the move will share their experiences and lessons learned. The differences may not be the ones you expect.

The latest details of panel members can be found on the Making an Impact page on the main conference website [euro2024cph.dk/programme/making-an-impact-2024]; the panel is expected to include Ruben Ruiz [Amazon, and a keynote speaker at this conference], Rodrigue Fokouop [Air Liquide], Martina Fischetti [ex-Vattenfall], chaired by João Paiva Fonseca [Portchain]",From Academia To Practice - Reflections From Those Who Have Taken The Leap,"[46961, 50835, 47530]",497,"[106, 151]",3340,From Academia To Practice - Reflections From People Who Have Taken The Leap,40,9,46,Making an Impact,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,40 [building - 324],"['Profession of OR', 'Practice of OR']",TC-46
"Practitioners who use MIP have a great choice of possible softwares. The huge choice can make it harder to make decisions; and that’s where this 90' session should be invaluable.
We bring together representatives from Xpress, Gurobi and Hexaly, with two heavy users of one or more of these we try to answer questions like - Why use a 3rd party solver instead of in house options? What problems have companies experienced in this approach? And of course… which solver is the best in practice? There will be plenty of opportunity for audience Q&A.
This is a reprise of a session first held in 2018. Since then, the landscape and the power of solvers has changed immeasurably, and we will be reviewing just how users can make the most of this.
For full details of speakers, please see the 'Making an Impact' page on the main conference website.",MIP Solvers In Practice,"[46961, 12336, 45600, 16880, 77660, 52787]",498,"[151, 134, 84]",3342,Which Solver?,40,14,46,Making an Impact,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,40 [building - 324],"['Practice of OR', 'Software', 'Optimization Modeling']",WC-46
"The speakers on this panel all have wide experience of working with stakeholders to deliver projects putting OR into practice.  In the first part of the session, each speaker will talk about their experiences and learnings, and how these have shaped their own views on which factors influence whether an application could be [or will be?] a success, a failure or somewhere in-between. We will then invite audience questions, and open up the discussion - do these sound familiar? What are your own experiences? What lessons would you like to share on how to address them?

For final details of speakers, please see the 'Making an Impact' page on the main conference website.","Success, Failure And The Factors That Influence Outcomes ","[46961, 71364, 45600, 5932, 31819]",499,"[151, 106]",3344,"Success, Failure And The Factors That Influence Outcomes",40,10,46,Making an Impact,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,40 [building - 324],"['Practice of OR', 'Profession of OR']",TD-46
"The integrated long-haul and local vehicle routing problem with an adaptive transportation network is a very challenging optimisation problem. The adaptive nature of the transportation network means that the resulting optimisation problem is extremely large and difficult to solve using general purpose solvers. As such, only very small instances of this problem can be solved without the use of decomposition techniques. We propose a Benders' decomposition approach that identifies transportation network design in the master problem and then solves the integrated long-haul and local vehicle routing problem in the subproblem. The Benders' decomposition approach is used to solve a relaxation within a parallelisation framework that concurrently solves an iterative algorithm for finding high quality primal solutions. The results will show that applying Benders' decomposition increases the scale of problems that can solved and improves the upper and lower bounds that can be achieved.",A parallel Benders' decomposition algorithm for solving the integrated long-haul and local vehicle routing problem,"[78505, 69531, 29543]",191,"[138, 102]",3345,Parallel Solvers,76,7,30,Software for Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,53 [building - 208],"['Supply Chain Management', 'Parallel Algorithms and Implementation']",TA-30
"Expand your skills with these two 45' practical sessions - Make Easy Changes, Make Changes Easy - A Tutorial On OR Coding Quality; followed by Tricks From The Trade For Large Scale Optimisation.  

[i] Make Easy Changes, Make Changes Easy - A Tutorial On OR Coding Quality [Michele Quattrone, Air Liquide] 
The implementation of well-thought-through quality strategies on the software lifecycle is becoming a requirement for many software developments, even for the early mock-up/prototyping phases. In this 45-minute tutorial, backed with a practical example, we will discuss together how our coding can benefit from this strong push, and what we can cherry-pick from hyping buzzwords such as DevOps, Continuous Integration, Continuous Deployment, MLOps...in practice!

[ii] Tricks from the Trade for Large-Scale Optimization in a practical context [Andreas Schmitt, Zalando SE]
Have you had to face up to optimization challenges, where you have had to develop and deploy heuristics in order to implement models into production environments? This session will use the speaker's experience of a successful application as a starting point for discussion.

Practical optimization problems often quickly increase in scale and complexity, necessitating tailored solution approaches. Our talk describes heuristics designed to work along-side a Lagrangian decomposition method, leading to almost-optimal solutions. These have been developed in the context of the frequently applied ’predict-then-optimize’ paradigm, more specifically for markdown pricing strategies in the online fashion industry. We present empirical evidence for the heuristics’ effectiveness, drawing on pricing applications at Zalando SE.

This exposition will serve as a starting point for discussions - Participants will be encouraged to explore questions such as - What are your experiences with similar optimization challenges? How do you approach heuristic development in their work? What are effective approaches for deploying such models into production environments and measuring their impact?",Practical Skills - Knowledge Sharing Tutorial -2-,"[46961, 48048, 55669]",500,"[151, 84]",3347,Practical Skills - Knowledge-Sharing Tutorials - 2,40,13,46,Making an Impact,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,40 [building - 324],"['Practice of OR', 'Optimization Modeling']",WB-46
"Considering growing number of TEUs shipped globally, there continue to be increasing resource constraints in container ports. In addition, changing and uncertain exogenous factors impact predictability and efficiency of container port operational planning. 
A DRSA approach is introduced, integrating a multi-criteria classification problem into the solving approach of the container space allocation problem. The approach is discussed in the wider context of port operations, emphasising the sensitivity of the problem solution on operational planning strategies within a port system. 
",Container Space Allocation using a Dominance-Based Rough Set Approach [DRSA],"[25559, 54300, 50208]",679,"[70, 65, 77]",3349,Container Stacking and Yard Planning II,52,9,62,OR in Port Operations,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S12 [building - 101],"['Maritime applications', 'Logistics', 'Multi-Objective Decision Making']",TC-62
"In this work, we study the sustainability performance of European Countries by applying a methodology that combines both multiple criteria decision-making and statistical techniques. Using individual sustainability development indicators for the European countries available in the EUROSTAT database, we first build composite indicators to assess the economic, social, and environmental dimensions of the territories. Based on the information obtained, secondly, a statistical analysis is done to regress the composite indicators only considering those individual indicators that are somehow controllable by policy makers. The main objective is to get some insights into the impact that a modification of these controllable individual indicators would have on the overall sustainable development. Finally, we focus on the Spanish case, whose sustainability situation can be improved, as this country does not reach the best possible values of the composite indicators of the three dimensions. However, to make a decision about how to improve its situation, further information is needed in order to know the extent of the possible improvement, the trade-offs existing among the dimensions, and how this improvement could be attained. Therefore, we build a multiobjetive optimization problem based on the statistical analysis previously performed, which is aimed at identifying the most desired compromise among the three sustainability dimensions to enhance the sustainability situation of Spain.",Assessing the Sustainability of European Countries Using Quantitative Techniques,"[57780, 23169, 6266, 23140, 78552]",110,"[139, 77]",3350,"MCDA and Composite Indicators - Issues, Advances and Applications 1",44,14,44,Multiple Criteria Decision Analysis,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,20 [building - 324],"['Sustainable Development', 'Multi-Objective Decision Making']",WC-44
"Given a directed graph with negative edge costs and possibly negative cycles, the elementary shortst path problem consists in finding a shortest elementary path, i.e., a path from a source node to a target node that visits each node at most once. In this talk, we discuss various formulations and relaxations for this NP-hard problem.
",Relaxations for the elementary shortest path problem,"[2481, 72323]",393,"[14, 0]",3351,Convex and conic optimization,68,13,38,"Conic Optimization - Theory, Algorithms, and Applications","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,34 [building - 306],['Combinatorial Optimization'],WB-38
"The energy transition is a global priority to address climate change and ensure energy security. While technological developments play an instrumental role in enabling this transition, the importance of human behaviour cannot be understated. People's energy consumption practices significantly impact energy demand patterns. Additionally, the adoption of renewable energy technologies depends on public acceptance, and energy projects often affect communities. Operational research [OR] models and methods have been successfully employed to address several complex problems in the energy sector, such as the location of infrastructures, the energy sharing in energy communities and market design, involving multiple stakeholders, including utilities, governmental agencies, and consumers. The objective of this study is to organise the relevant operational research challenges related to the energy transition, identify the associated behavioural aspects, and examine several disciplinary approaches used to address these challenges. Ultimately, the aim is to identify research gaps, contributing to the advancement of the behavioural OR discipline.",Advancing BOR - Structuring challenges and [inter]disciplinary approaches in the energy transition,"[37142, 123]",112,"[10, 37]",3352,BOR in public policy and environmental decisions,13,10,11,Behavioural OR,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,12 [building - 116],"['Behavioural OR', 'Energy Policy and Planning']",TD-11
"Recommender systems and informed assortment planning facilitate the display of goods or services tailored to individual customer needs/characteristics, promote the visibility of assorted products, and revenue growth. We address the problem of optimal assortment and display of online search results for goods or services. The objective is to maximize the platform's revenue and customer engagement by leveraging the menu of displayed search results to users, subject to catering to their individual search criteria or characteristics. Our analysis is based on a detailed data set from a leading online platform. Assortment planning involves optimization over a utility set-function. For example, a supermodular utility function is related to complementary goods, and a submodular utility function is related to substitutes. By analyzing our data, we identify properties of our utility function and implement the proper optimization algorithm tailored to individual users.
Recent advances in the discrete optimization field has enabled us to provide optimal or near-optimal algorithms in polynomial time to tackle the difficult problem of submodular function optimization",Data-Driven Submodular Set-function Optimization - Theory and Applications in Assortment Planning and Recommender Systems,[72046],323,"[7, 61, 136]",3354,Data science meets strongly NP-Hard CO ,14,10,03,Data Science Meets Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,1005 [building - 202],"['Analytics and Data Science', 'Inventory', 'Stochastic Optimization']",TD-03
"We consider estimation of parameters defined as linear functionals of solutions to linear inverse problems. Any such parameter admits a doubly robust representation that depends on the solution to a dual linear inverse problem, where the dual solution can be thought as a generalization of the inverse propensity function. We provide the first source condition double robust inference method that ensures asymptotic normality around the parameter of interest as long as either the primal or the dual inverse problem is sufficiently well-posed, without knowledge of which inverse problem is the more well-posed one. Our result is enabled by guarantees for novel iterated Tikhonov regularized adversarial minimax estimators based on minimax [robust] optimization over general hypothesis spaces.",Source Condition Double Robust Inference on Functionals of Inverse Problems,[78511],562,"[33, 127, 66]",3357,"Advancements of OR-analytics in statistics, machine learning and data science 15",16,10,06,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,1013 [building - 202],"['Economic Modeling', 'Robust Optimization', 'Machine Learning']",TD-06
"Maximizing the channel fidelity is a major problem in quantum information theory. Many methods have been developed for inner points and for some error models exact codes are available.  In general, however, one would be interested in certificates, i.e. outer bounds. In this talk, we will present a method for generating outer points and a hierarchy that also allows for inner points. The talk is based on the standard quantum de-Finetti hierarchy for problems with linear constraints [arxiv - 1506.08810]. Starting there, we are able to prove the existence of a sequence of inner points, which have similar error bounds as the outer bounds. This result yields that at least theoretically certification of actual error correcting codes becomes possible. 
",de-Finetti Methods for Channel Coding Theory,[75302],383,"[21, 0]",3358,Optimization in Quantum Information,83,5,42,Quantum Computing Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,98 [building - 306],['Convex Optimization'],MD-42
"Optimizing the quantum relative entropy under linear constraints is  a central problem in Quantum Key Distribution [QKD] and other fields of [Quantum] Shannon theory. In particular providing provable lower and upper bounds is a highly relevant task.
We provide a practical and resource efficient method for this problem.
At the core of our work stands a recently described, and pleasingly elegant, integral representation of the quantum  relative entropy, which we employ in order to formulate the problem of reliably bounding it as an iteration  of semi definite programs [SDP].
In contrast to existing techniques, our method comes with a provable convergence guarantee of quadratic order, whilst staying resource efficient with the matrix dimension of the underlying SDPs. We furthermore can provide an estimate for the gap to the optimum at each stage of the iteration. Combining this with some clever heuristics for the iteration, we find that convergence can in practice be actually achieved much faster then theoretically guaranteed.
",Optimization  of the Relative Entropy under linear constraints via semidefinite programming,[78500],383,"[115, 21, 113]",3359,Optimization in Quantum Information,83,5,42,Quantum Computing Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,98 [building - 306],"['Programming, Semidefinite', 'Convex Optimization', 'Programming, Nonlinear']",MD-42
"Green hydrogen can play a crucial role in the energy transition but this role remains largely unquantified.
This work presents the adaptation of REMix, an energy capacity expansion optimisation tool, for New Zealand [REMix-NZ]. With this, optimal pathways for achieving net-zero goals across energy sectors are evaluated. Unlike previous applications, REMix-NZ considers New Zealand's unique characteristics, including:
•	High share of hydropower - The model leverages existing variable hydropower resources as the backbone while integrating additional renewables like wind and solar PV.
•	Medium-sized islanded system - This allows for detailed analysis of optimal energy import and export strategies, particularly for green hydrogen and its derivatives.
We focus on the role and impact of green hydrogen, investigating its potential to decarbonise challenging sectors. This results in a large optimisation problem involving thousands of investments and millions of operation decision variables.  The results illustrate the trade-offs and synergies between domestic energy needs and potential hydrogen exports, providing valuable insights for policymakers and stakeholders.

","Big hydro, two islands, one goal - Exploring New Zealand's path to Net-Zero with Green Hydrogen integration","[77614, 74115, 74116, 54534, 54516]",839,"[37, 12, 93]",3361,The role of storage in energy problems,23,8,19,OR in Energy,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,44 [building - 116],"['Energy Policy and Planning', 'Capacity Planning', 'OR in Energy']",TB-19
"We study a resource constrained project scheduling problem. A set of jobs arrive in real-time and each of them is a sequence of tasks that require several resources for execution. There are minimum and maximum time lag constraints among the consecutive tasks, which may have uncertain durations. The objective is to minimize the total waiting time so that we guarantee the feasibility of the resulting schedule. 
The problem has a great practical interest in small-scale biomanufacturing systems, such as laboratory for drug design, where there can be large uncertainties in the duration of the tasks. For instance, the production of CAR T cells for personalized gene therapies of patients with a serious disease. The production process requires several steps including cell expansion where millions of CAR T cells are grown in a cell culture. The variance of cell expansion time may be several days, while the other production steps are deterministic and take a couple of days.
We have the following results:
1] We provide theoretical bounds about the quality of solutions we may expect.
2] We propose a polynomial time reactive scheduling policy for inserting tasks into a schedule. This method [i] can handle large instances, [ii] outperforms a simple heuristic with the feasibility guarantee, and [iii] starts each job as early as possible relative to the previously scheduled jobs.

Ack. - This work has been supported by the H2020 project AIDPATH, grant agreement number 101016909.",Resource constrained project scheduling with durational uncertainties and maximum time lags,"[78519, 49684, 4503]",873,"[129, 17, 127]",3362,Optimization problems in scheduling,64,12,26,Combinatorial Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,012 [building - 208],"['Scheduling', 'Computational Biology, Bioinformatics and Medicine', 'Robust Optimization']",WA-26
"US companies typically announce their earnings report after the stock market has closed. Their stock price usually experiences a significant move, up or down, the following day. Predicting the direction of this movement can be tricky, as the stock price may go down even after a good earnings report and vice versa. 

As a result, market players focus on non-directional options strategies, such as Iron Condors, to capture a quick profit overnight. Even then, if the magnitude of the post-earnings price move [in either direction] is too large, this strategy ends up losing money. Therefore, it is crucial to predict the magnitude of the post-earnings price change successfully. A highly successful predictive model could make a massive difference in a hedge fund's performance.

This paper aims to develop a Machine Learning model that predicts the magnitude of the post-earnings stock price change. We first noticed that Estimize.com is an acclaimed data provider in the finance industry, and it publishes consensus predictions for earnings reports based on the predictions of its best competing analysts. Also, Thomson Reuters, one of the two major data providers in finance, publishes Estimize's consensus predictions the day before earnings releases. We then decided to record the variance of the most confident analysts' predictions at Estimize. We then analyzed the relationship between this variance and the magnitude of the post-earnings stock price change. We have promising results.
",A Machine Learning Model for Options Traders that Predicts the Magnitude but not the Direction of Post-Earnings Stock Price Jumps,[76169],372,"[45, 66, 148]",3363,Advanced Options Strategies Using O.R. and Machine Learning,4,9,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S14 [building - 101],"['Financial Modelling', 'Machine Learning', 'Web-based Information Systems']",TC-63
"Humanitarian logistics is crucial in crisis and disaster scenarios, enhancing system resilience and facilitating rapid restoration. Post-disaster relief distribution is a critical challenge, requiring efficient and effective relief to minimize suffering. The complexity of relief distribution involves navigating damaged infrastructure, coordinating with stakeholders, and adapting to dynamic conditions. The social impact of relief distribution, especially on vulnerable populations, is paramount. Ensuring accessibility to essential resources and efficient connectivity can mitigate suffering. Colombia, a developing country, is most vulnerable to natural disasters, climate-related risks, and economic instability. The 2015 flash flood in Salgar, Antioquia, caused 93 deaths and affected over 1,263 people. The community faces challenges in post-disaster relief distribution, including damage assessment, accessibility, resource allocation, and efficient node placement. This research proposes an optimization framework for transportation networks to improve relief distribution efficiency and effectiveness in disaster-affected areas, leveraging a mathematical model. The framework uses multidisciplinary methods, including qualitative data analysis from humanitarian logistics experts and optimization algorithms for quick decision-making. It optimizes aid distribution to maximize coverage, minimize response times, and allocate resources.","Post-disaster Relief Distribution Optimization Framework in Salgar, Colombia","[77170, 78521, 53046]",773,"[58, 143, 84]",3364,Post-Disaster Relief Distribution,38,12,21,OR in Humanitarian Operations [HOpe],"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,49 [building - 116],"['Humanitarian Applications', 'Transportation', 'Optimization Modeling']",WA-21
"Balancing increasing shares of variable renewable generation requires flexible technologies such as hydropower. However, hydropower representation is often oversimplified in current power system investment models, leading to an overestimation of the flexibility potential. More accurate representations are therefore required to correctly map the available resources. A common simplification is reservoir aggregation, which removes complexity added by reservoir couplings. This information becomes increasingly important for pumped hydropower operation, to account for individual reservoir capacity limits and flow constraints between reservoirs. Additionally, deterministic formulations do not account for any uncertainty or variation in inflow or the availability of wind and solar resources. This is crucial to include for systems with a high share of generation from variable renewable sources.

This work aims to combine techniques from power system investment models with hydropower operational models to more accurately represent the flexibility from pumped hydropower storage. This framework is then to be used to compare the viability of large-scale pumped hydropower storage in Europe to other storage solutions, such as battery or hydrogen storage, under different policy scenarios for carbon emissions. ",Modelling competetive pumped hydropower,[78517],397,"[12, 36, 93]",3365,Decarbonized energy systems & markets,22,12,09,Energy Markets,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,10 [building - 116],"['Capacity Planning', 'Electricity Markets', 'OR in Energy']",WA-09
"In this presentation we report on a large-scale time study to estimate the expected duration of a set of common nursing tasks in 25 Belgian hospitals involving over 600 nurses. A direct time measurement by starting and stopping a timer at the beginning and end of every executed task proved to be too invasive and complicated if done by the nurse itself and too expensive if done by an outside expert. We therefore used an indirect method by equipping each nurse with a smart phone running an application that beeps at irregular time intervals. After every beep, within a time-out interval, the nurse registers what task type she was executing. In  addition, after the nurse's work shift is finished, she is presented with a list of 10 task types and asked to tally how many tasks of each type were executed during the shift. For a certain task type, the beeps allow us to estimate the fraction of time a nurse spends on tasks of this type, while the tallies estimate their rate, i.e. how many tasks of this type occur per time unit. The ratio of the fraction and rate is the expected duration.

We provide approximations of the bias and the variance of the resulting estimator using the Delta method leading to reliable confidence intervals and optimal strategies for deciding which task types to tally in future nurse shifts. In addition, we investigate the quality of our approximation by comparing to Monte Carlo simulations.",An indirect time study for nursing tasks in Belgian hospitals - methodological aspects and optimal sampling,"[59857, 78523, 59853]",950,"[56, 129, 135]",3367,Nurse rostering,3,10,15,OR in Health Services [ORAHS],"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,18 [building - 116],"['Health Care', 'Scheduling', 'Stochastic Models']",TD-15
"Travel time reliability [TTR] is an essential measure of service for traffic performance management, especially for congested freeway corridors. This paper proposes a systematic analytical framework to analyze the predictable and unpredictable variations in route TTR in corridor networks. More specifically, the predictable variation in route TTR is estimated with a deterministic fluid-based polynomial arrival queue [PAQ] model, while the unpredictable variation in route TTR is analyzed through the residual induced by the PAQ estimation model. Based on the output of the PAQ estimation model, a frequency-domain approach is proposed to decompose the observed time-domain travel time into underlying unobserved factors in different temporal resolutions. With the discrete-time Fourier transform method and the Butterworth low-pass filtering technique, it is capable of analytically uncovering different temporal scales of predictable variations in TTR, including the trend of day-to-day variations, the dynamics of within-day variations, as well as the stochasticity of within-period variations. Connecting with the unpredictable variations represented by the residual due to the PAQ approximation model, we can comprehensively elucidate TTR for congested freeway corridors. Case studies are conducted to show the effectiveness of the proposed frequency-domain approach in elucidating and measuring different contributing elements in TTR.",Using Frequency Domain Analysis to Elucidate Travel Time Reliability Along Congested Freeway Corridors,[78516],505,"[143, 0]",3368,Traffic flow modeling ,6,4,56,Transportation,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S04 [building - 101],['Transportation'],MC-56
"
This research pioneers a transformative path towards urban sustainability in developing countries, employing a Theory of Change [ToC] framework. Focused on Bozcaada, Canakkale, the study integrates circular economy principles, emphasizing a dynamic approach to sustainability assessment. Through the lens of the ToC, this research explores the anticipated impact of strategic interventions on the urban sustainability landscape.The methodology involves a multifaceted approach, incorporating sustainability assessment, network analysis, and the development of a novel circular economy framework. 
Findings highlight responsible resource use, waste reduction, and innovation management within the sustainability context, all evaluated through the lens of the ToC. Hence, utilises network analysis to uncover key relationships, collaborations, and influential stakeholders, presenting opportunities for strategic partnerships that align with the envisioned impact of the ToC.
The comprehensive framework developed herein serves as a guide for practical interventions and strategic resource management in Bozcaada, fostering circular economy principles. Stakeholders and decision-makers can leverage this research to assess and enhance urban sustainability, aligning with the Theory of Change impact, in developing countries.","Transformative approaches to urban sustainability - a case study of Bozcaada, Canakkale, Turkey, and circular economy strategies in developing countries",[78520],56,"[100, 25, 10]",3369,Developing green and sustainable communities [EWG-ORD Workshop 1],67,9,18,OR for Development and Developing Countries,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,42 [building - 116],"['OR in Sustainability', 'Decision Analysis', 'Behavioural OR']",TC-18
"The Varcities Horizon project aims to implement visionary nature-based solutions [VS], which contribute to the shaping of future green cities and the well–being of citizens in seven European cities.  
The present study aims to promote the replication of these solutions on a local level, providing a decision support tool to guide local administrations with a common baseline. As part of the tool, a spatial impact analysis will be presented, showcasing the positive impacts developed from the project. 
The theory of multiple benefits considers the positive impacts that can result from a NBS project without ranking or classification. The co-creation strategy for the project was structured into four main stages - co-identification, co-design, co-implementation, and co-evaluation. During the co-design phase, the stakeholders involved in each pilot selected a set of multiple benefits as goals for the development of the visionary solutions. During the co-monitoring phase, the solutions are being monitored through Key Performance Indicators [KPIs]. To verify the multiple benefits achieved by the solutions, a validation workshop with stakeholders is proposed, followed by a spatial visualization of the KPIs connected to each Visionary Solution. This will demonstrate the various benefits connected to the solutions.
The spatial impact assessment will assist administrations implementing VS, translating the theoretical concept of multiple benefits into a more comprehensible one.
",Multiple benefits spatial impact assessment for the project Varcities,"[77243, 53128, 77858, 6297]",912,"[26, 139, 77]",3372," Enhancement of circularity, inclusivity, and smartness in cities II",79,5,18,Sustainable Cities,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,42 [building - 116],"['Decision Support Systems', 'Sustainable Development', 'Multi-Objective Decision Making']",MD-18
"We aim to proposes a novel approach to estimating greenhouse gas [GHG] emissions by leveraging deep learning techniques and conducting gradient-based analysis to assess the influence of various inputs on CO2 emissions. The inputs considered encompass a range of socio-economic and environmental factors, including Gross Domestic Product [GDP], transportation modes, and energy generation mix. By constructing a deep neural network architecture, we aim to capture the complex relationships between these inputs and GHG emissions, enabling robust estimation and prediction capabilities.
Furthermore, our approach integrates gradient-based analysis to elucidate the relative contribution of each input towards overall emissions output. This allows for a granular understanding of the impact of individual factors, empowering policymakers and stakeholders to prioritize interventions effectively. Through iterative modifications of input values and analysis of resultant emission outputs, we can delineate the share of each input in driving emissions variations.
This research presents a promising methodology for GHG emissions estimation and analysis, offering insights into the intricate interplay between socio-economic drivers and environmental outcomes. By combining deep learning with gradient-based analysis, we provide a powerful framework for informing climate policy decisions and facilitating sustainable development strategies.
",Estimating Greenhouse Gas Emissions Through Deep Learning - A Gradient-Based Analysis of Input Impact,"[78527, 65726, 79503]",687,"[66, 143, 139]",3376,Toward Climate Neutrality,80,8,53,Sustainable and Resilient Systems,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,8007 [building - 202],"['Machine Learning', 'Transportation', 'Sustainable Development']",TB-53
"We present research results on the expressiveness of models and the robustness of recommendations for various approaches to solving the value-driven threshold-based multiple criteria sorting problem. The basic model we consider is UTADIS, which assumes the evaluation of alternatives based on monotonic, preferentially independent criteria. We also present several extensions to the model. Some of them relax monotonicity constraints, and others introduce interactions between criteria.The considerations focus on the preference disaggregation paradigm, reproducing the Decision Maker's preferences, expressed as assignment examples. We present the results of the performed experimental analysis for six different approaches, focusing on the aspect of expressiveness, which is the ability to reproduce indirectly expressed preferences, and robustness, which determines the stability of the recommended assignments, tested based on all models consistent with the provided recommendations, using Robust Ordinal Regression and Stochastic Multicriteria Acceptability Analysis. The analysis results were presented and discussed based on seven quality measures, divided into the considered problem dimensions, such as the number of classes, criteria, characteristic points in marginal functions, and the richness of knowledge provided by the DM. The analysis captured the trade-off between the considered aspects, indicating that more flexible approaches with richer representation provide more expressive ",A Comprehensive Framework for Multi-Criteria Sorting Model Selection Based on Experimental Analysis of Model Expressiveness and Robustness of Recommendation,"[71847, 19484, 41910]",887,"[25, 26, 77]",3378,Robustness analysis in MCDA 2,44,7,44,Multiple Criteria Decision Analysis,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,20 [building - 324],"['Decision Analysis', 'Decision Support Systems', 'Multi-Objective Decision Making']",TA-44
"This study introduces a composite indicator based on the Benefit of the Doubt [BoD] to support System Operation Centers in adjusting real-time schedules to meet demands of Electric Power Systems [EPS]. It addresses the challenge of selecting power plants for increased electricity generation in northeastern Brazil, undertaken by a company within the Brazilian electricity sector established in 1998. This entity is tasked with coordinating and overseeing the operation of generation and transmission facilities within the National Interconnected System. The evaluation focuses on the Northeast Requirement Load [NRL], with output indicators including operating time, response time, energy potential, and variable costs. Decision-makers analyze the results and concur with the prioritization of alternatives, employing the model to facilitate the selection of viable Thermoelectric Power Plants to alleviate the generation shortfall in northeast Brazil over a specified timeframe.",Incorporating the Benefit of the Doubt Indicator for Improved Electric Power Management,"[71456, 78587, 78588, 78634, 71122]",943,"[24, 35, 93]",3379,DEA applications in Policy Making and Planning II,89,12,48,Data Envelopment Analysis and its Application,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,60 [building - 324],"['Data Envelopment Analysis', 'Efficiency Analysis', 'OR in Energy']",WA-48
"Production planning problems have been extensively explored in the literature due to their wide-ranging applications and the necessity to adapt them to evolving technologies. Optimization techniques for production sequencing have garnered significant attention among companies striving for sustained high levels of production efficiency to compete effectively in both domestic and international markets. This study aims to propose and implement an optimization model that outlines production sequencing plans tailored to the chocolate milk industry. It takes into consideration the characteristics of the job shop environment, including asymmetric and sequence-dependent setup times. These challenges pose computational hurdles due to their high combinatorial complexity, particularly when managing many products and machines. In this study, three main features are emphasized due to the quantity and diversity of SKUs they produce. Such diversity often leads to frequent line stops for machine configuration, resulting in low productivity indicators. The proposed model integrates mathematical programming with heuristic approaches to address subproblems and minimize total setup time. Applying the model using real data resulted in a noteworthy 24% reduction in the average total setup time observed in the company under study.",An optimization model for the sequencing problem with dependent and asymmetric setups - an application in the chocolate milk industry.,"[8258, 78534]",805,"[105, 129]",3385,Lot-sizing with industrial applications I,32,4,49,"Lot Sizing, Lot Scheduling and Production Planning","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,M1 [building - 101],"['Production and Inventory Systems', 'Scheduling']",MC-49
"Algorithms for randomization of animal studies such as Irini [Weigle et al 2023] are used to allocate the subjects of a study at random into experimental groups. This paper presents a new method that offers an alternative to Irini random allocation a new criterion based on the Natural Hermite index.  The idea is that instead of comparing variables across samples individually, the new method compares the multivariate density estimators of the populations. A new method called differential Natural Hermite index was introduced in order to measure the dissimilarity among the multivariate density estimators of the various populations, and it was minimized by applying a genetic algorithm. The genetic algorithm  implements a process similar to the Theory of Evolution, for minimizing the differential Natural Hermite dissimilarity for partitions of a dataset into balanced subgroups. Simulations were performed for comparing the new dissimilarity to IRINI and to random allocation via exhaustive search. 
The simulations demonstrated the performance superiority of minimizing a fully multivariate dissimilarity such as the Natural Hermite one instead of the combination of univariate dissimilarities implemented by the Irini criterion. The Natural Hermite index that was able to find better partitions than the other two algorithms at a very comparable computational expense.
","A Natural Hermite dissimilarity index randomization for the design of pre-clinical studies, based on a genetic algorithm.",[78533],13,"[84, 7, 52]",3386,Data mining biomedical applications,47,9,40,Advances in Stochastic Modelling and Learning Methods,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,96 [building - 306],"['Optimization Modeling', 'Analytics and Data Science', 'Global Optimization']",TC-40
"In healthcare, association analysis between clinical risks is significant when balancing risks that are weighed against each other, such as stroke and bleeding risks with anticoagulant medication in atrial fibrillation [AF] patients. Traditional regression models are ill-suited due to standard errors in risk estimation. A novel two-stage Deming regression framework was proposed to address this issue, offering a tool to analyze associations between variables that are observed with standard deviations or estimated with standard errors. It supports personalized treatment recommendations based on identified clinical risk associations, as demonstrated with real-world AF patient data. Generalizing the model could extend its utility to differential privacy, facilitating the examination of relationships among privacy-protected data by effectively managing measurement errors and privacy errors. The model's adaptability in handling various sources of uncertainty in data indicates promising future developments.
",Two-stage Generalized Errors-in-variables Model with applications to Clinical Risks and Differential Privacy,[72168],13,"[7, 17, 5]",3387,Data mining biomedical applications,47,9,40,Advances in Stochastic Modelling and Learning Methods,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,96 [building - 306],"['Analytics and Data Science', 'Computational Biology, Bioinformatics and Medicine', 'Algorithms']",TC-40
"Technological advances have resulted in an explosion of information in all aspects of society. Much of
this information is unstructured in the form of text. Being unstructured means that machine learning and
artificial learning techniques cannot be applied.
The text mining field allows the extraction of frequent words from collections of texts in order to obtain
summary information in a more structured format. Machine learning techniques and artificial
intelligence may then be applied to this structured data.
However, as useful as it is to obtain a collection of commonly occurring words from texts, more specific
information may be obtained from texts in the form of commonly occurring phrases.
Despite this need, extracting frequent phrases is not commonly done due to inherent complications, the
most significant being double-counting. Double-counting occurs when words or phrases are counted
when they appear inside longer phrases that themselves are also counted, resulting in a large selection of
mostly meaningless phrases that are frequent only because they occur inside frequent super phrases.
Several papers have been written on phrase mining that describe solutions to this issue; however, they
either require a list of so-called quality phrases to be available to the extracting process, or they require
human interaction to identify those quality phrases during the process. In addition, those methods are
often very time-consuming.",Principal Phrase Mining - An Automated Method for Extracting Meaningful Phrases from Text,[78535],13,"[7, 5, 8]",3388,Data mining biomedical applications,47,9,40,Advances in Stochastic Modelling and Learning Methods,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,96 [building - 306],"['Analytics and Data Science', 'Algorithms', 'Artificial Intelligence']",TC-40
"Explainability in Artificial Intelligence [AI] is pivotal for enabling humans to comprehend both the outputs produced by algorithms and the underlying reasoning processes. Extending it to the realm of Business Process Management [BPM], it encompasses understanding the models generated by process discovery algorithms and deciphering the recommendations offered by process engines during process enactment.

Declarative Business Process Modelling [DBPM] emerges as a crucial framework within BPM, focusing on elucidating cause-effect relations between activities. This type of explanation is particularly valuable in knowledge-intensive processes with humans in the loop.

In this talk, I summarize our recent endeavors aimed at alleviating the cognitive load associated with understanding declarative business process models. Our approach draws insights from process modeling, event-based systems, human-computer interaction, and game design.

Specifically, we will explore techniques such as enhancing traceability, ensuring semantic transparency, leveraging explain-by-example methodologies, and employing design-via-metaphors. These strategies are designed to refine the visual representations of declarative process notations, thus enhancing the accessibility and comprehensibility of knowledge-intensive processes for novice users.

",Reducing Cognitive Load in Declarative Business Process Models,[77020],124,"[76, 26, 62]",3392,XAI in Business Processes,15,4,27,Mathematical Optimization for XAI,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,047 [building - 208],"['Modeling Systems and Languages', 'Decision Support Systems', 'Knowledge Engineering and Management']",MC-27
"The allocation of urban public charging resources plays a crucial role in facilitating EV diffusion. As a public resource, electric vehicle charging infrastructure [EVCI] also faces the conflict between equity and efficiency for its distribution, necessitating consideration of how to balance these two allocation principles. However, current research lacks in-depth exploration of the trade-off between equity and efficiency in the EVCI layout. Therefore, this paper uses an agent-based model [ABM] to depict the complex dynamic relationship between EVCI investment and EV diffusion, aiming to capture the evolution of EV uptake under different EVCI distribution principles. Empirical data from Beijing is further applied to calibrate and validate the model before conducting simulation experiments to analyze which principles should be prioritized with limited resources to fully realize the role of EVCI in promoting transportation electrification. Findings suggest the efficiency-oriented allocation strategy is clearly superior to the equity-oriented allocation strategy at the innovation and growth stage of EVs, and both are better than the baseline scenario. It is indicated that efficiency should be prioritized for EVCI allocation at the beginning stage of EV diffusion, which can eventually achieve a trade-off between efficiency and equity when the EV market maturates.",Exploring the trade-off between efficiency and equity of urban public charging investment - an agent-based simulation study,"[77656, 77644, 53530]",600,"[143, 3, 37]",3393,Simulation in transportation and logistics,77,9,43,"Agent-based Models in Management, Economic and Organisation Sciences","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,99 [building - 306],"['Transportation', 'Agent Systems', 'Energy Policy and Planning']",TC-43
"In this talk, we'll discuss the latest improvements in the solution methodology of MibS, an open source solver for solving mixed integer bilevel optimization problems. Recent progress has improved the performance of the solver substantially, as well as adding to its ability to solve related classes of problems. We'll provide some insights into the source of these improvements. ",Latest Development in the MibS Solver for Mixed Integer Bilevel Optimization,[78540],701,"[11, 50]",3395,Optimization Solvers,76,5,30,Software for Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,53 [building - 208],"['Branch and Cut', 'Game Theory']",MD-30
"Autonomous vehicles are set to transform urban mobility by embracing mobility as a service, presenting opportunities for cost reduction, diminished vehicular ownership, and enhanced road safety. Despite these advantages, shared autonomous mobility services face harsh restrictions, often confining them to fixed circuits and schedules. In the electric dial-a-ride problem on a fixed circuit [eDARP-FC], a fleet of electric shuttles provide on demand services while performing multiple laps on a designated circuit consisting of recharging depots and passenger stations. While such an operation has been investigated in the literature in a static setting, the challenges of a dynamic context have not been addressed so far. Therefore, we focus on the implications of dynamically arriving requests on both modeling aspects and solution techniques. This setting, requires making iterative decisions on request acceptance, assignment to vehicles and battery management. The objective function consists of several weighted components, including the number of accepted requests, users' total journey time, and the total number of vehicle laps. Our approach explores several online policies, employs an event-based simulation framework for high-resolution representation, and integrates a reinforcement learning model to solve large-scale instances effectively. The goal is to derive explainable dynamic operational policies that provide better balance between operational costs and the quality of service.",The Dynamic Electric Dial a Ride on a Fixed Circuit,"[78543, 46220, 23971, 35732]",181,"[143, 145, 131]",3398,Ridehailing & Ridepooling,85,4,54,Public Transport Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S01 [building - 101],"['Transportation', 'Vehicle Routing', 'Simulation']",MC-54
"Asthma exacerbations can occur due to various factors, including exposure to air pollutants generated by economic sectors, like the transportation sector. Here, we investigate the equity performance of different transportation modes using access to health facilities as key equity outcome. Patients with asthma must get to hospitals soon. Asthma, which affected 10% of the Canada's population in 2021, is one of the most prevalent chronic respiratory diseases affecting children in Calgary in Canada. 
To achieve transportation fairness, our study focuses on the critical component of spatial accessibility to medical facilities for the medical care of pediatric asthma. We intend to shed light on how accessible it is for patients with asthma to get to the closest medical facilities using different transportation modes. We have developed a comprehensive equity analysis framework to measure access to health facilities. It combines coverage and reachability-based methods and utilizes both vertical and horizontal equity techniques. We calculated accessibility metrics at different geographical levels to highlight how various transportation modes impact access to health facilities, such as at the community district or city sector level. Our findings demonstrate that transportation planners can leverage insights generated from both vertical and horizontal equity analyses to enhance transportation services in underserved communities, thereby improving access to various health facilities.
",Assess the Equity Impacts of different Transportation Modes using Access to Health Facilities for Asthma Patients an Indicator ,"[78526, 78861, 79053, 65726, 79050]",597,"[143, 56, 119]",3399,Mobility and transportation in healthcare,3,4,10,OR in Health Services [ORAHS],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,11 [building - 116],"['Transportation', 'Health Care', 'Public Local Transportation Systems']",MC-10
"We consider quadratically constrained quadratic programs [QCQP] in a general form with both linear and quadratic constraints. We will show that, if the number of quadratic constraints m and the number of decision variables n are large enough, there exist two random matrices T [with k rows and m columns] and R [with r rows and n columns] such that it is possible to apply them to the parameters of the original problem in order to obtain another QCQP with fewer decision variables and quadratic constraints which is easier and faster to solve.
We will provide theoretical results showing that the projected program is related to the original one in terms of both feasibility and optimality and a procedure that exploits a solution of the projected program to obtain a solution of the original one. 
Numerical experiments on some different classes of QCQPs will be presented.
",Random projections for quadratically constrained quadratic programming,"[67872, 8446, 65529]",720,"[114, 113]",3404,Mixed Integer Nonlinear Programming and Nonconvex Optimization ,86,13,04,MINLP,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1001 [building - 202],"['Programming, Quadratic', 'Programming, Nonlinear']",WB-04
"Order allocation is the dynamic problem of distributing incoming orders to fulfillment nodes. This problem appears in different sectors and contexts, where the nodes may belong to the same or independent vendors. We consider an Online Fashion Marketplace selling products owned by multiple vendors. In this setting, customers expect immediate feedback on their product fulfillment, such that order allocation decisions must be taken instantly.
This particular Marketplace diverges significantly from prior studies in order allocation literature due to the consistent low sales volume per item across the majority of the product portfolio, and the daily fluctuations in inventory by vendors that introduce a notable level of uncertainty.
Given the distinctive characteristics of this problem compared to existing literature, we propose an innovative methodology to generate heuristic fulfillment policies. Leveraging either Genetic Programming or Genetic Algorithms, we incorporate critical policy information, including shipping costs, basket coverage and stock depth into a machine learning pipeline to generate policies that are fast and effective.
To assess the efficacy of the proposed policies, a comprehensive comparative analysis is conducted against the retailer's existing policy and a perfect hindsight policy, from which managerial insights are extracted.",Order Allocation Optimization in a Fashion Marketplace,"[72296, 30652, 17649]",637,"[32, 65]",3405,Retail Distribution I,30,12,50,Retail Operations,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,M2 [building - 101],"['E-Commerce', 'Logistics']",WA-50
"We obtain new results about the existence and uniqueness of solutions to the Bellman equation of stationary stochastic dynamic optimization problems in discrete time and infinite horizon. We extend previous results for weighted and local contractions, through a generalization of the notion of Banach contraction which is especially well suited for problem where the shock variables are exogenous and/or the discount factor may depend on uncertain events. We apply the results obtained to an endogenous growth model and compare our approach with other well known methods, such as the weighted contraction method, countable local contractions and the Q-transform.
",Existence and uniqueness of solutions to the Bellman equation in stochastic dynamic programming,[21987],50,"[108, 0]",3406,Vector and Set Optimization II,33,3,41,Vector and Set Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,97 [building - 306],"['Programming, Dynamic']",MB-41
"This study is concerned with understanding and classifying the variety and difference among complex systems. Definitional differences usually stem from the disciplines where complexity was first detected and analyzed. They are also characterized by emphasis on one specific element of the system, being its composing entities, their interrelations, entropy, or emergence, to name a few of the most popular. These classifications generate a broad definitional space for complexity, leading to large differences among types of complexity. In other words, not all complexity is made equal. The issue of concern for this paper is to formally qualify these differences by using a series of agent-based computational simulation models where complexity is manipulated by modifying one component at the time. This should be able to provide a map of complexity, from more to less intuitive, from more to increasingly less predictable, and from single- to multi-layered. The research is structured around a model grounded in the social sciences, more specifically it illustrates intra-organizational dynamics as an exemplar of complexity in social systems. This research is aimed at improving our understanding and appreciation of complex systems in a way to guide the analytical tools and strategies that should accompany researchers when confronted with difference systems. The study concludes with a proposal for terminology to use in relation to different complex systems.",A Computational Account of Complex Systems,"[78551, 77525]",563,"[15, 3, 131]",3407,Simulation of organizations I,77,2,43,"Agent-based Models in Management, Economic and Organisation Sciences","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,99 [building - 306],"['Complex Societal Problems', 'Agent Systems', 'Simulation']",MA-43
"There is an abundant literature on integer partitions, with diverse applications, notably in computer science and more specifically in the field of image processing. In this paper, we present some new results. In the first part we  present the initial result,  concrenig the enumeration of triangles with fixed premeter and distincts side lengths and in the seceod part, we give  closed formula of the number of quadrilatrals with different  side lengths and a fixed perimeter, achieved through the utilization of integer partitions theory. Intermediate results are provided.Many open problems will be posed for further  exploration.

",Some particular combinatorial problem solved using integer partition theory techniques.,"[78547, 78562]",882,"[14, 0]",3408,Topics in Combinatorial Optimization I [Contributed],64,14,25,Combinatorial Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,011 [building - 208],['Combinatorial Optimization'],WC-25
"Semidefinite optimization hierarchies are the standard approach to Polynomial Optimization and generalized moment problems. Several authors have proposed hierarchies based on other conic alternatives lately. These alternatives replace the sum of squares with different classes of non-negative polynomials. The convergence of such hierarchies is rooted in Positivtellensatze, which guarantee particular types of non-negative certificates.  

We propose a universal approach to derive new Positivstellensatze for general semialgebraic sets from ones developed for simpler sets, such as a box or a simplex. We illustrate the approach by constructing Positivstellensatze over compact semialgebraic sets based on any given cone of non-negative polynomials. Also, we show how to use our results to derive sparse Psatz and to extend them to unbounded semialgebraic sets.",Lifting nonnegativity over general semialgebraic sets to nonnegativity over simple sets.,[78555],163,"[115, 113, 19]",3410,Applications of conic optimization,68,5,38,"Conic Optimization - Theory, Algorithms, and Applications","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,34 [building - 306],"['Programming, Semidefinite', 'Programming, Nonlinear', 'Continuous Optimization']",MD-38
"The protocol underwent rigorous testing in the agricultural sector, focusing on collective agri-environmental and climate schemes. While these initiatives hold potential [Pe'er et al., 2020], they often lack numeric data and standardized indicators, hindering comparisons [Rudolf in Udovč, 2022]. Consequently, decision-makers face challenges in assessing their impact on the environment and public goods. The protocol consists of four steps. In first step, SWOT results from various case studies are gathered, focusing on factors that impact the same goal. These results are then grouped into common themes and subthemes using the Stirling method [2001], creating a hierarchical decision tree of attributes. In second step, analytical hierarchical process is used to calculate the weights of SWOT factors and attributes, incorporating input from multiple professionals. In third step, a customized form is created for decision-makers to list SWOT factors for their specific case, following the decision tree of attributes. This form is linked to a tailored transformative formula, converting qualitative SWOT analysis into quantitative values. Building on the SWOT scorecard technique [Rudolf and Udovč, 2022], this formula adjusts using the individual weights assigned to SWOT factors in step two. The fourth step arranges numeric codes into groups, representing levels on a Likert scale. These levels are recognized within a model, aiding in the evaluation of case studies. ",Enhancing Decision-Making in Agricultural Policies - A Four-Step SWOT Protocol,"[78565, 62240]",653,"[25, 6, 139]",3414,Integer Programming for Decision Support,45,9,45,Decision Support Systems,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,30 [building - 324],"['Decision Analysis', 'Analytic Hierarchy Process', 'Sustainable Development']",TC-45
"The building sector is the largest consumer of energy in Europe. Energy efficiency is fundamental for the cost-effective transition towards a decarbonised energy system. The renovation rates of buildings reveal that there is an urgent need for action by leveraging private capital in investments. To this end, multiple actors of the value chain need to be engaged to enhance communication among them, overcome barriers and increase chances for renovation projects to be financed. The involvement of key stakeholders is useful to get alternative perspectives, which stimulates shifting the focus from project activities to intended outcomes in the early stage, and to clear communication of the scope of an energy efficiency project in terms of objectives, milestones and outputs. The specific paper introduces a stakeholder engagement process analysing the impact that an online marketplace promoting energy efficiency investments in buildings would have to the energy and financing community. This is done through a concrete methodological approach based on an institutionalised engagement process of the relevant stakeholders.  
Key findings, which have emerged from a dedicated survey and a training workshop with regards to the market needs and critical uncertainties affecting the building renovation investments popularity, support the co-creation of an energy efficiency marketplace that enables project aggregation and matchmaking with financing institutions in a standardised manner. ",Utilising decision-making methods and online tools to accelerate sustainable renovation investments - Insights from a stakeholder engagement approach  ,"[67269, 78575, 78585, 18714]",561,"[25, 37, 44]",3415,"Advancements of OR-analytics in statistics, machine learning and data science 14",16,9,06,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,1013 [building - 202],"['Decision Analysis', 'Energy Policy and Planning', 'Finance and Banking']",TC-06
"The aim of this 90' session is to explore the aspects of practice that do not get included in the materials submitted for awards such as the EURO Excellence in Practice Award. Submissions naturally focus on the criteria being judged - the originality of the methodology, the scientific excellence, the impact, and so on. But anybody who has been engaged in a practical application will know that this only tells part of the story. To get to the stage of an implemented application, there will have been many hurdles along the way -   getting all the data, making sure it is correct, convincing all the stakeholders, overcoming fears and doubts of the people who have to change their way of working, and so forth. In this session,  we will invite panellists to give a short  presentation of these aspects of their projects, consider how they dealt with the challenges and identify key lessons, to be followed by questions and discussion with the audience on the key issues and how we address them.
The latest details of panel members can be found on the Making an Impact page on the main conference website [euro2024cph.dk/programme/making-an-impact-2024]",Not The Excellence In Practice Award,"[46961, 8371, 17364, 5167, 79154, 67933, 73660]",541,"[151, 0]",3417,Not The Excellence In Practice Award,40,12,46,Making an Impact,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,40 [building - 324],['Practice of OR'],WA-46
"In situations of high uncertainty and high human involvement, it can be difficult to implement analytical tools; on the other hand those tools can serve to reduce and define the uncertainty. To achieve this level of use the human actors involved need to be empowered to understand and utilise the data that they produce and use. This requires both learning and experiment in a safe and creative space as mathematical tools, data and humans come together.
 
In this workshop we will use examples from industry interventions to introduce the ideas and concepts around HCA. We will discuss what works and what doesn’t, and why that is, and develop skills around choosing the right structuring methods and analytical approaches for differing situations. 

Design is a key part of the process, and we will look at ways to keep in mind the iterative and empathetic journey that this helps to facilitate, while also looking at ways to integrate HCA into common working practices such as lean projects.",Human Centric Analytics [HCA] - How To Foster The Human Centred Development Of Analytics That Augment Human Work Effectively.”,"[46961, 57905]",542,"[151, 7, 149]",3418,Human Centric Analytics [HCA],40,15,46,Making an Impact,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,40 [building - 324],"['Practice of OR', 'Analytics and Data Science', 'Problem Structuring']",WD-46
"Daily field maintenance activities are common in a wide range of service providers and industrial applications. Electricity distribution systems are a classical example, commonly characterized by a set of pending services that must be accomplished by a set of maintenance teams. There are three main daily decisions usually involved in the scheduling of such services - [i] what services will be executed on a specified date; [ii] what is the sequence of each service list of each maintenance team [usually based on routing constraints]; [iii] which team will execute each set of services. In this research, we address a real-world problem faced by an electricity distribution company. In this case, there are a set of activities available to the maintenance teams. For each activity is assigned a prize [strongly related to the activity's nature and deadline]. The time required to start an activity may be influenced by the previous task [e.g., there is no need to perform two security checks on activities executed at the same place]. This problem is faced as a derivation of the prize-collection vehicle routing problem [PCVRP]. This research presents a MILP formulation and proposes a meta-heuristic approach to generate daily maintenance schedules. Both solution strategies are applied to problem datasets based on historical data of large cities attended by the electricity distribution company. Our results show the applicability of the meta-heuristic to solve this real-world problem.",Scheduling electricity distribution field maintenance by a prize-collection VRP approach,"[73597, 78576, 78577, 78578, 78579]",862,"[129, 145, 74]",3419,Applications of combinatorial optimisation in industry and services II,64,8,29,Combinatorial Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,157 [building - 208],"['Scheduling', 'Vehicle Routing', 'Metaheuristics']",TB-29
"Predicting export product prices is important for export companies for negotiating contracts with the associated participants of the supply chain. In this study, extreme value techniques are used for normalizing data of price fluctuations, and then, machine learning methods are applied for estimating future price fluctuations. This methodology is applied to a case study for estimating the export price of sweet cherries. The results show low error in the price fluctuations’ forecast, with a MSE lower than 1. Besides the good performance, the proposed methodology is easy to implement and can be executed in a low computational time by using personal computers.",Predicting price fluctuations of commodities by the joint use of extreme value techniques and machine learning methods - a case study of cherry price forecast,"[74478, 12838]",589,"[66, 0]",3421,Agrifood supply chain decision problems,20,4,12,OR in Agriculture and Forestry ,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,13 [building - 116],['Machine Learning'],MC-12
"In the realm of supply chain dynamics, the influence of Blockchain technology has been extensively explored within the forward channel, yet its repercussions on reverse logistics and closed-loop supply chains [CLSC] remain understudied. For instance, Wipro, a prominent multi-billion-dollar technology company, actively employs Blockchain technology to combat counterfeiting challenges within the realm of product returns. This paper addresses this gap by delving into the implications of Blockchain adoption in CLSC. 

Focusing on the interplay between a manufacturer and a retailer, our study formulates a Stackelberg game wherein the manufacturer seeks to deploy Blockchain technology to enhance the accuracy of valuing end-of-life returned products. However, this adoption comes with a nuanced challenge—potential customer concerns about privacy and data collection within Blockchain, which can adversely affect the retailer by dampening customer demand.

Moreover, the manufacturer faces a choice between internally managing the collection of used products or outsourcing this task to the retailer. Through analysis of equilibrium strategies, we offer novel insights that contribute to a more comprehensive understanding of the multifaceted consequences of Blockchain integration in closed-loop supply chain dynamics. 

",Exploring Blockchain Implications in Closed-Loop Supply Chains,"[57929, 78586, 76261]",927,"[125, 50, 138]",3423,New technology for sustainable supply chains,18,14,23,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,82 [building - 116],"['Reverse Logistics / Remanufacturing', 'Game Theory', 'Supply Chain Management']",WC-23
"An fundamental optimization problem in quantum physics is finding the ground state of quantum many-body systems. 
Addressing this challenge, our presentation highlights numerical experiments that utilize a classical machine learning [ML] algorithm leveraging a geometric locality inductive bias to efficiently predict ground state properties. Our experiments on systems up to 45 qubits illustrate the model's ability to accurately predict ground state properties with only O[log[n]] instances from similar Hamiltonians, demonstrating a substantial efficiency improvement over previous state of the art methods with polynomial sample complexity. These findings validate theoretical improvements on the sample complexity bound for geometrically informed ML algorithms.

Additionally, if time allows, we will delve into preliminary findings from ongoing research in the domain of shadow tomography facilitated by diffusion models, exploring their potential to further advance our understanding of quantum systems.",Efficient Prediction of Quantum Ground State Properties via Classical Machine Learning with Geometric Locality,[78454],383,"[66, 0]",3425,Optimization in Quantum Information,83,5,42,Quantum Computing Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,98 [building - 306],['Machine Learning'],MD-42
"Priority rules are the most used methods in many commercial software tools for scheduling projects under limited resources because of their ease of implementation, intuitive working, and fast speed. The branch-and-bound [B&B] procedure is the most common way to deal with the resource-constrained project scheduling problem to find the optimal solution in a finite number of steps.  Meta-heuristics are the most effective methods to solve project scheduling. Although these methods have been developed, no single method has been shown to be the best. To fill this gap, we rely on machine learning classification and ranking models to find the relation between the characteristics of the project instances and the performance of solution methods.

This study not only aims to compare the performance of different machine learning models but also proves that machine learning can be used to solve the resource-constrained multi-project scheduling problem [RCMPSP]. The contribution is threefold - First, we extend two types of machine learning models to solve the multi-project scheduling problem. Second, we extend both machine learning models to priority rules, branch-and-bound procedures, and meta-heuristics, respectively. Third, we compare the performance of two types of machine learning models on the three categories of solution methods. An extensive computational experiment is set up to compare the performance of machine learning models with the existing methods from the literature.",Performance comparison of machine learning models in detecting the best-performing methods for the resource-constrained single-and multi-project scheduling problem,"[77765, 5083, 62308]",561,"[66, 118, 129]",3426,"Advancements of OR-analytics in statistics, machine learning and data science 14",16,9,06,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,1013 [building - 202],"['Machine Learning', 'Project Management and Scheduling', 'Scheduling']",TC-06
"When studying excess Covid deaths in the 50 US states, the responses is a tensor time series of 50 states by 15 major causes of deaths and the overall. This data is observed monthly since 2015 before covid until 2023 and the objective is to estimate the excess deaths during covid years.
We propose a model that combines a non-linear trend model that explains the over-all structure of the data, followed by a seasonality model, and a tensor autoregressive model for the residuals.
The seasonality model consists in a combination of sinusoidal trends at different frequencies.
The results give clear patters of excess deaths by state and disease during covid time that can be communicated and interpreted with data visualization.
",Forecasting high dimensional Tensor with relatively few observations.,[71956],13,"[7, 17]",3431,Data mining biomedical applications,47,9,40,Advances in Stochastic Modelling and Learning Methods,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,96 [building - 306],"['Analytics and Data Science', 'Computational Biology, Bioinformatics and Medicine']",TC-40
"Hospital institutions navigate daily through critical risk activities, giving rise to the substantial generation of medical waste. These activities involve tailored control and monitoring procedures, encompassing logistical planning for the disposal and elimination of waste as an integral part of achieving their mission.

This ethos extends universally across industries and supply chains associated with healthcare. Consequently, this study delves into the pharmaceutical supply chain from the perspective of conscientious disposal practices. It categorizes waste into hazardous and non-hazardous types, linking them to designated treatment centers, whether those be recycling hubs or remanufacturing facilities. This classification takes into account the lifespan parameter for non-hazardous waste and mandates direct incineration for hazardous materials.

Finally, the study introduces an optimization model to scrutinize various hospital institutions as prolific sources of waste. These institutions would be visited by a diverse fleet of waste collection vehicles, equipped to handle both hazardous and non-hazardous waste. The model aims to address the primary queries of the Inventory Routing Problem [IRP] - 1] When is the optimal time for visits? 2] What volume of waste should be collected? and 3] Which route should be pursued? Additionally, it incorporates an analysis of shared load considerations in each collection, particularly for smaller-scale instances.",Model for the Hospital Waste Collection Problem Considering Shared Load,"[78492, 40525, 68654]",777,"[145, 72, 65]",3432,Waste Collection,5,9,58,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S07 [building - 101],"['Vehicle Routing', 'Mathematical Programming', 'Logistics']",TC-58
"Against the backdrop of an ongoing debate on reforming the European electricity market, leading voices in academia and EU institutions agree that electricity markets must be supplemented with additional long-term options, including Contracts-for-Difference [CfDs]. A key design question for renewable support schemes in general, and CfDs in particular, is how to prevent electricity market distortions and preserve short-term, operational market integration without jeopardising the effectiveness of the schemes in leveraging investment of private capital for renewable energy deployment. Different experts advocate for different versions and implementation options of CfDs, including generation-based and generation-independent design approaches. Generation-based two-sided CfDs have so far been used in more than 200 auctions across ten European countries, namely Denmark, France, Greece, Hungary, Ireland, Italy, Poland, Portugal, Spain, and the United Kingdom. We show that generation-based CfDs in existing designs can be non-distortive for day-ahead markets. Remaining issues arise through spill-over incentive effects across market segments [e.g. towards intraday, balancing and futures markets]. We discuss generation-independent CfDs, which have theoretical advantages over generation-based designs, in particular in relation to intraday and balancing markets, but feature unresolved implementation challenges and would imply rather significant changes in the market. ",The role of CfDs in future Electricity Markets,[78611],177,"[37, 36]",3438,Long-term energy system planning,22,10,09,Energy Markets,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,10 [building - 116],"['Energy Policy and Planning', 'Electricity Markets']",TD-09
"The optimization of storage spaces in a terminal is a crucial factor for achieving high performance in its logistical operations. Temporary storage at a terminal is facilitated through a logistical structure known as a yard, where containers are stacked in blocks. The significance of this logistical infrastructure is underscored by the substantial volume of containers managed; for instance, nearly 2000 million tons of containers were stored in ports in 2022. The operations involving container movement and storage entail the coordination of various entities, including vehicles, yard cranes, and others. This complexity gives rise to a range of intricate logistical challenges that warrant attention.
This work presents a decision-support system aimed at managing, analyzing, and optimizing container storage and movement in the yard of a maritime container terminal. The system provides a detailed overview of the yard status and enables planning for future operations based on expected arrivals and departures of containers, thereby optimizing available space and reducing vehicle movement and associated emissions. The proposed system integrates a set of optimization techniques such as optimization models, heuristics, or metaheuristics, with the primary goal of addressing various yard problems such as block relocations or vehicle movement. Lastly, this decision-support system is integrated as a subsystem within a digital twin designed to manage all logistic operations within a port.
",Decision support system for managing integrated yard problems,"[36163, 78160, 58335]",680,"[14, 5, 65]",3439,Container Stacking and Yard Planning III,52,14,62,OR in Port Operations,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S12 [building - 101],"['Combinatorial Optimization', 'Algorithms', 'Logistics']",WC-62
"In this presentation, I will introduce some convergence results concerning the automatic differentiation of iterative algorithms, i.e., differentiating through the iterations, whether they are smooth or not. While the asymptotics of smooth problems are well understood [Gilbert 1992, Beck 1994], mainly thanks to the use of a fixed point theorem, the non-smooth case presents more difficulties. I will show how the framework of conservative Jacobians can achieve such results. I will also illustrate a strategy to adopt when the operator is not contracting, taking the differentiation of the Sinkhorn-Knopp algorithm as motivation. Finally, I will discuss one-shot differentiation, a method that combines the simplicity of automatic differentiation with the performance of implicit differentiation [in some cases]. This approach, particularly suitable for fast algorithms, will be illustrated through superlinear optimization methods and bi-level optimization scenarios.
Work in collaboration with Jérôme Bolte [Toulouse School of Economics] and Edouard Pauwels [Toulouse School of Economics].",[Automatic] Iterative Differentiation - some old [& new] results,[78617],338,"[81, 66, 19]",3440,Algorithms for machine learning and inverse problems - optimisation for neural networks,84,10,32,Advances in large scale nonlinear optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,41 [building - 303A],"['Non-smooth Optimization', 'Machine Learning', 'Continuous Optimization']",TD-32
"A sequential optimization model, the multi-armed bandit problem, is concerned with optimal allocation of resources between competing activities, in order to generate the most likely benefits.  

In this work, following the objective of a multi-armed bandit problem, we consider a  game theoretic model to approach to an ensemble of multi-armed bandits.
",Multi-armed bandits games ,[78616],539,"[136, 50, 27]",3441,"Advancements of OR-Analytics in Statistics, Machine Learning and Data Science 13",16,8,06,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,1013 [building - 202],"['Stochastic Optimization', 'Game Theory', 'Decision Theory']",TB-06
"Getting new grid access permits is challenging for Norwegian grid customers to a shortage of transmission system capacity. Grid reinforcements require long lead times and are not expected to ease the situation soon. If capacity limits are reached, requests face rejections for grid access. Grid managers have to ensure capacity limitations are met all the time. Enough capacity is available for most of the time, but some critical hours see spikes in which load profiles layer to unfortunate spikes. The consequences are severe and hinder economic development. So far, there are no signals indicating the grid's status on which coordinated actions could be aligned. Upcoming congestion hours are not forseeable, nor exist incentives to influence load profiles according to grid status. Alternatively, we advocate for congested customers to team up with entities with existing capacity rights and pool them in energy [capacity] communities. Steered by price signals from utility-cost equilibria, members coordinate their load profiles and assign capacity scarcity prices if a capacity violation occurs. We designed a two-stage combination of load flow optimal dispatch models with an equilibrium model to resolve the congestion. We demonstrate that this market design enables more customers to join the electricity grid through an efficient price-based trading mechanism, which adheres to grid restrictions, is incentive-compatible, and allows cost recovery. ",Local Network Coordination - A Market Design for Trading Congestions,"[78510, 57352, 57403]",244,"[36, 93, 37]",3442,Modelling European market coupling ,22,5,14,Energy Markets,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,16 [building - 116],"['Electricity Markets', 'OR in Energy', 'Energy Policy and Planning']",MD-14
"Despite its recent introduction in literature, the Best-Worst Method [BWM] is among the most well-known and applied methods in Multicriteria Decision-Making. The method can be used to elicit the relative importance of the criteria as well as to get the priorities of the alternatives on the criteria at hand. Here, we will present an extension of the method, namely, the parsimonious Best-Worst-Method [P-BWM] permitting to apply the BWM to get the priorities of the alternatives in case they are in a large number. At first, the Decision-Maker [DM] is asked to give a rating to the alternatives under consideration; after, the classical BWM is applied to a set of reference alternatives to get their priorities used to compute, then, the priorities of all the alternatives under consideration. We propose also a procedure to select reference alternatives, possibly in cooperation with the DM, providing a well-distributed coverage of the rating range. In addition to the standard approach, we propose one alternative way of inferring the priority vectors in BWM and P-BWM based on the barycenter of the space of alternative priorities compatible with the preferences given by the DM. Finally, an experiment with university students has been conducted to test the new proposal. A comparison among BWM, P-BWM, the Analytic Hierarchy Process [AHP], and the parsimonious AHP in terms of the amount of preference information provided by the DM in each method to apply it is also given.",Better Decisions with Less Cognitive Load - The Parsimonious BWM,"[36935, 5550, 40259]",888,"[26, 77]",3443,Preference Learning 2,44,3,44,Multiple Criteria Decision Analysis,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,20 [building - 324],"['Decision Support Systems', 'Multi-Objective Decision Making']",MB-44
"Hospital admission management is a fundamental challenge due to the uncertain demand for inpatient beds. This paper studies a stochastic variant of the well-known patient admission scheduling problem, which aims to assign patients to rooms during their hospitalizations while considering the overstay risk. We consider the problem as a two-stage stochastic programming problem where the first stage assigns patients to rooms on their planned hospitalization days, and the second stage evaluates the expected costs resulting from patient overstay. We propose two novel stochastic programming models to formulate the problem, including a scenario-based model and its equivalent state-variable model. The latter has a pseudo-polynomial number of variables and constraints, which is significantly smaller than the former. To solve the state-variable model efficiently, we apply the sample average approximation method to provide an initial feasible solution to the Gurobi solver. We carried out extensive computational experiments to evaluate the performance of the proposed models. The computational results revealed that our state-variable model finds high-quality feasible solutions with an average optimality gap of 3% for benchmark instances with 250 patients and 1.9E119 scenarios in 1 hour.",Stochastic patient admission scheduling problem with an exponential number of scenarios,"[78623, 65596, 40508]",610,"[56, 117, 135]",3446,Admission and discharge,3,15,10,OR in Health Services [ORAHS],"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,11 [building - 116],"['Health Care', 'Programming, Stochastic', 'Stochastic Models']",WD-10
"In this paper, we consider a class of non-convex and non-smooth sparse optimization problems, which encompass most existing nonconvex sparsity-inducing terms. These problems, moreover, are probably non-lipschitz around sparse solutions. We propose an damped iterative reweighted l1 algorithm to solve these problems. The algorithm is guaranteed to converge only to local minimizers when randomly initialized. By deriving a second-order sufficient optimal condition under the non-degenerate assumption, we show that the strict saddle property is generic on these sparse optimization problems.",Avoiding strict saddle points of nonconvex regularized problems,[56806],249,"[81, 19, 72]",3447,Lower-order composite optimization problems,70,10,41,Nonsmooth Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,97 [building - 306],"['Non-smooth Optimization', 'Continuous Optimization', 'Mathematical Programming']",TD-41
"Imagine a student who must obtain a certain amount of credits to graduate and needs to choose which courses to take. Naturally, they would like some of them more than others, but it may be the case that to attend a course they are required to complete also some of the less preferred ones. Thus, what is the best plan to employ?

This is an example of a precedence constrained knapsack problem in combinatorial optimization. In this talk we will introduce some continuous relaxations of this problem which have applications in the mining industry, and can be suitably used to model other investment planning problems.

We will present a new algorithm to compute a solution, which is devised studying the behavior of the classical primal simplex algorithm when applied to the LP formulation. This reveals one of the very few examples of a simplex algorithm with an underlying combinatorial structure.","On mines, flows and investments - a new algorithm for the precedence constrained continuous knapsack problem.","[72625, 25257, 78481]",254,"[5, 110, 150]",3448,Mixed Integer Optimization I,64,7,52,Combinatorial Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,8003 [building - 202],"['Algorithms', 'Programming, Linear', 'Network Flows']",TA-52
"The increasing usage of decision support systems typically hinges on the assumption that objective information provision can improve decision-making. This premise, however, disregards the potential for humans to deliberately ignore information in a manner that serves their own interests. We study willful information avoidance in the usage of decision support with the help of an online experiment where participants solve a task of high cognitive complexity, i.e. a knapsack problem. We indeed find first evidence for motivated reasoning in our experiment in one of two dimensions - First, we do not find evidence that participants willfully ignore decision support when it might suggest revising their initial choices. Yet, we find evidence that they willfully ignore information that might tempt them to act selfishly. Thus, participants use decision support to self-commit to more pro-social choices. Contrary to that, participants do not avoid decision support information to behave egoistically while maintaining a positive self-image. Although our results seem like good news, they show that even objective information provision in decision-support is no guarantee that humans acquire it in an unbiased way.",Do people willfully ignore decision support? Evidence from an online experiment,"[33361, 78627, 78626, 69819]",567,"[10, 26]",3449,Behaviour and decision support,13,10,07,Behavioural OR,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,1019 [building - 202],"['Behavioural OR', 'Decision Support Systems']",TD-07
"We are interested in optimizing noisy black-box problems that are non-convex in expectation. Such settings occur in a multitude of engineering and machine learning applications. For the noisy black-boxes that provide direct derivative observations, we devise new variants of trust-region methods that use adaptive sampling schemes and adjusted acceptance criteria to improve the iteration and sample complexity of these algorithms. The resulting methods will have clear implementable guidelines for speedy and robust finite-time performance.",First-order Trust-region Methods with Adaptive Sampling,[62809],917,"[19, 136, 35]",3450,New Algorithms for Nonlinear Optimization,84,7,34,Advances in large scale nonlinear optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,43 [building - 303A],"['Continuous Optimization', 'Stochastic Optimization', 'Efficiency Analysis']",TA-34
"The streaming industry has experienced exponential growth over the past decade. Streaming platforms provide subscribers with unlimited access to a diverse range of services, including movies, TV shows, and music, in exchange for a subscription fee. We take an axiomatic approach to the problem of how to share the overall revenue obtained from subscription sales among services or content producers. In doing so, we provide normative justifications for several distribution rules.",REVENUE DISTRIBUTION IN STREAMING PLATFORMS,"[58522, 78629, 1313]",384,"[50, 27]",3451,"Game Theory, Solutions and Structures II",88,3,36,"Game Theory, Solutions and Structures","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,32 [building - 306],"['Game Theory', 'Decision Theory']",MB-36
"Recent scientific literature has shown that combining drones with trucks is effective for last-mile delivery. With the growing volumes of next-day delivery, not all customers may be served within a given day, especially when considering a limitation of the drivers’ working time. To this end, we investigate the vehicle routing problem with drones and outsourcing [VRPDO], where customers are served either by a given truck-drone fleet [each truck equipped with a single drone] or by outsourcing, with each customer incurring an individual fixed cost. The objective is to minimize the transportation costs of trucks and drones as well as the outsourcing costs. We propose a variable neighborhood search [VNS] algorithm for this problem, where we represent a VRPDO solution as customer sequences. Each sequence is evaluated by solving a problem that we refer to as the fixed sequence assignment problem [FRAP]. Given a customer sequence, the FRAP assigns each customer to the truck, or the drone, or be outsourced, and determines truck and drone routes, in order to minimize total costs while satisfying a maximum-duration limit. We treat the FRAP as a resource-constrained shortest path problem and introduce an exact labeling algorithm to solve it. We further develop a heuristic labeling algorithm [HLA] for the problem and reinforce the VNS by presenting filtering strategies based on the HLA. Computational results show the effectiveness of our proposed approach in terms of solution quality.",The Vehicle Routing Problem with Drones and Outsourcing,"[75743, 55434, 19719, 78982]",205,"[145, 14, 74]",3453,Scheduling and Routing Problems ,64,5,52,Combinatorial Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,8003 [building - 202],"['Vehicle Routing', 'Combinatorial Optimization', 'Metaheuristics']",MD-52
"The pressing challenge of climate change and the need to reduce the levels of anthropogenic carbon dioxide emissions [CO2] have made Carbon Capture and Storage [CCS] an increasingly critical area of research. CCS represents a vital technology in the pursuit of mitigating climate change impacts by capturing CO2 emissions at their source and storing them underground. Recognizing the importance of this technology, our study delves into the optimization of CCS infrastructure, specifically focusing on the integrated design of CO2 pipeline networks and the development of effective price policies. Through a bilevel model tailored for the CCS context, we examine the potential of price policies to enhance the efficiency of CO2 pipeline networks in Germany. Herein, our findings illuminate the dual nature of CCS - on one hand, it offers a pathway to lower CO2 prices for emitters, thus promoting environmental welfare. On the other hand, it encounters challenges in market design, particularly in addressing the complex issue of preventing monopolistic control by pipeline operators . Additionally, we recognize the 'distance-to-sink' effect as a significant barrier, disadvantaging emitters situated far from storage sites. ",Towards an optimum carbon backbone - A Bilevel Model Approach for Efficient CO2 Pipeline Design and Price Policy in Germany,"[72549, 75037, 78632, 2650]",843,"[37, 79, 139]",3454,Towards sustainable development,23,12,19,OR in Energy,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,44 [building - 116],"['Energy Policy and Planning', 'Network Design', 'Sustainable Development']",WA-19
"Sustainable development strongly requires a shift away from fossil fuel-based energy systems and toward ones that rely on renewable resources. It is well recognized that the first step in improving sustainability is to evaluate the current degree of sustainability. This research proposes a unique approach for assessing the existing level of sustainability through renewable energy systems [RESs] using multi-expert multi-criteria decision-making based on a distance to ideal solution method under interval type-2 fuzzy information. The proposed method highlights underperforming sustainability components according to the linguistic data collected from experts, which helps decision-makers identify the obstacles to sustainable growth. The proposed approach may make it easier to create global sustainable development policies and plans that are more successful. This may be achieved via improving the quality of experts' linguistic assessments and by offering a thorough grasp of the sustainability of RES around the globe.",Evaluating Sustainability in Renewable Energy Systems - Introducing a Novel Fuzzy Multi-Criteria Distance to Ideal Solution Approach,"[76902, 2650]",886,"[49, 25, 139]",3457,MCDA in energy,44,13,47,Multiple Criteria Decision Analysis,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,50 [building - 324],"['Fuzzy Sets and Systems', 'Decision Analysis', 'Sustainable Development']",WB-47
"Goal programming [GP] is a special kind of linear programming [LP] technique where users seek to find a solution that deviates as little as possible from a set of goals, expressed as a series of linear equalities or inequalities. Applications include - planning manufacturing to match demand as closely as possible, given a limited number of raw materials; scheduling workers’ shifts to match their stated availability as closely as possible; etc.
Solving GP problems requires expanding the original goals into a larger LP problem. While the expansion is relatively straightforward, it is easy to make mistakes when utilising it manually in large problems. Furthermore, there are few software implementations of GP, with users mostly relying on general-purpose LP software. This makes solving large GP problems tedious and prone to errors.
We fill this gap by introducing goalp, an R package for GP. It allows solving basic, weighted, and lexicographic GP problems, as well as a mixture of them. The package can take as input a human-readable representation of the goals, or a set of matrices. Then it automatically sets up the corresponding LP problem, which it later solves using the powerful lp_solve solver. goalp also allows mixing goals with traditional linear optimisation constraints, i.e. those not allowing for deviations. Furthermore, it allows for continuous, discrete, and dichotomous decision variables.
goalp is currently available via CRAN, including a user guide [vignette].",goalp - Weighted and Lexicographic Goal Programming Interface for R,"[78640, 46127]",845,"[134, 77, 110]",3458,Optimization Frameworks,76,14,30,Software for Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,53 [building - 208],"['Software', 'Multi-Objective Decision Making', 'Programming, Linear']",WC-30
"Increasing the scale of kidney exchanges leads to improved outcomes for patients, as it increases the possibility of compatible donors being present. One way to increase scale is by collaboration between different hospitals or countries. However, it has been shown that in single period settings, no strategyproof mechanisms exist to organize such collaborations. This has led to an interest in credit mechanisms which function over multiple time periods. Individual agents reveal pairs to the collaboration, and receive credits in return. These influence which transplants are selected, where more credits lead the choice of solutions that are more beneficial to the agent. Based on the transplants received, credits are subtracted. Such mechanisms can lead to fairer outcomes, but also potentially escape the impossibility of strategyproof mechanisms in single period settings.
In this talk, we look in particular at credit mechanisms proposed by Klimentova et al. [2021] and Biro et al. [2020]. These mechanisms divide credits equal to the number of transplants obtained in an optimal solution, and each transplants “costs” one credit. We discuss the strategyproofness of their proposed mechanisms, and more in general of all mechanisms sharing some important properties with these.
",Strategyproofness of Credit Mechanisms for Kidney Exchange,[35964],592,"[50, 56]",3460,Kidney Exchange I,3,9,10,OR in Health Services [ORAHS],"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,11 [building - 116],"['Game Theory', 'Health Care']",TC-10
"This study aims to develop a method based on the radial epiderivative, a notion defined for non-convex functions. The method begins with an initial solution. Then, at each iteration, a direction is chosen and evaluated by using the value of the radial epiderivative at the current point. If the direction is decent, a new point is generated in that direction. If not, a new direction is determined, and the process is repeated. Determining directions is a crucial aspect to consider. The study utilizes particle swarm optimization [PSO] and cyclic coordinate [CC] to determine directions at each iteration. Two algorithms, radial epiderivative-based PSO [RPSO] and radial epiderivative-based CC [RCC], are then analyzed. The algorithms' performance is demonstrated on test problems from the literature, and the results are promising.",Radial Epiderivative based Method in nonconvex Lipschitz Optimization,"[47573, 77617, 78647, 78645]",509,"[81, 113, 5]",3461,Structured nonconvex optimization ,70,13,41,Nonsmooth Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,97 [building - 306],"['Non-smooth Optimization', 'Programming, Nonlinear', 'Algorithms']",WB-41
"We design a Mixed-Integer Linear Programming model for minimizing Covid-19 infections and deaths over a given horizon of several weeks. We allow two different strategies - centralized decisions applying homogeneously to all citizens at the national level, or decentralized decisions that can vary across regions or cities having different initial situations and risks. The decisions to make are the level of Non-Pharmaceutical Interventions [NPIs] implemented each week, like lockdowns, travel bans, school closures, or combinations of them. Shortages in doctors and hospital beds are considered in the model. Beyond the epidemic model that estimates the number of infections each week, one key aspect of this study is, in the decentralized scenario, the degree of compliance of the population facing more severe restrictions. Our results on real data show from which level of compliance the decentralized strategy starts to outperform the centralized strategy, under the same severity budget over the whole horizon. Compliance has been shown in previous studies to be decreasing in time, hence several kinds of compliance functions are considered to get robust results. Our results are also analyzed with or without the presence of vaccines. ",Decentralized vs centralized optimal strategies for Covid-19 control with compliance issues,"[4914, 69609]",963,"[56, 129, 84]",3464,COVID-19 [2],3,14,15,OR in Health Services [ORAHS],"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,18 [building - 116],"['Health Care', 'Scheduling', 'Optimization Modeling']",WC-15
"In this talk we describe a fused lasso approach for the regularized multi-period mean variance model, in a Markowitz framework. Multi-period portfolio selection aims at computing the optimal allocation of wealth among n assets within a time horizon of m periods. In addition, the investor can rebalance the portfolio at the beginning of each period. We define a multi-period minimum variance model with final expected return and introduce l1-regularization techniques to stabilize the solution process, which is well-known to be ill-conditioned because of assets correlation. Two l1-penalty terms are used - one on portfolio weights promotes sparsity in the solution; this allows investor to reduce both the number of positions to be monitored and the transaction costs. A second l1-penalty term is added to the objective function of the arising optimization problem. This is a penalization on the portfolio turnover, thus it limits the number of transactions by preserving the pattern of active positions. The model leads to a nonsmooth constrained optimization problem, where the inequality constraints are aimed to guarantee at least a minimum level of expected wealth at each date. We develop iterative algorithms based on first order methods. We validate the approach showing results of tests performed on real data.",Nonsmooth optimization in sparse portfolio selection,"[57364, 57067, 62357]",916,"[45, 81]",3465,"Nonsmooth optimization and applications, Part II",84,8,32,Advances in large scale nonlinear optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,41 [building - 303A],"['Financial Modelling', 'Non-smooth Optimization']",TB-32
"Addressing climate change requires the transportation sector to adopt renewable hydrocarbon fuels, particularly in aviation, maritime, and heavy-duty road transport, where electrification faces challenges. This transition necessitates the design of new production networks and supply chains, yet it faces uncertainties related to resources, technologies, and demand. It is vital to ensure that these renewable fuel supply chains are flexible and robust to uncertainties, such as supply and demand fluctuations or technological changes. We explore the impact of uncertainty on the design of renewable fuel supply chains, focusing on the variable capacity factors of renewable energy sources and their influence on electricity supply. We propose a multi-stage stochastic programming approach to optimize the renewable fuel supply chain design, taking into account seasonal and geographical variations in resource availability. We aim to minimize the expected total system cost while accommodating the uncertainty in the capacity factor of renewable electricity which will affect its supply. Scenario generation is based on 30 years of hourly data, using Latin Hypercube Sampling and Iman-Conover transformation to incorporate correlation among uncertain parameters. A fast-forward heuristic algorithm is developed to solve the model efficiently. Applied to a case study on the EU, results show promise for this approach in designing multi-period supply chain networks that work well under each scenario",A stochastic mixed-integer programming approach to optimize a renewable fuel supply chain under uncertainty of renewable electricity supply,"[70542, 75075, 2650]",484,"[136, 138, 93]",3466,Sustainable Supply Chain Design,19,2,24,Sustainable Supply Chains,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,83 [building - 116],"['Stochastic Optimization', 'Supply Chain Management', 'OR in Energy']",MA-24
"With the abundant availability of data, the importance of proper clustering algorithms has been growing over the past decades. This is even more the case when the objects to be clustered can be represent by either total or partial rankings. A well-known clustering method is the so called k-medoid algorithm. Still the major question remains how to define the median object of a cluster and what is the best one? The similarity measure to adopt in the clustering algorithm when comparing rankings is Kendall’s rank correlation coefficient.
Here we investigate the subproblem of maximizing the median of a set off rank correlation coefficients so as to determine the best median ranking by determining the optimal ranking. Since the optimal median ranking has to be determined, the rank correlation coefficients will also be variable. Hence, the sorting of Kendall’s tau’s in order to determine the median in the optimization model should be incorporated. In case tied rank values are absent, we will show that this optimization problem can be re-written as a linear mathematical program. If however ties are present, the linearization will only be possible if the different possible partitions of the rank values are preprocessed.
",On the optimal ranking maximizing a median rank correlation coefficient,[71242],883,"[14, 72, 111]",3467,Topics in Combinatorial Optimization II [Contributed],64,15,25,Combinatorial Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,011 [building - 208],"['Combinatorial Optimization', 'Mathematical Programming', 'Programming, Mixed-Integer']",WD-25
"We extend the model of designing a parcel lockers network as a solution to the logistics last mile problem, by choosing how many parcel lockers facilities to open, where to locate them, how many lockers to install in each facility, how to serve demand that exceeds the facilities’ finite capacities, and what discounts to offer to customers to motivate them to prefer the lockers’ option over the more convenient option of receiving the parcel at their doorstep. The objective is to maximize the total profit, consisting of the revenue from customers who use the parcel lockers solution, minus the facilities’ fixed and operational setup costs and the different discounts offered to customers, depending on their travel distance and on their mode of pick-ups [on time, with minor or major delays, or by-passing the lockers and picking up directly at designated counters inside stores that were contracted for this task]. The problem is expressed as a mixed integer linear program. We show that the model is tractable and can easily be solved by appropriate solvers available today. We present a few numerical examples to show how the model can be used in practice and demonstrate its sensitivity to additional constraints that might be imposed in certain circumstances.",Extending the Parcel Lockers' Solution to the Logistics Last Mile Problem,"[3746, 69610]",747,"[12, 43, 65]",3468,Last-Mile Delivery,5,12,64,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S16 [building - 101],"['Capacity Planning', 'Facilities Planning and Design', 'Logistics']",WA-64
"We study two supply chain models - [i] Buyer-Led Supplier Development and [ii] Supplier-Led Supplier Development, which are consisting of a buyer and a supplier. We analyze which player should initiate the carbon footprint reduction efforts in the supply chain under a scope 3 emission tax by using different levers. The supplier, as the producer, may invest to reduce the product carbon footprint. The buyer, as the downstream partner, may incentivize the supplier through supplier development initiatives. In the buyer-led model, the buyer offers a wholesale price premium rate contingent on improvement, and the supplier decides her carbon footprint reduction efforts accordingly. In the supplier-led model, the supplier offers sharing the carbon footprint reduction investment cost and the buyer decides the improvement level. We compare these two models and gain insights into their effectiveness under varying market conditions. Our results show that carbon footprint reduction is guaranteed if the supplier takes the lead, however, a wider range of improvement opportunities occur when the buyer initiates the reduction effort.",WHO LEADS THE WAY? BUYER VS. SUPPLIER INITIATIVES IN SUPPLY CHAIN CARBON FOOTPRINT REDUCTION,"[78653, 42083, 78654]",486,"[138, 50, 100]",3469,Carbon Regulation,19,13,24,Sustainable Supply Chains,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,83 [building - 116],"['Supply Chain Management', 'Game Theory', 'OR in Sustainability']",WB-24
"This presentation describes the formulation of a linear programming model and its use as a decision support tool to assess the phasing out of diesel vehicles and their replacement by low emission vans [LCV] and trucks [HGV]. The company concerned offered one of the largest fleets of commercial rental vehicles in the UK and was keen to maintain its credentials with respect to decarbonisation. 
The main considerations for management were the planned lifetime of the various existing diesel vehicles, the likely capital budget for new vehicles, a company-set reduction in CO2 emissions over the planning horizon, a reduction in the ratio of diesel to electric vehicles and all this in the context of a percentage annual growth rate for the total fleet size.  The objective function was to minimise emissions over 5 years and one of the main outcomes was whether the company’s planned profile for reduction in CO2 emissions was feasible given the capital spending budget for new vehicles. The difference in purchasing costs of diesel and electric vehicles is quite considerable and, so to both adhere to the purchasing budget and achieve the planned emissions reductions, it would not be possible to adopt a cliff edge change in the vehicle purchasing profile. The continued acquisition of some diesel vehicles would be necessary. The model [developed using the Xpress platform] is described in detail along with the reaction of the company management to the results obtained.
",Planning for a Transition to a Low Emissions profile for a large Commercial Vehicle Fleet,[458],226,"[110, 100, 14]",3470,Applications of combinatorial optimisation in industry and services I,64,7,29,Combinatorial Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,157 [building - 208],"['Programming, Linear', 'OR in Sustainability', 'Combinatorial Optimization']",TA-29
"Scheduling patient appointments at a hemodialysis center introduces a unique scenario. In contrast to other healthcare appointment scheduling challenges, patients undergo a series of dialysis treatment appointments rather than a single one. In this study, we formulate the multiple-appointment system as a set-partitioning problem and solve it using a Branch and Price [BP] algorithm. The pricing subproblem proves to be challenging for the dynamic programming algorithm. Hence, we further decompose the pricing sub-problem and propose a column generation-based algorithm for solving the subproblem. ",A Column Generation-Based Exact Algorithm for Multiple Appointment Patient Scheduling,"[62429, 73775, 54077]",596,"[13, 129, 151]",3471,Appointment planning,3,8,15,OR in Health Services [ORAHS],"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,18 [building - 116],"['Column Generation', 'Scheduling', 'Practice of OR']",TB-15
"The importance of online shopping has risen sharply in recent years. The Covid-19 pandemic brought up new customer segments, new industries and local shops were opened on a large scale by connecting to online retail. This leads to a significant increase in the volume of freight traffic.
Various innovative approaches have already been tested, but most initiatives for cooperation between logistics service providers have not succeeded so far. Appeals to individual responsibility for bundling orders on the customer side also appear to be ineffective. Therefore, restructuring the commissioning process for delivery services could be the way to go. 
The idea of an independent online platform for the organization of logistics services could provide a one-stop-shop solution for these problems - instead of the current decentralized placing of delivery orders through individual dealers, deliveries could be centrally coordinated in the interests of the society and the environment. Hence, spatial, temporal as well as modal optimizations should achievable. 
To evaluate the potential of such a solution the amount of load consolidation needs to be determined. A scenario-based approach is currently developed to derive appropriate estimates for Austria. First estimations show a reduction of greenhouse gas emissions caused be delivery vehicles of up to 20% for Austria in total but more detailed estimations are required to cover more realistic settings. ",Estimating the load consolidation potential for e-commerce deliveries in Austria – a scenario-based approach,"[67668, 78662, 78661]",540,"[32, 145, 100]",3472,Sustainable Logistics,19,8,24,Sustainable Supply Chains,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,83 [building - 116],"['E-Commerce', 'Vehicle Routing', 'OR in Sustainability']",TB-24
"The paper backtests the accuracy of 1-day Value-at-Risk [VaR], computed with variance-covariance methodology, using various alternative methodologies for estimating the risk factor’s volatility. The aim is to gauge whether using option-implied volatility [IV] produces superior results compared to other time-series based measures. In particular, we want to test if a forward-looking estimate – reflecting the sentiment of market investors – is more capable of capturing the tail behaviour of returns distribution during crisis periods, characterized by volatility spikes. The empirical analysis is performed on the S&P500 Stock Index, using both the VIX Index and VIX-1day Index, recently introduced by the Chicago Board Options Exchange. The crisis analysed are the pandemic period and the months following Russia’s invasion of Ukraine. Our results contradict our initial hypothesis, showing that IV-based VaR tend to be underestimated during market turbulences, both in terms of frequency of realized losses exceeding the threshold and in terms of average magnitude of the excess losses. However, quite a different picture emerges when we use the new VIX1D index, instead of the traditional VIX. The preliminary evidence indicates that the 1-day implied volatility derived from near term options may be effective for estimating VaR and superior to alternative methodologies. ",Forecasting the worst - is implied volatility forward-looking enough?,"[78648, 78559]",277,"[126, 47, 44]",3477,Risk Management and Cryptoassets,4,8,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S14 [building - 101],"['Risk Analysis and Management', 'Forecasting', 'Finance and Banking']",TB-63
"We propose an interior point method [IPM] for solving semidefinite programming problems [SDPs]. The standard interior point algorithms used to solve SDPs work in the space of positive semidefinite matrices. Contrary to that the proposed algorithm works in the cone of matrices of constant factor width. We prove global convergence and provide a complexity analysis. Our work is inspired by a series of papers by Ahmadi, Dash, Majumdar and Hall, and builds upon a recent preprint by Roig-Solvas and Sznaier [arXiv:2202.12374, 2022]. This talk is based on joint work with Felix Kirschner.",A predictor-corrector algorithm for semidefinite programming that uses the factor width cone,[21957],139,"[21, 60]",3479,Interior point methods,68,3,38,"Conic Optimization - Theory, Algorithms, and Applications","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,34 [building - 306],"['Convex Optimization', 'Interior Point Methods']",MB-38
"This talk reflects on the 1961 publication of Jay Forrester’s 'Industrial Dynamics', the founding text of the field of System Dynamics. It considers the circumstances that catalysed Forrester and explores the influences and published antecedents of the book. It presents a range of contemporary reactions, drawn from perspectives inside and beyond OR. It explores the ways in which the ideas in 'Industrial Dynamics' - ID - relate to other fields. From the perspectives of Systems Science, OR and Problem Structuring Methods/Soft OR, it explores - commonalities in intellectual provenance; similarities in central ideas; influences that the book had on the field. Via a biographical sketch, the talk argues that the provenance of the ideas in ID can be seen to be grounded in, and exemplified by, the life of Forrester himself.  It closes by drawing three conclusions. [1] The book continues to offer methodological challenges to OR, to contribute to debates on the nature and identity of OR. [2] It vivified an important thread of feedback thinking, providing a vehicle by which abstract ideas can be put to practical use. [3] ID anticipated developments in PSMs/Soft OR - meaning that the book’s ideas add weight to the case made by those in this area. In other words, whilst ID continues to be source and origin for the core ideas of the System Dynamics field, it offers bold and innovative ideas for all those interested in using practical techniques to solve messy, real-work problems.",Exploring the innovative nature of Forrester’s ‘Industrial Dynamics’,[37801],454,"[140, 131, 133]",3481,Moments in the history of OR  2,27,14,20,Moments in the history of OR,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,45 [building - 116],"['System Dynamics and Theory', 'Simulation', 'Soft OR']",WC-20
"The Capacitated Facility Location Problem [CFLP] is a well-known combinatorial optimization problem extensively studied in the field of location sciences. It has numerous applications in industrial engineering, humanitarian logistics, telecommunication networks, and other domains. Incorporating uncertainties in demands, stochastic programming emerges as a suitable approach to address this problem. However, the main drawback is that due to stochasticity, not all customer demand may be satisfied. To address this issue, we introduce the concept of backlogs into the CFLP. As a result, the CFLP with backlogs and normally distributed demands is proposed, where the cost of losing a customer is considered as a penalty cost in the objective function. To solve the problem we propose an exact and a heuristic method. The exact method is a Benders decomposition based on Branch-and-Cut. The heuristic method follows a Fixing-First scheme based on pricing strategies that efficiently solves the problem within a reasonable computational time. The effectiveness of the proposed algorithms is evaluated by comparing it against a deterministic equivalent solution given by the general-purpose solver Gurobi. Computational experiments are conducted on a set of challenging instances using a sample average approximation scheme. To validate the applicability of the problem under study, a real case study involving Mobile Health Clinics [MHCs] located in Mexico was analyzed.",The Stochastic Capacitated Facility Location Problem with Backlogs - Exact and Heuristic Algorithms,"[73759, 73847, 73856, 73817]",614,"[64, 11, 117]",3483,Location under uncertainty,29,5,61,Locational Analysis,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S10 [building - 101],"['Location', 'Branch and Cut', 'Programming, Stochastic']",MD-61
"In the past decades, real-life security problems occurring on board passenger ships in the open sea have become a great challenge. Evacuating a large number of evacuees in a limited time period is a difficult task, especially in a ship with a very complex structure inside. In this paper, we focus on the development of a fast algorithm for the emergency evacuation from a moving or sinking ship to safe points [such as lifeboats on the board]. The main objective is to minimize the total network clearance time. We propose a cell-transmission-based mixed integer programming model that accounts for evacuation dynamics, congestions and balanced outflows from exits. A Lagrangian relaxation is used to decompose the problem into two subproblems. For the first subproblem, we develop a time-expanded network and propose an alternative cycle-canceling algorithm to solve it. To demonstrate the efficiency of the proposed solution technique, we provide computational results on a realistic example in the numerical part.",Emergency Evacuation Routing from the Passenger Ship,[57751],688,"[150, 70, 129]",3487,Disaster & Emergency Management,80,5,53,Sustainable and Resilient Systems,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,8007 [building - 202],"['Network Flows', 'Maritime applications', 'Scheduling']",MD-53
"The sports industry has a significant global reach, with a value of $480 billion in 2023, which is expected to increase to $629.81 billion by 2028. These data reveal various profit potentials, particularly for the betting industry, which generated a total turnover of €1.41 trillion in 2022, of which €730 billion was attributed to football alone. Due to its predominant role in the global sporting landscape and its significant economic impact, this work focuses on football, specifically on Expected Goals [xG]. The accuracy of predicting xG plays a critical role in the financial and strategic decisions of football teams, as well as for investors and bookmakers, as it allows them to assess a team’s performance and identify value bets more accurately. This work proposes an innovative Neural Network approach to predict Expected Goals in the main Italian league, Serie A, with the aim to provide a significant advancement in the application of predictive methods to football, opening new perspectives for analyzing and optimizing sporting and financial performance. Implementing this methodology could improve on-field performance and optimize team investments. Additionally, it could provide important insights for investors and bookmakers, enabling them to make more informed and profitable decisions in the context of professional football. This work is part of a larger research project that aims to evaluate financial contracts indexed to the performance of professional athletes.",A Neural Network Approach to Evaluate Expected Goals in Football,"[78671, 57067, 57364]",665,"[47, 66]",3488,Football analytics,37,9,16,OR in Sports,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,19 [building - 116],"['Forecasting', 'Machine Learning']",TC-16
"This work presents a novel adaptation of the Stochastic Gradient Descent [SGD], termed AdaBatchGrad. This modification seamlessly integrates an adaptive step size with an adjustable batch size. An increase in batch size and a decrease in step size are well-known techniques to tighten the area of convergence of SGD and decrease its variance. A range of studies by R. Byrd and J. Nocedal introduced various testing techniques to assess the quality of mini-batch gradient approximations and choose the appropriate batch sizes at every step. Methods that utilized exact tests were observed to converge sublinearly. Conversely, inexact test implementations sometimes resulted in non-convergence and erratic performance. To address these challenges, AdaBatchGrad incorporates both adaptive batch and step sizes, enhancing the method's robustness and stability. This makes AdaBatchGrad markedly more robust and computationally efficient relative to prevailing methods. To substantiate the efficacy of our method, we experimentally show, how the introduction of adaptive step size and adaptive batch size gradually improves the performance of regular SGD. The results imply that AdaBatchGrad surpasses alternative methods, especially when applied to inexact tests.",AdaBatchGrad - Combining Adaptive Batch Size and Adaptive Step Size,[78674],360,"[8, 21, 66]",3489,Adaptive and Polyak step-size methods,84,12,32,Advances in large scale nonlinear optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,41 [building - 303A],"['Artificial Intelligence', 'Convex Optimization', 'Machine Learning']",WA-32
"Prescriptive Process Monitoring [PresPM] is an emerging field dedicated to devising methods for suggesting interventions in business processes to optimize them. Machine learning models are applied using event logs to attain desired outcomes for each case of a process. Current research in PresPM reveals two significant gaps. The first involves the underutilization of synthetic data in existing literature. Synthetic datasets offer easy customization and the ability to generate ground truth counterfactuals for a given case, providing precise model evaluation and explainability of an intervention recommendation. However, these datasets are often overlooked and can be unrealistic. The second gap revolves around the often exclusive focus on a narrow range of machine learning approaches and intervention types, restricting a holistic understanding of PresPM methodologies. To bridge these gaps, we conduct a thorough comparison of machine learning methods using a synthetic dataset mimicking a local bank's loan application process. This dataset strikes a balance between real-life complexity and simplicity, allowing meaningful comparisons. Our approach involves variants of the established methods Causal Inference and [online] Reinforcement Learning and we compare insights obtained from the synthetic dataset with historical datasets to assess consistency when applied to real-world data. This research aims to contribute to a more comprehensive understanding of current methodologies.",Balancing Realism and Simplicity - A Synthetic Dataset Approach for Comprehensive Model Benchmarking in Prescriptive Process Monitoring,[78673],935,"[8, 66]",3490,Advances in Process Mining,15,8,27,Mathematical Optimization for XAI,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,047 [building - 208],"['Artificial Intelligence', 'Machine Learning']",TB-27
"Educational games hold particular significance in elementary school education, as they serve as a popular tool for engaging children in learning. However, a common challenge faced by these platforms is the risk of children becoming disinterested. To mitigate this, it becomes desirable to sustain their engagement with the platform to foster continuous learning and retention. This raises the crucial question - Which games should children play? While the approach may be effective in the classroom, children who engage with these games outside of school settings have unrestricted access to all available games, often without knowing which ones are best suited to their needs. Consequently, if they struggle, they are prone to becoming bored or frustrated, ultimately leading them to disengage. To enhance engagement and optimize learning outcomes, it is essential for children to play games where they perform adequately yet are still challenged enough to promote learning. In this work, we develop state-of-the-art recommender systems based on graph-based neural networks for an educational game platform for children called Tak-Tak-Tak. The machine learning process is guided by the choices made by the users in terms of the next game to play, as well as the efficacy of a user's learning experience within a game and the duration of the play sessions. We conduct a comparative analysis between a wide variety of recommender system approaches to evaluate their effectiveness in this context.",A network-based recommender system to enhance learning in educational games,"[78675, 78676, 78678, 47369, 18420]",149,"[7, 66, 92]",3491,Learning Analytics using Mathematical Optimization and XAI,15,12,27,Mathematical Optimization for XAI,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,047 [building - 208],"['Analytics and Data Science', 'Machine Learning', 'OR in Education']",WA-27
"Shop-floor space in urban areas is scarce and expensive. To avoid a time-consuming zigzag movement when replenishing densely arranged shelves, store-specific shipment buildups [SSSBs] mirror the store layout in the packing pattern of the carriers [e.g., roll cages] on which merchandise arrives in brick-and-mortar sales outlets. If a carrier is pre-sorted according to the layout of its designated store, then the sales personnel can follow a clear route through the store while replenishing the shelves. The time savings in the stores come at the price of additional effort in the retail warehouses having to assemble SSSBs, however. If picker-to-parts order picking is applied to assemble SSSBs, then it is the order pickers who must zigzag through the warehouse. In this paper, we develop and test the performance of novel storage location assignment policies that consider order frequencies and store layouts when deciding on the storage positions of stock keeping units [SKUs] on the shelves of a picker-to-parts warehouse. We evaluate these novel policies both with the theoretical optimum if perfect information were available and with traditional SLA policies merely based on order frequencies. These computational tests show that our novel policies can greatly reduce the picking effort during the assembly of SSSBs in picker-to-parts warehouses.",Storage assignment policies for retail warehouses processing store-specific shipment buildups,"[71040, 5934, 64320, 50403]",635,"[146, 65, 74]",3492,Warehouse Management,30,5,50,Retail Operations,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,M2 [building - 101],"['Warehouse Design, Planning, and Control', 'Logistics', 'Metaheuristics']",MD-50
"This work explores the intricate dynamics of the multiarmed bandit [MAB] problem, augmented with a critical realworld consideration - the cost implications of switching decisions. This study is at the forefront of integrating risk considerations into MAB scenarios, where decision-makers face penalties each time they transition between options. Such scenarios are not just theoretical constructs but are reflective of numerous practical applications. Our work distinguishes itself by addressing the largely unexplored domain of risk-averse MAB problems compounded by switching penalties. Our contribution is threefold - firstly, we explore the qualitative aspects of optimal policies. Secondly, we present novel theoretical results, including the development of the Risk-Averse Switching Index [RASI], which addresses the dual challenges of risk aversion and switching costs, demonstrating its near-optimal efficacy. Lastly, through rigorous numerical experiments, we validate our algorithm’s effectiveness and practical applicability.",Risk-Averse Multiarmed Bandits with Switching Penalties,[68392],958,"[136, 8, 82]",3493,Stochastic optimization - theory and applications,49,13,35,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,44 [building - 303A],"['Stochastic Optimization', 'Artificial Intelligence', 'Optimal Control']",WB-35
"This study investigates the impact of managerial gender on portfolio risk, based on the assumption that women are more risk averse than their male counterparts. The research focuses on a key question - Does this risk aversion manifest itself in the stock market performance of firms that are majority-led by women? To answer this question, we constructed two distinct portfolios based on the gender of top managers - one composed of predominantly female-led companies and one composed of exclusively male-led companies. The selection of companies was based on MSCI’s annual global gender diversity reports, and the analysis was conducted over a sample period from 2018 to 2022. Portfolio risk was assessed using the value-at-risk [VaR] metric, with dynamic optimisation procedures applied through an AR[1]-EGARCH[1,1] model to minimise variance. Our results reveal that portfolios composed of predominantly female-managed companies exhibit lower risk, as measured by VaR, compared to portfolios of male-only managed companies. ",Gender as a tool for diversification,"[57463, 62886, 78677]",470,"[126, 0]",3494,Applications to Economics and Finance,4,14,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S14 [building - 101],['Risk Analysis and Management'],WC-63
"Vehicle Routing Problem with Time Windows [VRPTW] is a well-studied challenging combinatorial optimization problem. In this work, we focus on an extension of VRPTW where each vehicle can travel multiple routes, each customer has multiple time windows and each vehicle can start their route from their latest destinations. These kind of problems usually arise in marine routing problems, where a fleet of ships need to visit multiple customer and supplier ports and the trip may end up in any of the ports. Then, their next trip will start in the last port they have visited in their previous trip. Usually the optimization problem is defined over a planning horizon, however the routing scheme and the initial/final locations of the ships are dynamic depending on the planning horizon cut-off dates.

A mixed integer linear programming formulation is developed to represent the problem and various small-sized instances are solved exactly, using commercial solvers. For larger instances various heuristic and meta-heuristic methods are being developed and tested.",Multi-Trip Vehicle Routing Problem With Multiple Time Windows and Dynamic Starting Locations,[42196],784,"[145, 14, 111]",3495,Dynamic Vehicle Routing 2,5,10,64,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S16 [building - 101],"['Vehicle Routing', 'Combinatorial Optimization', 'Programming, Mixed-Integer']",TD-64
"We study a family of discrete optimization problems asking for the maximization of the expected value of a concave, strictly increasing, and differentiable function composed with a set-union operator. The expected value is computed concerning a set of coefficients taking values from a discrete set of scenarios. The function models the utility function of the decision maker, while the set-union operator models a covering relationship between two ground sets, a set of items, and a set of metaitems. This problem and it can be modeled as a mixed integer nonlinear program involving binary decision variables associated with the items and metaitems. Its goal is to find a subset of metaitems that maximizes the total utility corresponding to the items it covers. It has applications to, among others, maximal covering, location of critical infrastructures and influence maximization problems.",Critical infrastructures location via submodular maximization ,"[25257, 37844, 22042]",254,"[14, 109]",3496,Mixed Integer Optimization I,64,7,52,Combinatorial Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,8003 [building - 202],"['Combinatorial Optimization', 'Programming, Integer']",TA-52
"The global net-zero transition requires new international energy links based on renewable energy carriers. Moreover, recent global energy supply disruptions emphasize the need to integrate risk into future energy systems. Against this background, we aim to construct affordable, secure energy supply chains for both importing and exporting countries by incorporating risk in energy system design. Our linear bi-objective quadratic optimization model analyzes total costs and security risk trade-offs in renewable energy production. Using the e-constraint in Python and Gurobi, we apply our model to a case of renewable energy production in Africa and its supply to Europe.
Our findings reveal how characteristics of exporting countries, e.g., socio-political, environmental, and technological features and overall network structures, impact costs and security risk. Contrary to common assumptions of marginal risk improvements leading to significant additional costs, our model shows efficient risk mitigation is possible without a substantial cost increase. Our research further confirms that Africa's renewable potential can both satisfy its domestic energy demand and contribute to Europe's energy imports, underscoring the continent's role in global energy transitions.
This work provides a comprehensive tool for decision-makers to evaluate cost efficiency vs. supply security trade-offs in renewable energy system design, enhancing resilience and reliability during global energy transition.",Balancing Costs and Supply Security Risk in Renewable Energy Systems -  A Bi-Objective Optimization Approach and an African-European Case Study,"[72931, 75079, 2650]",545,"[93, 112, 28]",3500,Clean Energy Supply Chains,19,4,24,Sustainable Supply Chains,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,83 [building - 116],"['OR in Energy', 'Programming, Multi-Objective', 'Developing Countries']",MC-24
"Deep learning has emerged as the predominant method for text analytics owing to its capacity to learn language patterns from vast datasets and subsequently apply this knowledge to specific tasks. In this study, we leverage deep learning techniques to enhance the categorization of public security reports submitted by ordinary individuals through a mobile application in Chile. Users select a category to report an incident from a wide array of options, encompassing everything from lost pets and disturbing noises to accidents involving injuries or burglary. They also have the option to provide a description of the incident. A primary challenge arises from users often misreporting events in the first category presented in the app.
While the app has proven invaluable for municipalities and police departments in efficiently deploying resources and patrols to address incidents and prevent crime, its prioritization scheme relies on the category provided by the user. This study introduces a machine learning solution employing BERT and other Transformer architectures to learn from incident descriptions and accurately assign labels, thereby improving event prioritization. Our results show a positive predictive performance. Additionally, the model shows potential to extract further insights from descriptions, augmenting prioritization by, for instance, assessing the severity of accidents or crimes. 
",Dealing with noisy labels in text analytics - an application in crime analytics for prioritizing reports,"[57734, 18420, 11616]",62,"[8, 7, 26]",3503,Crime Analytics,17,3,31,Analytics,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,54 [building - 208],"['Artificial Intelligence', 'Analytics and Data Science', 'Decision Support Systems']",MB-31
"We consider a robust Markov decision process for which we assume that the true transition distribution can be uniquely specified by a parametric distribution. We enforce this by the construction of a parametric ambiguity set that ensures the worst-case distribution is from the same parametric family of distributions. To conduct robust value iteration, we formulate the Bellman update as a linear program by discretising the ambiguity set. This scales poorly with problem size and requires significant pre-computation. We develop two algorithms for solving the update. Firstly, a cutting surface algorithm that reduces the time to solve the linear program but still requires the pre-computation. Secondly, a projection-based bisection search algorithm that removes the need for discretisation and pre-computation. Using a multi-period Newsvendor problem as an example, we compare these methods with non-parametric phi-divergence based methods from the literature. We show that the projection-based algorithm completes robust value iteration significantly faster than the other parametric algorithms, and is faster than the phi-divergence equivalent.",Robust Markov decision processes under parametric transition distributions,"[9228, 74841, 21665]",826,"[127, 108]",3504,Robust Optimization - Theory and Applications,49,14,35,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,44 [building - 303A],"['Robust Optimization', 'Programming, Dynamic']",WC-35
"Predictive policing represents a productive avenue of research utilizing analytics to anticipate potential criminal activities. This study aims to develop a learning machine capable of forecasting the occurrence of specific crime types in a given area based on historical data. The primary objective of this model is to furnish municipalities and police departments with a tool to deploy patrols and allocate resources efficiently. Data was collected from a Chilean crime report app, encompassing various criminal events and suspicious activities reported by the public. Under the assumption that past reports serve as reliable predictors of future criminal activities, we formulate a classification problem to forecast burglary and motor vehicle theft in a designated area. Promising outcomes were achieved using conventional statistical and machine learning methods such as logistic regression, decision trees, and gradient boosting, attaining a balanced accuracy exceeding 80%. We also discuss the used of graph-based deep learning and other network architectures to take advantage of the spatial structure of the problem.",Using crime report app data to predict burglary via machine learning,"[18420, 57734, 11616]",62,"[8, 7, 22]",3505,Crime Analytics,17,3,31,Analytics,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,54 [building - 208],"['Artificial Intelligence', 'Analytics and Data Science', 'Critical Decision Making']",MB-31
"We consider a periodic review inventory system in which multiple customers are jointly and periodically replenished from a single supplier by a vehicle with limited capacity. The customers have stochastic demand, and at every replenishment, their inventory is supposed to be filled up to a so-called ‘base-stock’ level. However, when their cumulative demand between replenishments exceeds the vehicle capacity, it is impossible to replenish all the customers up to their base-stock and shortfall occurs.
When the distribution of this shortfall is approximated with a mass exponential function, closed expressions for the expected inventory holding costs and stockout costs at the customers can be derived. This allows us to optimize the base-stock levels of the customers, minimizing the total holding and stockout costs. As a result, we can evaluate several rules-of-thumb for allocation the shortfall across the customers. Moreover, we can optimize the shortfall allocation in an iterative procedure.
In an extensive computational study, we compare the cost effectiveness of various shortfall allocation rules across a wide range of scenarios where the customers have varying cost parameters and storage capacity limitations. Furthermore, the convergence of the iterative shortfall allocation optimization procedure is inspected, and the accuracy of the approximation is evaluated using simulation experiments.",Shortfall Allocation and Safety Stock Optimization for Capacitated Joint Replenishment,"[4741, 76654]",788,"[65, 61, 16]",3506,Logistics 2,5,14,58,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S07 [building - 101],"['Logistics', 'Inventory', 'Complexity and Approximation']",WC-58
"Dual cycling, which executes loading and unloading simultaneously, is acknowledged for reducing handling time at the port, thus enabling slow steaming and GHG reduction. Previous studies on dual cycling mainly focused on container shipping with deterministic cargo arrivals, which are unsuitable for the RoRo [Roll-on/Roll-off] sector’s inherent uncertainties. Our research bridges this gap by proposing a two-stage stochastic model that aims to minimise the total discharging and loading operation time under uncertain cargo arrivals. The model is based on several assumptions - cargo is distinguished as unaccompanied trailers and classified by weight, and it is assumed that the ship’s space is divided into slots for distinct cargo types based on a predetermined weight-based plan; cargo arrivals follow a stochastic pattern, and the ship should wait until all planned cargoes have arrived and are loaded. Loading and unloading follow specific precedence rules reflecting the deck layout and safety requirements, and the number of tugs during the entire operations remains constant.  Addressing the computational challenge, we propose a heuristic rolling horizon approach and a scenario reduction framework. As a case study, the proposed model was implemented on a shuttle vessel’s tank deck. The results demonstrated that, compared to the classical discharge-then-load policy, dual cycling achieved an 11.11% turnaround time reduction and a 38% tug utilisation improvement.",A rolling horizon approach to a two-stage stochastic model for dual cycling under uncertainty in RoRo terminals,"[67073, 78298, 31857]",669,"[70, 136]",3508,Seaside Planning I,52,2,62,OR in Port Operations,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S12 [building - 101],"['Maritime applications', 'Stochastic Optimization']",MA-62
This study evaluates the potential benefit of retrofitting existing conventional cascade hydropower stations [CCHSs] with reversible turbines so as to operate them as pumped hydro energy storage [PHES] systems. We examine the energy generation and storage problem for a CCHS with two connected reservoirs that can be transformed into a PHES system in a market setting where the electricity price can be negative. We formulate this problem as a stochastic dynamic program [SDP] under uncertainty in the streamflow rate and electricity price. We analytically derive an upper bound on the profit improvement that can be obtained from the PHES transformation. We conduct numerical experiments with data-calibrated time series models and observe that the PHES system provides a greater benefit under more limited streamflow conditions or more frequently observed negative prices.,Operational benefit of transforming cascade hydropower stations into pumped hydro energy storage systems,"[77084, 56904, 43460]",453,"[136, 93, 108]",3509,Optimization of energy storage systems,21,7,22,Energy Management,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,81 [building - 116],"['Stochastic Optimization', 'OR in Energy', 'Programming, Dynamic']",TA-22
"Policy uncertainty affects firms’ investment decisions and the corresponding societal welfare [total surplus]. One common perception is that, policy uncertainty has negative welfare effects, and therefore, the regulator should make policy changes “transparent” by announcing future policy changes. This paper investigates a firm’s green technology investment decision in a dynamic setting, where it decides about the investment timing and size under the market uncertainty in the form of stochastic investment costs, and under the policy uncertainty in the form of unannounced subsidy retractions. In particular, we consider two scenarios, depending on whether the regulator announces the subsidy retraction in advance. We show that, a larger policy uncertainty makes the firm invest earlier and less. Besides, the subsidy retraction announcement can motivate the firm to invest early to “take” the subsidy, which is a typical rent-seeking behavior. This has two negative welfare implications - an inefficiently small investment size due to the early investment, and a large subsidy cost due to the firm “taking” the subsidy. Comparing the total surplus generated by firm’s investment decision in the two scenarios, we conclude that the rent-seeking behavior in the announced scenario makes it possible that not-announcing the retraction threshold yields a larger total surplus. This result is robust for several common subsidy types.",Green Technology Investment - Announced vs. Unannounced Subsidy Retraction,"[61148, 61268, 61089]",854,"[37, 25, 33]",3511,Dynamics of the Firm II,90,4,33,Optimal Control Theory and Applications,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,42 [building - 303A],"['Energy Policy and Planning', 'Decision Analysis', 'Economic Modeling']",MC-33
"We analyse a differential Fish Wars game with n players and the infinite time horizon. We calculate the social optimum, a Nash equilibrium and partial cooperation equilibria. The Nash equilibrium always leads to depletion of fish, while the social optimum results in sustainability for a whole interval of parameters. Partial cooperation is in between, with the chance for sustainability growing with the number of cooperators, but only a coalition of size 2 is stable, which makes sustainability unlikely also in this regime. This is the tragedy of the commons in its most drastic form due to logarithmic payoff. We solve the tragedy by two frameworks to - either by an environmental agreement or by a tax-subsidy system. In the first framework, we analyse agreements in which each signatory reduces fishing to the social optimum level until they detect that another signatory has defected. We also assume it takes time to detect a defection. We are interested in the maximal detection time for which the agreement still remains self-enforcing. Counter-intuitively, it turns out that  the more  players, the higher this maximal detection time, so the more stable the agreement. Concerning the second framework, besides solving our specific tragedy, we also propose a general algorithm for finding financial incentives enforcing the optimal profile in a large class of differential games and a large class of financial incentives or proving that such an incentive does not exist.","Self enforcing agreements, delayed information and taxes in a differential game modelling extraction of common renewable resources","[52070, 77698]",846,"[33, 20, 50]",3515,Heterogeneity in optimal control problems,90,7,33,Optimal Control Theory and Applications,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,42 [building - 303A],"['Economic Modeling', 'Control Theory', 'Game Theory']",TA-33
"Intrahospital patient transportation is a crucial activity to hospital operations that also impacts patients' satisfaction. The problem consists in assigning each of the incoming transport requests to one of the available porters, in such a way that patients arrive to their destination on time, or their lateness is minimized. Some patients [i.e., hospitalized patients] are transported on the bed they occupy, while others [i.e., external patients] require a wheelchair provided by the hospital. Handling different types of transport equipment may require porters to switch equipment between two transport requests. For instance, a porter may have to visit an equipment depot to pickup [drop off] a wheelchair before [after] serving a request which requires it, increasing the total traveled distance, and reducing the availability of porters to answer other requests. To the best of our knowledge, this work is the first modeling and solving this realistic version of the intra-hospital patient transportation problem with various types of rolling equipment. We propose and compare the performance of several approaches [including a mixed integer linear programming model, a constraint programming model, a constructive heuristic, and a local search heuristic] with respect to metrics widely used by hospital managers over a set of instances inspired by a real mid-size hospital. The local search heuristic produced excellent results and proved its suitability to be used in real applications.",The intra-hospital patient transportation problem with diverse rolling equipment,"[44542, 72539, 54992]",597,"[56, 143, 129]",3516,Mobility and transportation in healthcare,3,4,10,OR in Health Services [ORAHS],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,11 [building - 116],"['Health Care', 'Transportation', 'Scheduling']",MC-10
"The set partitioning problem can be generalized. One generalization is to consider that certain tasks must be covered several times. So it's no longer a partition that's sought, but a set of parts that verify the right number of coverages. 

Unlike the set partitioning problem, the graph of integer solutions is no longer connected when the possible arcs are simplex pivots. Howerver, in order to restore the connectedness of the graph, it is possible to add artificial variables locally, which allow new pivots to be made while preserving the integrity of the solution.

We give priority to pivots that keep the solution integer. When this is no longer possible, we solve a linear program to obtain a non-degenerate sequence of simplex pivots to perform in order to improve the solution. When this improved solution is still not integer, in some cases the pivot sequence allows to add an artificial column associated with an integer artificial point. The linear program is solved again from this point to find the remaining pivots to be performed in order to obtain a better integer solution.  If integrity is still not reached, a branching is performed.

We present results for large pairing problems [around 2,000 constraints and 300,000 variables] and show that the method is around 2 to 5 times faster than traditional methods [CPLEX] and that in many cases, the addition of the artificial variable effectively allows moves on integer points not directly accessible by pivot.
",Integral simplex for generalized set partitioning problems,"[76897, 51646, 24885, 10966]",395,"[72, 63, 109]",3517,Large Scale Optimization in Air Transportation,64,13,29,Combinatorial Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,157 [building - 208],"['Mathematical Programming', 'Large Scale Optimization', 'Programming, Integer']",WB-29
"Pricing used cars is a complicated process that involves various influencing factors. This study deploys a series of interpretable machine learning models to predict used car prices with elaborate feature engineering. A comparative analysis reveals that ensemble tree algorithms can achieve the highest performance, with an R-squared value exceeding 0.93. Through examining the interpretable feature attributes, the used cars' age, make, and engine capacity are ranked as the top three most significant determinants of price. Furthermore, our wrapper method explores the heterogeneity of features and datasets, by which the performance of neural networks with numerical features on more specific data is highlighted. Finally, the treatment effects of significant binary variables are summarized as suggestions for consumers. By integrating predictive modeling, interpretability, and causal inference, this study presents a novel Predict-Interpret-Suggest framework for the used car market.",A methodology of predict-interpret-suggest for used car pricing,"[78692, 46514]",562,"[7, 66, 47]",3520,"Advancements of OR-analytics in statistics, machine learning and data science 15",16,10,06,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,1013 [building - 202],"['Analytics and Data Science', 'Machine Learning', 'Forecasting']",TD-06
"Reduction of Greenhouse Gas [GHG] emissions in the shipping industry has been of great focus over the past years. Within Roll-on/Roll-off [RORO] shipping, GHG emissions can be reduced by decreasing the turnaround time at port calls.
Decreasing the turnaround time for RORO vessels results in more time being available for sailing between port calls. As more time is allotted for sailing, the vessel can significantly reduce its speed and, thereby, fuel consumption.
If RORO vessels have several port calls, the cargo is handled following an approximate First-in-last-out queue. Therefore, in some cases, cargo to be discharged at the port can become stuck behind cargo not to be unloaded. This results in a necessary re-handling of cargo, increasing time spent at the port.
Optimizing the loading sequence, positioning of cargo, and cargo routing within the vessel reduces the number of cargo that needs to be re-handled.
Prior research has utilized mathematical models and heuristics to solve similar problems. The authors present preliminary results for using a reinforcement learning framework to optimize the RORO stowage planning problem. Leveraging reinforcement learning allows for the development of an algorithm that can provide executable actions at the port based on current information in real time. ",RORO Stowage Planning with Reinforcement Learning,"[71531, 71916, 31857]",671,"[70, 8]",3522,Machine Learning and Optimization in Ports I,52,12,62,OR in Port Operations,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S12 [building - 101],"['Maritime applications', 'Artificial Intelligence']",WA-62
"In this presentation, I give an overview of my Ph.D. dissertation about enhancing explainability and fairness in Machine Learning [ML] via the Mathematical Optimization approach. 
The use of ML to aid Data-Driven Decision-making is increasing dramatically. The wide availability of ML algorithms brings important advantages, such as the improved accuracy of decisions and the reduction in the resources required to make them. Despite excellent accuracy, state-of-the-art ML tools are effectively black boxes that complicate model trustworthiness and may provide unfair outcomes. In my Ph.D. dissertation, I address this issue and model the trade-off between accuracy and transparency.",Novel Mathematical Optimization Models for Explainable and Fair Machine Learning,[67331],458,"[66, 72]",3523,YW4OR_3,39,14,12,WISDOM - Women in OR,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,13 [building - 116],"['Machine Learning', 'Mathematical Programming']",WC-12
"As urban populations continue to grow, traditional static and fixed-route transportation systems face unprecedented challenges to meet societies' dynamic and diverse mobility needs. Demand adaptive systems [DAS], which combine traditional fixed bus lines and demand-responsive systems, have emerged as a promising paradigm in public transportation to address these challenges. DAS provides traditional transit-line service for a set of compulsory stops. These compulsory stops are bound to a schedule with fixed time windows during which the vehicle serving the line has to leave each compulsory stop. Passengers may issue requests at optional stops, inducing detours in the vehicle routes. 
Focusing on the operational phase, where we adjust vehicle routes and schedules in response to user requests, we formulate and solve a variant of the dynamic single-vehicle routing problem with time windows and profit. We propose an algorithmic framework based on a set of dynamic user requests containing both optional and compulsory stops. We present a machine learning enriched integer programming framework that enables a DAS operator to select a subset of requests to serve while maximizing its expected profit. Here, we determine a route serving all selected requests, respecting compulsory stops' time windows. With this algorithmic framework, we compare traditional fixed-route transportation systems with DAS in Munich and present managerial insights from the operator's and user's perspectives.
",Online routing for Demand Adaptive Systems,"[52258, 75243]",822,"[143, 145, 72]",3524,Demand-responsive public transport 3,85,10,54,Public Transport Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S01 [building - 101],"['Transportation', 'Vehicle Routing', 'Mathematical Programming']",TD-54
"Sequential-decision making is an active area of research in Operations Research [OR], where real-time decision-making, machine learning and data-driven approaches are stepping forward.
The Dynamic and Stochastic Inventory-Routing Problem [DSIRP] is one of the most fundamental problems companies seek to optimize, given its meaningfulness at strategic and operational levels. 
The challenge at hand involves the integration of inventory management and vehicle routing problems while at the same time effectively handling the dynamic and stochastic nature of customer demands unveiled over time.
A promising research path lies in combining Reinforcement Learning and OR to achieve novel inventory and routing policies with significant cost savings. Accordingly, Policy Function Approximation [PFA] emerges as an encouraging strategy.
This research intends to give new insights into solving the single-item, single-vehicle DSIRP with a one-to-many endpoint structure, where decisions must be released periodically. Therefore, we propose novel delivery policies based on PFA, exploring two evolutionary algorithms, i.e. Genetic Programming and Genetic Algorithms.
The proposed methodology enabled us to derive novel rules that present competitive results compared to several literature benchmarks, optimizing the balance between holding and stockout costs, reducing waste and decreasing travel distances.",Evolutionary Approaches to Solve the Dynamic and Stochastic Inventory-Routing Problem,"[72149, 30652, 36154]",637,"[61, 145, 108]",3525,Retail Distribution I,30,12,50,Retail Operations,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,M2 [building - 101],"['Inventory', 'Vehicle Routing', 'Programming, Dynamic']",WA-50
"Travel demand management measures providing personalized route advice to receptive road users are potentially powerful in redistributing traffic for the benefit of network performance, typically expressed using indicators on efficiency, safety, emissions and equity. With the well-known trade-off between individual and societal objectives, system-beneficial route advice may involve a detour compared to the habitual choice. Sustained effects however can only be achieved if travelers [incidentally] comply with such advice. Real-world experiments show that travelers can be persuaded or nudged to do so provided that the suggested detour is bounded and acceptable. Determining the system optimal advice while meeting user constraints is a difficult task since travel times depend on the compliance and the potential re-routing behavior of all travelers, which are subject to uncertainty when the advice is given. In this research, we formulate an optimization problem with equilibrium constraints determining the best possible advice in terms of network performance while explicitly accounting for  compliance rates, different perceptions of road users and variations in travel times. We use techniques from variational analysis to develop a numerical method to solve the problem. Results reveal that it is key to account for uncertainties in travel times to prevent negative experiences with social route advice and to assure that intended effects are achieved.",Traffic rerouting under uncertainty,"[62655, 62298]",445,"[143, 79, 50]",3526,Supply Chain Network Optimization,6,15,55,Transportation,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S02 [building - 101],"['Transportation', 'Network Design', 'Game Theory']",WD-55
"The relationship between urban amenities and property values has been widely studied, yet the specific contribution of retail accessibility to housing prices in European urban areas remains underexplored. This paper makes a significant contribution to this field by introducing a new classification of retail categories within the Hedonic Pricing Method and Geographically Weighted Regression [GWR] frameworks. It categorizes retail into five types - shopping, convenience, supermarkets, bars & restaurants, and traditional markets, to capture the influence of the retail landscape on housing values. A key innovation is the use of a gravity-based accessibility index to quantify the proximity of various retail types to residential properties, using a decay parameter that optimizes the explanatory power of retail accessibility. The findings suggest that shopping, bars & restaurants have a positive effect, while convenience retail and traditional markets have a negative impact. The study did not find a significant relationship between housing values and accessibility to supermarkets. GWR was used for the first time in this research domain, providing new insights into the spatial dynamics at play. The study focuses on Turin, Italy, and provides valuable information for urban planners and policymakers regarding the differential impacts of retail accessibility across urban areas. It highlights the importance of strategic support for the retail sector to enhance the value of investments.","Understanding the Value of Retail Accessibility in Private Housing Markets - A Study from Turin, Italy","[77487, 63151, 7119, 57936]",912,"[33, 64, 37]",3528," Enhancement of circularity, inclusivity, and smartness in cities II",79,5,18,Sustainable Cities,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,42 [building - 116],"['Economic Modeling', 'Location', 'Energy Policy and Planning']",MD-18
"Protecting coastal roadways and other transportation infrastructure from the impacts of climate change is crucial to the long-term performance of transportation networks and the welfare of the communities they support. Climate change is responsible for sea-level rise, increased storm intensity and frequency, and larger storm surge events. Actions, like building seawalls and elevating streets or rail, are typical engineering methods for mitigating the impacts of these changes and related events. These actions often involve major capital investments and implementation schedules. Thus, it is critical to determine a best protective investment strategy and strategy implementation timing. This talk will describe OR-based methods for creating an implementable tool to support the taking of timely capital investment decisions toward transportation system protection against uncertain coastal flooding events and other risks of SLR and changing climate conditions.",Optimal Transportation Infrastructure Protection Investment Timing for Climate Change,"[26227, 73485]",688,"[26, 94, 139]",3531,Disaster & Emergency Management,80,5,53,Sustainable and Resilient Systems,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,8007 [building - 202],"['Decision Support Systems', 'OR in Environment and Climate change', 'Sustainable Development']",MD-53
"In this proposed talk, we delve into the promising realm of quantum computing, specifically focusing on the utilization of neutral atom-based Quantum Processing Units [QPUs] and analog quantum computing methodologies to tackle combinatorial problems. We will discuss how neutral atom-based QPUs, with their precise control and manipulation capabilities, provide a robust platform for quantum computation. Practical examples and case studies will be presented to demonstrate the efficacy and potential of these quantum computing approaches. Additionally, we will discuss the current state of research, challenges, and future prospects in this rapidly evolving field.",Analog Quantum Computing for Combinatorial Optimization Problems,"[77343, 76189]",382,"[5, 76, 114]",3535,Quantum Computing for Discrete and Combinatorial Optimization,83,4,42,Quantum Computing Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,98 [building - 306],"['Algorithms', 'Modeling Systems and Languages', 'Programming, Quadratic']",MC-42
"With the transition towards renewable energy sources, there is a growing need to capture uncertainty within energy market models, with firms facing stochasticity in terms of costs, demands, and generation capacity. With this heightened variability comes additional exposure to risk, leading firms to exhibit risk-averse decision-making. In order to account for this, risk measures can be appended to models. A number of such measures, particularly conditional value-at-risk, stochastic dominance constraints, and concave utility functions, have gained varying degrees of traction in energy market modelling literature. This study aims to investigate the consequences of these risk measures on the behaviour of price-making players acting competitively and seeking to maximise profits. It comprises a discussion of these risk measures and their relative merits. It further includes an analysis of the impact of incorporating risk measures into stochastic equilibrium models, both from an analytic and numerical perspective. Moreover, it discusses the circumstances under which these complementarity problems can be converted to an equivalent problems in convex optimisation, and the use of Arrow-Debreu securities to conceptually complete the market. This work aims to elucidate and illustrate the behaviour of these risk measures and their appropriate application to energy markets.",Risk Measures in Stochastic Complementarity Models of Energy Markets,"[78697, 41933]",631,"[36, 93, 126]",3536,Game Theoretic Market Equilibrium Modelling,22,8,09,Energy Markets,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,10 [building - 116],"['Electricity Markets', 'OR in Energy', 'Risk Analysis and Management']",TB-09
"Commodity prices are a strategic concern to countries and governments [e.g., energy or food security]. Producers and end-users are sensitive to commodity price swings and volatility, which may imply lower profits and higher energy bills respectively. Commodity prices depend on supply and demand fundamentals but also specific drivers such as weather-linked seasonality and events, regional/country-specific infrastructures [e.g., bottlenecks] and regulatory/compliance constraints.
Commodity price forecasting is important to allow market participants to hedge their risk exposures [i.e., losses due to large price swings]. We apply a deep learning approach to few commodities for forecasting prospects. In the area of big data and artificial intelligence, deep learning allows for detecting key patterns in commodity prices and exploit them for forecasting purposes. Such an approach is data driven and relies on neural network analysis without requiring explicit fundamental factors. Moreover, the dynamic complex behavior of commodity prices is captured. A study of several forecasting windows is also performed. Besides, we also perform an improved deep learning approach by including very few key fundamental factors to check if these ones improve the forecasting degree of the method and/or increase the forecast horizon. Hence, we test for the usefulness of an augmented information space on the forecasting power and accuracy of deep learning in the context of few commodity markets.",A deep learning approach to forecasting commodity prices,[57451],276,"[45, 47, 66]",3537,Optimal Portfolio Strategies,4,13,02,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,Glassalen [building - 101],"['Financial Modelling', 'Forecasting', 'Machine Learning']",WB-02
"By assuming a proportional alteration in the output/input ratio, DEA can estimate the additional resources necessary for police units in anticipation of criminal activities. However, reallocating human resources and capital poses significant challenges within public administrations, often influenced more by political considerations than technical merits.

This study proposes a novel approach that integrates non-parametric frontier estimations with a prioritization reasoning to guide the redistribution of cases, rather than personnel, within law enforcement agencies. We assess the production capacity of police units utilizing Data Envelopment Analysis. A Multicriteria framework is incorporated to facilitate reassigning criminal cases from police stations working at full capacity to units with available resources. This approach provides an efficient and fair allocation of cases, prioritizing factors such as case complexity, severity, and costs across different levels.

As a result, our methodology identifies police units operating at full capacity, allowing the transfer of less complex, low-severity, and low-cost cases to neighboring units with spare production capacity. This surplus capacity enables fully operational units to concentrate on critical cases that otherwise might be overlooked.",Improving the Efficiency of Criminal Investigations with a Multi-needs Approach for Case Reassignments,"[71122, 53634, 78704, 79792]",943,"[24, 12, 35]",3539,DEA applications in Policy Making and Planning II,89,12,48,Data Envelopment Analysis and its Application,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,60 [building - 324],"['Data Envelopment Analysis', 'Capacity Planning', 'Efficiency Analysis']",WA-48
"Disruptions that impact transport operations are an inevitable reality that pose significant challenges. These disturbances, escalating in both frequency and impact, have far-reaching implications on seamless freight transport operations. From unexpected weather conditions to unplanned maintenance of the transport network, each disruption brings with it a ripple effect that can drastically alter the planned movement of freight. This, not only negatively impacts the immediate transport operations but also has profound implications on the broader supply chain landscape, necessitating innovative solutions to ensure resilience and reliability. This paper proposes a simulation-based optimisation framework for adaptive disruption management strategies. It incorporates real-time events dynamically to facilitate selection of modes, routes, and handling points. It leverages an event-based model to capture the dynamics of the system at the event of a disruption. The stochastic optimisation model lying at its core generates disruption-aware reactive solutions to minimizes their potential impact. These solutions are informed by optimal disruption management policies such as rerouting, transshipment, or holding, thus ensuring enhanced operational resilience and efficacy in response to disruptions. This adaptive approach to disruption management facilitates more resilient, robust, efficient, and sustainable supply chain operations in the face of increasing uncertainties and disruptions.",A Simulation-Based Optimization Framework for Adaptive Disruption Management Strategies in Synchromodal Transport,"[75770, 78701, 73411, 78702, 78703]",167,"[143, 123, 30]",3540,Sustainable freight transportation,52,10,62,OR in Port Operations,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S12 [building - 101],"['Transportation', 'Reliability', 'Disaster and Crisis Management']",TD-62
"We consider a multi-stage generalization of the interval-based stochastic dominance [ISD] principles introduced by Liu et al. [2021]. The ISD criterion was motivated specifically in a financial context to
allow for contiguous integer SD orders on different portions of a portfolio return distribution against a benchmark distribution. A continuous spanning of SD conditions between first, second and third order stochastic dominance principles was introduced relying on a reference point. Here by extending the partial order to random data processes, we apply ISD conditions to a multi-period portfolio selection problem and verify the modeling and computational implications of such generalization. Several theoretical and
methodological issues arise in this case that motivate this contribution. The problem is formulated in scenario form as a multistage stochastic recourse program and we study two possible generalization of
ISD principles in which we either enforce ISD constraints on each stage, independently from the scenario tree process evolution, or we do so conditionally along the scenario tree. We present a comprehensive set
of computational results to show that, depending on the benchmark investment policy and the adopted ISD formulation, stochastic dominance conditions of first or second order can be enforced dynamically
over a range of possible values of the reference point and their solution carries a specific rationale. ",Portfolio optimization based on multiperiod continuous stochastic dominance principles,[78706],126,"[117, 45, 27]",3543,"Dynamic portfolio selection - stochastic optimization, filtering, and learning techniques",74,4,57,Modern Decision Making in Finance and Insurance,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S06 [building - 101],"['Programming, Stochastic', 'Financial Modelling', 'Decision Theory']",MC-57
"This study introduces a sophisticated model designed to optimize operational efficiency and revenue management for airlines operating within monopolized markets. Unlike previous models that separately address scheduling, fleet assignment, and pricing strategies, our approach integrates these components into a unified framework. The model leverages advanced algorithms to simultaneously determine optimal flight schedules, fleet allocation, and pricing strategies, taking into account demand elasticity, competitive dynamics, and operational constraints. We incorporate real-time data analytics to dynamically adjust the model parameters, ensuring the airline's ability to respond to market fluctuations and passenger preferences effectively.","Optimizing Monopoly Airline Markets - An Integrated Approach to Scheduling, Fleet Management, and Dynamic Pricing",[25980],698,"[124, 129, 4]",3544,Pricing Strategies,11,9,59,Pricing and Revenue Management,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S08 [building - 101],"['Revenue Management and Pricing', 'Scheduling', 'Airline Applications']",TC-59
"We derive four monetary policy indices from FOMC [Federal Open Market Committee] minutes to analyze exchange rate changes in several foreign currencies. These monetary policy indices are found to have significant impacts on foreign currency changes in emerging markets while exchange rate changes in some developed or industrial countries could depend on monetary policy indices from meeting records of their own central banks or central banks of their competitors. Additionally, a monetary policy uncertainty indicator based on entropy is introduced to further improve the prediction of exchange rate changes in certain currencies. We find that the uncertainty of meeting minutes could reduce the market belief in monetary policy stance, and then reduce the impact of monetary policy sentiments on exchange rate variations.",Prediction of exchange rate changes with monetary policy indexation of FOMC meeting minutes,"[71750, 78711]",518,"[7, 44, 47]",3545,"Advancements of OR-analytics in statistics, machine learning and data science 9",16,13,28,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,065 [building - 208],"['Analytics and Data Science', 'Finance and Banking', 'Forecasting']",WB-28
"This paper proposes solutions for pricing American options on stocks in markets with daily price limits. We first extend the intraday density function of Guo and Chang [2020] to a multi-day one. Next, we adopt the fast Fourier transform [FFT] to derive accurate and efficient formulae for American options in the framework of Kim [1990] and Chang et al. [2016] and, further, employ the three-point Richardson extrapolation to accelerate the computation. Finally, the accuracy of our proposed methods is verified by simulations. We also note that more restrictive daily price limits could force put options to be exercised earlier.",Efficient Approximations for American Options in Markets with Daily Price Limits,"[71750, 78712]",798,"[45, 83, 135]",3546,Financial Modelling,50,14,39,Stochastic Modelling,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,35 [building - 306],"['Financial Modelling', 'Optimization in Financial Mathematics', 'Stochastic Models']",WC-39
"Performance and citation impact of scientific journals are measured by traditional metrics such as impact factor, article influence score, journal citation indicator, and others. While the impact factor is based on the total number of citations and does not reflect the quality of journals cited, the article influence score considers the past importance of the citing journals. This paper aims the analyze the possibility of measuring the performance of journals by data envelopment analysis [DEA] models. We applied traditional radial and SBM DEA models with weight restrictions where the outputs of the models are the citation counts from Q1 to Q4 categories and other journals according to the impact factor. This basic model is extended by considering the impact factor of the journals from the previous year as one of the inputs of the model. The results of the study are illustrated in the set of 80 journals from the Web of Science category Operational Research and Management Science [ORMS]. The dataset for the study was obtained from the Journal Citation Reports in the period from 2017 until 2022. The relative efficiency scores and the ranking of the journals obtained by the models are presented and compared with traditional metrics.",Analysis of citation impact of ORMS journals by DEA models,[1591],939,"[35, 24]",3548,DEA applications in Education and Health II,89,7,48,Data Envelopment Analysis and its Application,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,60 [building - 324],"['Efficiency Analysis', 'Data Envelopment Analysis']",TA-48
"Current supply chains operate under uncertain environments causing risks in disruptions and sub-optimal performance due to operational failure or lack of coordination. The inventory control problem, a sequential decision-making problem, is challenged by stochastic and volatile factors such as lead times and seasonal demand patterns, often resulting in sub-optimal performance. Reinforcement learning [RL] remains a promising alternative to enhance decision-making in supply chains. However, as the supply chain grows in size, the complexity of decision-making can grow significantly which may hinder the performance of traditional RL algorithms as they struggle to scale efficiently. The extension of single-agent RL to multi-agent RL allows for scalability as well as a decentralised decision-making framework of individual entities. Our methodology leverages on the inherent graph structure of a supply chain, developing a multi agent RL framework with Graph Neural Networks for a multi-echelon multi-product inventory management system. We show the benefits of a collaborative approach by testing the policies on a series of disruptions. Additionally, the framework moves computational costs from online to offline, ensuring faster decision making than most efficient optimisation methods. As a result, the methodology proposed shows promising scalability with number of agents for a decentralised and online decision-making framework whilst still ensuring collaboration between entities.  
 ",Multi Agent Reinforcement Learning and Graph Neural Networks for Inventory Management ,"[78652, 68126]",652,"[8, 26, 53]",3549,Artificial Intelligence and Machine Learning for Decision Support,45,8,45,Decision Support Systems,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,30 [building - 324],"['Artificial Intelligence', 'Decision Support Systems', 'Graphs and Networks']",TB-45
"The Weighted Circles Packing Problem [WCPP] is an extension of the classical circle packing problem with practical applications across various industries, including logistics, facility layout, automotive, manufacturing, telecommunications, and materials science. In WCPP, each circle is characterized not only by its size [i.e., radius] but also by a weight value, representing cost, value, priority, or other relevant metrics. This introduces complexity to the problem, as algorithms need to simultaneously consider the physical arrangement of circles and optimize their total weighted configuration. This study aims to maximize both the number of unequally sized circles packed in a container and the total layout score of the packed weighted circles while adhering to the constraint of non-overlapping circles or between each circle and the container. For this problem, we develop an Iterated Local Search metaheuristic algorithm where the initial feasible solution generated using polar coordinates and geometric methods is improved iteratively by local search and shaking procedures. Computational tests on a relaxed version of this problem using the benchmark published in López and Beasley [EJOR, 2016] clearly indicate the effectiveness and significant time-efficiency of the develop",A heuristic for packing weighted unequal circles in circular containers,"[77975, 76078, 78725, 79697]",501,"[23, 0]",3551,Cutting and Packing 2 - 2D irregular,81,3,07,Cutting and Packing [ESICUP],"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,1019 [building - 202],['Cutting and Packing'],MB-07
"This paper evaluates the effect of the variation of PV’s effective capacity across the day-hours and the seasons of the year on the optimal capacity mix, generation levels and prices of a wholesale electricity market in which power producers use natural gas [NG] and photovoltaic [PV] technologies. The actual levels of PV’s effective capacity during the day hours resemble the probability distribution function of a normal variable; it is very low in the morning, grows steadily until it reaches its maximal level during the midday hours, and then continuously declines during the afternoon hours. The shape of this distribution varies across the seasons of the year. Applied to Israel’s stylized electricity market in 2030, we find that representation of PV's effective capacity by only two levels [high during midday hours and low during morning and afternoon hours] yields very similar optimal capacity mix to a representation in which PV's effective capacity changes during every hour of the day and over the four seasons of the year. We also show that using some NG during the day hours mitigates daytime price spikes and enhances consumer welfare. Finally, we show how technology improvements to the PV technology reduce the optimal PV capacity and electricity prices during the day hours.",The effect of the variation of PV's effective capacity during the day-hours on the optimal capacity mix in a Cournot wholesale electricity market,"[24492, 24494, 53941]",343,"[12, 36, 33]",3554,Uncertainties in the Energy Transition,22,5,09,Energy Markets,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,10 [building - 116],"['Capacity Planning', 'Electricity Markets', 'Economic Modeling']",MD-09
"In this presentation, we address the vehicle routing problem with time windows and synchronized visits with uncertain service and travel times. Specifically, a subset of the clients require simultaneous service by two vehicles, which is initiated only after the arrival of both vehicles and, consequently, enforces a waiting period for the vehicle that arrived earlier. To tackle this problem, we propose an optimization-simulation framework. Through an iterative process, a stochastic optimization model generates potential solutions which are then assessed through a simulation model. If the solution is found to lead to a bad average and/or worst-case performance, new scenarios suggested by the simulation are added to the stochastic optimization model. This iterative process continues until all performance criteria are satisfied. We validate our approach through experiments on benchmark instances from the literature.",Synchronization in Vehicle Routing - Optimization-Simulation Approach,"[46526, 57605, 49803]",782,"[145, 72, 117]",3555,Vehicle Routing Under Uncertainty 2,5,7,64,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S16 [building - 101],"['Vehicle Routing', 'Mathematical Programming', 'Programming, Stochastic']",TA-64
"Green hydrogen has been regarded as a promising substance for sustainable energy conversion and storage. However, the by-product pure oxygen, from water electrolysis, has not yet been widely utilized. Meanwhile, the municipal wastewater treatment system is facing great challenges to upgrade its original capacity and the advanced tertiary treatment. This work proposes to integrate electrolysed oxygen with activated sludge aeration and effluent post-oxidation. An onsite wind/multi-electrolysers’ water electrolysis/oxygen model is simulated to satisfy the potential oxygen demands from the Benchmark Simulation Model No.2 [BSM2]. The oxygen productivity fluctuates because of the uncertainty from intermittent wind energy; meanwhile, the oxygen demands of aeration and effluent oxidation are also non-linear dynamic variables. The imbalance of day-ahead supply and demands of oxygen is solved by a virtual oxygen storage unit in the model. The optimization goal is to - maximize the real-time oxygen applications of aeration or effluent oxidation; minimize the volume requirements, and usage counts of oxygen storage units. To simplify the simulation system by making the oxygen production and demands deterministic, the short-term forecasting intelligence of linear regression [LR] algorithm is employed. Then the LR is incorporated with reinforcement learning [RL] for scheduling flexible control of oxygen dispatches, so as to make wastewater treatment processes more efficient and accurate.",Demand response control of wind-based electrolyzed oxygen for advanced activate sludge wastewater treatment ,"[78546, 78803, 78806, 78804]",929,"[104, 147, 139]",3557,Sustainable supply chains,18,15,24,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,83 [building - 116],"['Process Systems Engineering', 'Water Management', 'Sustainable Development']",WD-24
"
The vehicle scheduling problem [VSP] is one of the sub-problems of public transport planning. It aims to minimize operational costs while assigning exactly one bus per timetabled trip and respecting the capacity of each depot. Public transport planning is subject to various endogenous and exogenous causes of uncertainty, notably affecting travel time and energy consumption. Despite the uncertainties involved, the VSP and its variants are usually solved deterministically to address trackability issues. However, considering deterministic travel time in the VSP can compromise schedule adherence, whereas considering deterministic energy consumption in the electric VSP [E-VSP] may lead to solutions with sub-optimal true costs [including recourse costs and the cost of ownership of battery electric buses]. 

This presentation proposes a methodological framework aimed at integrating uncertainties, specifically travel time and energy consumption uncertainties, into the VSP. Three distinct stochastic, data-driven mathematical models and branch-and-price algorithms are introduced to address two variations of the problem - the multi-depot VSP [MDVSP] and the E-VSP. The objective is to find bus schedules that offer a good trade-off between operational costs and service reliability, as well as between operational costs and battery degradation in electric buses.
",Data-driven optimization of bus schedules under Uncertainties,"[75786, 73537, 18350, 53667]",205,"[145, 13, 136]",3558,Scheduling and Routing Problems ,64,5,52,Combinatorial Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,8003 [building - 202],"['Vehicle Routing', 'Column Generation', 'Stochastic Optimization']",MD-52
"We consider a manufacturer who sells two vertically differentiated products - a high-quality product and a low-quality product. Using blockchain is assumed to have a multiplicative increase of consumer perception of the product’s quality and a loss of consumer utility due to privacy concerns. It is also associated with a fixed cost implementation and a per unit marginal cost. Given this, we build a stylized model based on customer utility and self-selection to study the impact of using blockchain on the conditions of co-existence of the two types of product on the market, the products’ optimal prices, their respective market shares, the manufacturer’s profit and his differentiation strategy.

Preliminary results show that the two products will continue to coexist while blockchain is being used only if the loss of consumer utility due to privacy concerns is contained within a certain interval [neither too high nor too low]. The results also demonstrate a cannibalization effect of blockchain, while the manufacturer’s overall market coverage remains the same. Moreover, the results establish the sensitivity of the benefit from using blockchain to the level of differentiation between the two products, and the impact of blockchain usage on the product differentiation level.
",Impact of blockchain on product pricing and vertical differentiation,"[70929, 78722]",700,"[124, 67]",3559,Pricing and applications 2,11,12,59,Pricing and Revenue Management,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S08 [building - 101],"['Revenue Management and Pricing', 'Management Information Systems']",WA-59
"This study investigates return and volatility spillovers between different regional private equity markets and investment styles to analyze the dynamics of interconnectedness. This is done using the LPX Group’s listed private equity indices for the period January 2004 to September 2023. The results show that the LPX America index is a net transmitter for the LPX Europe index. The transmission of volatility spillovers from the US to Europe precedes the impact of return spillovers by several months. Furthermore, Europe only becomes a transmitter for the US in a few specific European events. For the investment style indices, the LPX Buyout index consistently acts as a net return spillover transmitter throughout the sample period, while the LPX Mezzanine index consistently exhibits higher net volatility spillovers. The LPX Venture index consistently acts as a net receiver. The catalysts for risk contagion within the LPX indices are explicitly stated to be the differential impact of major economic and financial events. Another reason for the observed spillover effects could be herd behavior as a transmission channel.",Return and volatility spillover among  the global private equity markets,"[62886, 63054, 78723]",142,"[44, 45, 126]",3561,Risk management in finance,9,2,51,Risk management in finance,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,M5 [building - 101],"['Finance and Banking', 'Financial Modelling', 'Risk Analysis and Management']",MA-51
"Counterfactual Analysis is a powerful tool for Explainable Artificial Intelligence. In Supervised Classification, this means associating with each record a so-called counterfactual decision, i.e., an instance close to the record [closeness measured by an appropriate and context-dependent dissimilarity] and whose probability of being classified by a given classifier in the positive [desired] class is high. In other words, the counterfactual analysis is a reference for a given individual - how he/she should minimally change to improve the probability of being classified in the positive class.  
While the literature has focused on the case of finding one counterfactual, we propose in the talk how to address the problem in which a sequence of k decisions, gradually increasing the probabilities of positive, and implying an acceptable cost at each stage. 
A mathematical optimization model is considered and theoretical properties are derived.   ",Multistage counterfactual decisions,"[4607, 67604, 22145]",120,"[66, 7, 72]",3562,Mathematical Optimization for Counterfactual Explanations,15,5,27,Mathematical Optimization for XAI,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,047 [building - 208],"['Machine Learning', 'Analytics and Data Science', 'Mathematical Programming']",MD-27
"The adoption of self-driving robots across various delivery applications presents a transformative shift in urban logistics, offering enhanced efficiency, agility for fast and on-time deliveries, sustainability through zero emissions, and reduced human contact. The onset of the COVID-19 pandemic expedited the execution of robot delivery roll-out strategies, leading to market size growth and attracting new competitors. This study investigates the integration of autonomous delivery/collection robots into last-mile logistics for synchronised collection of medical samples in the healthcare sector, where vans are equipped with self-driving robots. A multi-echelon network topology is adopted by leveraging insights from the UK public health service's [known as NHS] hub and spoke network strategy which prioritises faster and more reliable screening test results, including for cancer. The study addresses the new operational challenges associated with implementing this new self-driving robot technology, focusing on routing and scheduling logistics problems. To tackle these challenges, a mixed-integer linear programming model and an Adaptive Large Neighbourhood Search [ALNS] metaheuristic algorithm are developed. Experimental results demonstrate the effectiveness of both the model and the algorithm in optimising last-mile service in the healthcare sector, shedding light on the potential of self-driving robots to revolutionise healthcare logistics.",The adoption of self-driving robots in last mile healthcare logistics,"[76078, 78725, 78726, 78727]",953,"[14, 65, 56]",3565,Healthcare logistics and routing,3,5,10,OR in Health Services [ORAHS],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,11 [building - 116],"['Combinatorial Optimization', 'Logistics', 'Health Care']",MD-10
"We present a new accelerated stochastic second-order method that is robust to both gradient and Hessian inexactness, which occurs typically in machine learning. We establish theoretical lower bounds and prove that our algorithm achieves optimal convergence in both gradient and Hessian inexactness in this key setting.  We further introduce a tensor generalization for stochastic higher-order derivatives. When the oracles are non-stochastic, the proposed tensor algorithm matches the global convergence of Nesterov Accelerated Tensor method. Both algorithms allow for approximate solutions of their auxiliary subproblems with verifiable conditions on the accuracy of the solution.","Advancing the lower bounds - An accelerated, stochastic, second-order method with optimal adaptation to inexactness","[78705, 76160, 49308, 67234, 76856, 35353, 29152]",363,"[21, 0]",3568,Beyond First-Order Optimization Methods,84,13,32,Advances in large scale nonlinear optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,41 [building - 303A],['Convex Optimization'],WB-32
"Distributed learning has emerged as a leading paradigm for training large machine learning models. However, in real-world scenarios, participants may be unreliable or malicious, posing a significant challenge to the integrity and accuracy of the trained models. Byzantine fault tolerance mechanisms have been proposed to address these issues, but they often assume full participation from all clients, which is not always practical due to the unavailability of some clients or communication constraints. In our work, we propose the first distributed method with client sampling and provable tolerance to Byzantine workers. The key idea behind the developed method is the use of gradient clipping to control stochastic gradient differences in recursive variance reduction. This allows us to bound the potential harm caused by Byzantine workers, even during iterations when all sampled clients are Byzantine. Furthermore, we incorporate communication compression into the method to enhance communication efficiency. Under quite general assumptions, we prove convergence rates for the proposed method that match the existing state-of-the-art [SOTA] theoretical results.",Byzantine Robustness and Partial Participation Can Be Achieved Simultaneously - Just Clip Gradient Differences,[76161],370,"[5, 63]",3569,Distributed and Federated Optimization,84,15,32,Advances in large scale nonlinear optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,41 [building - 303A],"['Algorithms', 'Large Scale Optimization']",WD-32
"Landscape planning is not only about material production, but also about the provision of other services. The conservation and enhancement of landscapes to ensure ecosystem services [ES] for the benefit of society is an important issue in the field of OR. The aim of this study was to gain experience on how a decision support system based on MCDA can be used in practice for long-term landscape management. We present a planning tool that integrates DEA, Fuzzy AHP and DP. As a first step, it was crucial to know the current state of the landscapes. To this end, we used DEA to assess the operational efficiency of landscapes depending on a specific region [landscape unit], considering land use, land cover and ecosystem condition. Knowing that the provision of EFP is site-specific and dynamic due to changing natural conditions and influences as well as various other socio-economic and political factors, decision-makers define alternative management programs/scenarios for each unit under consideration. In the second step, the management criteria were selected and evaluated by the stakeholders. The Fuzzy AHP was chosen as the preferred weighting method for comparing the programs for each unit. Once all the alternatives, criteria and weightings are defined for each alternative per unit, the next step is to create a DP network for each unit to forecast the evolution of the unit to the target state, which depends on the ES supply/demand ratio in the unit.","Resilience of the long-term balance of ecosystem services in the landscape through integration of DEA, fuzzy AHP and DP - a case study in Slovenia","[11522, 50415, 47744]",621,"[78, 31, 108]",3570,OR in Forestry II,20,8,12,OR in Agriculture and Forestry ,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,13 [building - 116],"['Natural Resources', 'Dynamical Systems', 'Programming, Dynamic']",TB-12
"This article challenges the conventional notion that minimizing shortest paths guarantees the fastest passengers arrivals at destinations. Instead we present a fresh perspective in the school bus routing problem. This article introduces the cumulative school bus routing problem, which the objective of the problem is to select a drop-off point for each student among potential locations within a certain walking distance and to generate routes such that the sum of arrival times of all students from their school to their homes is minimized. The article describes six polynomial-size mixed integer linear programming formulations based on original and auxiliary graphs, and the formulations are numerically compared on real instances. The article reports the results of computational experiments performed to evaluate the performance of the proposed models.",The Cumulative School Bus Routing Problem - Polynomial-Size Formulations,"[78655, 6803, 2435]",779,"[145, 72, 53]",3571,MILPs for Vehicle Routing 1,5,10,58,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S07 [building - 101],"['Vehicle Routing', 'Mathematical Programming', 'Graphs and Networks']",TD-58
"Multiobjective mixed-integer nonlinear programming is a challenging type of problem to solve even when the continuous relaxation is convex. This work presents two new methods which attempts to solve convex biobjective MINLPs efficiently. This is done by considering the so-called epsilon-constraint method for a biobjective problem. Since the set of nondominated points is in general not convex due to integrality constraints, the classic weighted sum method is not suitable. The epsilon-constraint method solves a sequence of similar convex MINLPs to approximate the set of nondominated points. Each subproblem can be solved using outer approximation, where a polyhedral approximation is obtained. The main contribution of the work is the two methods presented of how to reuse the polyhedral approximation from a solved subproblem when solving the next one. The methods are derived from two observations. First, the subproblems are by construction increasingly restrictive, which means that a valid cut will be valid for all the following subproblems. Second, the problems are only slightly changed, which means that the integer combinations needed to be visited should be similar between subproblems. Preliminary results show that there is a noticeable improvement in both solution time and the number of iterations needed to solve each subproblem.",Warm Starting of Outer Approximation for Biobjective MINLPs,"[78681, 70639]",478,"[111, 112, 113]",3573,Algorithms for Mixed-Integer Nonlinear Programming and Nonconvex Optimization,86,12,04,MINLP,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1001 [building - 202],"['Programming, Mixed-Integer', 'Programming, Multi-Objective', 'Programming, Nonlinear']",WA-04
"This talks introduce an optimisation-based method for generating robust neural networks against adversarial attacks for image classifications. Instead of using large training data and heaving computing expense in classic training process, the proposed method only requires  a small size of adversarial dataset in each outer iteration to improve the robustness a model trained with default settings. This algorithm includes two levels of updating. In the inner iteration, we add cuts to improve the linear approximation for the nonlinear constraints regarding the adversary behaviour and level of loss function.  Then, to balance the performance of the model on both clean dataset and the adversarial dataset,  we select the Pareto set by line search and update the model with a weighted objective function.  In the outer iteration,  we generate new adversarial data and repeat the same process until stopping criteria are satisfied. This talk concludes with numerical restyle with CNN and ResNet models. 
",Optimization-based algorithm for robustness enhancement of neural networks ,"[69402, 70639]",176,"[66, 113, 112]",3574,Topics in Mixed Integer Programming and Nonconvex Optimization,86,7,04,MINLP,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,1001 [building - 202],"['Machine Learning', 'Programming, Nonlinear', 'Programming, Multi-Objective']",TA-04
"Many mathematical models and algorithms to address logistics around mass casualty incidents [MCI] have been proposed in the literature. However, hardly any data sets exist that can be used to analyse, validate and evaluate approaches and solutions. As MCI are comparably rare, real-world data is scarce and often very sensitive, making it difficult for researchers to get access and build test datasets. In this talk, we therefore present the first steps towards a repository to share datasets to facilitate validation and comparison. We first provide an overview of potential stakeholders such as working groups, review existing disaster-related databases and existing repositories for other fields followed by a preliminary taxonomy to identify and structure the relevant entities and their characteristics within models for disaster management.",Towards a repository for datasets of mass casualty incidents,"[33694, 73030]",954,"[30, 26]",3575,EMS and crisis logistics,3,3,10,OR in Health Services [ORAHS],"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,11 [building - 116],"['Disaster and Crisis Management', 'Decision Support Systems']",MB-10
"Framework agreements [FAs] are  procurement mechanisms used in private and public organizations by which a central procurement agency selects an assortment of products, typically through auctions, and then affiliated organizations can purchase from the selected assortment as needs arise. In Chile's central procurement agency [ChileCompra], FAs accounted for 23% of the procurement expenditures during 2018-19. However, descriptive analysis of purchase transaction data suggests that some FAs exhibited low levels of competition in  the auctions used to select suppliers, 
which could potentially result in larger government expenditures. We collaborated with ChileCompra to redesign FAs to enhance competition introducing two important changes - [i] standardize the product catalogue using natural language processing algorithms; [ii] use this product  standardization to induce more competition in the auctions to select of suppliers. These changes were implemented through an experimental design in the new Food FA to measure its impact, showing that inducing more intense competition in the auction stage reduced transaction prices by 8%. This pilot study ultimately led ChileCompra to implement a similar design in all of its FAs, and many of the improvements in the design of the FAs were included in the new regulation on government purchases. If we were to extrapolate the savings from our pilot re-design to all of these FAs, the total savings would amount to around US\$64 million in 2022.",Saving Millions in Government Procurement Through Data Science and Market Design ,"[73777, 73859, 24589, 78736]",639,"[9, 101, 7]",3578,Market Design 1 - Auctions,87,10,43,Market Design,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,99 [building - 306],"['Auctions / Competitive Bidding', 'OR/MS and the Public Sector', 'Analytics and Data Science']",TD-43
"We consider a class of nonsmooth fractional optimization problems over the fixed-point constraints, where the numerator is a convex function and the denominator is a concave function. To solve this problem, we propose iterative algorithms whose idea is to perform subgradient steps with respect to the convex numerator and the concave denominator separately.  Under certain assumptions, we obtain the convergence properties of the proposed methods. Furthermore, to tackle large-scale optimization problems, we propose an incremental subgradient algorithm to deal with the case where the objective function is a sum-of-ratios fractional optimization problem. ",Subgradient Methods for Nonsmooth Fractional Optimization Problems over the Fixed-Point Contraints,"[78729, 78741]",357,"[81, 63, 5]",3580,Subgradient-based methods,70,9,41,Nonsmooth Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,97 [building - 306],"['Non-smooth Optimization', 'Large Scale Optimization', 'Algorithms']",TC-41
"A sudden surge in emergency surgeries strains the critical and limited resources of the operating room [OR]. Relying solely on human judgment for emergency surgery decisions may worsen congestion, leading to potential delays or cancellations of both emergency and elective surgeries. In the worst case, if the initially allocated hospital lacks capacity, the patient may face a second transfer, causing additional treatment delays. To address these risks, we develop a stochastic mixed-integer programming model for emergency patient allocation with a hybrid emergency surgery strategy. We then design a simulation-optimization approach to assess the expected maximum capacity for accommodating emergency surgeries. Our approach expedites decision-making in urgent cases, allowing decision-makers to make more rational choices swiftly. Specifically, we propose an improved sample average approximation method with a stopping rule for single-hospital cases, integrating it into the optimal computing budget allocation algorithm for multi-hospital settings. To demonstrate the efficiency of the proposed algorithms, we conduct experiments with real data from a 3A hospital in China. Our results illustrate the robustness of our methods through a sensitivity analysis of internal factors [overtime length], external factors [release time of emergency patients], and expected elective surgery durations. Additionally, we highlight the benefits of collaborative decision-making in multi-hospital setting.",Emergency surgery allocation - Simulation-optimization approach incorporating scheduled elective surgeries under a hybrid emergency strategy,"[78735, 41246]",973,"[56, 73, 131]",3581,Capacity and treatment planning in healthcare,3,14,10,OR in Health Services [ORAHS],"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,11 [building - 116],"['Health Care', 'Medical Applications', 'Simulation']",WC-10
"Energy poverty, a multidimensional socio-economic challenge, significantly affects the welfare of many people across Europe. This paper aims to alleviate energy poverty by exploring sustainable energy practices and policy interventions, using household survey data from Portugal and Denmark. A Multidimensional energy poverty index [MEPI] is developed to assess energy poverty through different dimensions such as heating and cooling comfort, financial strain, access to energy-efficient appliances, and overall health and well-being. In a next step, for selecting features, machine learning techniques are employed including recursive feature elimination and random forest analysis. These methods help to reduce the number of irrelevant and mutually correlated predictors. Subsequently, a logistic regression model is used to predict energy-poor households based on selected socio-economic, and policy-related factors. The logistic regression results indicate that sustainable energy-saving behaviors and supportive government policies can mitigate energy poverty. Furthermore, for analyzing the impact of determined features the Shapley additive explanations [SHAP] method is being utilized. Finally, the main findings are evaluated further via scenario simulation analysis. The result shows that fully adopting waste-compositing and energy-efficient microwave ovens can decrease the proportion of energy-poor households by 93% and 79%, respectively. ",Integrating machine learning in measuring multidimensional energy poverty - new insights from a survey analysis in Europe,"[78664, 21108]",672,"[66, 33, 93]",3584,"Advancements of OR-analytics in statistics, machine learning and data science 18",16,14,06,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,1013 [building - 202],"['Machine Learning', 'Economic Modeling', 'OR in Energy']",WC-06
"Bias detection and mitigation have received more and more attention over the last few years in Natural Language Processing [NLP]. This is mainly due to the societal implications. While often research into these debiasing techniques focuses on English and mostly monolingual models, our research aims to provide insights into the cross-lingual transferability of these debiasing techniques. More specifically, we look into what happens to the other languages when debaising a multilingual model in one language. We examine several languages such as English, French, German, and Dutch. The CrowS-Pairs dataset includes stereotypes associated with historically disadvantaged groups in the United States encompassing multiple types of bias, among which gender, race, and religion. From this dataset also a French version exists, addressing stereotypes against specific demographic groups in France. For the other languages, we used translations of the dataset that were checked by native speakers. Next, we analyzed the effects of cross-lingual debaising on multilingual BERT [mBERT]. Using these translations of the CrowS-Pairs dataset, we find that cross-lingual debiasing is possible. Moreover, we identify SentenceDebias as the best-performing debiasing technique from our set. Finally, we find that the debiasing techniques that add an additional pretraining step are best employed on the lowest resource languages.",Cross-Lingual Debiasing of Large Language Models,"[71710, 67964, 78745, 68588, 10234]",415,"[8, 7, 41]",3585,Learning Analytics and other Text Analytics tasks,17,13,31,Analytics,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,54 [building - 208],"['Artificial Intelligence', 'Analytics and Data Science', 'Ethics']",WB-31
"Owing to the lack of land-based infrastructure connecting islands, ferry-based public transport is often the only feasible means of reliable mass transportation. Ferry transport operating in populated archipelagos consists of a combination of fixed and flexible services, highly variable stopping patterns within lines, a lack of regular operating cycles and a small heterogeneous vehicle fleet.While water bodies separating islands create barriers, the unique geographic layout with many islands in close proximity also offers large routing freedom.Taking advantage of novel pathways may require cost-inducing efforts such as depth surveys, dredging or marking shallows. We present a joint method for ferry routing and scheduling based on a multi-trip vehicle routing problem approach. The proposed model incorporates many of the observed characteristics of real systems. Traffic requirements are included as minimum stop visits over the scheduling period and as window-constraints.Evaluation of novel pathways is supported in the form of arc-bundles with fixed opening costs. The computation time is shown to be sensitive to the number of window constraints. Full system solutions with an optimality gap between 10 and 15 % are obtained within a time-scale reasonable for strategic planning. Our results show that pathway evaluation and operational assessment need to consider the traffic requirements and fleet characteristics at hand.",Ferry network design and scheduling - an exact optimization model for analysis of pathway alternatives,"[78744, 64072]",817,"[145, 70, 109]",3586,Network Design for Public Transport,85,10,51,Public Transport Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,M5 [building - 101],"['Vehicle Routing', 'Maritime applications', 'Programming, Integer']",TD-51
"We intend to provide a different perspective on the class of optimisation problems that are related to transportation systems. As a transportation system, we understand a situation that involves locations, means of transport, subject of transportation and the question at hand is finding an optimal way of distribution over this system, and we specifically focus on such systems that can be represented and solved using mixed-integer linear programming [MILP]. Instead of following the usual solution approach, we focus on optimising the very system structure rather than finding an optimal solution over a system with fixed parameters.
For demonstration purposes, we assume - for its simplicity - the traditional transportation problem of Hitchcock and Koopmans with possibly multiple objectives. In addition, it is considered that the capacity levels of suppliers [or customers] are bounded by given maintenance costs that increase with the increasing capacity. Given the constrained budget, we propose an optimal [in the cost-minimising sense] transportation system design that respects all objectives. We realise that these costs may be subject to uncertainty and therefore propose the robust-optimal transportation system design. We combine the De Novo optimisation approach with Gamma-robust optimisation to demonstrate our methodology which can be extended to any other transportation problem representable by MILP.
",Towards efficient transportation systems – designing a transportation system under uncertainty,"[50731, 10202]",828,"[143, 127, 77]",3590,Transportation and Logistics under Uncertainty,49,14,34,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,43 [building - 303A],"['Transportation', 'Robust Optimization', 'Multi-Objective Decision Making']",WC-34
"This presentation covers a newly introduced variant of the Vehicle Routing Problem - the Hierarchical Multi-Switch Multi-Echelon Vehicle Routing Problem. It is a real-world problem originating from the policies of a Nordic distribution company and includes; a single depot, a non-predetermined hierarchy of intermediate facilities, and two different fleets consisting of homogeneous depot and homogeneous local vehicles pulling swap-bodies. The central depot dispatches vehicles with attached swap bodies, which can either directly serve customers [if only one swap body is attached] or visit switch points to transfer loaded swap bodies to local vehicles. The local vehicles then serve customers with the transferred swap bodies, while the original vehicle continues serving customers with the remaining loaded swap body. The presentation will cover the problem and model formulations, properties and difficulties in solving it using a commercial solver, solution approaches for realistic-sized instances, and decomposition-based methods to generate lower bounds and evaluate the performance of heuristic algorithms.",Exact and Heuristic Methods for the Hierarchical Multi-Switch Multi-Echelon Vehicle Routing Problem,[73138],754,"[145, 74, 13]",3591,Heuristics for Vehicle Routing 1,5,14,64,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S16 [building - 101],"['Vehicle Routing', 'Metaheuristics', 'Column Generation']",WC-64
"The objective of this paper is to study an incumbent-entrant model
under uncertainty. The entrant knows the realization of the random vari-
able[s] before it makes its decision on entry and eventual capacity choice.
So all the uncertainty is on the incumbent's side. The sources of uncertainty consider the characteristics of the entrant's product and the entry
cost the entrant needs to incur before becoming active. We know from the
literature that the incumbent-entrant setup could result in three di¤erent
outcomes - blockaded entry, i.e., the incumbent behave like a monopolist
and the entrant does not enter, deterred entry, i.e., the incumbent overinvests to make the market unprofitable for the entrant, and accommodated
entry. The main result from our work is that under uncertainty there can
be four outcomes - apart from blockaded entry and accommodated entry,
it can be either 100% entry deterrence or entry deterrence with a certain
probability.",Optimal entry deterrence under uncertainty,"[61089, 10538, 19666]",292,"[33, 19, 50]",3594,Dynamics of the Firm I,90,3,33,Optimal Control Theory and Applications,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,42 [building - 303A],"['Economic Modeling', 'Continuous Optimization', 'Game Theory']",MB-33
"This study introduces a dynamic investment model for pension fund managers, aiming to optimize mean-risk performance over a 21-year horizon. The model reacts to past asset returns, with annual rebalancing, and seeks to outperform benchmarks using second-order stochastic dominance. We explore the possibility of identifying a universally preferred dynamic strategy for all risk-averse investors and examine its characteristics compared to benchmarks. The model maximizes mean-0.5*CVaR with stochastic dominance constraints, utilizing historical extraction for future returns. Benchmarks follow a 1/N portfolio strategy, evolving asset allocations over time according to pension fund regulations. Results for the benchmark show an average terminal wealth of 2.7 EUR after 21 years, with a 5% probability of falling below 1 EUR. The optimal strategy, without ESG restrictions, outperforms the benchmark, exhibiting higher cash and bond investments, resulting in an expected terminal wealth of 6 EUR. When considering only ESG attractive assets, the optimal strategy is less profitable [expected terminal wealth of 3.6 EUR], yet it outperforms the benchmark in second-order stochastic dominance. The optimal strategy demonstrates sensitivity to asset bounds and ESG considerations, emphasizing the importance of strategic asset selection over risk aversion parameters.
The project has received funding from the Research Council of Lithuania [LMTLT], agreement No S-MIP-21-32.",Dynamic Investment Model for Pension Funds - Maximizing Mean-Risk Performance with SD constraints,"[24791, 12024, 46721, 45493]",413,"[77, 45, 126]",3595,Portfolio risk management,9,5,51,Risk management in finance,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,M5 [building - 101],"['Multi-Objective Decision Making', 'Financial Modelling', 'Risk Analysis and Management']",MD-51
"In recent years, there has been a rising demand for transparent and explainable machine learning [ML] models. A large stream of works focuses on algorithmic methods to derive so called counterfactual explanations [CE]. 
Although significant progress has been made in generating CEs for ML models, this topic has received minimal attention in the Operations Research [OR] community. However, algorithmic decisions in OR are made by complex 
algorithms which cannot be considered to be explainable or transparent. In this work we argue that there exist many OR applications where counterfactual explanations are needed and useful. We translate the concept of CEs into 
the world of linear optimization problems and define three different classes of CEs - strong, weak and relative counterfactual explanations. For all three types we derive problem formulations and analyze the structure of it. We show 
that the weak and strong CE formulations have some undesirable properties while relative CEs can be derived by solving a convex optimization problem. We test all concepts on a real-world diet problem and we show that relative CEs can be calculated efficiently on NETLIB instances.",Counterfactual Explanations for Linear Optimization Problems,"[50496, 406, 72596]",120,"[8, 72, 66]",3596,Mathematical Optimization for Counterfactual Explanations,15,5,27,Mathematical Optimization for XAI,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,047 [building - 208],"['Artificial Intelligence', 'Mathematical Programming', 'Machine Learning']",MD-27
"This study addresses the improvement of order fulfilment in industrial settings with centralized order consolidation and distributed warehouses. To ensure homogeneity and customer satisfaction when transporting products to the central facility, logistics operators need to make decisions about warehouse management, transportation modes, stock unit configurations, and product features. Many companies rely on manual information flow to achieve these objectives, resulting in costly and time-consuming operations. Here we propose a Decision Support System [DSS] that integrates information flow digitalization and material flow optimization. Optimization is accomplished through an integer linear programming model that considers all the factors listed above. Digitalization relies on a scalable software infrastructure that periodically gathers commercial information, checks product availability, runs the optimization model, and integrates decisions into the enterprise resource planning system. The DSS was developed in collaboration with an international ceramic tile company. Compared to manually implemented operations, the results reveal a significant reduction in transportation costs, ranging from 24% to 40%, along with estimated time savings of about 120 hours per month. Therefore, the implementation of the DSS offers substantial cost savings by optimizing stock transfers and maximizing valuable working hours.",Optimizing Industrial Order Fulfilment - A Decision Support System for Efficient Intra-logistics,"[78641, 73468, 7965, 67821]",68,"[26, 59, 65]",3598,Decision Support for Sustainable Operations,45,10,45,Decision Support Systems,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,30 [building - 324],"['Decision Support Systems', 'Industrial Optimization', 'Logistics']",TD-45
"In this research, we focus on the stochastic Advance Scheduling Problem with Downstream Capacity Constraints [ASPDCC] in the case where the decision maker has access to side data. We begin formulating the stochastic ASPDCC as a two-stage stochastic program using a sample average approximation [SAA] approach. Unlike the common method to characterize uncertainty, which assumes a lognormal distribution as the true data-generating process for each surgical specialty, we propose a data-driven framework that integrates a machine learning algorithm [decision trees] to estimate the conditional probability distribution for the surgery durations and the length of stay [LoS], in the presence of side data. Finally, to speed up the solution of the SAA model, we use a benders decomposition incorporated into a branch-and-cut framework. To justify the value of using side data, we perform extensive experimentation using real data from a reference hospital in Bogota, Colombia. Results show that our algorithm generates schedules that outperform the traditional method of fitting a lognormal distribution for each surgery specialty.",Stochastic scheduling in the presence of side data - case - surgery planning,"[78751, 41246]",632,"[7, 56, 14]",3599,"Advancements of OR-analytics in statistics, machine learning and data science 16",16,12,06,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1013 [building - 202],"['Analytics and Data Science', 'Health Care', 'Combinatorial Optimization']",WA-06
"Accurate analysis of production lines is of critical importance at strategic and tactical level for the sustainable economy of companies. Production rate, which is defined as the amount of production per unit time. The processing times, failure times and repair times [reliability parameters] of the machines and the number of buffers used between machines to reduce idle times greatly affect the production rate, which is one of the main performance indicators. Unreliable lines, where machines are subject to random failures, and balanced production lines, where the processing time of all machines is equal or expressed by independent identically distributed random variables. Exact analytical methods, approximate analytical methods and simulation are widely used for production rate calculation. Estimation of the production rates with artificial intelligence-based methods are very limited in terms of production line configuration and belong to recent years. In this study, an artificial neural network model is proposed to accurately and quickly estimate the production rate of small and medium-sized serial production lines with all machines in the line balanced, unreliable, finite buffer size. Parameter tuning is performed for the proposed artificial neural network model and additional tests are conducted to increase the prediction efficiency of the model. The results obtained encourage the proposed model to be applied to large size lines.  ",An artificial neural networks model to production rate estimation of short and medium sized serial production lines ,[32445],672,"[66, 69, 135]",3601,"Advancements of OR-analytics in statistics, machine learning and data science 18",16,14,06,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,1013 [building - 202],"['Machine Learning', 'Manufacturing', 'Stochastic Models']",WC-06
"In energy distribution networks, adequate investment planning is crucial not only for meeting the demands of established load points within the network but also for maintaining specific reliability standards. These two objectives, which involve meeting the demand economically and ensuring adequate service reliability levels, are highly dependent on the investments made in the electrical network. Therefore, this paper presents a mathematical model that incorporates both criteria to determine investment decisions in distribution networks. The developed model incorporates various types of investments, commonly found in long-term projects, such as investments in substations and circuits, which are evaluated over multiple investment periods. Moreover, a set of constraints is utilized to account for potential outage scenarios in the networks and assess the robustness of the investments against them; this includes considering circuit breakers as essential network structures for conducting network reconfigurations that minimize the number of users affected by outages. As a result, cost benefits were observed when incorporating decisions regarding the investment in circuit breakers, circuits, substations, and necessary upgrades due to the increase in network demand. The final study was conducted using a 14-node and a 27-node model.
",Multiperiod Planning Model for Long-Term Expansion of Energy Distribution Systems Considering Advanced Reliability Topics,"[71302, 78755, 73953]",640,"[21, 84, 123]",3602,Capacity expansion planning for energy systems,21,13,22,Energy Management,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,81 [building - 116],"['Convex Optimization', 'Optimization Modeling', 'Reliability']",WB-22
"This paper investigates the impact of three oil price shocks, i.e. oil supply shock, oil aggregate and specific demand shocks, on firm-level distress using a dataset including 8,130 firms across 48 countries from 2002 to 2022. The study also analyzes the role of energy diversification in the relationship between oil-specific demand shock and firm distress. The paper finds that aggregated demand and specific demand shocks increase firm distress risk while supply shocks reduce such risk. Furthermore, the findings suggest that energy diversification mitigates the impact of specific demand shocks on firm distress. The paper also implements several robustness checks, and the results remain the same. Potential policy implications are also discussed.",Do Oil Market Shocks Affect Financial Distress? Evidence from Firm-level Global Data,[61961],340,"[126, 37]",3603,Enhanced statistical methods for energy challenges,22,7,14,Energy Markets,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,16 [building - 116],"['Risk Analysis and Management', 'Energy Policy and Planning']",TA-14
"Nowadays, pigs are raised in large industrial-scale operations. The fattening farms operates under an all-in–all-out management policy. In general, the fattening period depends on the starting weight, batch homogeneity, feeding system, growth curve, reward system and chain management policies. Although pigs are fed with the same diet, not all of them reach market weight at the same time. Hence, partial sales produce better economic rewards. Once the old batch leaves, and before the new batch arrives, the facilities are cleaned and sanitized.
The viewpoint of an abattoir collecting fattened pig from many different farms belonging to the same supply chain has not yet been considered. Farms vertically integrated sell their entire production to a single abattoir in agreement with the supplier contract signed with the integrator company in a long-term relationship. The routes to collect pigs and the arrival to the abattoir need to be balanced over time and scheduled beforehand with uncertainty in pigs’ liveweight to avoid workload peaks and a fair operation for the PSC. The problem is a vehicle routing problem with uncertain demand since the deliveries depend on the inventory of pigs. Thus, the objective is to formulate a mixed-integer linear-programming [MILP] model to optimize daily routes of trucks and transports of fattened pigs to the abattoir. A metaheuristic algorithm is proposed to approximate the solution of the model and to efficiently plan transportation.",Vehicle routing problem to optimize pig deliveries to the abattoir,"[243, 78762, 78761]",676,"[89, 145, 74]",3606,DSS in Agriculture,20,9,12,OR in Agriculture and Forestry ,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,13 [building - 116],"['OR in Agriculture', 'Vehicle Routing', 'Metaheuristics']",TC-12
"Trust and reputation systems play a pivotal role in modern decentralized environments, fostering cooperation and mitigating risks in various online interactions. The aim of our work is to introduce a variational formulation approach to model and analyze trust and reputation systems. By formulating trust and reputation as variational problems, this approach offers a novel perspective on understanding the underlying mechanisms governing trust establishment. The variational formulation provides a mathematical framework to determine the equilibrium weighted trust values, taking into account that each trustee tries to maximize its gain, namely the utility minus the costs. We illustrate the applicability of this variational formulation through a pletora of simulations, demonstrating its effectiveness in modeling trust and reputation systems. Insights gained from this approach offer valuable guidance for the design and implementation of more reliable and efficient trust and reputation mechanism in decentralized environments.",A Variational Formulation for a Trust and Reputation System,"[68626, 68705, 22982, 37018]",900,"[132, 0]",3607,Equilibrium detection in applications,63,12,40,"Interfaces Between Optimization, Hierarchical Problems and Equilibrium Detection with Applications","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,96 [building - 306],['Social Networks'],WA-40
"Indoor localisation is an essential area with vast implications for navigation, emergency services, and a range of Internet of Things contexts. The challenge lies in the intricate nature of indoor spaces and the limitations inherent in current technologies for precise indoor localisation. This research addresses these issues using a supervised autoencoder [SAE] approach on the UjiIndoorLoc dataset, encompassing Wi-Fi fingerprint data from various buildings and levels at Jaume I University. An AutoML search was employed to identify the best hyperparameters, including learning rate, number of epochs, layers in both the autoencoder and multilayer perceptron, nodes per layer, and dropout rate. Bayesian optimisation, an efficient search strategy, was used to test different hyperparameter combinations and neural network layer configurations to find the most effective models. The findings demonstrate the model's high accuracy rates - 99.91% for building identification, 91.45% for floor identification, and an overall accuracy of 91.18%. The SAE model's performance showcases its capability and competitive edge over a standard model and various methods, underscoring its potential to overcome the difficulties of complex indoor localisation tasks.",Enhancing Wi-Fi Fingerprinting through the Use of an Automated Supervised Autoencoder,[27627],215,"[66, 42, 141]",3609,Applications of Machine Learning in Optimization,64,10,25,Combinatorial Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,011 [building - 208],"['Machine Learning', 'Expert Systems and Neural Networks', 'Telecommunications']",TD-25
"Catering trolleys are one of the aircraft’s standard load equipment for food-and-drink services in the cabin. Airlines must prepare sufficient scales to cope with all their passenger flight operations. The catering work must start a few hours before the flight departs, and loaded onboard trolleys cannot be directly reused after arrival before the cleaning process. Trolley types can be divided into regular and half carts by size and volume and are versatilely used on every aircraft type. Aircraft galleys have specific areas with multiple blocks for the side-by-side storage of trolleys, with the principle of “all carts or no cart” for a block, even storing empty ones, to avoid trolleys arbitrarily moving while flying. This study exploited a time-space network graph to express the movement of trolleys between airports and airports. An integer programming model based on this graph was proposed with the decision variables for the numbers of full and half trolleys on the arcs. The objective minimizes the total number of equipment scales subject to the flow conservation and constraints in handling catering trolleys with a safety stock level at each airport. This preliminary research reported the numerical experiments for a Taiwanese airline operating international flights. Tested cases consisted of 43 airports and 643 flights operated with four kinds of fleet and were solved within a reasonable time.",Modeling equipment scales of catering trolleys for flight operations,[78668],196,"[4, 72, 150]",3610,"Discrete, continuous or stochastic optimization and control in networks, transportation and design I ",64,2,25,Combinatorial Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,011 [building - 208],"['Airline Applications', 'Mathematical Programming', 'Network Flows']",MA-25
"Multiproduct pricing optimization is a challenging process. Calibrating demand models based on transactional data tends to produce bias in cross-elasticity effects, mainly due to endogeneity issues, getting nonsensical or extreme solutions in the optimization process. We explore and compare different supervised and unsupervised methods to extract constraints from the data to formulate a robust feasible space for the pricing optimization procedure. These constraints or pricing rules uncover latent business rules where relationships between product prices happen. Also, they can identify price solutions correlated with good product category performance. We apply our methodology using the orange juice category dataset for different supermarket stores, obtaining robust improvements from 15% to 40% in expected category profit. Some of the methods used to extract pricing rules from data are association rules, SVM classifiers, robust optimization, and ellipsoidal kernels.",Using machine learning for constraint learning in multiproduct pricing optimization,"[46880, 78772]",414,"[124, 127, 66]",3612,Pricing and learning 2,11,4,59,Pricing and Revenue Management,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S08 [building - 101],"['Revenue Management and Pricing', 'Robust Optimization', 'Machine Learning']",MC-59
"Since the ratification of Solvency II, the determination of the Value-at-Risk in the insurance industry has gained renewed importance. We examine the convergence of estimators of the Value-at-Risk or, more specifically, of quantiles within the nested Monte Carlo framework as they occur in the internal models of [typical life] insurance companies. Due to the numerous model uncertainties, robust methods and model-free results are of particular importance. To this end, we provide sharp results regarding the deviation of the quantiles of a perturbed random variable to the quantile of the undisturbed random variable depending on the perturbation, while reducing the assumptions to a minimum without sacrificing the sharpness of the results. These results are combined with classical results from estimation theory to derive almost-sure convergence rates for the Value-at-Risk estimator under rather weak assumptions.",Value-at-Risk estimation in nested simulations,[78771],280,"[126, 131, 44]",3613,Decision making in Insurance and Pensions,74,3,57,Modern Decision Making in Finance and Insurance,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S06 [building - 101],"['Risk Analysis and Management', 'Simulation', 'Finance and Banking']",MB-57
"The design of resilient hub-and-spoke networks has been previously explored in the literature, but in most cases, both the attack and defense strategies are deterministic and known by both the attacker and the defender.
We study the problem faced by a company that aims to design a hub-and-spoke network despite the existence of one or more potential threats to the network operation.
The company is tasked with a complex decision-making process. It must strategically determine the locations of its hubs, meticulously design the resulting hub-and-spoke network, and judiciously select which infrastructure to protect, all while considering potential threats to the network operation.
The company faces one or more attackers, whose actions could potentially degrade the network operation. Importantly, the company operates in an environment of uncertainty, as it does not possess knowledge of the actual attack strategy, but has prepared several attack scenarios.
The company’s primary objective is to minimize design and protection costs. In contrast, the attacker’s objective is to maximize them.
We expect that the resulting models will provide valuable insights on how to design resilient hub-and-spoke networks, together with tools to quantify the impact of using heuristic protection strategies.",A stochastic hub network design and protection problem,"[54824, 66185, 1646, 69008]",582,"[64, 143, 123]",3615,Hub Location,29,4,61,Locational Analysis,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S10 [building - 101],"['Location', 'Transportation', 'Reliability']",MC-61
"We consider crew scheduling on the day before operation - because of a lack of train drivers [e.g., due to sick leave] crew members need to be rescheduled. We are given a feasible crew schedule for a subset of the drivers, a set of unassigned tasks, and a set of standby drivers with fixed start and end time of their working hours. We aim to generate a new crew schedule with as few unassigned tasks as possible --- which will in turn decrease the number of trains to be cancelled. We build a Mixed Integer Linear Programming [MILP], which we solve using the commercial solver Gurobi 11.0, and we develop a heuristic approach based on Tabu Search. We consider the same restrictions for the MILP model and our heuristic, this includes constraints on the total working hours, on break duration, maximum working time without a break etc. The data of a one-day schedule is provided by Mälartåg with 153 drivers in total, in which there are 16 standby drivers with fixed depot, start and end working time. For an instance with 11 drivers being on leave, we can assign all but 5 tasks [from a total of 37, which were unassigned after the leave], and we obtain a result in 23 seconds. We compare the results, computational time and space between our model and approach to show that our Tabu-Search-based approach can achieve a ‘good-enough’ result with less time and space needed. ",Last-Minute Crew Rescheduling - Model and Heuristic Approach,"[78749, 78818, 36664, 78822]",630,"[122, 111, 143]",3616,Railway Applications,6,14,56,Transportation,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S04 [building - 101],"['Railway Applications', 'Programming, Mixed-Integer', 'Transportation']",WC-56
"Over the last years, hybrid genetic search [HGS] algorithms for the vehicle routing problem [VRP] and its variants have demonstrated encouraging results in particular with the use of a route-first-cluster-second heuristic. As an individual is represented by a “grand tour” [TSP Solution], the advantages of this method are an easiness regarding the population management and crossover operators combined with an optimal split method for the transition from an individual to a full solution. The goal is to improve the efficiency of this method using a trained deep learning model to identify good individuals in the population, thus avoiding the need to switch search space and saving computational time. To this end, a reverse split method is used to switch freely between population and solution space to train the deep learning model using cost evaluation and feature extraction. ",Deep Learning Based Hybrid Genetic Algorithm for the CVRP,"[78773, 56339, 87]",334,"[143, 74, 8]",3617,Big data analysis and AI in transportation,6,14,55,Transportation,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S02 [building - 101],"['Transportation', 'Metaheuristics', 'Artificial Intelligence']",WC-55
"In the dynamic landscape of football, predicting the emergence and success of top players is a crucial aspect that has gained momentum with the advent of advanced analytics. This presentation explores the fascinating world of forecasting top players in football, focusing on the intriguing facts surrounding data preparation and analysis. By delving into the intricacies of collecting and processing vast datasets, this presentation unveils the methodologies employed to identify key performance indicators and potential breakthrough talents. Leveraging cutting-edge analytics, teams and analysts aim to unravel patterns and trends that transcend traditional scouting methods. This talk highlights the synergy between technology and talent evaluation, offering insights into the challenges, breakthroughs, and captivating facts that define the landscape of predicting top players in football. ",Top Players Prediction in Football - Interesting Facts on Data Preparation and Analysis,[78776],668,"[18, 66, 7]",3618,Performance and scouting in football,37,10,16,OR in Sports,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,19 [building - 116],"['Computer Science/Applications', 'Machine Learning', 'Analytics and Data Science']",TD-16
"Managers seek ways to survive pandemic situations to avoid huge financial losses. Preventing staff from infecting each other plays a prominent role in survival. To do so, the infectious among employees should be modeled at first. Secondly, the members who directly or indirectly infect the maximum number of other employees are identified. This problem is considered as an instance of Influence Maximization [IM] general problems. We use a Mixed Integer Linear Program [MILP] analytical optimization for IM problem. The MILP optimization guarantees the global optimal solution. However, due to the uncertain nature of influence, the IM problem is formulated as a stochastic optimization based on a limited number of scenarios. Therefore, the whole stochastic nature of the influence process may not be captured. It is of high importance to check whether the number of scenarios used in stochastic MILP is adequate. To do so, the result of the optimization is simulated with numerous scenarios to evaluate the gap between the objective function and the exact expected value. The proposed stochastic MILP methodology is examined on a company with twelve employees. The efficiency of the method and adequacy of scenarios are then discussed.

",Identify the most influential employees in Covid-19 Pandemic by A stochastic MILP influence maximization,"[78775, 78778, 78779]",963,"[14, 136, 56]",3619,COVID-19 [2],3,14,15,OR in Health Services [ORAHS],"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,18 [building - 116],"['Combinatorial Optimization', 'Stochastic Optimization', 'Health Care']",WC-15
"The mining industry, recognized for its high energy consumption and carbon emissions, faces pressing demands from customers for more sustainable practices. This work outlines a comprehensive approach to integrating renewable energies, into the mining sector to address these challenges. While the potential benefits of such integration include reduced energy consumption and emissions, it also presents significant operational complexities. Mining equipment operates under intricate dynamics, and the fluctuating nature of renewable energy can complicate these operations further. Moreover, the mining industry's need for a continuous power supply necessitates that renewable energy integration is reliable and capable of meeting energy demands.
To tackle these challenges, we propose the construction of optimization problem tailored to the complex operational dynamics of mining equipment and corresponding process, aimed at minimizing energy consumption while maximizing the use of renewable energy. Given the uncertainties inherent in renewable energy production and mining operations, we employ a rolling horizon optimization approach. Additionally, we integrate discrete event simulation to describe the whole process of ore processing to overcome the issue of missing parameters, enabling more accurate estimation of critical variables in the optimization problem. Preliminary results from this integrated approach demonstrate significant promise for the mining industry. ",Advancing Sustainable Mining - Integrating Renewable Energy through Optimization and Simulation,"[78777, 78780, 78781, 78782]",605,"[38, 93, 104]",3620,Renewable Energy Challenges,21,12,22,Energy Management,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,81 [building - 116],"['Engineering Optimization', 'OR in Energy', 'Process Systems Engineering']",WA-22
"Central Sterile Supply Departments [CSSD] play a critical role in the operation of hospitals and for their overall environmental footprint. Operationally, the batching of jobs and their scheduling and allocation to washer-disinfectors and sterilizers is a complex decision-making problem, with CSSDs being potential bottlenecks in surgery scheduling. Environmentally, on the one hand, CSSDs enable the reusing of surgical and other sterile equipment and thus help to reduce the environmental impacts of production and waste treatment associated with alternative disposable solutions. On the other hand, CSSDs themselves consume substantial amounts of electric and thermal energy, chemical cleaning agents, water, and disposable equipment [e.g., face masks, gloves, fleeces]. Critically, operational and [various] environmental objectives are often strongly conflicting. In this work, we develop a two-stage batch-scheduling flexible flow shop model of a CSSD, which is parameterized with Life Cycle Assessment-based environmental data. The model and its parameters are validated by real-world cases of CSSDs in German and Danish hospitals. We anticipate the model to identify possible compromise solutions between conflicting objectives [e.g., by adjusting utilization rates and time-temperature profiles in autoclaves], thereby concurrently contributing to the operational efficiency and sustainable transformation of hospitals.",Environmental and operational optimization of Central Sterile Supply Departments in Danish and German hospitals,"[61102, 75133, 15060, 26634, 1609]",546,"[100, 77, 73]",3621,Sustainable Operations,19,5,24,Sustainable Supply Chains,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,83 [building - 116],"['OR in Sustainability', 'Multi-Objective Decision Making', 'Medical Applications']",MD-24
"Despite the prevalence of Poisson process modeling in social science, little research has explored how the periodic collection of data affects the applicability of this model.  Our research studies how discretization impacts the Poisson process model’s interarrival distribution and introduces the periodically-observed time-homogeneous Poisson process [PTPP] model as a viable alternative.  The long-run behavior of the PTPP is quantified and compared to the analogous Poisson process.  We show the PTPP consistently provides a good approximation of process outputs regardless of discretization, extending the capabilities of the traditional model.  The applicability of the PTPP model is demonstrated through multiple case studies. ",Interarrival Distribution of a Periodically-Observed Time-Homogeneous Poisson Process,"[5920, 78786, 61287]",450,"[135, 101]",3622,Analysis of Stochastic Models I,50,10,39,Stochastic Modelling,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,35 [building - 306],"['Stochastic Models', 'OR/MS and the Public Sector']",TD-39
"Driven by an application from chromatography, we model an optimization problem with linear objective subject to robust constraints that depend on uncertain particle size distributions [psd].
The ambiguity set of our model can exploit information on moments as well as confidence sets. Moreover, we present a duality-based reformulation approach for distributionally robust problems, where the objective of the adverserial is allowed to depend on univariate indicator functions. This renders the problem nonlinear and nonconvex. In order to be able to reformulate the resulting semiinfinite constraints nevertheless, a safe approximation is presented that is realized by a discretized counterpart. Its reformulation leads to a mixed-integer linear problem that yields sufficient conditions for distributional robustness of the original problem. Furthermore, it is proven that with increasingly fine discretizations, the discretized reformulation converges to the original distributionally robust problem.
Computational results for the chromatographic setting show that the safe approximation yields robust solutions of high-quality within short time.",A Safe Approximation of Distributionally Robust Problems Depending on Univariate Indicator Functions,"[72641, 78830, 14713]",720,"[127, 111, 104]",3623,Mixed Integer Nonlinear Programming and Nonconvex Optimization ,86,13,04,MINLP,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1001 [building - 202],"['Robust Optimization', 'Programming, Mixed-Integer', 'Process Systems Engineering']",WB-04
"Over the last decade the European legislator has set ambitious recycling targets for municipal waste and packaging waste so to achieve a more circular economy and to contribute to the European Green Deal’s sustainability goals. In this paper, we evaluate the European countries performance evolution with respect to the constantly evolving waste management legal framework and their response to the definition of increasingly demanding targets. To do so, we suggest an innovative dynamic composite indicator to evaluate the country performance with respect to different packaging waste categories and taking into account their deviation from the set targets. In particular, the proposed synthetic indicator distinguishes targets in terms of essential thresholds and aspiration levels. The essential thresholds do not allow for compensations between strengths and weaknesses of the evaluated units and grants that certain fundamental standards are met. The aspiration levels are targets that countries should aim at to foster their sustainability, but they are not compulsory. We monitor the evolution of 27 European countries from 2015 to 2020, where the packaging legislative framework has undergone important developments. The tool provides policy-relevant findings, important to identify best practices and to define priority actions so to enhance the transition of lagging-behind countries towards a more circular economy.",EU Waste Recycling Targets - Where are we at? A Performance Evolution Assessment,"[24128, 60983, 4960, 59522, 62442]",907,"[100, 77]",3624,"MCDA and Composite Indicators - Issues, Advances and Applications 2",44,15,44,Multiple Criteria Decision Analysis,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,20 [building - 324],"['OR in Sustainability', 'Multi-Objective Decision Making']",WD-44
"Two primary goals of grocery retailers when managing perishable products [e.g., food] are [i] to offer products with a high remaining shelf life to consumers, as those are perceived to be of high quality, and [ii] to minimize waste driven by products that exceed their maximum remaining shelf life. Both goals prompt retailers to integrate a minimum remaining shelf-life requirement, known as “Minimum Life on Receipt” [MLOR], into the contracts with their suppliers. However, little is known about how such an MLOR requirement affects the contractual agreement between retailers and food producers and how it further affects replenishment and production decisions, profitability, and waste of the retailer and the food producer. In this paper, we investigate how an MLOR requirement of the retailer affects the operational performance of a wholesale price contact in a two-echelon perishable-product supply chain. We consider that both the retailer and the food producer follow a basestock policy, where the basestock levels are optimized depending on the wholesale price set by the food producer and the MLOR requirement. Our results challenge widely accepted findings that retailers benefit from organizing their inventory management following a first-in-first-out [FIFO] issuing policy compared to a last-in-first-out [LIFO] policy.           ",Shelf-life requirements and their impact on contractual agreements in perishable-product supply chains  ,"[9110, 78791, 78792]",425,"[61, 105]",3625,Retail Inventory Management I,30,3,50,Retail Operations,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,M2 [building - 101],"['Inventory', 'Production and Inventory Systems']",MB-50
"Cross-docking, a rapid logistics strategy, emerges not just as an option but as a strategic necessity in the era of just-in-time inventory and manufacturing systems. In this context, we study a novel stochastic cross-dock scheduling problem considering uncertain truck arrival times to minimize the total penalty of untransferred goods and truck departure delay. A new hybrid genetic simheuristic framework is proposed that embeds a simulation-based local search into a variable neighborhood decent [VND] algorithm. VND optimizes on the deterministic version, in each iteration, if the current solution is identified as a promising solution [not necessarily a local optimal solution], SBLS, where the neighborhood is evaluated by the simulation instead of the deterministic objective value, is triggered in the expectation of finding a local optimal scheduling policy under the stochastic version. A fast evaluation technique for sample scenarios is designed, independent of the objective function, which allows the expectation of the objective under a given sample to be evaluated without simulation, thereby improving the efficiency of the search. And we extend the fast evaluation technique to scheduling problems with a similar structure. Finally, systematic experiments and analysis prove the efficiency of our proposed algorithmic framework and fast evaluation technique.
",A new Simuheuristic for Stochastic Cross-Dock Scheduling Problem With Time Windows Under Uncertainty,"[78790, 40508, 65596]",786,"[136, 129, 131]",3626,Heuristics for Vehicle Routing 3,5,3,58,VeRoLog - Vehicle Routing and Logistics,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S07 [building - 101],"['Stochastic Optimization', 'Scheduling', 'Simulation']",MB-58
"Due to the escalating congestion at airports, there has been a growing interest in using optimization methods for airport slot allocation and flight scheduling. Level 2 and 3 airports nowadays have a substantial surplus of demand relative to airport capacity, which requires sophisticated slot allocation. This process is not only a computationally complex task but needs to satisfy the requirements of different players following individual objectives. The resulting allocation of slots and subsequent flight schedules significantly impacts the market power of airlines. From the passengers' perspective, the key consideration is connectivity, emphasizing the importance of viable connections between destinations. We propose an innovative slot allocation model that prioritizes destination coverage while considering limitations on market power for participating airlines. Based on an extensive computational study, we elaborate on how varying degrees of market power restrictions can positively or negatively influence connectivity.",Airport Slot Allocation for Maximal Destination Coverage,"[50997, 39372]",331,"[14, 143, 4]",3627,Airline Applications I,6,4,55,Transportation,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S02 [building - 101],"['Combinatorial Optimization', 'Transportation', 'Airline Applications']",MC-55
"Recoloring methods applied to optimally edge colored complete graphs of even size are fundamental components of local search procedures aiming to obtain 1-factorizations with specific properties. Then, there is an interest in finding in a 1-factorization, colored subgraphs that allow a local recoloring to obtain a new 1-factorization. The occurrence of bichromatic cycles, lanterns and optimally colored complete subgraphs in certain classes of 1-factorization has been previously studied. In this note we investigate the existence of optimally colored complete bipartite subgraphs in canonical 1-factorizations. We conclude that, whenever $n-1$ is a prime number, no optimally colored complete bipartite subgraphs with more than 4 nodes occur in canonical 1-factorizatoins of complete graphs with $n$ nodes. For other values of $n$ we show that optimally colored complete bipartite subgraphs with more than 4 nodes do occur and we show how to construct some of them. For some small bipartite subgraphs the construction given is proven to be unique.
","On the occurrence of optimally colored $K_{t,t}$ in canonical 1-factorizations of complete graphs","[70617, 4789]",202,"[53, 0]",3628,Applications of combinatorial optimization I,64,7,25,Combinatorial Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,011 [building - 208],['Graphs and Networks'],TA-25
"The Roof of Africa Project is an initiative aimed at promoting sustainable development in Tanzania, focusing on access to education, food, health, and family well-being. Our methodological approach employs system dynamics modelling and scenario simulation to comprehensively analyse the project's contributions to key UN Sustainable Development Goals related to developing access to education, food, health and supporting local families. This project will explore critical factors impacting the initiative's success, including infrastructure development and educational resources, community livelihood uplifting, food and healthcare provision. Through system dynamics lenses, we explore   the complex interactions among these factors and their combined effect on elevating the local communities. Preliminary findings suggest that the Roof of Africa Project offers a transformative and replicable model for tackling the multi-dimensional challenges of sustainable development in impoverished communities.",The Roof of Africa Project - scenario simulation for supporting sustainable development in Sub-Saharan Africa,"[78887, 78796, 47801, 78797]",104,"[92, 100, 41]",3631,Just and ethical sustainability transitions,28,3,20,OR and Ethics,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,45 [building - 116],"['OR in Education', 'OR in Sustainability', 'Ethics']",MB-20
"This paper proposes the development of a decision support tool aimed at evaluating the impacts of regulatory measures within the circular chemical economy framework. Building upon previous research in the field and addressing identified barriers and challenges, a system dynamics simulation tool is being developed. This tool will enable stakeholders to assess the effects of regulatory interventions on investor and consumer behaviours, aiding policy decision-making processes. By employing simulation scenarios that consider different regulatory frameworks, the tool facilitates the evaluation of potential shifts in investment patterns and consumer preferences, offering insights crucial for policy formulation and implementation. The transition towards a circular chemical economy necessitates proactive policy measures to promote sustainable practices and mitigate environmental impacts. However, understanding the multifaceted impacts of regulatory interventions on investor and consumer behavior remains crucial for effective policy decision-making.",Scenario Simulation and Forecasting of Circular Chemical Economy - Assessing Policy Impacts on Investor and Consumer Behaviour for Policy Decision-Making,"[78797, 78799]",111,"[140, 139, 131]",3632,Scenarios and foresight practices - Behavioural issues I,13,12,11,Behavioural OR,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,12 [building - 116],"['System Dynamics and Theory', 'Sustainable Development', 'Simulation']",WA-11
"In this talk, we propose a novel branch and bound algorithm for solving the linear multiplicative programming [LMP] problem. A new quadratic relaxation is proposed. We first show that the Hessian matrix of the multiplication of two linear functions has a decomposition form, given by the difference of two rank-one matrices, resulting in a decomposition each quadratic term of the objective function. And then the minus term is relaxed into a linear one. To find the global optimal solutions, branch and bound method is adopted. Theoretical analysis is provided that guarantees the convergence of our algorithm. Numerical experiments show that the new algorithm runs faster than several typical algorithms for certain types of LMP.",A novel branch and bound algorithm for solving the linear multiplicative programming problems,[75683],903,"[114, 19, 5]",3638,Advances in polynomial optimization and its applications,68,15,38,"Conic Optimization - Theory, Algorithms, and Applications","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,34 [building - 306],"['Programming, Quadratic', 'Continuous Optimization', 'Algorithms']",WD-38
"While a shift from individual transport to public transport is necessary to reach climate goals, also the public transport system consumes a non-negligible amount of energy. Modern electric motors are able to regenerate energy while braking. In the context of rail traffic, the most efficient way to use the regained energy is to transfer it via the catenary to an accelerating train close by. Therefore, it makes sense to select the timetable so that braking and acceleration processes of trains near each other take place simultaneously.
In this talk a new MIP model is introduced that finds a feasible periodic timetable optimizing the usage of energy obtained from regenerative brakes. Assigning arrival and departure times, the model also decides between which pairs of trains a transfer of energy takes place and it maximizes the total brake traction overlap. The new model based on the PESP is compared to previous models that optimize the usage of regenerative energy and computational results are presented.
While the usage of regenerative energy profits from synchronized brake and traction phases of two trains, this prevents the transfer of passengers from the braking to the accelerating train. We analyze the tradeoff between energy and travel-time optimization and show first properties of Pareto-optimal timetables for the special case of one transfer station.",Sustainable Timetabling - Integrating Regenerative Braking into a PESP Model,"[78811, 72792, 1601]",823,"[142, 0]",3639,Timetabling 1,85,14,51,Public Transport Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,M5 [building - 101],['Timetabling'],WC-51
"We propose a new dataset of Capacitated Vehicle Routing Problem instances which are up to two orders of magnitude larger than those in the currently used benchmarks. Despite these sizes might not have an immediate application to real-world logistic scenarios, we believe they could foster fresh new research efforts on the design of effective and efficient algorithmic components for routing problems. We provide computational results for such instances by running FILO2, an adaptation of the FILO algorithm proposed in Accorsi and Vigo [2021], designed to handle huge-scale CVRP instances. Solutions for such instances are obtained by using a standard personal computer in a considerably short computing time, thus showing the effectiveness of the acceleration and pruning techniques already proposed in FILO. Finally, results of FILO2 on well-known literature instances show that the newly introduced changes improve the overall scalability of the approach with respect to the previous FILO design.",One Million ... and Beyond! Solving Huge-Scale VRPs in a Handful of Minutes,"[24902, 58559]",763,"[145, 74, 63]",3640,Public Transport,5,8,58,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S07 [building - 101],"['Vehicle Routing', 'Metaheuristics', 'Large Scale Optimization']",TB-58
"This paper addresses the strategic placement of nature-based solutions [NBSs]
to enhance sustainability and resilience in cities. Leveraging Operations Research [OR] tools and convolution theory, we propose a comprehensive and
flexible approach to maximize the benefits of urban green infrastructure while ensuring equitable distribution across urban areas. Our formulation integrates multiple variables, benefits, and constraints, including fairness considerations, to identify optimal locations of NBSs such as parks, green walls, green walls, and tree plantings. Through several case study analyses, we demonstrate the effectiveness of our approach in improving environmental and social indicators. This research contributes to advancing the understanding and implementation of NBSs in urban planning, offering valuable insights and decision support for re-designing resilient and equitable cities.",Optimal Placement of Nature Based Solutions in Urban Greening,"[71727, 69151, 69969]",76,"[64, 72, 100]",3642,"Assessment Methods for Shaping the Green, Inclusive, and Digital Cities  I",79,2,18,Sustainable Cities,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,42 [building - 116],"['Location', 'Mathematical Programming', 'OR in Sustainability']",MA-18
"Because Sigma-2- and Sigma-3-hardness proofs are usually tedious and difficult, not so many complete problems for these classes are known. This is especially true in the areas of robust and bilevel optimization [we focus in this talk on min-max-regret, interdiction, most vital vertex, and two-stage robust optimization problems]. Even though these areas are well-researched for over two decades and one would naturally expect many [if not most] of the problems occurring in these areas to be complete for the above classes, almost no completeness results exist in the literature. We address this lack of knowledge by introducing over 70 new Sigma-2-complete and Sigma-3-complete problems. We achieve this result by proving a new meta-theorem. This meta-theorem applies to most classical problems, like clique, vertex cover, knapsack, TSP, facility location and many more. In summary, our work reveals the interesting insight that a large amount of NP-complete problems have the property that their min-max versions are 'automatically' Sigma-2-complete.",A large and natural Class of Sigma-2- and Sigma-3-complete Problems in Bilevel and Robust Optimization,[69660],466,"[16, 127, 14]",3645,Robust and Multi-Level Optimization,86,10,04,MINLP,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,1001 [building - 202],"['Complexity and Approximation', 'Robust Optimization', 'Combinatorial Optimization']",TD-04
"We present a python framework, pyO3F, which aims to support the development of optimization approaches for wildfire related problems. We describe the pyO3F main modules, their interactions, and the information they use to model fire behaviour, fire spread scenarios, and different types of firefighting resources. At the core of pyO3F is the use of the minimum travel time principle in a network with nodes representing locations and arcs representing potential fire transmissions between adjacent locations. Fire spread is the modelled as a set of quickest paths with respect to the fire transmission times associated with the arcs [estimated with fire behaviour models].
We exemplify the use of pyO3F in two types of problems - fire suppression and fire-aware forest management. In the former, it is intended to manage a set of available firefighting resources to minimize the impact of fire [e.g. the burned area]. In the latter, it is intended to select a set of prescriptions, one for each forest stand, to maximize the net present value of the timber, taking into account bounds on other ecosystem services [e.g. carbon stock] and fire spread simulations.
",pyO3F - A python framework for wildfire related optimization,"[430, 78823, 3842, 43766, 67922]",5,"[134, 48, 111]",3646,OR in Forestry I,20,3,12,OR in Agriculture and Forestry ,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,13 [building - 116],"['Software', 'Forestry Management', 'Programming, Mixed-Integer']",MB-12
"The transition towards an electricity market with a high penetration of renewable energy is characterized by higher price volatility compared to the more traditional electricity market dominated by conventional generation. The increasing share of intermittent generation requires a method of securing supply and matching demand with supply. Integrating batteries as fully controllable energy storage into the system is a solution to handling this issue. The inclusion and operation of batteries can contribute to the green transition if market participants can operate them profitably. This project aims to quantify these batteries' economic viability, increasing their incentive to be invested in and participate in the electricity markets.
 
In this paper, we run an optimization model based on forecasts of electricity prices in the day-ahead and in the intraday market. The model's performance relies on forecasts of the electricity prices. 
 
We compute the profit from operating the batteries according to our algorithm during 2020 and 2021 in the UK electricity market. The chosen forecast methods can obtain profits in the range of 30-40\% compared to the profit generated from perfect foresight of price developments, i.e., supplying the models with realized prices. In addition, we investigate how participating in both the day-ahead and the intra-day markets yields superior results compared to focusing on just one of the markets.",Assessing the Profitability of Large-Scale Batteries in the UK Electricity Market,"[71189, 19761]",146,"[83, 36, 47]",3647,Modelling commodity markets dynamics,74,2,57,Modern Decision Making in Finance and Insurance,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S06 [building - 101],"['Optimization in Financial Mathematics', 'Electricity Markets', 'Forecasting']",MA-57
"There have been developments recently in interpretable and explainable AI as well as moves to understand ways to do human centred analytics design. As technology and the use of algorithms in business become more ubiquitous we need to understand what parameters in algorithm design can facilitate human centred approaches. Ways to do this ethically and sustainably become ever more pressing as business moves into a world of ESG reporting and increasingly cybernetic systems.   

This talk initially outlines the parameters [defined by prior research] needed by algorithms and AI/ML techniques that make them amenable to integration with human practice. We then take a quick look at the philosophical underpinnings needed by research into cross paradigm work. This sets the stage for a tour of some currently available mathematical tools and techniques that can work with human practice and the contexts they are likely to work under. ",The 'je ne sais quoi' of algorithms that foster human centric analytics design,[57905],981,"[7, 151, 41]",3653,Impact of AI on Soft OR - B,26,9,13,Soft OR and Problem Structuring Methods,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,15 [building - 116],"['Analytics and Data Science', 'Practice of OR', 'Ethics']",TC-13
"In energy systems dominated by renewables, the high share of volatile and uncertain electricity generation raises the need for flexibility for balancing supply and demand. Operating a substantial share of energy storage capacities increases the smoothing impacts on electricity prices. Therefore, a system-oriented approach considering uncertainties as well as endogenous price effects is crucial to optimize the operation of energy storage systems in renewable-dominated energy systems.
The valuation of energy storage systems under consideration of uncertainties is usually solved by determining optimal operating decisions for given price realizations. Current state-of-the-art approaches, such as least squares Monte Carlo, approximate value functions to recursively determine a dynamic decisions considering the uncertainty of exogenous electricity prices.
Our work introduces a hybrid model that combines a least squares Monte Carlo approach with a fundamental model for determining endogenous electricity prices based on a piecewise linear merit order. Here, interactions between storage operations on price determination are explicitly depicted. The developed model thus enables the stochastic valuation of storage operation in the system context and considers the uncertainties of renewable energies, electricity demand, and the technical availability of generation capacities. 
",Extending Least-Squares Monte Carlo to a System-Oriented Study on Storage Operation,"[50545, 72992, 24773]",341,"[36, 136, 93]",3655,Modelling and Economics of Storage Technologies in Energy Markets,22,2,09,Energy Markets,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,10 [building - 116],"['Electricity Markets', 'Stochastic Optimization', 'OR in Energy']",MA-09
"The Minimum Stretch Spanning Tree Problem [MSSTP] is a variant of the classical minimum spanning tree problem. Consider a simple, connected, undirected, and unweighted graph G=[V, E], where V is the set of vertices and E is the set of edges. 
The objective of MSSTP is to identify a spanning tree T=[V, E'] within G that minimises the stretch, i.e., reduces the maximum distance in the spanning tree between adjacent nodes in the original graph [1]. This problem has practical applications in areas such as distribution systems, network design, transportation networks, sensor networks, phylogenetic tree reconstruction, and parallel machine architectures. 
This study introduces a promising and efficient Carousel Greedy [CG] algorithm for tackling the MSSTP. The CG algorithm is a generalised greedy algorithm designed to address the conventional limitations of greedy strategies [2]. We will show that our CG algorithm outperforms the General Variable Neighbourhood Search [GVNS] in terms of solution quality, computational time, and consistency across benchmark instances.

[1] Kardam, Y. S., Srivastava, K., & Marti, R. [2023]. General variable neighbourhood search for the minimum stretch spanning tree problem. Optimization Letters, 17, 2005-2031.

[2] Cerrone, C., Cerulli, R., & Golden, B. [2017]. Carousel Greedy - A generalised greedy algorithm with applications in optimisation. Computers & Operations Research, 85, 97-112.",Carousel Greedy Algorithm for the Minimum Stretch Spanning Tree Problem,"[38462, 9412]",872,"[74, 5, 53]",3656,Optimization problems on graphs,64,5,26,Combinatorial Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,012 [building - 208],"['Metaheuristics', 'Algorithms', 'Graphs and Networks']",MD-26
"Physicians and operating rooms are two of hospitals' most important, expensive, and scarce resources. Therefore, effective and efficient scheduling of these resources is among hospitals' most relevant planning tasks. The decisions to be made on the daily planning level regarding sequencing patients’ surgeries and assigning appropriate staff to surgeries. Since there are several interdependencies between surgery schedules and physician rosters, it is a meaningful approach to consider both planning problems within one integrated optimization problem. We provide mixed-integer programming models within a column generation algorithm that solves both scheduling problems. We create schedules for surgeries and operating room staff. We use test data based on a real-world dataset to provide meaningful insights. The algorithm leads to optimal solutions in more than 75% of all test cases and solves the problem efficiently within a desirable amount of time. We further evaluate our algorithm with respect to different aspects of flexibility in the context of surgery and staff scheduling and generate key insights about their interdependencies.",Integrated surgery and staff scheduling using column generation,"[15060, 70363]",965,"[56, 13]",3658,Surgery Scheduling and Operating Room Planning [2],3,15,15,OR in Health Services [ORAHS],"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,18 [building - 116],"['Health Care', 'Column Generation']",WD-15
"We develop a novel framework for designing cluster-based heuristics to the Vehicle Routing Problem with Time Windows  [VRPTW]. Similarly to state-of-the-art cluster-based VRPTW approaches, our framework consists of a clustering and a routing phase. However, we also incorporate an intermediate pre-routing phase that allows using any existing VRPTW solution method [e.g. exact or metaheuristic] in the routing phase and scaling the method's performance. In this way, we can achieve different desired trade-offs between solution quality and running time efficiency. Further, our framework consists of configurable components in the clustering phase - namely, the number of clusters, clustering objective, and cluster feasibility - that can be customized to enhance the performance of obtained cluster-based VRPTW heuristics. Also, we embed an approach based on time discretization and incompatibility constraints in the clustering phase to ensure that time windows are respected. We computationally validate the effectiveness of our framework and its components using the Solomon benchmarks. 
",A Framework for Cluster-Based Heuristics for Vehicle Routing With Time Windows,"[77879, 77044, 77033]",749,"[145, 65, 14]",3659,Vehicle Routing Problems With Time Windows,5,13,64,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S16 [building - 101],"['Vehicle Routing', 'Logistics', 'Combinatorial Optimization']",WB-64
"We study the pricing decisions of a medical service provider who operates both an emergency department and a non-emergency department with different priority schemes and admission fees. Each patient independently makes her attendance decision according to the urgency level of her medical condition, the admission fees, and expected waiting time at the two departments. We characterize patients' equilibrium attendances to the two departments. Effects of adjusting the admission fees on patient attendances are examined both in an analytical model and with empirical data from public hospitals in Hong Kong.",Admission Pricing In Public Emergency Departments - Theory And Evidences From A Natural Experiment,"[22658, 78835, 78836]",103,"[130, 0]",3661,Healthcare services,3,5,15,OR in Health Services [ORAHS],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,18 [building - 116],['Service Operations'],MD-15
"This study centers on the Time Window Assignment Traveling Salesperson Problem with Stochastic Travel Times, marking a shift from a cost-focused strategy to a more balanced approach that considers both cost and service quality aspects in delivery operations. This research aims to address the challenge of integrating real-world uncertainties, particularly stochastic travel times, into the optimization process. When treated as a two-stage stochastic problem, the first stage involves making decisions about routing and time window assignment using binary and possibly continuous variables. This is followed by handling continuous variables associated with time window violations in the subsequent stage.

This work contributes to the ongoing exploration of vehicle routing, emphasizing the critical role of time window assignment in the optimization framework. Existing literature provides limited methodologies for this specific problem [Celik et al. 2023]. In response, we introduce a novel formulation inspired by the Fox et al. three-index formulation for the Time-dependent Traveling Salesperson Problem [Fox et al. 1980]. Employing Benders Decomposition as a versatile solution framework, our approach exhibits substantial advancements over prior state-of-the-art solutions for this problem.
",An Exact Approach For The Time Window Assignment Traveling Salesperson Problem With Stochastic Travel Times,"[78424, 30921, 19719, 72604]",779,"[145, 136, 14]",3662,MILPs for Vehicle Routing 1,5,10,58,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S07 [building - 101],"['Vehicle Routing', 'Stochastic Optimization', 'Combinatorial Optimization']",TD-58
"Since the release of ChatGPT in 2022, Generative Artificial Intelligence [AI] has been transforming people’s work lives. There is increasing research interest in leveraging Generative AI on human resource management, change management, innovation management, and management education. However, integrating Generative AI and leadership, especially leadership development, needs more attention. Previous work reviewed AI in learning and development and studied gender bias of AI-generated content in leadership training. To further existing research, we want to explore the potential of augmenting leadership development by leveraging Generative AI on leadership intervention and organizational context.","Augmenting Leadership Development with Generative Artificial Intelligence - Perspectives, Research Methods, and Future Directions","[78838, 45621]",981,"[8, 0]",3666,Impact of AI on Soft OR - B,26,9,13,Soft OR and Problem Structuring Methods,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,15 [building - 116],['Artificial Intelligence'],TC-13
"This paper aims to initiate a dialogue between fintech and sustainability by evaluating the impact of sustainability disclosure on the outcomes of 370 Italian crowdfunding campaigns, collected from the Kickstarter platform between 2015 and 2024. Utilizing an NLP technique called zero-shot classification, we analyzed and classified the blurbs of the campaigns as sustainable, employing a rigorous decision framework. The model utilized was the BART-large model, trained with the MultiNLI dataset. The blurbs classified as sustainable by the model were further validated through human supervision to ensure accuracy. Furthermore, leveraging this classification, we constructed a measure of sustainability, and with a probit regression model, we assessed the impact of sustainability on the success of crowdfunding rounds. Several control variables were included such as the funding goal, campaign duration, blurb length, and the number of investors. The findings indicate that sustainability has a positive and significant impact. Disclosing sustainability information increases the probability of success by 7.73%. Additionally, the number of investors also has a positive and significant effect, while the funding goal has a negative effect. This paper contributes to the literature by providing evidence that sustainability acts as a signal of quality for investors, who are more inclined to invest in sustainable projects, resulting in a higher probability of success.",Assessing the impact of Sustainability disclosure in crowdfunding - a Natural Language Processing analysis.,[78840],245,"[44, 139, 66]",3667,Portfolio optimization and sustainability,53,2,08,AI & Innovation in Sustainable Finance,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,1020 [building - 202],"['Finance and Banking', 'Sustainable Development', 'Machine Learning']",MA-08
"Interconnected agents such as firms in a supply chain make simultaneous preparatory investments to increase chances of honouring their respective bilateral agreements. Failures cascade - if one fails their agreement, then so do all who follow in the chain. Thus, later agents' investments turn out to be pointless when there is an earlier failure. How losses are shared affects how agents invest to avoid the losses in the first place. In this way, a solution sets agent liabilities depending on the point of disruption and induces a supermodular investment game. We characterize all efficient solutions. These have the form that later agents—who are not directly liable for the loss chain—still shoulder some of the losses on the premise that they might have failed anyway. Moreover, we find that such indirect liabilities are necessary to avoid unbounded inefficiencies. Finally, we pinpoint one efficient solution with several desirable properties.",Managing cascading network disruptions through optimal liability assignment,"[76209, 57023, 53687]",642,"[50, 0]",3669,Market Design 2,87,12,43,Market Design,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,99 [building - 306],['Game Theory'],WA-43
"In financial econometrics, the focus is often on improving covariance matrix estimations rather than addressing optimization problems with constraints for better portfolio management. This paper argues that combining these advanced estimation methods with optimization that includes specific limits, like cardinality constraints, enhances decision-making and investment strategies. Cardinality constraints limit the number of assets in a portfolio, potentially making simpler estimators like the sample covariance sufficient for investment decisions, especially when dealing with large dimensions that typically introduce significant estimation errors affecting portfolio performance.

We also address the issue of managing portfolios when there are fewer data points than assets, leading to non-invertible, noisy covariance matrices. Cardinality constraints simplify this challenge, making it possible to aim for a global minimum variance portfolio despite these limitations.

Empirically, we find that smaller portfolios, constrained by cardinality to include only a subset of available assets, can achieve diversification similar to market portfolios while reducing transaction costs and simplifying analysis. This suggests focusing on smaller, strategically selected portfolios could offer investors efficient and cost-effective outcomes.

", Cardinality Constraints Meet Large-scale Portfolio,"[78518, 8503, 78849, 72518]",863,"[45, 151, 84]",3673,Mixed Integer Optimization II,64,8,52,Combinatorial Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,8003 [building - 202],"['Financial Modelling', 'Practice of OR', 'Optimization Modeling']",TB-52
"Deep models have shown promising results in solving vehicle routing problems [VRPs]. However, existing models are often trained on instances from specific distributions and their worst-case performance is largely underexplored, thereby hindering the understanding and improvement of their robustness. In this paper, we present a generic framework to generate hard instances for obtaining robust VRP solutions. Given a pretrained deep model, we first develop an attack method that comprises an autoregressive sampling network [ASN] and a hardness measurement network [HMN]. The two networks are trained alternately by reinforcement learning, aiming to generate hard instances for the given deep model and gauge the attack effect [i.e., hardness] of the instances, respectively. Then, we propose a simple yet effective training algorithm to robustify the deep model, which is progressively replaced by the continually trained HMN. Experimental results show that the attack method significantly degrades the performance of various deep models and conventional heuristics. Moreover, the training algorithm showcases the ability to enhance the robustness of the deep model, demonstrating its promising zero-shot generalizability.",Learning to Generate Hard Instances - Towards Robust Solutions for Vehicle Routing Problems,"[78850, 78889, 51230]",732,"[66, 145]",3674,	[Deep] Reinforcement Learning for Combinatorial Optimization 3,14,7,03,Data Science Meets Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,1005 [building - 202],"['Machine Learning', 'Vehicle Routing']",TA-03
"Cutting and packing [C&P] problems are hard combinatorial optimisation problems that arise in a variety of manufacturing and process industries. These problems arise whenever a larger object needs to be divided into smaller parts in order to minimise waste, and their optimal solution contributes to the reduction of raw material costs. However, many companies ignore optimisation approaches to their C&P problems in favour of production efficiency, prefering simple ways of cutting raw materials [cutting patterns], which require less human labour and less sophisticated and cheaper machinery, to complex, but in terms of raw material consumption optimised, cutting patterns. Therefore, it becomes critical to explicitly consider the impact of pattern complexity when solving C&P problems.
In this talk, we will present a number of characteristics that influence pattern complexity, such as the number of different piece types, the total number of cuts, different types of strip structures, etc. Based on the floating-cuts mathematical model for two-dimensional rectangular cutting problems, these characteristics will be modelled, requiring new variables, additional constraints and resulting in new components of the objective function. Within a multi-objective optimisation framework, the conflicting objectives of waste and pattern complexity minimisation are addressed. Computational results are presented that show and quantify this trade-off. 
[Projeto CIBELE doi - 10.54499/2022.02767.PTDC]",Control of pattern complexity in two-dimensional rectangular cutting problems,"[1999, 78851, 15471, 663]",862,"[23, 14]",3675,Applications of combinatorial optimisation in industry and services II,64,8,29,Combinatorial Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,157 [building - 208],"['Cutting and Packing', 'Combinatorial Optimization']",TB-29
"The bioeconomy is a crucial concept for addressing major environmental challenges. Transitioning towards a bioeconomy requires coordinated efforts from various stakeholders and has the potential to significantly transform value chains, particularly in the agriculture sector. However, the pathway to this transition is complex and dynamic. This paper aims to shed light on this complexity by employing a hybrid simulation approach that integrates system dynamics and agent-based simulations. Our focus is to explore both the intended and unintended changes that a bioeconomy transition might bring to dairy livestock farmers in Norway. 
The contributions of this paper are twofold. First, it examines the advantages and drawbacks of using hybrid simulations  to model farming systems.
Second, it offers practical insights into the impact of introducing and promoting new biotechnologies in the industry. In particular , the simulation results highlight significant challenges regarding a bio-based economy for dairy farmers resulting on the potential exacerbation of the previous trend towards fewer, larger, and more intensive farms. 
Moreover the simulation results also indicate a potential geographical concentration of farms in small areas of Norway. The potential concentration of farms in very few hotspots for dairy farming is a consequence of the scales needed to justify investments in supportive infrastructure like bioreactors.
","Bioeconomy transition, using hybrid simulation to assess challenges and opportunities for livestock farmers in Norway",[71404],590,"[140, 131, 137]",3676,OR in Agriculture,20,5,12,OR in Agriculture and Forestry ,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,13 [building - 116],"['System Dynamics and Theory', 'Simulation', 'Strategic Planning and Management']",MD-12
"This study proposes a novel method for assessing engineering programs. It combines Data Envelopment Analysis [DEA] and Bootstrap techniques to evaluate the efficiency of educational institutions. By employing DEA models [CRS and VRS], the approach pinpoints areas where foundational skills for professional development are lacking, revealing opportunities to enhance educational quality. Furthermore, it creates targeted action plans for specific improvements. This data-driven approach fosters accurate and reliable evaluations, promoting continuous improvement in Colombian engineering education. Ultimately, DEA and Bootstrap offer a robust and objective method to optimize educational efficiency, identify best practices, and define achievable learning objectives.",Enhancing Engineering Education - A Novel DEA-Bootstrap Approach for Efficiency and Improvement,"[78855, 78900, 79366]",939,"[24, 34, 35]",3679,DEA applications in Education and Health II,89,7,48,Data Envelopment Analysis and its Application,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,60 [building - 324],"['Data Envelopment Analysis', 'Education and Distance Learning', 'Efficiency Analysis']",TA-48
"We analyze a service provider's optimal design of service contracts, which specify service priority and pricing for consumers [e.g., patients] of different types. The analysis incorporates both static and dynamic contracts and considers consumer heterogeneity following either discrete or continuous distributions. It demonstrates the profit advantage of dynamic contracts over static contracts and characterizes the conditions under which the advantage is most pronounced.",Dynamic Contracts For Queuing Service Systems,"[78835, 22658, 78857]",103,"[121, 130]",3680,Healthcare services,3,5,15,OR in Health Services [ORAHS],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,18 [building - 116],"['Queuing Systems', 'Service Operations']",MD-15
"The anticipated surge in demand for elective surgical services highlights the urgency of optimising hospital resources, especially in publicly funded systems with limited budgets. Effective coordination of materials and resources under operating theatre [OT] capacity management is essential to meet the rising volume of elective surgeries. This study introduces a novel multi-stage care facility model, emphasising the significance of both OT and non-OT processes, aiming to balance financial constraints with community satisfaction. It explores enhancing OT scheduling using the theory of variation and uncertainty buffering, incorporating a time buffer into the surgical process. Furthermore, the study addresses uncertainties related to emergency arrivals and processing time, employing robust optimisation to ensure adequate capacity at each stage of the surgery process, thereby enhancing the model's relevance and realism. Solving this model, the study proposes meta-heuristic algorithms like Hybrid Nondominated Sorting Genetic Algorithm and hybrid Multi-Objective Firefly Optimisation algorithms with Hybrid Non-dominated Sorting Genetic Algorithm to efficiently address real-world scale scheduling problems. These meta-heuristics facilitate rapid adjustments in schedules in response to evolving circumstances, making them valuable tools for healthcare practice. This study enhances surgical service delivery and resource management in hospitals by advancing understanding of OT scheduling.",Advancing elective surgery scheduling considering operating theatre and non-operating theatre resources with time and demand uncertainties,"[78801, 78860]",599,"[56, 129, 127]",3681,Surgery Scheduling and Operating Room Planning,3,8,10,OR in Health Services [ORAHS],"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,11 [building - 116],"['Health Care', 'Scheduling', 'Robust Optimization']",TB-10
"This paper studies a scheduling problem for automated guided vehicles [AGVs], in which a fleet of homogeneous AGVs are used to deliver parts to workstations in a shop floor environment.  During delivery the batteries of AGVs can be recharged, in order to ensure all parts are transferred from a central warehouse to their respective destinations without stops.  A partial charging policy, which allows to flexibly manage the charging operations for AGVs, is employed.  The transportation tasks undertaken by each AGV and the charging operations on each AGV need to be scheduled.  The objective is to minimize the makespan to complete all transportation tasks.  A mixed integer programming model is presented to describe the AGV scheduling problem.  A two-stage heuristic approach is proposed for problem-solving.  For stage one, two metaheuristics, i.e., a genetic algorithm [GA] and a hybrid GA and whale optimization algorithm [WOA], are developed to assign transportation tasks to each AGV.  For stage two, a rule-based heuristic is proposed to schedule the assigned transportation tasks and charging operations for each AGV.  Computation experiments show that the proposed solution procedures can solve the scheduling problem effectively and efficiently.",An AGV scheduling problem based on partial charging policy,"[78858, 28027]",624,"[129, 111, 74]",3682,Advancing mobility towards sustainable solutions III,6,12,56,Transportation,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S04 [building - 101],"['Scheduling', 'Programming, Mixed-Integer', 'Metaheuristics']",WA-56
"The Convex Nonparametric Least Squares [CNLS] method can be used to develop a benchmarking model represented by a set of hyperplanes. CNLS assumes that the regression function is either concave or convex. However, there may be instances where the regression function exhibits concave and convex patterns, rendering this assumption invalid. This paper addresses this drawback by proposing a new method called Concave-Convex Nonparametric Least Squares [C2NLS], which incorporates concavity and convexity constraints in CNLS. It is proved that C2NLS will have better goodness-of-fit performance than CNLS, but the number of hyperplanes will also increase. Since C2NLS contains both concave and convex portions, it is not sufficient to rely solely on the concavity assumption [or convexity assumption] during the benchmarking process. To tackle this issue, it is suggested that both concave and convex portions be used separately and combined with the resulting benchmarking scores. An illustrative example is provided, and the energy performance of Hong Kong secondary schools is used to demonstrate the goodness-of-fit of C2NLS.",Benchmarking Building Energy Consumption Performance by A Nonparametric Least Squares Method ,[6927],282,"[37, 93, 26]",3683,Advancements in energy system optimization and analysis tools,21,4,22,Energy Management,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,81 [building - 116],"['Energy Policy and Planning', 'OR in Energy', 'Decision Support Systems']",MC-22
"The aging population and declining birth rates have become social issues in many Asian countries, especially Taiwan. To alleviate the significant shortage of manpower, hiring second-career certified nursing assistants has emerged as a new trend in care institutions. This study combines interviews and the Analytic Hierarchy Process to explore the job satisfaction and key factors influencing turnover decisions of second-career care workers. The findings of this study will contribute to understanding the factors affecting job satisfaction among second-career care workers and serve as a reference for training programs in long-term care institutions.",Identifying the key factors of job satisfaction of the second-career certified nursing assistants in long-term care,"[78868, 78560]",950,"[77, 6, 56]",3685,Nurse rostering,3,10,15,OR in Health Services [ORAHS],"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,18 [building - 116],"['Multi-Objective Decision Making', 'Analytic Hierarchy Process', 'Health Care']",TD-15
"In public transit networks, timetable synchronization is crucial for offering passengers attractive transfer services, while vehicle circulation significantly impacts operating costs. These two problems are closely related and well worth integration since the number of required vehicles is influenced by train timetables, and both focus on service connections. However, there is very limited research on integrating these two problems.

In this paper, we present an integration of these two problems into a novel quadratic semi-assignment problem [QSAP] formulation, based on a simplified time-discretized network. The objective is to simultaneously minimize passenger total transfer time and vehicle operation costs. We include features such as multiple period lengths, flexible transfer connections, and vehicle connections within a pure QSAP formulation. By avoiding the inclusion of additional variables and constraints, this formulation optimally leverages existing algorithms designed specifically for QSAP. Furthermore, we reformulate the problem into a Quadratic Unconstrained Binary Optimization [QUBO] formulation, solved with advanced quantum annealing approaches. Our model is also linearized into an integer linear programming [ILP] format, ensuring compatibility with solvers like Gurobi. To verify the accuracy and computational efficiency of our models, we applied them to numerous virtual case studies and real-world cases from the Beijing Metro Network.
",Integration of timetable synchronization and vehicle circulation for public transit networks - A novel quadratic semi-assignment problem formulation,"[78871, 78873]",823,"[142, 129, 143]",3686,Timetabling 1,85,14,51,Public Transport Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,M5 [building - 101],"['Timetabling', 'Scheduling', 'Transportation']",WC-51
"In this talk, we propose a novel Mixed-Integer Non-Linear Optimization formulation to construct a risk score. A trade-off between prediction accuracy and sparsity is sought. Previous approaches are typically designed to handle binary datasets, where numerical predictor variables are discretized in a preprocessing step by using arbitrary thresholds, such as quantiles. In contrast, we allow the model decide for each continuous predictor variable the particular threshold that is critical for prediction. The resulting optimization problem is tested in synthetic and real-world datasets.",On the optimization of risk scores for continuous predictors,"[56862, 22410]",141,"[66, 111, 113]",3687,Machine Learning for and with Mathematical Optimization,15,15,27,Mathematical Optimization for XAI,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,047 [building - 208],"['Machine Learning', 'Programming, Mixed-Integer', 'Programming, Nonlinear']",WD-27
This paper models the labour-leisure choice problem with Cobb-Douglas preferences and the short-run profit maximization problem of a monopsonist with Cobb-Douglas technology as a bilevel programming problem. The solution of the problem is discussed and the results are enriched by economic interpretations and numerical examples.,Modelling the choice between labour and leisure and short-run profit maximization of the monopsonist - bilevel programming approach,"[62175, 1783, 41932]",850,"[33, 84, 25]",3693,Simulation in economics II,77,5,43,"Agent-based Models in Management, Economic and Organisation Sciences","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,99 [building - 306],"['Economic Modeling', 'Optimization Modeling', 'Decision Analysis']",MD-43
"Food waste levels in the food service sector can be up to 30%, mainly caused by overproduction, as food is produced upfront rather than on demand. Therefore, there is a significant risk of meals or meal components to be left uneaten. Leftover meal components might be used in other dishes; however, due to the short shelf life of these products, time is a limiting factor. Adequate and flexible meal planning might increase the reuse of these leftover products and, therefore, reduce the environmental impact of the food sector. In this study, we propose an innovative menu planning model for the food service sector. The model incorporates the common meal planning decisions on which ingredients to purchase and which meals to offer on a day, but also allows storing excess meal components and using them in the following days. This model aims to provide a menu with the least environmental impact but also aligns with the requirements of the customers and the budgetary constraints of the food service. Using Adjustable Robust Optimization, the model incorporates the uncertainties inherent in food service operations, such as fluctuating customer demand.",Menu planning for food services - reducing food waste by the use of leftovers under consideration of demand uncertainty,"[67698, 78880, 58404, 72182]",853,"[100, 127]",3694,Reducing Food Waste,78,15,13,Secure & Sustainable Food Supply,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,15 [building - 116],"['OR in Sustainability', 'Robust Optimization']",WD-13
"Products [e.g., cattle and pig] can be processed into several types of products [parts] targeting different segments of customers, which belong to the so called coproducts. Mismatch risk, alliance stability and production efficiency are three significant issues in such coproduct logistics and supply chain. First, we use the unbalanced ratio to reflect the degree of mismatch between supply and demand among different parts of the coproduct and study how the tradeoff between the bargaining power and the mismatch cost, by different mismatch risk allocations, influences the optimal decisions and the performances of the two parties as well as the whole logistics and supply chain. Then, we investigate how the coproduct alliances are formed considering tradeoff between processing cost and mismatch ratio among heterogeneous retailers. We establish a general collaborative game model and study the existence of the core of the grand coalition and extend the model to the multi-retailer settings, and the general distribution with the cost-least consideration. Finally, in view of the product line design of the coproduct supply chain, considering the uncertainty of the market size, the size and length of the product line of the coproduct supply chain are analyzed and we expand to multiple products and general distribution of customer types and market size. We endeavor to provide some new insights into coproduct logistics and supply chain management at the theoretical level.
","Mismatch Risk, Alliance Stability and Production Efficiency in a Coproduct Supply Chain","[65630, 78885, 78882, 78884]",633,"[25, 72, 46]",3695,Retail Cooperation and Competition,30,13,61,Retail Operations,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S10 [building - 101],"['Decision Analysis', 'Mathematical Programming', 'Flexible Manufacturing Systems']",WB-61
"The fast growth of renewable energies increases the power congestion risk. To address this issue, the French Transmission System Operator [RTE] has developed closedloop controllers to handle congestion. To ensure their proper functioning, RTE wishes to estimate the probability that the controllers ensure the equipment’s safety. The naive approach to estimate this probability relies on simulating a large amount of randomly drawn scenarios, and use all the outcomes to build a confidence interval around the probability. 

Although theory ensures convergence, the computational cost of power system
simulations makes such a process intractable. The target of the present paper is to propose a faster process using machine-learning-based proxies. The amount of required simulations is greatly reduced thanks to an accuracy-aware proxy built with Multivariate Gaussian Processes. Using a proxy instead of the simulator however adds uncertainty to the outcomes. An
adaptation of the Central Limit Theorem is thus proposed to include the uncertainty of the outcomes predicted with the proxy into the confidence interval. As a case study, we designed a simple simulator that is tested on a small IEEE network. Results show that the proxy learns to accurately approximate the simulator’s answer, allowing a significant time gain for the machine-learning based process.
",Certification of MPC-based zonal controller security properties using accuracy-aware machine learning proxies,[59688],282,"[66, 150, 93]",3698,Advancements in energy system optimization and analysis tools,21,4,22,Energy Management,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,81 [building - 116],"['Machine Learning', 'Network Flows', 'OR in Energy']",MC-22
"Designing policies for managing the Biodiversity Climate Society [BCS] Nexus claims for a better understanding of the complex and dynamic interactions among its components and an effective engagement of the stakeholders. 
To this aim, a Participatory System Dynamic Modeling [PSDM] framework was adopted in BIOTRAILS project [https://biotrailsproject.eu/]. The PSDM framework is composed by several steps. This contribution focuses on the system mapping phase, whose scope is to map the interactions among the different elements of the BCS nexus, accounting for the stakeholders’ understandings and diversity of problem framings. Causal Loop Diagram was used to formalize and analyse the stakeholders’ knowledge. Methodological approaches based on Ambiguity Analysis and structural analysis were combined to identify the BCS nexus challenges and detect leverage points. Those elements will be, in a later stage of the project implementation, used to inform the stakeholders’ involvement in Nexus policy design. 
The developed methodology was experimentally implemented to the gold mining in Ghana, to analyse their impacts on biodiversity losses and the dynamic evolution of the BCS nexus. A PSDM exercise was held in Kumasi to map the BCS Nexus system. The preliminary analysis of the obtained CLD show how stakeholders perceived gold mining impacts on the local environment and socio-economic context, and allowed us to start the process for the Nexus policy design.
",Causal Loop Diagrams to address Nexus Biodiversity- Climate- Society governance challenges - insights from the gold mining in Ghana case study.,"[51066, 77859, 54579, 78888]",718,"[133, 78, 140]",3702,OR Innovations in Policy Making - B,26,7,13,Soft OR and Problem Structuring Methods,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,15 [building - 116],"['Soft OR', 'Natural Resources', 'System Dynamics and Theory']",TA-13
"In contemporary collaborative learning environments, characterized by intricate activities and numerous stakeholder interactions, there's a pressing need to gather insights from diverse sources to attain a comprehensive understanding of learning dynamics and students' behaviours. While Learning Management Systems serve as the cornerstone of these environments, their data analysis capabilities are often limited to basic metrics such as visit frequency, completion rates, and score statistics. Consequently, there's a high demand for specialized tools to support instructional designers. Decision support in the form of Learning Analytics, rooted in the domain of business intelligence, has emerged as a forefront area of research in technology-enhanced learning. In this paper, we present the application of predictive analytics within a multi-criteria dataset structure to categorize students based on their performance in collaborative activities. Our approach was deployed in a real-world scenario involving two cohorts of an undergraduate course conducted entirely online due to the pandemic. The first cohort comprised 337 students organized into 81 groups, while the second cohort consisted of 341 students divided into 76 groups. We applied various machine learning algorithms to determine the algorithm yielding optimal performance.",Application of Predictive Analytics in Assessing Student Performance - A Case Study in a Computer Supported Collaborative Learning Environment,"[78721, 23003, 78894, 78896, 78898, 78899]",66,"[34, 26]",3708,Emerging Trends in Decision Analysis,45,5,45,Decision Support Systems,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,30 [building - 324],"['Education and Distance Learning', 'Decision Support Systems']",MD-45
"As closed-loop supply chains [CLSCs] are gaining importance in many industries as key enablers of reduced waste and ecological footprint, new challenges arise in the integration of forward and reverse logistic flows. This research focuses on a variation of the production inventory problem with open routes and explores the benefits of vendor-managed inventory [VMI] systems for the reverse logistics operations of reusable transport items [RTIs]. A stochastic two-stage program considering uncertainty of return flows is developed and solved under use of sample average approximation [SAA] and relaxation-based bounds. We find that the benefits of vendor-managed inventory strategies are not only linked to lower operational cost but also lower stockout risk in long-term planning. Further, in hybrid networks of partially VMI and non-VMI nodes, the strategic choice for which nodes to implement VMI can positively impact the network performance. This application of VMI in reverse logistics has been scarcely explored in the field and provides new managerial insights for practitioners in the industry. ",Benefits of Vendor-Managed Inventory Strategies for the Reverse Logistics Operations in Closed-Loop Supply Chains,"[74986, 72509]",928,"[125, 117, 61]",3709,Information sharing in sustainable supply chains,18,15,23,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,82 [building - 116],"['Reverse Logistics / Remanufacturing', 'Programming, Stochastic', 'Inventory']",WD-23
"We introduce a new solution concept of [robust] least chance decisions for cooperative games under uncertainty and distributional ambiguity, which is motivated by the concept of least core solutions for deterministic cooperative games. We develop a framework to find those decisions and compute their [robust] least chance dissatisfaction for cooperative games under normally distributed uncertainty and moment-based distributional ambiguity. We demonstrate how the proposed framework can be applied to operations research games such as linear production games with detailed analytical results.",Operations Research Games under Uncertainty and Distributional Ambiguity,"[47560, 78908]",342,"[50, 127, 136]",3713,"Game Theory, Solutions and Structures III",88,4,36,"Game Theory, Solutions and Structures","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,32 [building - 306],"['Game Theory', 'Robust Optimization', 'Stochastic Optimization']",MC-36
"Behavioral OR [BOR] is positioned between hard and soft OR and provides analyses of human behavior in decision making so that it can be modeled with OR tools. Although behavioral analyses have always been integral to OR, scientific rigor and especially theory-based analyses are required to strengthen the research field. From our perspective, BOR bridges distinguishable world views and describes how behavioral dimensions influence OR problems and solutions. The goal of this research is to clarify the areas where BOR research can provide insights to increase scientific rigor. We provide a theoretical foundation that improves the positioning of BOR using the concept of “wickedness”. It structures typical OR problems according to their degree of wickedness that increases with the complexity of problems and the difficulty regarding stakeholders and institutions. Decision making in very wicked settings is grounded in belief functions that can differ between individuals due to insufficient knowledge or deviating goals of individuals involved. By identifying relevant assumptions from theories, this research aims to clarify beliefs so that they can be calculated in models. This analysis serves as a foundation to derive relevant types of theories that can be used from other fields [mainly from related fields e.g. psychology]. Moreover, novel specific OR theories are intended to be developed based on the adopted ones. Our work contributes to the understanding of BOR as a discipline.",The role of theory in behavioral operations research,"[27704, 53040, 37142, 45470, 54104]",105,"[10, 0]",3715,Behavioral OR general papers,13,8,11,Behavioural OR,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,12 [building - 116],['Behavioural OR'],TB-11
"We consider an optimal replacement problem for a mission-based system with partially observable degradation levels, where a mission process modulates the system's degradation process. The system is partially observed at predetermined time points, where perfect information related to the mission process [fully observable] is obtained, and only partial information related to the system's degradation level [partially observable] is obtained. We formulate the optimal replacement problem as a partially observable Markov decision process to minimize the long-run expected total discounted cost. We provide sufficient conditions to guarantee a control-limit type optimal replacement policy.",Optimal Replacement of Mission-Based Systems with Partially Observable Degradation Levels,"[78906, 539]",800,"[135, 136]",3716,Analysis of Stochastic Models II,50,15,39,Stochastic Modelling,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,35 [building - 306],"['Stochastic Models', 'Stochastic Optimization']",WD-39
"In this work, we address the parallel machine scheduling problem where processing times are stochastic. The objectives to maximize are the makespan service level, i.e. the probability that the makespan is smaller than a given threshold, and the due date service level, the [weighted or not] sum of the probabilities that the jobs are completed on time. Mathematical models for both objectives, that rely on the generation of scenarios and on positional binary variables, are presented. Heuristics are also proposed, and numerical results are discussed",Minimizing service levels in the stochastic parallel machine scheduling problem,"[78907, 16259]",401,"[14, 129, 136]",3717,Parallel Optimization and Scalability,64,13,52,Combinatorial Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,8003 [building - 202],"['Combinatorial Optimization', 'Scheduling', 'Stochastic Optimization']",WB-52
"In the landscape of blockchain consensus mechanisms, Proof of Stake [PoS] has emerged as a prominent alternative to traditional Proof of Work protocols, as evidenced by the consensus change of the second largest cryptocurrency, Ethereum. In PoS systems, validators are tasked with securing the network and validating transactions based on the amount of cryptocurrency they 'stake' as collateral. However, ensuring the integrity of PoS networks requires mechanisms to incentivise validators to behave honestly and to penalise malicious actors. Drawing on insights from economic theory, in particular principal-agent theory, this paper examines the role of liability in the PoS consensus, focusing in particular on the incentives created by voting mechanisms designed to enforce compliance. By developing a mathematical model and using an agent-based simulation approach, I analyse the impact of these mechanisms on validator behaviour and network security. My research highlights the importance of aligning incentives to deter undesirable actions while promoting network resilience and decentralisation. By comprehensively examining liability within blockchain consensus, this paper contributes to a deeper understanding of the dynamics at play in PoS systems. By elucidating the interplay between stakeholder incentives and protocol design, my findings provide valuable insights for the development and governance of blockchain networks in the pursuit of robust and secure decentralised ecosystems.",Liability in Blockchain Consensus - An Analysis of the Incentives Set by Proof of Stake Voting Mechanisms,[69805],564,"[3, 33, 131]",3719,Simulation in economics I,77,4,43,"Agent-based Models in Management, Economic and Organisation Sciences","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,99 [building - 306],"['Agent Systems', 'Economic Modeling', 'Simulation']",MC-43
"Providing care and treatment for people with disordered eating is a complex undertaking requiring clinical input from multiple disciplines. The psychological underpinning of the disorder involves mental health services, the physiological impact brings in multiple inpatient and outpatient care services and support is required from General Practitioners in the community. The Integrated Care Board in Devon, UK have initiated an integrated care pathway for people with Anorexia Nervosa. However, diagnoses of Anorexia Nervosa account for only 14% of disordered eating cases. This study sought to understand the current care provision for people with other eating disorders to inform the development of defined care pathways for people with eating disorders other than Anorexia Nervosa. A network-based operational modelling approach was employed to analyse, the care provision for people with disordered eating across Devon. This analysis was based on an integrated dataset containing data from the majority of NHS care providers in the geography. The most likely treatment pathways were simulated from the model to determine what the current provision of care looks like. We saw a high reliance on primary care services, community mental health teams and general internal medicine. The models and findings from this study are being further interrogated to inform the development of new care pathways to improve integrated care provision for this patient group.",Examining inter-organisational care provision for people with disordered eating,[45963],967,"[150, 7, 56]",3722,Integrated planning in healthcare,3,13,10,OR in Health Services [ORAHS],"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,11 [building - 116],"['Network Flows', 'Analytics and Data Science', 'Health Care']",WB-10
"Optimizing biomass supply chains necessitates the allocation of feedstock supply and biomass conversion facilities. This presentation introduces an innovative approach that combines Multi-Attribute Decision Making [MADM] with Multi-Objective Decision Making [MODM] to harmonize diseconomies of supply with technological economies of scale, thereby evaluating various biomass valorization pathways. Leveraging MADM and GIS-based location analysis, we establish a comprehensive location suitability criterion integrated into a two-fold linearized Mixed Integer Linear Programming [MILP] model, incorporating economic and ecological objectives. The multi-objective formulation is efficiently solved using the augmented epsilon constraint method. Finally, we employ the Technique for Order Preference by Similarity to an Ideal Solution [TOPSIS] method to assess the location decision of biomass conversion facilities and determine optimal designs for biomass supply chains. This approach not only estimates the potential for energetic or chemical biomass valorization in Southwest Germany but also facilitates strategic decision-making by integrating multi-criteria location analysis. By elucidating the specific methods employed and outlining potential outcomes, this research contributes to the advancement of location analysis methodologies in the context of biomass supply chain optimization.",Combining MADM and MODM for allocating biomass conversion plants in Southwest Germany ,"[47422, 2675]",769,"[64, 100, 112]",3724,Applications of Location Methods,29,12,61,Locational Analysis,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S10 [building - 101],"['Location', 'OR in Sustainability', 'Programming, Multi-Objective']",WA-61
"Originally introduced to allocate parliamentary seats among states or provinces, apportionment methods can distribute any scarce discrete resource based on a series of claims or budgets. Here, we focus on the price of representation - how one can turn the claims, such as the number of voters, into a share of the resource, such as the parliamentary seats. The D'Hondt method naturally emerges as a competitive equilibrium. We show that the class of parametric divisor methods is fully characterised by a competitive equilibrium augmented by a uniform credit or debt with respect to the original claims. 

While in a competitive equilibrium, all pay the same price, states or provinces are not equally efficient at converting their claims. Optimization apportionment methods such as the Leximin focus on minimizing price differences. We demonstrate how the D'Hondt and Adams methods can be formulated as optimization methods and highlight their connection to the Leximin method.","One Man, One Vote, One Price","[16474, 63046]",641,"[55, 59, 50]",3728,"Game Theory, Solutions and Structures VI",88,8,36,"Game Theory, Solutions and Structures","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,32 [building - 306],"['Group Decision Making and Negotiation', 'Industrial Optimization', 'Game Theory']",TB-36
"Risk-seeking behavior has been extensively documented across various research fields such as decision sciences, economics and finance, psychology. In this research, we propose a decision support model based on reverse second-order stochastic dominance [RSD] to accommodate incomplete risk-seeking preferences. Specifically, we establish dominance conditions according to the RSD criterion in discrete state-space. We then develop a stochastic optimization model that enables to identify an optimal decision alternative whose dominance over a pre-specified benchmark is robust for all risk-seeking decision makers. Furthermore, we demonstrate that RSD-based optimization model can be formulated as a mixed-integer linear programming problem to generate decision recommendations. The developed decision support model is well-suited to support data-driven decision analytics problems, including production and operations management, logistics and supply chain management, and healthcare management, particularly in the presence of incomplete preference information.",Decision Support Model for Incomplete Risk-Seeking Preferences,"[57704, 78914]",653,"[25, 136, 111]",3732,Integer Programming for Decision Support,45,9,45,Decision Support Systems,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,30 [building - 324],"['Decision Analysis', 'Stochastic Optimization', 'Programming, Mixed-Integer']",TC-45
"We introduce the two-echelon covering tour vehicle routing problem [2E-CTVRP] - a variant of the two-echelon vehicle routing problem to address the transportation and provision of essentials in humanitarian logistics. In the first echelon, a fleet of trucks is responsible for transporting supplies and drones from the depot to satellites located near the affected area. The drones then take on last-mile delivery from the satellites to the demand cluster centroids in the affected area. A clustering algorithm is employed to group the victim location and determine the delivery points within the clusters. This study formulates the 2E-CTVRP as a Mixed Integer Linear Program [MILP] with the objective of minimizing the sum of arrival time of all trucks. A hybrid metaheuristic, called the Greedy Randomized Adaptive Search Procedure with evolutionary path relinking [GRASP-EvPR], is proposed to solve larger instances within reasonable computational time. 
The numerical results show that GRASP-EvPR can effectively handle the proposed instances in terms of solution quality and computational time. Furthermore, this study compares the effects of different truck fleet types on three dimensions of humanitarian logistics - efficiency, efficacy, and equity.",GRASP reinforced by Evolutionary Path-relinking for two-echelon covering tour vehicle routing problem,"[78915, 3051, 22084]",879,"[58, 145, 74]",3733,Combinatorial optimization issues in transportation [Contributed],64,15,26,Combinatorial Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,012 [building - 208],"['Humanitarian Applications', 'Vehicle Routing', 'Metaheuristics']",WD-26
"Benefit of the Doubt [BoD] models are increasingly popular in the literature with flexibility around technology assumptions and ease of understanding for managers and practitioners. Most of the focus has been on the efficiency scores with little attention paid to benchmarking in terms of target values and the appropriateness of peers. Specifically, there are problems that arise when weight restrictions are imposed, particularly the effects on efficient frontiers, further compounded when virtual weight restrictions are used. These affect target values and benchmarking peers with nonintuitive effects as the lower bounds on the restrictions are increased. 
A further question concerns how size can be incorporated to allow for scale economies. Further issues arise with respect to units-invariance properties for output oriented models. 
We illustrate these issues using sustainability data comprising GRI measures and provide some thoughts towards addressing these problems.
",SOME PROBLEMS WITH BENEFIT OF THE DOUBT MODELS,"[62311, 29959, 78916]",946,"[24, 0]",3735,DEA methodological developments II,89,15,48,Data Envelopment Analysis and its Application,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,60 [building - 324],['Data Envelopment Analysis'],WD-48
"Presently, the domestic aviation sector in New Zealand emits approximately 825 kilotons of CO2 equivalents, positioning the country among the top 10 OECD nations with the highest per capita CO2 emissions from domestic flights. Therefore, transitioning away from conventional fossil fuels to more sustainable alternatives, notably Sustainable Aviation Fuels [SAF], is inevitable. Enabling an effective transition to SAFs requires understanding the share of different SAFs to meet future demand, the timing and location of production plants, fuel transport between different regions and airports, the resource use for SAF production, and all associated economic investments. To address this gap, we present a mixed-integer linear programming model to support the development of SAFs in New Zealand. The proposed framework optimizes the SAF supply chain, considering different production processes, to provide an evidence base to support the transition towards SAF integration. In this first case study, we consider the domestic aviation industry and domestic SAF production using indigenous resources. Ultimately, integrating these fuels will lead to decarbonization in the aviation sector and contribute to the country’s environmental and climate change goals.",Green Skies Ahead - Streamlining the Sustainable Aviation Fuel Supply Chain in New Zealand,"[78924, 73715, 74115, 74116, 2650, 72344]",843,"[138, 37, 4]",3736,Towards sustainable development,23,12,19,OR in Energy,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,44 [building - 116],"['Supply Chain Management', 'Energy Policy and Planning', 'Airline Applications']",WA-19
"Practitioners using problem structuring methods [PSMs] help participants in groups make sense of a situation they consider to be problematic and find consensus positions that enable action to be taken to improve the situation. Many approaches encourage participants to co-create models by picking up pens or markers and getting involved. Less attention is paid to how participants or practitioners listen to different perspectives, integrate different points of view and document findings. Listening is more than a single ability and can be conceptualised in different ways such as listening as information processing, listening as comprehension, and listening as retention, among others. This paper introduces listening using Rich Notes, a novel approach to note taking and diagramming during meetings that uses a non-linear, non-hierarchical, unconstrained and distributed layout that documents what participants say, recognises interdependencies and generates options for action. Rich Notes has been developed within an action research programme consisting of over 300 real-world participant interactions. ",Listening to act - Using Rich Notes for problem structuring in small groups,[69057],130,"[149, 0]",3737,Methodological Developments in Soft OR and PSMs,26,5,13,Soft OR and Problem Structuring Methods,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,15 [building - 116],['Problem Structuring'],MD-13
"This work considers the minimum sum-of-squares clustering [MSSC] problem, which has an intrinsic bi-level nature and the significant characteristic of being nondifferentiable.
To overcome these difficulties, the proposed method adopts a smoothing strategy, which engenders an unconstrained completely differentiable single-level alternative.
The proposed algorithm also applies a partition of the set of observations into two non-overlapping groups - data in frontier and data in gravitational regions.
The article introduces a novel incremental procedure to produce starting points in an iterative way, which begins with only one centroid, and which adds one cluster centroid at a time.
To show the distinct performance of the new algorithm, we perform numerical experiments with a set of thirteen large test problems from the literature.
In short, except for a few solutions, numerical results show a high level of performance of the IncAHSCM algorithm according to different criteria of accuracy and speed, in compassion with the best-established clustering algorithms. 
. The accuracy performance can be attributed to the complete differentiability offered by the hyperbolic smoothing approach, as well as, by the strategy to get good initial starting point.
The high speed of the algorithm can be attributed to the partition of the set of observations into two non-overlapping parts,  which drastically simplify the computational tasks.
",Incremental Accelerated Hyperbolic Smoothing Clustering Method - Computational Results,"[1905, 78911]",678,"[52, 81, 63]",3739,Machine Learning and Ensemble Learning with optimization methods,15,14,27,Mathematical Optimization for XAI,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,047 [building - 208],"['Global Optimization', 'Non-smooth Optimization', 'Large Scale Optimization']",WC-27
"An Euclidean Laman graph is a geometric graph on n vertices in the
plane with exactly 2n − 3 edges such that, for all k > 1, every
k-vertex subgraph has at most 2k − 3 edges. In this study, we
introduce a variant of the well-known Euclidean Steiner tree problem,
say the Euclidean Steiner Laman graph problem [ESL for short]. Given a
point set P in the plane, the goal is to find an Euclidean Laman graph
of minimum total length such that any point in P is contained as one
of its vertices. We also consider the problem of finding a plane
[i.e., non-crossing] solution under the same setting, say the
Euclidean Steiner plane Laman graph problem [ESPL for short].

First, we prove an optimal solution for ESL has no crossing, i.e., an optimal solution for ESPL is also optimal for ESL. Then, we design a polynomial time approximation algorithm that outputs a plane Laman graph and guarantees an approximation ratio at most asymptotically 1.5+ Ɛ with an arbitrarily positive Ɛ. ",On Minimum Euclidean Steiner Laman Graphs,"[3297, 78810, 45244, 42276, 78856, 78807, 78802, 78859, 59781, 78824, 78809]",202,"[14, 79, 53]",3741,Applications of combinatorial optimization I,64,7,25,Combinatorial Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,011 [building - 208],"['Combinatorial Optimization', 'Network Design', 'Graphs and Networks']",TA-25
"Industry 5.0 promotes human-centric industries. Human-robot collaboration is a rapidly developing production mode. By combining the strengths of humans and robots, it not only improves production efficiency but also reduces ergonomic risks for workers. We study how to schedule workers and robots in a collaborative assembly line. We formulate a mixed integer programming model and propose an adaptive simulated annealing algorithm. We compare our algorithm with genetic algorithm and artificial bee colony algorithm. The experimental results show that our algorithm outperforms the others. Furthermore, we analyze the impact of the human-machine collaboration on the assembly line balancing plan. We find that the fairness of workload distribution among workers may deteriorate due to human-robot collaboration, which is something to be aware of.",Scheduling of workers and robots in collaborative assembly lines,[78927],808,"[129, 74]",3742,Machine scheduling problems,32,14,49,"Lot Sizing, Lot Scheduling and Production Planning","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,M1 [building - 101],"['Scheduling', 'Metaheuristics']",WC-49
"The introduction of foldable containers is anticipated to reduce the number of vehicles required for repositioning empty containers. However, when using foldable containers, folding time [FT] is necessary in addition to waiting time [WT] until the same number of containers as the stacking tier accumulates. These factors could potentially decrease the operational efficiency of vehicles and containers. Therefore, this study focuses on the relationship between FT, WT, and the operational status of an inland depot, employing a stochastic simulation approach to verify how efficiently foldable containers can facilitate empty container repositioning by vehicles. The results of numerical experiments reveal that the variation in FT duration significantly impacts the economic viability of foldable containers.",Impact of Foldable Container Implementation on Vehicle Operations - Focusing on Folding Time,"[16234, 78930, 78931, 16227]",627,"[65, 143, 131]",3744,Freight transportation and logistic II,6,9,55,Transportation,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S02 [building - 101],"['Logistics', 'Transportation', 'Simulation']",TC-55
"Post-pandemic logistics showed that just-in-time was not always the most successful supply alternative. The effect in the supply chain was a disruption, causing scarcity of material, delays in deliveries, and capacity constraints in transportation. This paper proposes to answer whether the amount of data or high level of uncertainty exist. The ability of models created with large amounts of data may be not necessarily the answer. The proposal is a novel alternative forecast model when the amount of data is not available [< 4 records] and integrates the past demand as a Markov Chain and offers a model's characterization to discriminate a set of data that is suitable for the Grey Systems Theory and a Markov Chain Grey Model MCGM [1,1]. The model - 1] incorporates the past demand behavior, 2] establish the transition probability matrix, 3] defines enhanced forecast model through the GST GM [1,1], 4] develops the forecast, 5] measure its performance. The performance measurement was MAPE. The results exceed other models and methodologies such as - exponential smoothing, moving average and linear regression. The MAPE with the traditional methods was 6.58% vs. the MCGM [1,1] was 2.28% and provided a forecast range comparable to ARIMA.

","Grey systems theory and a Markov chain model MCGM [1,1] applied to demand forecast for a 3PL provider","[73813, 45744]",860,"[138, 47, 65]",3746,"Discrete, continuous or stochastic optimization and control in networks, transportation and design IV",64,5,25,Combinatorial Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,011 [building - 208],"['Supply Chain Management', 'Forecasting', 'Logistics']",MD-25
"A Miniratna company of the Government of India, under the Ministry of Textiles [ministry, henceforth] has been serving in the field of marketing of cotton for last five decades. As a sovereign function of Government of India, the organization undertakes Minimum Support Price [MSP] operations to safeguard the cotton farmers from any distress sale and offers its cotton stocks through e-auction on regular basis to meet out the demand of quality cotton to domestic textile Industry at competitive rates. In this paper, an objective model to decide the variety-wise optimal floor price of daily e-auction of cotton is developed and presented. Before the model was proposed and developed, a team of subject matter experts and top management of of the organization considered the [i] variety-wise prevailing prices of cotton in Indian market, [ii] general price behaviour of international market and [iii] previous days’ variety-wise sale rates over & above the floor price and decided the floor price for daily auction. The organization currently uses the optimal floor pricing model to scientifically determine the floor price for the 15 odd cotton varieties grown in India and traded globally in the e-auction mode.    

",Optimal floor price for e-auction of a seasonal crop,[46209],543,"[89, 151, 149]",3747,Facilities Routing and Planning in Developing Countries,67,14,18,OR for Development and Developing Countries,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,42 [building - 116],"['OR in Agriculture', 'Practice of OR', 'Problem Structuring']",WC-18
"This study addresses a problem of scheduling automated guided vehicles [AGVs], in which a fleet of heterogenous AGVs are used to deliver materials in a flexible manufacturing environment. During the transportation process, the batteries of AGVs can be recharged so that materials are delivered timely to various workstations without interruptions. A partial charging policy which enables flexible management of charging operations is applied.  By effectively scheduling transfer jobs and potential charging operations on the AGVs, the objective is to minimize the total weighted tardiness. A mixed integer programming formulation is presented to describe the scheduling problem. A hybrid approach combining variable neighborhood search [VNS] and linear programming is developed to solve this NP-hard problem. VNS is firstly used to schedule transfer jobs on the AGV fleet without considering charging operations, based on which a linear programming model is formulated to determine the best schedule of charging operations on each AGV. Numerical experiments show that the hybrid solution procedure can solve the AGV scheduling problem effectively and efficiently.",Scheduling AGVs based on partial charging policy to minimize the total tardiness,"[78933, 28027]",625,"[129, 111, 74]",3751,Advancing mobility towards sustainable solutions IV,6,13,56,Transportation,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S04 [building - 101],"['Scheduling', 'Programming, Mixed-Integer', 'Metaheuristics']",WB-56
"This study explores the impact of government contractors’ supply chain characteristics on contractors' financial reporting decisions related to risk, cost, and compliance. Supply chain contracts are economically important and subject to substantial risks, as shown by reporting of supply chain risks in firms’ Item 1A disclosures; however the exact influence of supply chain risks on financial reporting outcomes remains unclear due to the lack of publicly available contract details for commercial supply chains. To overcome this issue, I use detailed data from US government contracts. 
Government contractors an institutionally interesting setting for studying supply chain contract characteristics because they operate within complex bureaucratic frameworks and accounting guidelines, which require robust financial operations to remain in compliance with regulations. Government contracts also account for approximately 13% of public firms' major customers.
Given the importance of government contracts and the detailed contract data that is available, this study investigates how supply chain contract characteristics influence financial reporting decisions. Specifically, leveraging data from the Federal Procurement Data System, I analyse the impact of these characteristics on financial reporting outcomes, including how managers identify, assess, and mitigate risks associated with government contracts; track and control contract costs; and manage audit risk and control weaknesses.",The Government Contracting Supply Chain and Financial Reporting ,[78480],255,"[1, 138, 126]",3752,"OR in Accounting - Planning, Taxation, and Reporting",7,13,59,OR in Financial and Management Accounting,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S08 [building - 101],"['Accounting', 'Supply Chain Management', 'Risk Analysis and Management']",WB-59
"In the case of traditional neural style transfer, the aim is to transfer the style embedded within an image, song, or literary work to that of another, e.g. to transfer the art style of Van Gogh’s Starry Night to another image. Contemporary approaches have demonstrated notable efficacy in respect of capturing and translating embedded properties within complex data representations. In this study, an investigation is carried out into the application of neural style transfer to optimisation algorithms. More specifically, the extent to which neural style transfer can capture and transfer salient features of high-quality solutions to other solutions is investigated. The proposed approach entails various modifications to different facets of conventional neural style transfer, such as data set construction, solution encoding, neural architecture, and loss functions. Different benchmark optimisation problems are considered in respect of evaluating the utility of the proposed approach.",Neural style transfer inspired optimisation ,[61651],215,"[66, 74, 5]",3754,Applications of Machine Learning in Optimization,64,10,25,Combinatorial Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,011 [building - 208],"['Machine Learning', 'Metaheuristics', 'Algorithms']",TD-25
"Pointwise ranking models diverge from conventional recommendation systems by tailoring item rankings directly to individual user preferences, thereby offering nuanced recommendations aligned with user tastes. However, overlooking pairwise, listwise and position based factors in ranking output can diminish the overall user experience. To obtain user-specific rankings while considering the whole ranked list, we propose a framework that synergistically leverages machine learning and optimization techniques. Focused on personalizing product recommendations within the Trendyol app, our approach integrates xGBoost Classifier and integer programming to derive item scores and optimize rankings while addressing diversity concerns across categories and brands. Our framework starts with using nearly 200 features to learn user action probabilities. Then, an integer programming model determines rankings of products by maximizing expected purchases while ensuring diversification. This process operates in near real-time, updating personalized rankings every 2-3 minutes for 150,000 users. To tackle scalability challenges, we propose an alternative formulation and decomposition algorithm. Lastly, we introduce an ML-based multi-objective optimization framework to extend our focus to other performance metrics such as position-based utilities and clicks, alongside purchases. Rigorously tested via our effective AB testing system, we comprehensively analyze and discuss our results.",From Personalization to Optimization - A Framework for Enhanced Product Recommendations,"[78890, 77668, 52407, 57737, 79058]",528,"[32, 66, 109]",3756,E-Commerce,30,14,50,Retail Operations,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,M2 [building - 101],"['E-Commerce', 'Machine Learning', 'Programming, Integer']",WC-50
"Transfer learning methods for remaining useful life [RUL] prediction of rolling bearings have rarely focused on online feature drift scenario. Bearing degradation trends are categorized into several states, as normal, slight degradation and severe degradation states, each with distinct feature distributions. During the degradation process, changes in state lead to online feature drift, which reduces the accuracy of RUL prediction. To overcome this challenge, this study suggests aligning feature distributions for each state using state-awareness to adapt to online domain shifts. State awareness is achieved by deriving the Health Index [HI] based on the Wasserstein distance, applying smoothing techniques, and statistically detecting the First Prediction Time [FPT] and acceleration point. This provides state label during online degradation process. The online self-supervised domain adaptation model leverages the current degradation data as target, with the source domain comprising previous run-to-fail data. Aligning feature distributions between these domains and states enhances RUL prediction accuracy by extracting domain-invariant and domain-specific features, as well as state-invariant and state-specific features. Experimental validation using IEEE PHM Challenge 2012 bearing data demonstrates the effectiveness of the proposed methodology in achieving state awareness and accurate RUL prediction through online self-supervised domain adaptation in dynamic operating environments.",State-aware online self-supervised domain adaptation for bearing remaining useful life prediction,"[76913, 25193]",713,"[123, 62, 66]",3758,Experimental economics and game theory 3,73,15,40,Experimental economics and game theory,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,96 [building - 306],"['Reliability', 'Knowledge Engineering and Management', 'Machine Learning']",WD-40
"Electric carsharing systems allow individuals to borrow electric cars on a short-term basis for a rental rate charged by the time or the distance driven. These systems are seen as a promising way to both lower the CO2 emissions linked to urban mobility and reduce urban congestion through a decrease in the number of privately owned cars parked in cities.  We focus on the optimal long-term design of a station-based carsharing system in which shared cars should be picked-up and dropped off by customers at stations. More precisely, we seek to determine the number of chargers to build in each station and the size of the vehicle fleet to deploy in the network. One major difficulty here is that to be practically relevant, the modelling of this strategic problem should be able to estimate the future revenue that one may get by operating the proposed system on a daily basis.  Our model should thus consider operational decisions such as trip request acceptance, car dispatching to stations, intra-day car charging and relocation. Furthermore, in practice, uncertainties on the trip duration and energy consumption may prevent cars from being available when and where needed with a sufficient battery state of charge.  To handle this difficulty, we first propose a new scenario-based two-stage stochastic programming approach. We then seek to assess the value of a stochastic solution by discussing numerical results carried out on randomly generated instances. ",Optimal design of an electric carsharing system under uncertain trip duration and energy consumption,"[29574, 78940]",612,"[143, 64, 117]",3762,Optimization of sustainable urban mobility,79,7,18,Sustainable Cities,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,42 [building - 116],"['Transportation', 'Location', 'Programming, Stochastic']",TA-18
"In the transformation towards sustainable transportation, a good public transport offer is a crucial element. However, in rural areas, public transport providers [PTPs] face challenges in introducing a good offer using classical line-based services within a restricted budget due to the low demand. One potential solution is on-demand buses, which operate on flexible schedules and routes that depend on the actual demand. Still, the low demand in these regions remains a challenge to PTPs. They want to maximize the served passengers but are forced to accept or reject customer requests immediately within the dynamic booking process and without knowing the true impact of the total number of server passengers due to potential future requests. This raises the question of predicting the customer's fit within a passenger maximizing dial-a-ride problem [PM-DARP].

To address this problem, we introduce the request fit predictor [RFP] framework that allows for fast decisions about the acceptance or rejection of customer requests. Within this framework, we model the request fit as a binary classification task and learn the fit with supervised machine-learning models from a static PM-DARP using historical data.

The framework is tested on real-life data, and the results show that the PTP can serve 7.48% more passengers using the RFP compared to a cheapest insertion benchmark and more than 25% more than the status quo. As a side effect, we also reduce the driven distance by 6.68%.",A Supervised Machine Learning Framework to Predict the Request Fit for Dynamic Dial-a-Ride Problems,"[39359, 78942, 59666]",784,"[143, 66, 111]",3763,Dynamic Vehicle Routing 2,5,10,64,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S16 [building - 101],"['Transportation', 'Machine Learning', 'Programming, Mixed-Integer']",TD-64
"Quantum search, spurred by Grover's algorithm in 1996, has long held promise in quantum computing for its quadratic speedup over classical algorithms in unsorted set searches. However, demonstrating a quantum advantage on finding the best solution to a combinatorial problems is not straightforward due to the existence of efficient classical heuristics, unsufficient speedup and inherent quantum noise. In this study, we investigate a novel approach - constructing a superposition of states of reduced size by leveraging problem structure before employing quantum search algorithms. We apply this methodology to a multi-machine job scheduling problem characterized by specific time window structures, local job constraints within machines, and resource limitations. Our objective is to minimize costs associated with job dates, aiming to identify solutions below a predefined threshold. This model is particularly suited for multi-year planning in power plant maintenance, where machines represent production units and jobs signify maintenance outages for specific units. We exploit the unique structure of spacing constraints between jobs and structured time windows to create a superposition of state subspaces. Subsequently, we apply amplitude amplification by designing two oracles to account for resource constraints and the cost function. This innovative approach holds promise for improving the efficiency and effectiveness of quantum optimization techniques in real-world applications.",A quantum search algorithm exploiting specific time windows structure. Application to production units planning problems.,[78909],375,"[5, 53]",3765,Decomposition methods for Quantum Optimization,83,3,42,Quantum Computing Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,98 [building - 306],"['Algorithms', 'Graphs and Networks']",MB-42
"Urban population growth and the expansion of e-commerce have driven up the volume of deliveries, the need for faster shipping services, and a greater occurrence of delivery failures in cities. Consequently, there has been a noticeable increase in the number of trips made within urban areas to meet these demands, which intensifies the burden of freight transport. Driven by these considerations, we collaborate with local authorities in Bergen, Norway to analyse the impacts of freight transport on city liveability. In this work, we analyse the adoption of parcel lockers to provide authorities with insights on how the city could benefit from them as a sustainable delivery solution. By adopting clustering and vehicle routing models of freight carriers at a strategic level, we offer guidance to authorities on essential factors for improving city liveability through the utilization of parcel lockers. In this regard, we use data on the road network and population from local authorities, as well as the distribution data from a major carrier in Norway. ",Guiding Local Authorities in Parcel Locker Adoption - A Case Study in Norway,"[73605, 3393]",956,"[100, 145, 65]",3767,Optimization of sustainable urban mobiltiy II,79,8,18,Sustainable Cities,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,42 [building - 116],"['OR in Sustainability', 'Vehicle Routing', 'Logistics']",TB-18
"Personnel selection in competitive two-sided markets presents challenges for organizations seeking to attract high-quality candidates while managing costs. However, existing models often overlook the optimization of job-offer packages to address both candidate preferences and organizational objectives. This paper addresses this research gap by proposing a novel mathematical model for determining the optimal job offer package. Our model integrates candidate preferences and organizational costs, offering a comprehensive solution to the selection process. Utilizing a two-stage Mixed-Integer Linear Programming [MILP] approach, we employ the Gale and Shapley algorithm to facilitate efficient matching. Through a small case study, we validate the effectiveness of our model in providing actionable insights for organizations seeking to enhance their hiring processes and attract top talent.",Optimizing Job Offer Packages - How Can Organizations Enhance Personnel Selection?,"[78778, 78945, 78943, 78946]",658,"[25, 57, 144]",3769,Decision Support for Operations Management,45,12,45,Decision Support Systems,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,30 [building - 324],"['Decision Analysis', 'Human Resources Management', 'Utility Systems']",WA-45
"In today's competitive market, characterized by demanding levels of product customization and variant production, the significance of a reliable and agile logistics system cannot be overstated in promptly meeting material demands at production lines. However, the uncertainties inherent in the production process and unplanned changes to the production plan pose significant challenges to achieving optimal in-plant material delivery for logistics planners.
This study proposes a solution comprising a mathematical optimization model and a simulation-based optimization approach to assist material logistics planners in manufacturing companies in swiftly adjusting material delivery to accommodate changes in the production plan. The effectiveness of the proposed solution is validated through the resolution of a real case problem, demonstrating its practical applicability and benefits in modern manufacturing environments.",Optimizing Material Logistics in Dynamic Production Settings,"[78936, 78984]",760,"[65, 74, 111]",3770,Logistics 1,5,13,58,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S07 [building - 101],"['Logistics', 'Metaheuristics', 'Programming, Mixed-Integer']",WB-58
"Fares for public transport usage affect the number of people traveling by public transport, the passenger satisfaction and they are additionally important for covering the costs of the public transport operator. Thus, fare design is an important step of public transport planning. In our work, we focus on the design of flat, affine distance and zone fares. In practice, the new fares should often be similar to former fares while having a specific structure, e.g., a zone tariff. The old prices are called “reference prices”. As the objective function, we hence concern the sum of absolute deviations from reference prices. We show that the fare design problems are closely related to median problems. Further, we analyze the complexity of such problems, which ranges from linear solvability for flat fares to NP-completeness for zone fares. The problems can be solved by formulating them as linear programs or with solution methods based on solving known median problems. As an additional aspect for zone tariff design problems, we concern monotonicity of the prices as a desirable requirement.",Fare design for public transport - minimizing the sum of absolute deviations from reference prices,"[69300, 1601]",824,"[84, 119, 110]",3773,Transit,85,15,51,Public Transport Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,M5 [building - 101],"['Optimization Modeling', 'Public Local Transportation Systems', 'Programming, Linear']",WD-51
"Aircraft engine maintenance, repair, and overhaul [MRO] exemplifies a closed-loop remanufacturing system in which all components are recovered. As a critical process ensuring aircraft safety and reliability, MRO faces significant challenges due to the inherent uncertainty in maintenance workloads and the stochastic nature of the process. Aircraft engines contain life-limited parts, replaced at predetermined intervals, and on-condition parts, which are inspected during each maintenance visit and replaced as needed. The presence of on-condition components introduces additional uncertainty, as the full scope of required maintenance is only known after disassembly and inspection.  Consequently, effective buffer allocation between the disassembly, repair, and reassembly stages is crucial for absorbing this variability. To optimize buffer allocation in this stochastic environment, this study employed discrete-event simulation to model the detailed MRO process. A multi-objective meta-heuristic algorithm was then applied to identify near-optimal buffer allocations that simultaneously maximize engine inter-arrival rates and minimize work-in-process. The results demonstrate that strategically designed buffers, particularly between major process stages, can significantly enhance performance in the face of uncertainty inherent to MRO operations. This simulation-based optimization approach offers valuable insights for managing complex remanufacturing systems such as aircraft engine MRO.","Buffer Allocation in Remanufacturing Systems and its Applications in Aircraft Engine Maintenance, Repair, and Overhaul Industries","[78669, 78936, 79309]",924,"[14, 131, 69]",3774,Remanufacturing and refurbishing operations,18,10,23,"Circular Economy, Remanufacturing and Recycling ","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,82 [building - 116],"['Combinatorial Optimization', 'Simulation', 'Manufacturing']",TD-23
"The flexible job shop scheduling problem [FJSSP] is a complex combinatorial optimization CO problem [COP] in manufacturing, involving the allocation of operations across multiple machines while considering varying processing requirements and machine capabilities. In contrast to normal job shop scheduling operations in the FJSSP may be processed on multiple machines with different processing times, introducing additional complexity. 
Current deep reinforcement learning implementations for the FJSSP make use of graph neural networks, which are limited to local message passing and therefore underperform compared to the attention model [AM] on various COPs. Especially for the FJSSP with complex inter and intra node relationships, the self-attention mechanism of the AM could enhance feature representations. However, the AM is defined for problems with a single entity type, like customer nodes in the TSP, whereas machines and operations of the FJSSP pose different node types in a complex heterogeneous graph. Therefore, this work proposes an extension of the AM, integrating self- and cross-attention blocks to allow for message passing between all nodes and thus to capture inter and intra node relationships. Further, we present a factorized action-space formulation for the FJSSP, which – other than the composite action-space used in current implementations – does not grow quadratically. Consequently, a better sampling efficiency and scalability of our approach is to be expected.",AM4FJSSP - An Attention Model formulation for the Flexible Job Shop Scheduling Problem,[27073],732,"[14, 46]",3775,	[Deep] Reinforcement Learning for Combinatorial Optimization 3,14,7,03,Data Science Meets Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,1005 [building - 202],"['Combinatorial Optimization', 'Flexible Manufacturing Systems']",TA-03
"Containerized shipping has recently been deeply affected by events such as Covid-19 pandemic and regional wars. These events led to unexpected slumps or surges in demand, hampered cargo handling capacity of ports, and compelled liners to streamline their services to mitigate operational difficulties and logistical hurdles. In this paper, we address the design of a hub-and-spoke container shipping network by assuming that the values of origin-destination cargo demands and port capacities are imprecise. The imprecision in these parameters is represented with trapezoidal possibility distributions. The problem involves the location of transshipment hub ports, allocation of non-hub ports, and routing of cargo flows in the resulting network so as to minimize the expected network cost. We formulate the problem as a possibilistic mixed integer linear programming model and convert it to a chance constraint model using a fuzzy measure. To solve the chance constraint model, we develop a Tabu Search algorithm. In addition, we design an upper bound procedure that decomposes the problem into three subproblems and solves them iteratively. The computational experiments using the data of 50 ports in the Mediterranean show that the proposed solution method finds optimal or near optimal solutions in a very short computational time. In addition, analysis of the resulting networks provides important insights into the impact of uncertainty on the network cost and number of hubs.  ",Hub-and-spoke container shipping network design under uncertainty,"[55866, 2465, 12970]",627,"[143, 64, 65]",3778,Freight transportation and logistic II,6,9,55,Transportation,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S02 [building - 101],"['Transportation', 'Location', 'Logistics']",TC-55
"In electricity markets, participants employ diverse strategies when submitting their bids. From a realistic perspective, it can be considered that participants aim to increase profitability and thus maintain their market presence. However, unlike profit maximization, participants' bidding behaviors are shaped by considerations like available capacity, market prices, market risk, etc. This research aims to investigate the bidding behaviors of electricity market participants using Inverse Reinforcement Learning [IRL]. IRL aims to identify an objective function [reward function] from the historical bidding behaviors of an optimally behaving market participant. In this study, different IRL structures are utilized for linear or nonlinear reward function assumptions. In addition, the bidding behaviors of power plants using different energy sources, such as wind, gas, coal, etc., are comparatively analyzed. The research relies on data sourced from the Australian electricity market.",Bidding Behavior Analysis in Electricity Markets Using Inverse Reinforcement Learning,"[68170, 63048, 78760]",644,"[36, 66]",3779,Electricity Market Design,22,3,09,Energy Markets,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,10 [building - 116],"['Electricity Markets', 'Machine Learning']",MB-09
"Driven by environmental pollution concerns and legislative pressure, an increasing number of Industry 5.0 initiatives and impact startups have recently emerged, focusing on establishing circular economy business models. One emerging industry is combating plastic waste through innovative reusable packaging systems.

In this study, we examine the case of the German startup Vytal, which offers a deposit-free reusable packaging system for take-out food, utilizing serialized containers. We develop a forecasting framework to estimate the distribution of net demand, consisting of container issues and returns. Accurate net demand forecasts are critical for optimizing asset utilization and inventory management of reusables. We propose a forecasting framework based on a pool of distributed lag models for estimating container issues and returns. Each model exploits the relationship between container issues and returns to make accurate predictions, and we combine them into an optimal model using an adaptive Markov Chain Monte Carlo algorithm. Furthermore, we examine several extensions of our basic model and analyze the benefits of including additional covariates, as well as leveraging cross-location forecasts into our model.

We use a unique dataset from Vytal to demonstrate that our proposed forecasting framework outperforms existing models and algorithms from both the literature and our case company in terms of forecast accuracy and computational inference efficiency.
",The Analytical Edge in Circular Economy -  Predictive Analytics for Reusable Packaging Success,"[78925, 23418]",428,"[47, 125, 66]",3780,Retail Analytics,30,7,50,Retail Operations,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,M2 [building - 101],"['Forecasting', 'Reverse Logistics / Remanufacturing', 'Machine Learning']",TA-50
"This study investigates a congested hub location problem with the aim of optimizing a service provider's profit by determining hub locations, capacities, and prices for serving commodities. The study incorporates a price-dependent demand function with a general functional form and models hub operations as M/G/1 queueing systems to address congestion effects. The problem is initially formulated as a mixed-integer nonlinear program, which is subsequently transformed into a mixed-integer second-order cone program, demonstrating solvability with standard off-the-shelf solvers. Through numerical analysis, the study evaluates the influence of different demand functional forms and service time constraints on the optimal network configuration and the service provider's profitability.",Congested Hub Location Problem with Price-dependent Demand,"[46195, 78951]",766,"[64, 121, 111]",3782,Nonlinear Location Optimization,29,8,61,Locational Analysis,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S10 [building - 101],"['Location', 'Queuing Systems', 'Programming, Mixed-Integer']",TB-61
"With the current economic and energy situation, it's crucial for large electricity consumers to thoughtfully consider their electricity options. This work aims to determine the optimal approach for cement manufacturers to decide their electricity procurement sources. The available options include grid electricity purchase, bilateral contracts, and the utilization of a solar photovoltaic installation and batteries. Furthermore, we analyze the flexibility in electricity consumption by modeling all stages of cement production. The proposed approach is   a mid-term decision-making tool considering different sources of uncertainties, formulated through a two-stage stochastic programming approach. To validate our approach, we have conducted several case studies based on a real cement producer based in Spain.",Modeling electricity consumption flexibility to determine the optimal medium-term electricity procurement for cement producers,"[18498, 71270, 67439]",869,"[36, 136, 12]",3784,Flexibility in future energy systems,22,3,14,Energy Markets,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,16 [building - 116],"['Electricity Markets', 'Stochastic Optimization', 'Capacity Planning']",MB-14
"In railways, Temporary Capacity Restrictions [TCRs], such as track closures and speed reductions, can make it very difficult to operate trains according to the timetable. Depending on the severity of the TCR, the timetable might require relatively simple repairing actions [e.g., retiming, resequencing] or major modifications [e.g., cancellations, short-turning, rerouting]. In general, the objective for the new timetable is to be as similar as possible to the original, but the precise definition is based on opinions and experience from the route planners, making it difficult to formalize. In contrast to most of the current literature that tries to approximate this objective, we propose the following iterative process - 1] route planners use an interactive GUI to sketch a new [potentially infeasible] reference timetable, and 2] mathematical programming is used to find a feasible timetable that is closest to the reference one. Step 1 and 2 can be repeated until the route planners are satisfied. We call this method Incremental Timetabling, to highlight the fact that route planners can iteratively steer the timetable towards the desired one, while our model simply takes care of its feasibility. We present an application of this procedure to parts of the Norwegian and Italian Railways.",Incremental timetabling for handling Temporary Capacity Restrictions,"[62586, 37212, 78974, 3763]",182,"[143, 142, 111]",3788,Europe's Rail MOTIONAL - Algorithms for railway planning,85,5,54,Public Transport Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S01 [building - 101],"['Transportation', 'Timetabling', 'Programming, Mixed-Integer']",MD-54
"In recent years we have witnessed various supply chain disruptions that severly affect the normal flow of goods and profitability. The purpose of this paper is to analyze a dynamic game that involves a supply chain dealing with a normal-disruption cycle, which is characterized by a random and unpredictable environment. The supplier can invest in building resilience to minimize the potential disruption's impact. We derive the feedback equilibrium strategies and compare the policies and payoffs under different supply chain structures [centralized/decentralized]. Finally, we propose a cost-sharing scheme to achieve coordination.",Building resilience - a dynamic approach,[67625],897,"[20, 31, 33]",3789,Optimal control and resilience,90,9,33,Optimal Control Theory and Applications,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,42 [building - 303A],"['Control Theory', 'Dynamical Systems', 'Economic Modeling']",TC-33
"In railway planning, effective disruption management tools are necessary to limit the impact of a disruption. The duration of a disruption plays a central role in this process and is typically assumed to be known with certainty. However, in practice the duration is uncertain, and neglecting this aspect may lead to inefficiencies.
We propose two different approaches for incorporating robustness when rescheduling rolling stock units while considering a disruption of uncertain duration. The first, termed strict composition robustness, obtains a schedule that is feasible for all possible realizations of the disruption duration and, therefore, can be easily modified in the case that the disruption lasts longer than expected. The second, termed light trip robustness, focuses on the notion of trip robustness'', and requires that out of a set of critical trips a pre-specified percentage should be robust with respect to their incoming composition across different disruption durations. 
We propose extensions of a well-known rolling stock scheduling model to respectively capture the different robustness criteria and, on a diverse set of real-life instances from the Dutch railway network, compare their performance to a naive approach that assumes a given disruption duration.
The results show that all methods are worthwhile and that the choice of which to apply is highly dependent on the likelihood that the disruption will last longer than anticipated.",Robust Rolling Stock Rescheduling,[53331],179,"[122, 129, 14]",3790,Disruption management in passenger railways,85,2,54,Public Transport Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S01 [building - 101],"['Railway Applications', 'Scheduling', 'Combinatorial Optimization']",MA-54
"In this talk, we introduce a multi-objective recommender system aimed at
enhancing both the accuracy and diversity of top-n recommendation lists.
To address this optimization challenge, we present a customized hybrid
AMOSA_NSGA-II [HAN] algorithm, facilitating the creation of a Pareto set
for top-n lists. Additionally, we provide a methodology for selecting the
optimal list for each user within this Pareto set. Initially, we generate
preliminary top-n lists through item-based collaborative filtering.
Subsequently, the second stage addresses a bi-objective optimization
problem related to recommendation lists, utilizing the customized HAN
algorithm. Finally, the third stage focuses on producing optimal
personalized top-n lists for individual users. To assess the effectiveness of
our method, we implement it on real-world datasets, conduct a thorough
performance evaluation, and compare its results with existing approaches
in the literature.",Multi-objective optimal recommender systems,"[75135, 45155]",726,"[77, 86]",3792,Optimization in Online Environments,14,3,03,Data Science Meets Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,1005 [building - 202],"['Multi-Objective Decision Making', 'OR and the Internet']",MB-03
"In the field of process system engineering [PSE], particularly in the oil and gas industry, a novel approach is presented that significantly enhances sustainability. This approach is characterized by integrating effective scheduling and planning strategies that transition from traditional deterministic optimization to a robust framework. This framework is adept at managing the uncertainties inherent in the industry, a challenge often overlooked in conventional methods. Central to this new strategy is the development of a Mixed-Integer Nonlinear Programming [MINLP] model for crude oil tank farm management. This model integrates deterministic mathematical principles with stochastic components to address the unpredictability of oil volumes and market prices, aiming to improve operational efficiency and promote oil sector sustainability. Furthermore, the integration of the Industrial Internet of Things [IIoT] and Process Intensification [PI] optimizes value chains, marking a substantial advancement in addressing the complex challenges of current industrial processes and establishing a foundation for future sustainable industrial developments. This comprehensive approach balances ecological, social, and economic dimensions and represents a significant step towards the sustainable development in chemical engineering and PSE, combining advanced technologies with robust optimization techniques.",Stochastic optimization with automated number of storage tank selection for crude oil scheduling,"[72786, 78954, 78955, 78956]",830,"[117, 129, 26]",3796,Optimization under Uncertainty in Manufacturing and Supply Chain Management,49,15,35,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,44 [building - 303A],"['Programming, Stochastic', 'Scheduling', 'Decision Support Systems']",WD-35
"Forced displacement entails relief aid distribution efforts among en route refugees to alleviate their migration hardships. This study aims to assist humanitarian organizations in cost-efficiently optimizing logistics of mobile facilities to deliver relief aid to transiting refugees in a multi-period setting. The problem, referred to as the Capacitated Mobile Facility Location Problem with Mobile Demands [CMFLP-MD], involves refugee groups following specific paths and receiving aid at least once every fixed number of consecutive periods. The overall costs associated with capacitated mobile facilities are minimized. We formulate a mixed integer linear programming [MILP] model and propose two solution methods - an accelerated Benders decomposition approach and a matheuristic algorithm relying on an enhanced fix-and-optimize agenda. We evaluate our methodologies using realistic instances based on the 2018 Honduras migration crisis. Numerical results reveal that the accelerated Benders decomposition excels MILP with a 46% run time improvement on average while acquiring solutions at least as good as the MILP. Our matheuristic acquires high-quality solutions with a 2.4% average gap compared to best-incumbents rapidly. Sensitivity analysis highlights the managerial advantages of implementing CMFLP-MD solutions.",Capacitated Mobile Facility Location Problem with Mobile Demand - Efficient Relief Aid Provision to En Route Refugees,"[13778, 69465, 22044, 15639]",90,"[58, 64, 111]",3797,Humanitarian aid provision and disposal,38,7,21,OR in Humanitarian Operations [HOpe],"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,49 [building - 116],"['Humanitarian Applications', 'Location', 'Programming, Mixed-Integer']",TA-21
"There are various empirical phenomena relating to individual investors' behavior, such as how their emotions and opinions influence their choices. Sentiment refers to all of these emotions and opinions. In finance, stochastic changes can occur in response to investor sentiment levels. Machine Learning methods are well-known and helpful tools for prediction problems, and they have already been applied successfully to handle a wide range of financial problems. In this study, we focus on the behavior of financial difficulties based on the sentiment levels of investors, rather than pure financial problems. The goal of this study is to evaluate sentiment index prediction performance using two-stage MARS-NN, MARS-RF, RF-MARS, RF-NN, NN-MARS, and NN-RF hybrid models. We further prepare to describe people's sentiments toward the economy according to their level of confidence. For this objective, we employ HMM to estimate the underlying state change of the Consumer Confidence Index [CCI] and investigate its relationship with some macroeconomic indices [CPI, GDP, and currency rate] at monthly intervals. The purpose is to monitor and comprehend the transitions between these phases, as well as to chart a course through them. We also plan to apply volatility models to each subgroup we receive in HMM to see if we can improve our predicting results.",Applying multiple indicators to assess the relationship between finance and human factors,[58855],632,"[83, 66]",3798,"Advancements of OR-analytics in statistics, machine learning and data science 16",16,12,06,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1013 [building - 202],"['Optimization in Financial Mathematics', 'Machine Learning']",WA-06
"A series of lab and field studies were conducted with managers, executives, and postgraduates to test the hypothesis that priming effects exist within any given scenario planning workshop [Crawford, 2021]. The studies show that priming scenario practitioners with real-world, yet biased views of the external business environment can significantly alter how they view the future, scenario development, and implications for action. Early indicators also suggest some of these biases may be internally consistent. If true, biases based on priming effects suggests potentialities for both facilitated nudging [Bryson, Grime, Murthy & Wright, 2016; Crawford & Wright, 2024] and predictive modelling [i.e. machine learning and AI; Spaniol & Rowland, 2023]. These empirical studies are part of a larger grounded theory investigation. Results support three key claims. First, all scenario planning workshops are susceptible to implicit influences. Second, there may be as much consistency as inconsistency in biased scenario thinking and foresight across teams. Third, facilitators need to understand these variabilities in order to help advance meaningful strategies.",Biased Foresight - A Facilitator’s Playbook,[78774],677,"[22, 27, 55]",3803,Scenarios and foresight practices - Behavioural issues III,13,14,11,Behavioural OR,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,12 [building - 116],"['Critical Decision Making', 'Decision Theory', 'Group Decision Making and Negotiation']",WC-11
"Modern solvers for mathematical optimization problems are complex pieces of software. They usually make use of a combination of solution methods. Though the correctness of these methods is often proven on paper, with today's tools it seems prohibitively difficult to verify that they are implemented correctly in complex software. Even thoroughly tested state-of-the-art solvers used by many are not immune to bugs and numerical issues. We present a proof system allowing for establishing correctness of results produced by optimization algorithms. Particular focus is placed on mixed-integer programming [MIP], extending on work for binary programs by Bogaerts, Gocht, McCreesh, and Nordström [2022]. Our proof sytsem covers a broad range of MIP solving techniques, including cutting planes, presolving, and symmetry handling.",Certifying Symmetry Handling Reductions in Integer Programming,"[70975, 67145, 19147, 55298]",702,"[111, 109, 19]",3804,Specialized Optimization Algorithms,76,13,30,Software for Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,53 [building - 208],"['Programming, Mixed-Integer', 'Programming, Integer', 'Continuous Optimization']",WB-30
"We take into account the blocking job shop scheduling problem [BJSS] where processing, release and sequence-dependent setup times have uncertain interval durations. Scheduling problems which take uncertainties into account are computationally complex and the literature on stochastic and uncertain scheduling is relatively limited compared to the that on deterministic scheduling. As such, analysts often use deterministic models that incorporate simplified representations of the most relevant stochastic aspects in order to determine or select the schedule to implement. We propose a method for the evaluation of the risk that the makespan of deterministically computed solutions worsens under the worst-case realizations of the uncertainty.  An interval-valued network approach is proposed to model the feasible solutions characterized by uncertain values on the given constraints’ durations. The study assumes the Value-at-Risk and the Conditional Value-at-Risk as measures, and addresses both modeling and computational issues. The impact of different sources of uncertainty on the overall performance of the proposed approach is analyzed. The results of the experimental campaign show that the method, for both the computational time and the quality of the evaluations, has broad applicability and can support the decision-makers for taking into account their risk sensibility.",Evaluation of VaR and CVaR for the Makespan in Interval Valued Blocking Job Shops,"[50677, 10238, 71490]",931,"[129, 126]",3808,Job shop scheduling,35,13,60,Project Management and Scheduling,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S09 [building - 101],"['Scheduling', 'Risk Analysis and Management']",WB-60
"When given a multi-objective optimization problem, one usually aims to find efficient solutions. Yet, often this is hard. Solutions that are often easier to identify than efficient solutions are supported efficient solutions. Intuitively, the set of supported efficient solutions can be described as the set that contains all solutions to the weighted-sum scalarization for “reasonable” choices of weights. However, different authors work with different definitions of supported efficiency [and—if in the image space--supported nondominatedness, respectively].

We examine definitions found in the literature and show that there are in fact differences. We are able to categorize them into four layers with an implicit hierarchy - The characterization at the highest level being the most restrictive. Moreover, we show that these layers only coincide in special cases, such as discrete, linear or bi-objective problems, while counterexamples show that these characterizations are not equivalent for more general problems. We argue which characterizations are best suited to be used for different kinds of multi-objective optimization problems.

Last, we extend our results to concepts of extreme supported efficiency and extreme supported nondominatedness, respectively.",Characterizations of supported efficiency,"[61718, 1601]",602,"[77, 0]",3810,Theory of Multiobjective Optimization,34,14,37,Multiobjective Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,33 [building - 306],['Multi-Objective Decision Making'],WC-37
"An important aspect of optimising public transport is finding a good timetable.  On the one hand, short travel times are important from the passengers' point of view. On the other hand, tight timetables without buffer times are prone to delays, which are inevitable in practice and highly dissatisfactory for the passengers. Hence, a good timetable should also have some degree of delay resistance. Often a periodic timetable is desirable, i.e. a timetable which repeats in a regular pattern [e.g. every hour]. However, delays do in general not occur periodically, so many robust timetable models only consider aperiodic timetables.
In our work we apply the concept of recoverable robustness to periodic timetabling with aperiodic delays, resulting in the integration of two well-known problems - the periodic event scheduling problem [PESP] and delay management [DM]. We show how we can bridge the gap between the periodic PESP and the aperidoic DM, yielding MIP formulations. However, solving the problem to optimality is unrealistic for large instance due to its high complexity. Therefore, we pursue heuristic approaches. We present iterative heuristics computing periodic timetables with increased robustness at the cost of only a small increase in the nominal travel time.
",Heuristics for recovery robust periodic timetabling,"[69308, 1601]",818,"[119, 142, 127]",3811,Timetabling 2,85,12,51,Public Transport Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,M5 [building - 101],"['Public Local Transportation Systems', 'Timetabling', 'Robust Optimization']",WA-51
"Startups drive economic growth through technological innovations, market expansion, and increased production. This trend is evident globally, with countries like India fostering entrepreneurship through well-funded support structures, notably business incubators. Incubator performance assessment is challenging due to existence of diverse frameworks and objectives. Current literature often overlooks improvement pathways for individual incubators. To address this, we employ Data Envelopment Analysis [DEA] for relative performance evaluation, identifying improvement opportunities without preconceived assumptions. A Classification Decision Tree based on DEA outcomes distinguishes incubator profiles, notably highlighting differences between university-based units and others. Mentoring, infrastructure, and technology support emerge as crucial, especially for Deep Tech startups, which require distinct assistance due to their capital intensity and technological uncertainty. Our study assesses 53 Indian non-profit incubators and reveals variations in profiles that are discussed in the context of supporting Deep Tech startups, making it vital for policymakers and incubator managers navigating the evolving entrepreneurial landscape.",Analysing the performance of Business Incubators in India using a DEA-Decision Tree approach,"[78938, 79345]",942,"[24, 0]",3812,DEA applications in Policy Making and Planning I,89,10,48,Data Envelopment Analysis and its Application,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,60 [building - 324],['Data Envelopment Analysis'],TD-48
"In today's highly industrialized and specialized pig production landscape, efficient decision-making is essential to navigate the complexities and maximize benefits. This study investigated the delivery of fattening pigs from farms to slaughterhouses, considering diverse growth patterns. An optimal marketing strategy is developed to maximise profits by adopting an all-in, all-out management approach. The research utilized the K-means algorithm to categorize actual growth data of fattening pigs into three clusters, each analyzed using the Gompertz animal growth model to assess growth rate, acceleration, and relative growth rate. Subsequently, a simulation of a fattening farm was conducted using a mixed integer linear programming [MILP] model to evaluate various parameters and develop optimal marketing strategies. These strategies, oriented for pig delivery schedules, cost analysis, and profitability assessment, were designed without needing individual control and weight measurement of fattening pigs. Sensitivity analyses were performed on key production factors affecting profitability, such as pig transaction price, feed price, and pig mortality during fattening, leading to the development of marketing strategies for each scenario.",Maximizing profits by performing optimal marketing decisions in pig fattening farms  ,"[78761, 71202, 243]",591,"[89, 131, 71]",3813,OR in Livestock farming,20,7,12,OR in Agriculture and Forestry ,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,13 [building - 116],"['OR in Agriculture', 'Simulation', 'Marketing']",TA-12
"Infrastructure owners face challenges in maintenance decision-making due to the process’s multidisciplinary nature, spanning mathematics to cognitive science. This study optimizes maintenance for complex infrastructure systems, specifically in scenarios where a single primary owner must consider the preferences and requisites of multiple stakeholders. We propose a generic conceptual model facilitating shared views, eliciting objectives early, and aligning agendas with desired outcomes. It offers valuable guidance for decision modelers, reinforced by case studies.
The decision-making process in infrastructure management varies based on industry standards, organization, and public expectations. Evolving approaches recognize infrastructure as part of a complex process, intensifying challenges such as multi-disciplinary issues and stakeholder conflicts. In this matter, effective management involves understanding uncertainty, navigating complex systems, and establishing optimal solutions. Moreover, social complexity emerges when stakeholders hinder collective inquiry due to singular problem perceptions. In response, decision agents play crucial roles, facilitating consensus and commitment while promoting shared understanding and restructuring education for a complexity mindset.",Crafting Consensus - A conceptual model for enhancing urban infrastructure maintenance decisions,"[75625, 78966]",716,"[25, 55, 149]",3814,Supporting Planning and Sustainability,26,2,13,Soft OR and Problem Structuring Methods,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,15 [building - 116],"['Decision Analysis', 'Group Decision Making and Negotiation', 'Problem Structuring']",MA-13
"Recently, a number of energy system analyses have emerged, which highlight the potential roles of hydrogen in future energy systems. The results however differ substantially in terms of the outcomes. This is partly due to hydrogen being an expensive energy carrier, which requires a number of conditions to become feasible in large scale some of which include e.g. high carbon taxes, the availability of low-cost renewable electricity and low availability of biomass. To investigate in more detail where the differences stem from, a number of studies of the role of hydrogen in Denmark has been compared and a modeling and reporting framework is proposed. The goal of the framework is to ensure transparency and understanding of results highlighting important input parameters and conditions. The 7 main features identified can be summarized as:
1.	Model type
2.	Modeling of time [aggregation, pathways and foresight etc.]
3.	Modeling of space [aggregation, scope, grids etc.]
4.	Scenario boundary conditions [e.g. CO2 tax/cap, import/export possibility etc.]
5.	Resource assumptions [potentials, prices, variance etc.]
6.	Technology assumptions [conversion, storages etc.]
7.	Demands [energy carriers, sectors included, amounts & timing & flexibility etc.]
In order to be able to compare and understand differences between different studies it is recommended to document and reflect upon these model characteristics as well as the main assumptions and modeling choices. 
",Important features when modeling hydrogen as part of integrated energy systems,"[62517, 58629, 78961, 78962, 78965, 78963, 78964]",643,"[37, 110, 93]",3815,Hydrogen Modeling and Regulation II,22,15,09,Energy Markets,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,10 [building - 116],"['Energy Policy and Planning', 'Programming, Linear', 'OR in Energy']",WD-09
"This study develops and evaluates a linear optimization model for an optimal channel mix for persuasive communication. It is based on the premise that different disclosure channels induce different processing costs and that a listener's processing capacity for information is constrained. Under these conditions, a persuader benefits from distributing positive and negative information differently to maximize the listener's understanding of the positive and to minimize it for the negative information. By doing so, a persuader effectively increases her persuasive power. We show that our model predicts actual channel choices made by real-world persuaders in the setting of earnings announcements with an out-of-sample approach. Although our model only considers the information’s tone and readability, it predicts up to 69% of channel choices correctly, which is statistically significant. We provide robustness tests by assessing the sensitivity of the model’s predictive power to changes in numerical assumptions. Additional analysis indicates that persuasive channel choices are a selfish act, as these are predominantly done by firms with low ESG scores and high institutional ownership. A critical discussion of our results and an outlook for future research is provided.",Persuasive channel choices - evidence from manager-investor interactions,"[75277, 75279, 75280]",661,"[1, 10, 55]",3816,OR in Accounting - Wealth and Risk,7,15,59,OR in Financial and Management Accounting,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S08 [building - 101],"['Accounting', 'Behavioural OR', 'Group Decision Making and Negotiation']",WD-59
"A physical limitation in quantum circuit design is the fact that gates in a quantum system can only act on qubits that are physically adjacent in the architecture. To overcome this problem, SWAP gates need to be inserted to make the circuit physically realizable. The nearest neighbor compliance problem [NNCP] asks for an optimal embedding of qubits in a given architecture such that the total number of SWAP gates to be inserted is minimized. In this paper we study the NNCP on general quantum architectures. Building upon an existing LP formulation, we show how the model can be reduced by exploiting the symmetries of the graph underlying the formulation. The resulting model is equivalent to a generalized network flow problem and follows from an analysis of the automorphism group of Cayley graphs generated by transpositions. As a byproduct of our approach, we show that the NNCP is polynomial time solvable for several classes of symmetric quantum architectures. Numerical tests on various architectures indicate that the reduction in the number of variables and constraints is on average at least 90%. This opens the way to solving NNCP instances on large quantum circuits that are far beyond the computational capacity when solving them without the exploitation of symmetries.",Exploiting symmetries in optimal quantum circuit design,"[76886, 78967, 71382]",378,"[110, 53]",3817,Recent advances in LP and SDP for discrete optimization problems,68,12,38,"Conic Optimization - Theory, Algorithms, and Applications","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,34 [building - 306],"['Programming, Linear', 'Graphs and Networks']",WA-38
"Firms’ growth, the darling measure of investors, comes from higher revenues. Thus, sales and marketing departments make extreme efforts to accept as many customer orders as possible. Unfortunately, not all orders contribute equally to profits, and some orders may even reduce net profits. Thus, saying no [i.e., not accepting an order] may be a necessary condition for net profits growth. For understanding the impact of rejecting orders on profitability, we propose an order acceptance and scheduling problem [OAS]. Although the OAS has extensively been studied in the literature, there is still some gap between these papers and real-life problems in industry. In an attempt to close that gap, the OAS we propose considers orders revenues, machines costs, holding costs and tardiness costs. We develop a mixed integer linear programming [MILP] model for solving this problem. Since the complexity of the problem makes it impossible for the MILP to solver large-scale instances, we also propose a metaheuristic algorithm. Numerical experiments show that the metaheuristic finds good quality solutions in short computational times. In the last part of the paper we confirm some managerial insights - higher holding and tardiness costs imply a lower acceptance of orders, forcing production has a concave negative impact on net profits, and accurately estimating costs is essential for good planning.",Acceptance Ordering Scheduling Problem - The impact of an order-portfolio on a make-to-order firm’s profitability,"[13353, 59017, 22831]",409,"[129, 0]",3820,Manufacturing scheduling with sustainability considerations,35,12,60,Project Management and Scheduling,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S09 [building - 101],['Scheduling'],WA-60
"The growth of e-commerce has led to increased requests for home deliveries, including attended home deliveries for subscription-based platforms, which require frequent customer presence at home. To accommodate customer availability, many online retailers offer different delivery time slots. Providing these time slots is convenient for customers but results in higher uncertainty for the retailer. This paper introduces a novel model tailored for subscription-based e-retailers in which customers' decisions regarding delivery slots and their corresponding impact on routing costs are explicitly considered. The slots' assortment and discounts on prices are considered control variables in our model. Our research employs a mixed-integer linear programming framework integrated with a mixed logit model, capturing customer preferences' stochastic and diverse nature in delivery time slots. We use an adaptive large neighborhood search to be able to solve this problem for large instances of this problem. Numerical experiments are conducted to show the effectiveness of these approaches. Results indicate the importance of accounting for the uncertain heterogeneous behavior of customers when deciding about the assortment and price discount rates of time slots.",A tactical time slot management problem under mixed logit demand,"[71393, 36097]",695,"[145, 32, 10]",3821,Customer behaviour,11,5,59,Pricing and Revenue Management,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S08 [building - 101],"['Vehicle Routing', 'E-Commerce', 'Behavioural OR']",MD-59
"Given recent social and technological developments, the consideration of the Dial-a-Ride-Problem [DARP] in large-scale ridepooling has gained more importance as a sustainable and increasingly viable alternative to personal motorized vehicles. In reality, routing decisions for DARPs in large-scale ridepooling effectively influence traffic conditions and the resulting travel times in the network. Since routing decisions are mostly based on travel times, a mutual dependency between routing decisions and travel times emerges. Recent research on DARPs in large-scale ridepooling provides solutions for congestion-aware routing and vehicle assignment while the modelling of infrastructure capacity is based on traffic dynamics within links. The main feature of the presented approach is the modelling of infrastructure capacity and its impact on driving and waiting times, extending beyond links to include intersections with adaptive signal control. The novelty of the suggested approach is twofold. Firstly, a new analytical approach for modelling infrastructure and its capacity is provided while considering mutual dependencies between flow capacities within intersections in case of adaptive signal control. Secondly, a heuristic is derived to optimize a given initial solution of a DARP with the aim to optimize capacity consumption of infrastructure while maintaining constraints of a typical DARP. The results are shortly demonstrated within a small case study.",Rerouting Heuristic for the Congestion-Aware Dial-a-Ride-Problem Considering Intersection Capacities,"[75373, 69958, 75353]",822,"[143, 145, 119]",3823,Demand-responsive public transport 3,85,10,54,Public Transport Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S01 [building - 101],"['Transportation', 'Vehicle Routing', 'Public Local Transportation Systems']",TD-54
"The inclusion of sustainable goals in the portfolio selection process may have an actual impact on financial portfolio performance. Environmental, Social, and Governance [ESG] indices provided by the rating agencies are generally considered good proxies for the performance in sustainability of an investment, as well as, appropriate measures for Socially Responsible Investments in the market. In this framework, the lack of alignment between ratings provided by different agencies is a crucial issue that inevitably undermines the robustness and reliability of these measures. Indeed, the ESG rating disagreement may produce conflicting information, implying a difficulty for the investor in the portfolio ESG evaluation. This may cause underestimation or overestimation of the market opportunities for a sustainable investment. In this paper, we deal with a multicriteria portfolio selection problem taking into account risk, return, and ESG criteria. We present a new approach to manage the ESG ratings disagreement between different agencies. We propose a nonlinear optimization model for our three-criteria portfolio selection problem. We show that it can be reformulated as an equivalent convex quadratic program by exploiting the k-sum optimization strategy. An extensive empirical analysis of the performance of this model is provided on real-world financial data sets.",Managing ESG Ratings Disagreement in Sustainable Portfolio Selection,[78903],693,"[139, 112, 72]",3826,Portfolio Optimization - Models and Methods,4,14,02,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,Glassalen [building - 101],"['Sustainable Development', 'Programming, Multi-Objective', 'Mathematical Programming']",WC-02
"Millions of individuals are subjected to exploitative labour practices worldwide. This project focusses on intervention at the recruitment stage, where deceptive job advertisements often proliferate unchecked. We propose a machine learning approach to identify high-risk online job adverts potentially leading to labour exploitation. With the help of natural language processing techniques such as word embeddings, we extract relevant features from a sample of job adverts, obtained in collaboration with a non-profit organisation. Using these features, we then train a predictive model to distinguish high-risk adverts from others. We validate our model using a different set of job adverts which are unseen to the model. Through this work we demonstrate the use of machine learning to help vulnerable job seekers by facilitating timely intervention. This work represents a crucial step towards mitigating the risk of labour exploitation in the digital age and safeguarding the rights of workers. ",Identifying high-risk job advertisements on social media to help mitigate labour exploitation. ,"[78969, 76764, 40226]",632,"[66, 58, 7]",3827,"Advancements of OR-analytics in statistics, machine learning and data science 16",16,12,06,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1013 [building - 202],"['Machine Learning', 'Humanitarian Applications', 'Analytics and Data Science']",WA-06
"Alcohol consumption is a recognised public health issue, and it is believed that the resultant harms are rising since the COVID-19 pandemic. Alcohol drinking affects healthcare including the demands of alcohol-attributable diseases; mental health and housing services demands on social care; and demands on the criminal justice system resulting from violent and aggressive behaviours. This talk will present an overview of the application of operational research [OR] methods to model alcohol drinking behaviours, specifically within healthcare, social care, and criminal justice settings. Some OR methods are utilised frequently in alcohol consumption literature, namely agent-based modelling [ABM], Markov models, and network analysis. The recurrence of ABM and network analysis highlight the social component of alcohol consumption. Other methods such as system dynamics are utilised less frequently but could be useful models to include in the literature to gain a whole-system perspective. Alcohol consumption data collected in Wales can be analysed geographically by health board, local authority, and area deprivation, to gain insights into the diverse alcohol drinking patterns reported, and how resultant harms vary geographically and by consumption behaviour. Based on the literature evidence, alcohol consumption and alcohol-related harms data, and collaboration with stakeholders, an initial proposal on how to model alcohol drinking behaviours across the whole system is presented. ","An overview of the application of OR methods to alcohol drinking behaviours, and a geographical analysis of alcohol consumption in Wales","[78976, 63586, 28046, 47682]",971,"[56, 132, 131]",3830,Simulation models in healthcare,3,4,17,OR in Health Services [ORAHS],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,40 [building - 116],"['Health Care', 'Social Networks', 'Simulation']",MC-17
"Efficient machine-part cell formation is crucial for modern manufacturing systems, ensuring high throughput, reduced setup times, and increased flexibility. This paper introduces a novel 2-phase MILP-based Co-clustering model tailored for optimal cell formation in manufacturing systems. Addressing both binary and ratio [workload] input datasets, the model introduces the noble similarity coefficient yielding better results. The objective is to provide an efficient and versatile solution, with a particular emphasis on minimizing inter-cell workloads and reducing idle time within the cells. Unlike probabilistic methods, our model guarantees a deterministic, globally optimal grouping of machines and parts using a mathematical model. The model is successfully applied to over 50 benchmark datasets of binary and workload drawn from Cellular Manufacturing Systems. The model demonstrates superior grouping efficacy and lower time complexity compared to existing methods. Furthermore, a streamlined heuristic follows the mathematical model, enabling efficient machine or part reallocation, further improving the model's efficiency.", A Generalized MILP Model based Co-Clustering Approach - An Application in Cellular Manufacturing,"[78975, 19376]",836,"[111, 72, 69]",3832,Lot-sizing with industrial applications II,32,5,49,"Lot Sizing, Lot Scheduling and Production Planning","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,M1 [building - 101],"['Programming, Mixed-Integer', 'Mathematical Programming', 'Manufacturing']",MD-49
"In the context of airborne Anti-Submarine Warfare [ASW], we focus here on active sonar systems, i.e. systems based on the transmission and reception of an acoustic wave reflected by the target [echo]. Furthermore, our case study involves acoustic buoys, or sonobuoys, which can be classified into three main categories - transmitter-only [Tx], receiver-only [Rx] and transmitter-receiver [TxRx]. When transmitter and receiver are co-located - TxRx buoy - we refer to this as monostatism, while when transmitter and receiver are located in two distinct geographical locations, we refer to this as bistatism. A Multistatic Sonar Network [MSN] is therefore defined as a combination of sonar systems in monostatic and/or bistatic configuration. In this context, given a limited number of heterogeneous sonobuoys and probabilistic detection models, we seek to determine the optimal MSN, i.e. the one that maximizes the area covered. To address this issue, we propose a constructive greedy heuristic relying on a QuadTree-based approach [QT] enhanced by a local search through Variable Neighborhoud Descent [VND]. This heuristic is based on a recursive principle - akin to branch & bound - dividing the area of interest into sectors and accelerating the search for locally optimal positions through judicious computation of upper and lower bounds. Finally, we compare our method with the most efficient Mixed-Integer Linear Program [MILP] in the literature as well as a Simulated Annealing [SA].",A quadtree-based heuristic and variable neighborhood descent for the optimal location of heteregenous multistatic sonar network,"[78834, 78980, 56339, 87, 78981]",793,"[14, 75, 64]",3834,"Military, Defense, and International Security I",65,5,20,"Military, Defense, and International Security","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,45 [building - 116],"['Combinatorial Optimization', 'Military Operations Research', 'Location']",MD-20
"A particular form of extremum problems which attracted the attention of the optimization community over more than the past decade are a so-called optimization problem with vanishing constraints. This follows from the fact that optimization problems of such a type have been served as a model for many extremum problems from structural and topology optimization. In most of works in the optimization literature, optimality and duality results have been proved only for differentiable scalar optimization problems with vanishing constraints. In our considerations, we shall investigate optimality results for a new class of nondifferentiable multiobjective programming problems with vanishing constraints. Namely, we derive both necessary and sufficient optimality conditions for a new class of such nonsmooth vector optimization problems.",On optimality conditions for nonsmooth multiobjective programming problems with vanishing constraints,[31093],50,"[112, 81, 113]",3835,Vector and Set Optimization II,33,3,41,Vector and Set Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,97 [building - 306],"['Programming, Multi-Objective', 'Non-smooth Optimization', 'Programming, Nonlinear']",MB-41
"One of the great challenges in reaching zero hunger is to secure the availability of sufficient nourishment in the worst of times such as humanitarian emergencies. Food aid operations during a humanitarian emergency are typically subject to a high level of uncertainty. We develop a novel robust optimization model for food aid operations during a humanitarian emergency, where we include uncertainty in the procurement prices, which is one of the primary sources of uncertainty in practice. Due to the multi-period and dynamic nature of food aid operations, we extend this robust optimization model to an adaptive robust optimization model.
Moreover, we analyse a folding horizon approach for the nominal, robust, and adaptive robust optimization models in which decisions can be altered in later time periods. We compare the different approaches based on a food operations case in Syria. We show that the [adaptive] robust optimization approach outperforms the nominal approach in the non-folding horizon case, while the nominal approach performs best in the folding horizon case. Consequently, in case decisions have to be made early on, we show that applying robust optimization to food aid operations can make a difference. However, in case small adaptations can be made to the decisions taken in later time periods, then food aid operations can use a relatively simple approach in practice and apply a folding horizon approach each month to optimize decisions.",A robust approach to food aid supply chains,[32308],419,"[58, 127, 30]",3839,Humanitarian Aid,78,13,13,Secure & Sustainable Food Supply,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,15 [building - 116],"['Humanitarian Applications', 'Robust Optimization', 'Disaster and Crisis Management']",WB-13
"Despite their relevance in policy evaluation, social impacts and socio-demographic distributional effects of transportation systems have been less addressed than economic or environmental considerations. This lack of social indicators mainly has to do with the way the demand is modeled for policy analysis [e.g., by assuming an aggregate representation and/or constant demand elasticities]. In this research, we derive a tractable measure of social welfare that revolves around the notion of expected maximum utility as given by discrete choice models based on the random utility principle. In addition, their disaggregate nature allows to carry out distributional analyses for different groups with similar socioeconomic characteristics. To illustrate this methodology, we consider an urban multi-modal system where both private and public transport modes are managed by a single planning authority that has control of the related decisions [e.g., price, frequency] and the welfare measure is to be maximized. The optimization problem governing these decisions is constructed using the choice-based optimization framework previously introduced by the authors, in which a linear representation of discrete choice models is embedded in a mixed-integer linear programming formulation to capture the interaction between the expected demand and the supply-related decisions. We propose a semi-synthetic case study on the introduction of a road toll in a corridor connecting two cities in Switzerland.",Welfare maximization for a user-centric transportation system,"[50839, 36405, 26236]",824,"[10, 143, 111]",3846,Transit,85,15,51,Public Transport Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,M5 [building - 101],"['Behavioural OR', 'Transportation', 'Programming, Mixed-Integer']",WD-51
"We provide a model for shock propagation and resilience in a temporal network. Each node is a bank and two banks are connected through an arc if a bank lends money at least once to another bank in a period. We assume that each node is associated with two latent parameters, i.e. the fitnesses, describing its propensity to create incoming and outgoing links at any time. At each time, the presence of an arc in the network is an independent realization of a Bernoulli random variable with probability specified by a link function of the fitness of two nodes. We assume that the fitness vector evolves accordingly to a VAR[1] through which it is possible to model the lagged interactions between fitness. In this framework, our aim is to study the resilience of a temporal network by proposing a modification of the impulse response analysis, which, in the network case, is a non-linear function of the shock intensity. We study on how a shock on a node of a temporal network propagates and how the system relaxes back to the equilibrium state. We also propose a novel econometric estimation model that combines the Maximum Likelihood Estimation method and the Kalman filter to estimate the fitness'dynamics. We test our methodology to synthetic networks and then we apply it to the electronic Market of Interbank Deposit network. The analysis is suitable to detect evolutionary features and the relationships among financial institutions highlighting which are the critical banks in shock spreading",Modelling shock propagation and resilience in financial temporal networks,"[78770, 78039]",387,"[45, 135, 7]",3847,New Tools in Insurance Risk Management ,4,10,02,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,Glassalen [building - 101],"['Financial Modelling', 'Stochastic Models', 'Analytics and Data Science']",TD-02
"Many Global South countries currently suffer from high degrees of agricultural food waste due to the warm climate and lack of an integrated national cold chain supply network. This can lead to many societal issues that hinder growth and development, including poor outcomes for population health and nutrition metrics, a “poverty trap” for smallholder farmers without market options for selling their produce, and for the national economy through lack of cold-chain traceability that would allow access to more lucrative export markets. Design of a national cold-chain network as critical infrastructure requires consideration of a wide range of market forces and behaviours in such a tightly-coupled system, as well as dealing with the inherent uncertainty in such interactions, to ensure that placement and operation of cold staging facilities can be optimally planned to achieve the desired national outcomes for public health and economic wealth. Agent-based modelling [ABM] can assist in capturing these phenomena to allow a future scenario of cold-chain infrastructure deployment to be tested for efficiency and resilience in the face of uncertainty and black-swan events such as extreme weather events caused by climate change. We present preliminary results, applied to Rwanda as a case study, of a bespoke ABM solution designed to show the effects of offering cold chain as an alternative business model to farmers and showing how network placement affects national outcomes.",Resilient national cold chain deployment for addressing food waste and achieving economic prosperity in the Global South - An agent-based approach,"[77594, 71831, 77593]",601,"[28, 58, 3]",3848,Simulation in sustainability,77,8,43,"Agent-based Models in Management, Economic and Organisation Sciences","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,99 [building - 306],"['Developing Countries', 'Humanitarian Applications', 'Agent Systems']",TB-43
"The maintenance of an electricity distribution network involves numerous daily technical interventions. In this problem, we are given a set of interventions each with associated time windows, location, necessary skills and duration, as well as a set of teams of technicians with associated set of skills. We need to find feasible routes on the interventions for each team, considering the time windows and skills, and ensure that each team returns to its departure depot before the end of the day. The primary objective is to maximize the total duration of completed interventions and as a secondary objective, we aim to minimize the overall routing cost. This problem can be formulated as a capacitated vehicle routing problem with time windows. Due to the large number of teams and interventions, this results in a large-scale optimization problem, and its operational nature limits the time available for exact solving. Here, we propose a column generation approach where one subproblem per vehicle has to be solved and each potential route of a vehicle is considered as a new column in the master problem. To generate these routes, we rely on dynamic programming. Real-world instances from EDF [Electricité de France] of historical technicians' interventions will be used to evaluate the effectiveness of the proposed methods.",A Column Generation Approach for the Routing of Electricity Technicians,"[78743, 71428, 50839]",755,"[145, 13, 108]",3849,Column Generation for Vehicle Routing,5,4,58,VeRoLog - Vehicle Routing and Logistics,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S07 [building - 101],"['Vehicle Routing', 'Column Generation', 'Programming, Dynamic']",MC-58
"In this talk, we present a stochastic and dynamic variant of the Control Disease Testing Program [CDTP] that originated in the late COVID-19 global pandemic. The CDTS was proven to be a key strategy to contain and control the pandemic, where suspected cases required to remain isolated and be tested. These results must be available quickly for the CDTS to be effective. We address the stochastic and dynamic version of the problem remark - repetition of the first sentence, maybe this part can be deleted, where only some suspected cases are known in advance, and new suspected cases could appear randomly throughout the course of the day or planning horizon. When a new suspected case arrives, it must be decided whether to include the new test request in the current plan or reject it. The specimens of the accepted requests must be collected on the same day; either by assigning the case to a time slot in a test-center or by visiting the patient with a mobile test-team. On the other hand, rejected requests must be included in the plan for the next day. The aim of this problem is to decide how many mobile test-teams to use, how many test-centers to open and where, which suspected cases to visit with a mobile test-team and which to assign to a test-center, and design the vehicle routes for the mobile test-teams. To solve this problem, we propose a solution method based on value function approximation and compare the effectiveness of the algorithm with respect to benchmark solution approa",Solution Strategies for the Stochastic Dynamic Contagious Disease Testing Program,"[55814, 61710, 74181, 2769]",782,"[65, 136, 145]",3850,Vehicle Routing Under Uncertainty 2,5,7,64,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S16 [building - 101],"['Logistics', 'Stochastic Optimization', 'Vehicle Routing']",TA-64
"In the classical capacitated facility location problem [CFLP], a set of facilities needs to be chosen in order to cover the demand of customers. Customers are assigned to any open facility such that the capacity of the facility is not exceeded and the total cost consisting of opening and assignment costs is minimised. However, in many real-world applications customers are not willing to travel to any open facility assigned to them but want to select an open facility according to their preferences. Such deviations can turn feasible solutions for the CFLP infeasible. The capacitated facility location problem with preference constraints [CFLP-PC] takes this behavior into account by assigning customers to their most preferred open facility. 
In this talk, we focus on two types of cover-based inequalities for the CFLP-PC. The first type corresponds to the classical cover inequalities, which we strengthen by taking advantage of a specific structure arising from the combination of capacities and preference constraints. In the second inequality type, we make use of information on the set of open facilities that arises from covers. We discuss the complexity of the corresponding separation problems and evaluate the performance of the inequalities for two preference types in a computational study.",Valid inequalities for the capacitated facility location problem with preference constraints,"[67804, 17092, 12046]",765,"[64, 109]",3851,Covering Location Problems,29,7,61,Locational Analysis,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S10 [building - 101],"['Location', 'Programming, Integer']",TA-61
"Mobility as a Service aims to integrate the offerings of Mobility Service Providers [MSPs] within a single mobile phone application for journey planning, booking, and ticketing. Multi-modal journeys, including public transit, supplemented with shared modes such as e-scooters and hire modes such as taxis, can patch-up deficiencies of public transit alone. The objective is to generate Pareto optimal sets of multi-modal journeys with respect to the minimisation of cost, travel time, CO2 emissions, inconvenience, and calorie expenditure. To address this complex combinatorial optimisation problem for the case of a large-scale multi-modal transport network, such as the Solent region of the UK, a Transfer Point Sample Approach [TPSA] was proposed, where non-dominated multi-modal journeys were composed from sequence of single mode trips via the sampled transfer points. This work proposes a novel approach that finds optimal alternative transfer points. The approach integrates adapted versions of A* and RAPTOR shortest path algorithms and uses machine learning travel time predictions to guide the journey planning process. Our experimental results show that the new approach reduces TPSA solution travel times in millisecond query times via globally optimised transfer points. 
This research is part of the Solent Future Transport Programme funded by the Department of Transport, UK and led by Solent Transport.
",Multi-modal journey optimisation using integrated shortest path algorithms and machine learning,"[78604, 22418, 71000, 61782, 74447, 78993, 77695]",624,"[5, 66, 143]",3853,Advancing mobility towards sustainable solutions III,6,12,56,Transportation,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S04 [building - 101],"['Algorithms', 'Machine Learning', 'Transportation']",WA-56
"The recent Industry 4.0 trend, followed by the technological advancement of collaborative robots, has convinced many industries to shift towards semi-automated assembly lines with human-robot collaboration [HRC]. Moreover, moving towards the human-centric and resilient industries proposed by the Industry 5.0 vision, the increasing use of HRC-based collaborative assembly systems is anticipated in the near future. In the HRC environment, robot agility can support human skill upon efficiently balancing and scheduling tasks among the stations and operators. However, in this attempt, the cycle time [CT], the total energy cost [TEC] of robots, and the total ergonomic risks [TER] of humans are among three conflicting objectives. Thus, this study deals with the balancing and scheduling of HRC lines where a trade-off between CT, TEC of robots, and TER of humans is sought. A mixed-integer linear programming model is proposed to formulate the problem. In addition, a multi-objective optimization approach based on ε-constraint is developed to address a case study from the automotive industry and a set of generated test problems. The computational results show that promising Pareto solutions in terms of CT, TEC, and TER can be obtained using the proposed approach.","Balancing and scheduling human-robot collaborated assembly lines considering cycle time, energy costs, and ergonomic risks","[78984, 78936]",834,"[72, 69, 111]",3857,Flow shop scheduling problems,32,13,49,"Lot Sizing, Lot Scheduling and Production Planning","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,M1 [building - 101],"['Mathematical Programming', 'Manufacturing', 'Programming, Mixed-Integer']",WB-49
"We study the problem where a population has to determine the truthfulness to certain commonly shared information. Problematics like this seems particular relevant in the current often tense political climate where spreading of false news repeatedly is at the center of accusation across the political aisle. To this aim we introduce a prototype model by considering “an index-of-truth” [IoT] and two types of agents - i] a percentage rho of rational agents, and ii] 1-rho of irrational agents. i] the rational agents use rational expectation in their decision making whether to believe in the given level of an IoT. By X we denote the uncertainty in the information used by the agents. If through rational expectations such an agent believe the IoT is right no action is taken. On the other hand, if the agent disagrees, a decision is taking to signal that the level of IoT is wrong influencing the value of the IoT index in proportion to the number of agents that reacts. ii] agents that are irrational on the other hand don’t try to gauge the proper level of the IoT index, but rather try to position themselves in alignment of what they believe will be the future value of the IoT. The decision making of the irrational agents is also publically signaled and influence the IoT index in proportion to the number of agents that react   We present an analytical expression showing how the range of the IoT index depends on rho and X. ",RATIONAL REACTION TO IRRATIONAL BEHAVIOR,[72390],436,"[3, 10, 15]",3858,Novel Optimization Models in Finance,4,12,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S14 [building - 101],"['Agent Systems', 'Behavioural OR', 'Complex Societal Problems']",WA-63
"At k-level uncapacitated facility location, ensuring each client is served by a sequence of k facilities is common in complex logistical systems, where central depots distribute goods through smaller depots. Due to its complexity, especially when k > 1, approximation algorithms are crucial for swiftly obtaining high-quality solutions. This study focuses on a four-echelon supply chain design facility location problem, involving plants [P], ware-houses [W], distribution centers [D], and retail stores [S]. The objective is to select locations of plants, retail stores, warehouses and distribution centers to maximize the total profit of serving selected retail stores. We formulate the problem using a bipartite Boolean quadratic programming [BBQP] model. Its solution is obtained through a heuristic approach that employs Tabu search with an r-Opt sequence embedded diversification. ",Four-echelon facility location supply chain design,"[45646, 42520]",858,"[5, 14, 43]",3859,"Discrete, continuous or stochastic optimization and control in networks, transportation and design II",64,3,25,Combinatorial Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,011 [building - 208],"['Algorithms', 'Combinatorial Optimization', 'Facilities Planning and Design']",MB-25
"The Moore-Penrose [M-P] pseudo-inverse has a prominent place in matrix theory and applications. It is well-known that the M-P pseudo-inverse is characterized by the four M-P properties. But not all of these properties are needed for the use of it in applications like least-squares fitting. In particular, when a matrix is not full rank, as is common in modern applications, there are much sparser [and even block structured, for explainabilty] generalized inverses than the M-P pseudo-inverse that solve the least-squares problem for arbitrary response vectors. Besides sparsity and structured sparsity, we are interested in low-rank and low-norm solutions, for further explainability and numerical stability. So, we attack the problem of generating such generalized inverses using optimization methods. Our techniques include - linear programming [LP], second-order cone programming [SOCP], local-search based approximation methods, the alternating direction method of multipliers [ADMM], and accelerations of these ideas via new structural results on generalized inverses.",Efficient and effective optimization methods for sparse generalized inverses,"[38731, 25963, 78999, 79000]",132,"[52, 21, 14]",3860,Topics in Mixed Integer Nonlinear Programming 1,86,8,04,MINLP,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,1001 [building - 202],"['Global Optimization', 'Convex Optimization', 'Combinatorial Optimization']",TB-04
"Snow removal problem in cities is a challenging task in nordic countries. Doing the snow removal efficiently is important. Since the amounts of snow vary a lot from day to day, and from year to year, fixed plans are not the best. Optimization of the snow removal tours could save much time and money.

We study the multi-vehicle urban snow removal problem from a mixed integer programming perspective. It is a very hard problem, and obtaining the exact optimum seems to be out of reach. Therefore, we study relaxations of the problem. Our goal is simply to find the best bounds for the optimal objective function value that is possible in limited time.

Since the problem has many sets of constraints with complicated structures, using Lagrangian relaxation might be beneficial. We discuss different possibilities of relaxing sets of constraints and develop a Lagrangian heuristic which consists of a suitable Lagrangian relaxation of the problem, a subgradient optimization method for solving the Lagrangian dual, and procedures for obtaining feasible solutions. The heuristic has been implemented and applied to artificial and real life city networks.

Extensive computational tests show that the lower and upper bounds can be improved.
",Lagrangian and Other Relaxations for the Urban Snow Removal Problem,"[38017, 62369]",743,"[14, 26, 5]",3864,Arc Routing,5,8,64,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S16 [building - 101],"['Combinatorial Optimization', 'Decision Support Systems', 'Algorithms']",TB-64
"We consider a computationally demanding bi-objective MILP, arising from an industrial maintenance and workshop scheduling problem subject to a turn–around time contract. Since, for industrial scale instances, this problem cannot be solved in reasonable computing time, we employ [i] so-called job variables in the workshop model, and [ii] a lower bounding procedure based on Lagrangean relaxation of the complicating constraints of an epsilon-constraint scalarized problem combined with math-heuristics to identify approximately non-dominated feasible points. A Lagrangean dual of the scalarized problem is maximized using subgradient optimization, which is shown to provide lower bounding functions of the objective of the scalarazed problem over the domain of the epsilon-constrained objective. As a means to reduce computational complexity we introduce so-called job variables, however resulting in an in-definiteness of the Lagrangean dual variables. This is remedied by aggregating, over jobs, the constraints defining the objective, which in turn entails a relaxation of the scalarized problems in terms of lower optimal values. To mimic the turn–around time contract, the aggregated variables’ values are split into values for the original job variables. The lower bounds computed are, however, still valid, albeit slightly weaker as compared to when constraint aggregation is not employed. A topic for further research is tightening of these lower bounds.",Lagrangean lower bounding and math-heuristics for approximating the set of non-dominated points of a computationally demanding bi-objective MILP,"[72584, 79279, 79280, 79281]",584,"[14, 81, 77]",3865,Multiobjective Mixed-Integer Linear Optimization,34,15,37,Multiobjective Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,33 [building - 306],"['Combinatorial Optimization', 'Non-smooth Optimization', 'Multi-Objective Decision Making']",WD-37
"Decarbonising the German energy system entails gradually substituting carbon-intensive power plants with renewable-based technologies. The need to integrate sizeable renewable quantities into the German electricity system affects the operation of future transmission grids. Impacts also include an increasing congestion management requirement and associated costs.
Managing grid congestion involves adjusting the market-based dispatch of single generators subject to grid constraints to find the least-cost system equilibrium. Grid constraints comprise rated thermal power flow capacities limiting electricity transmission. However, line capacities, to some extent, are subject to uncertainty. Increasing the line capacity is frequently practised by TSOs within certain boundary conditions, commonly called dynamic line rating. This gives operators greater flexibility, and indicates the importance of determining efficient capacity values. 
In this research, we propose to apply chance-constrained programming to investigate the economic feasibility of systematic power line overloading in the German transmission grid, utilising joint chance constraints to model probabilistic flow restrictions. Expected results will indicate its economic value and effects on congestion management cost depending on the probability level assumed. Outcomes also comprise line-specific information on realised single and simultaneous overload events across the time and lines providing TSOs with decision support.",Evaluating the Economic Feasibility of Systematic Power Line Overloading in the German Electricity Transmission Grid Applying Chance-Constrained Programming,"[45991, 79004, 3287, 14876]",343,"[36, 38, 93]",3866,Uncertainties in the Energy Transition,22,5,09,Energy Markets,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,10 [building - 116],"['Electricity Markets', 'Engineering Optimization', 'OR in Energy']",MD-09
"A securities transaction is an exchange of securities versus payment in currency between two financial actors. The transaction is settled, if, inter alia, there are sufficient amounts of resources on the corresponding cash and securities accounts. Each night more than four hundred thousand transactions over a billion euro are settled in batches. Banque de France is in charge of Mathematical Optimization Module [MOM], which is a component of a large European transactions settlement platform named T2S. Given a batch and the limited time, MOM looks for a subset of transactions to settle whose size is as large as possible respecting all business constraints and taking advantage of some financial features reducing the number of failed transactions. It is composed of preparatory, construction and improvement solution phases. Since the batch size is too large, no algorithm can find neither optimal nor approximate solution for the entire batch on a conventional computer. Thus, MOM’s preparatory phase splits a batch of transactions into the subsets to address smaller problems. The current splitting procedure has long runtime and its quality affects the final number of settled transactions. In this work, we present an approach that would improve the splitting results and the MOM’s outcome. It is based on a Quadratic Unconstrained Binary Optimization solved by a Quantum Approximate Optimization Algorithm and implemented on the available quantum devices via Qiskit platform.",Application of quantum computing in securities transactions settlement framework,"[36084, 79006]",374,"[44, 26, 18]",3870,Hybrid Classical-Quantum Algorithms,83,2,42,Quantum Computing Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,98 [building - 306],"['Finance and Banking', 'Decision Support Systems', 'Computer Science/Applications']",MA-42
"Automated Guided Vehicles [AGVs] play a pivotal role in modern transportation systems, revolutionizing the movement of goods and materials in warehouses and production systems. Yet, exploiting the performance of AGVs requires careful consideration of their operational constraints during the planning phase. In this study we consider the AGVs scheduling problem, where a fleet of AGVs are used to perform a set of transport requests. The AGVs are energy constrained and their battery needs to be swapped regularly. The problem is to assign transport activities to AGVs, sequence them, and determine optimal battery swap times, such that the makespan is minimized. In essence, this problem can also be viewed as a parallel machine scheduling problem with job-dependent activity cycles and constant maintenance time.
For this problem, we introduce a novel mixed-integer linear programming formulation that surpasses existing state-of-the-art models. Then, we develop a logic-based Benders decomposition algorithm embedded in a single search tree framework and strengthened by new sets of lower bounds and valid inequalities. Preliminary computational results indicate the algorithm's capability to solve previously unsolved instances reported in the literature.
",An exact solution for the energy constrained AGV scheduling problem,"[73594, 35097]",233,"[129, 11, 14]",3873,Novel topics and recent advances in solution approaches in scheduling,64,3,26,Combinatorial Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,012 [building - 208],"['Scheduling', 'Branch and Cut', 'Combinatorial Optimization']",MB-26
"Using machine learning, we implement a novel investing strategy that integrates environmental, social, and governance [ESG] factors, coupled with economic and risk considerations to enhance portfolio performance. The strategy involves a monthly screening of the stocks within the Russell 3000 index based on three filters. The first filter considers the economic cycle and uses leading indicators to identify future outperforming sectors. Then, using a clustering algorithm, the second filter ranks the sector-winning stocks by their ESG scores and selects the top-rated stocks from each ESG group. Finally, the remaining stocks are grouped via a clustering algorithm that focuses on risk metrics. Within each cluster, we select the stocks which exhibit the lowest loss potential as measured by the conditional value at risk [CVaR].

The resulting stock portfolio is constructed with the selected stocks and related asset allocation is achieved by using a convenient optimization method. The proposed investing strategy demonstrates the potential for achieving a desirable balance between risk, return, and ESG considerations. It proposes an active portfolio management architecture based on three layers, the main drivers being the sector rotation and ESG-factor investing policy.",ESG factors driven asset management - a multi-layered approach for enhanced returns and risk mitigation,"[71087, 57451]",660,"[66, 139, 126]",3874,"Advancements of OR-analytics in statistics, machine learning and data science 17",16,13,06,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1013 [building - 202],"['Machine Learning', 'Sustainable Development', 'Risk Analysis and Management']",WB-06
"Nurse rostering research has traditionally focused on algorithms, goals and constraints. Encouragingly, there has also been recent collaboration with commercial partners. It is only in the last few years that some of the scheduling researchers have started to pay significant attention to the health issues of scheduling. However, the human aspects of shift work have been studied for decades, but separately from research on scheduling optimization. It is still the case that health professionals publish articles on which health considerations are important in shift work planning and how best to reconcile them in theory, and optimization professionals publish articles on best models to represent and best methods to solve a wide variety of goals and constraints. Despite some good news, the most important thing from the employer's point of view is maximizing the use of the available working time, i.e. securing that the number of employees at work cover the expected workload as well as possible. The problem is the considerable number of indirect and hidden interactions between these two major targets - maximizing the working time and considering the human aspects. We examine the effect of three important issues:
1] The method of selecting the work that will be left over when the total need for work exceeds the total resources available to the employees.
2] The accepted deviation of the employee’s total working time from the target.
3] The weight set to the accepted deviation.",Nurse rostering - How to efficiently maximize the use of the available working time,[18783],950,"[128, 129, 56]",3875,Nurse rostering,3,10,15,OR in Health Services [ORAHS],"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,18 [building - 116],"['Rostering', 'Scheduling', 'Health Care']",TD-15
"Train unit scheduling optimisation[TUSO] involves allocating train unit vehicles to fulfil timetabled trips, with the option to couple or decouple units to allow flexible rolling stock configurations for a train. TUSO typically balances two conflicting objectives by Pareto multiobjective [MO] optimisation - meeting passenger demand and minimising rolling stock use. However, certain emergencies, eg adverse weather or signal failure, require a significantly reduced timetable, making it challenging to meet full passenger demand with fewer services. In this case, a different rule is used that as long as adjusted demand is met, a solution is acceptable regardless of other objectives. This is beyond the capability of Pareto methods, as setting a large weight on demand is oversimplification. Pareto methods also face challenges in decision making involving uncertainty/fuzziness. Experts may find real-time selection of frontier solutions impractical. We propose a novel MO TUSO model for both normal and emergency cases on a synthetic TUSO instance. By tuning its parameters, the model can switch between the two, or intermediate states. It provides a probabilistic metric for gauging solution desirability across a continuum, as seen by different experts/rules via tuned parameters, instead of binary choices in efficient and inefficient options.This is valuable for uncertainty or real-time scenarios where the ‘best’ solution relies on complex factors, eg hierarchically prioritising demand.",A multiobjective model for adaptable train unit scheduling in normal and emergent conditions,"[49632, 79013]",193,"[77, 122, 49]",3878,Resilience in Public Transport Planning,85,7,54,Public Transport Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S01 [building - 101],"['Multi-Objective Decision Making', 'Railway Applications', 'Fuzzy Sets and Systems']",TA-54
"Zero Hunger, a UN Sustainable Development Goal, aims for a hunger-free world while also targeting the reduction of food waste. Supermarkets are major contributors to food waste due to surplus from wholesale operations. Food banks play a vital role in salvaging and distributing surplus food to the needy, but they often face budget constraints and therefore lack of transportation means. This study proposes collaboration among food banks to address these challenges, where those with transport capabilities assist smaller ones. This collaborative effort introduces a new logistical challenge, characterized by the perishable nature of foods and the selective nature of the problem—determining which supermarkets to visit, which foods to collect from visited supermarkets, which food banks to visit to deliver collected foods. This multi-objective problem aims to optimize the amount of food delivered to foodbanks as well as their fair allocation, and to minimize costs and carbon emissions associated with transportation operations. To solve this problem, we introduce a matheuristic algorithm. This algorithm iteratively solves single-vehicle subproblems and employs a master model to produce a solution for the original multi-vehicle problem, leveraging single-vehicle solutions. Computational experiments conducted with both real-life and synthetic data demonstrate the efficacy of this proposed methodology. The work described in this study is supported in part by TUBITAK 1001 grant 121M585.",A matheuristic for the foodbank routing problem,"[55402, 8542]",419,"[100, 145, 77]",3879,Humanitarian Aid,78,13,13,Secure & Sustainable Food Supply,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,15 [building - 116],"['OR in Sustainability', 'Vehicle Routing', 'Multi-Objective Decision Making']",WB-13
"As customer demand changes over time, reevaluation of strategic decisions concerning network design becomes imperative. However, this entails significant investment in capital goods, necessitating resilience to future disruptions. Consequently, this crucial decision often gets deferred in practice, leading to inefficient distribution networks. To address this challenge, we introduce a robust decision-support tool for evaluating strategic choices under varying scenarios. Specifically, our focus lies on the optimization problem of facility location and network design [FLNDP] within a distribution network. This involves selecting the optimal subset of new warehouses to open from a set of potential locations, taking into account the significant return flow of items. In addition, a strategic planner can opt to equip the opened warehouses with facilities for inspecting returned items at fixed costs. Our contribution is twofold. First, we develop a mathematical model to solve the FLNDP that allows for transshipments between warehouses, in contrast to earlier literature. This results in notable differences in the optimal solution of the network design, due to the imposition of fixed setup costs for transportation. Second, we provide managerial insights by evaluating a use case with real-world data, featuring disruptions from a pandemic and geopolitical shifts within Europe. ",A location and assortment problem for e-commerce distribution with returns and transshipments,"[78992, 56013, 46298, 63457]",356,"[43, 79, 138]",3884,Recommender systems,17,2,31,Analytics,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,54 [building - 208],"['Facilities Planning and Design', 'Network Design', 'Supply Chain Management']",MA-31
"Integrated container logistics rely on efficient management of first-mile and less-than-container-load [LCL] processes, particularly in drayage, which involves the short-distance movement of full container loads to consolidation centers or for modality changes. Given that shipping companies often engage external carriers for drayage operations, effective drayage procurement becomes crucial. This study introduces a dynamic programming approach to tackle the drayage procurement problem, integrating capacity planning optimization to devise a reservation plan that maximizes operational value. The scalability of the proposed algorithm is assessed, with an extension to a model-free approach inspired by advancements in artificial intelligence aimed at mitigating computational burdens associated with executing the dynamic program for evaluating capacity plans. Execution times and optimality gaps between exhaustive and approximate dynamic programming methods are compared on small instances of the problem. Furthermore, the scalability of the approximate method to larger instances is demonstrated. Additionally, a parameterization of carrier capacities is proposed to further reduce the complexity of capacity optimization programming with minimal losses in optimality. ",Integrated optimization of operations and capacity strategy for drayage procurement in first-mile & LCL container logistics,[79009],660,"[12, 108, 8]",3885,"Advancements of OR-analytics in statistics, machine learning and data science 17",16,13,06,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1013 [building - 202],"['Capacity Planning', 'Programming, Dynamic', 'Artificial Intelligence']",WB-06
"Signed networks are graphs in which each edge has associated a sign. They were introduced as a simplified model to analyze social relationships. A positive sign in an interaction represents friendship while a negative one means animosity. Central notions in signed networks relate to the possibility of partition the vertices into clusters so that the positive edges join vertices in the same cluster and the negative edges join vertices in different clusters. This leads to two relevant notions - structural balance [having such a partition into two clusters] and social equilibrium [having such a partition].  We study signed graphs with respect to these two notions from the point of view of cooperative game theory. We first introduce some new families of simple games on signed network and analyze several parameters and properties.   We compare them to the ones known for other subclasses of simple games, in particular with social disruption games which were introduced regarding social equilibrium.   In addition, we give some complexity results on the considered properties.

[Supported by Spanish AEI grant MICINN PID2020-112581GB-C21]
",Clustering prevention games on signed networks ,"[36990, 19815, 70944]",641,"[50, 132, 16]",3886,"Game Theory, Solutions and Structures VI",88,8,36,"Game Theory, Solutions and Structures","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,32 [building - 306],"['Game Theory', 'Social Networks', 'Complexity and Approximation']",TB-36
"This talk is devoted to the new class of strongly convex sets with variable radii in Hilbert spaces recently introduced by L. Thibault and the author. Such a strong convexity property can be seen as the convex counterpart of the famous phi-convexity introduced by A. Canino in 1988 and thoroughly developed in a 2010 survey by G. Colombo and L. Thibault under the name prox-regularity with variable thickness [also known in the case of constant radius/thickness as proximally smooth sets, positively reached sets, weakly convex sets, O[2]-convex sets, etc.].

Roughly speaking, a strongly convex set with a variable radius [is obviously convex and] has its curvature locally bounded from below by an appropriate function. Whenever the latter function is constant, our strong convexity coincides with the usual strong convexity of sets [see, e.g., a recent survey by V.V. Goncharov and G.E. Ivanov]. Strongly convex sets have been involved in various mathematics, including - separation properties, minimal time problem, preservation of prox-regularity, linear differential games, sweeping processes, etc. 

The main aim of the presentation is to provide numerous properties and characterizations of strongly convex sets with variable radii through the differentiability of the so-called farthest distance function, the Lipschitz behavior of the farthest point mapping and the strong monotonicity of truncated normals. Perspectives and open questions would be also provided.",Strongly convex sets with variable radii,[61616],295,"[81, 19, 21]",3887,Variational Methods in Vector Optimization,82,8,42,Variational Analysis and Continuous Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,98 [building - 306],"['Non-smooth Optimization', 'Continuous Optimization', 'Convex Optimization']",TB-42
"Benders decomposition has been applied significantly to tackle large-scale optimization problems with complicating variables, which, when temporarily fixed, yield problems significantly easier to solve. Still, in its standard form, Benders decomposition shows several shortcomings, for instance upper bound zigzagging [for a minimization problem]. We propose the Primal Benders Decomposition where the upper bound decreases at each iteration. It fully profits from primal information generally available in practice. As consequence, the number of cuts needed  to converge is drastically reduced. Tests on large scale academic and industrial problems will be presented.",Primal Benders Decomposition,"[24885, 73660, 10966]",865,"[109, 63, 65]",3889,Topics in Integer Programming I,64,12,25,Combinatorial Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,011 [building - 208],"['Programming, Integer', 'Large Scale Optimization', 'Logistics']",WA-25
"We present a case exercise teaching students how to apply mathematical programming to a real-life context.
The case deals with food donation supply chain management. Using a project-based approach, the case proposes a realistic scenario simulating the consultancy interaction with the [fictional] nonprofit company. 
The Logica&Co acts as a two-sided platform connecting supply and demand. The objective is to define an effective strategy for collecting and delivering food donations from local businesses to soup kitchens over a semester-long timeframe.
The problem exhibits non-linear characteristics; however, it is designed with adjustable difficulty levels.
An interactive offline tool, the SoS simulator, can be furnished to students for performance assessment. the tool is publicly available for download and can be customized by the instructors.
The case was proposed as a competitive group challenge to students of the Bachelor's or Master's program in Management Engineering at Sapienza University of Rome.
Students appreciated both the teaching methodology and the teamwork aspects; they highlighted the utility of the SoS simulator tool.
", Optimizing food donation delivery for nonprofit company Logica&Co,"[79015, 70498]",914,"[84, 22, 65]",3890,"OR Initiatives for Education, Sustainability, and Developing Countries",48,5,16,OR Education,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,19 [building - 116],"['Optimization Modeling', 'Critical Decision Making', 'Logistics']",MD-16
"The strategic allocation of tenants within shopping centers, known as tenant mix, is crucial for enhancing profitability in the retail sector. This research delves into creating an ideal combination of retail categories and their placement to boost rental income, a primary financial source for mall operators. Utilizing integer linear programming, we propose a model that integrates the concept of tenant synergy at its core—a critical yet underexplored aspect in the literature. This model includes constraints based on the total leasable space available and the strategic distribution of store units. Drawing upon a dataset from 27 shopping centers in Spain, we construct a regression model to estimate base rent, positioning it as the critical component of our objective function to maximize rental income. Additionally, this objective function features a synergy-based scoring system as an extra component, designed to enhance sales revenues and create a balanced retail environment through strategic tenant placement. The effectiveness of our model is demonstrated through several case studies, highlighting its potential to increase rental income and sales. Our findings offer mall operators a practical tool to optimize vacant spaces, facilitating strategic decision-making in the retail industry.
Acknowledgments - This research was partially supported by ANID [Chile] through Ph.D. scholarship #72190065 and AGAUR through Ph.D. project #2019DI098.",Enhancing Shopping Centre Profitability through Optimized Tenant Mix and Synergistic Placement,"[74020, 5708]",634,"[109, 64]",3892,Retail Optimization,30,15,50,Retail Operations,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,M2 [building - 101],"['Programming, Integer', 'Location']",WD-50
"This annotation proposes an algorithm for solving combinatorial optimization problems with fractional-linear objective functions. Such problems are ubiquitous in diverse domains including operations research, scheduling, and network optimization, where the goal is to find an optimal arrangement of elements from a finite set.
The proposed algorithm uses the properties of combinatorial configurations of permutations and their graphs.
The essence of the algorithm is to perform the following steps:
the coefficients of the numerator and denominator of the fractional linear function are determined, the elements of the permutation are specified;
for a selected pair of adjacent permutations, the difference in the values of the linear fractional function is calculated;
based on the calculated coefficients and signs of the differences, a structured permutation graph is formed, which allows you to analyze changes in the values of the objective function at the vertices of the graph;
for the selected elements of the permutation in the graph, a Hamiltonian path is formed along which the values of the objective function change;
extreme values of the objective function are calculated, which are achieved at the extreme points of the permutation graph.

This algorithm allows you to solve combinatorial optimization problems with fractional linear objective functions by analyzing the function values for various permutations and constructing a structured permutation graph.",Algorithm for Solving Combinatorial Optimization Problems with Fractional-Linear Objective Functions,[58037],880,"[14, 5, 113]",3894,Exact methods in combinatorial optimization [Contributed],64,9,52,Combinatorial Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,8003 [building - 202],"['Combinatorial Optimization', 'Algorithms', 'Programming, Nonlinear']",TC-52
"Efficient primal and dual algorithms have been developed recently by the authors for the n-dimensional minimum covering Euclidean ball problem of several types. These include the minimum covering ball of a given finite set of distinct points, the minimum covering ball of a given finite set of balls each with different radius and center, and the minimum covering ball of a set of distinct points with weighted Euclidean distance between the points and the center of the covering ball. Each problem type corresponds to an equivalent one-center location problem.

The common approach for each algorithm is a search-path method, where the search path is determined by the intersection of bisectors of pairs of points. Each problem has a different structure for the bisectors of pairs of points [hyperplanes, hyperboloids, and hyperspheres, respectively]. The step size along the search path is computed exactly at each iteration. Each point on the search path is primal [dual] feasible and satisfies complementary slackness for the primal [dual] algorithm. 

Example problems are presented for each problem type and illustrate the bisectors, their intersection, and the optimality conditions. Computational results are presented. 

Current work is presented on the combined problem of finding the minimum covering Euclidean ball of a given set of balls with weighted distance. The bisector of a pair of points is a quartic surface.",The minimum covering Euclidean ball problem - review and report,"[70040, 64927]",766,"[64, 113]",3898,Nonlinear Location Optimization,29,8,61,Locational Analysis,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S10 [building - 101],"['Location', 'Programming, Nonlinear']",TB-61
"Problem definition - When selling online, manufacturers can either sell directly to customers through the platform [agency channel], sell their products to the platform [reselling channel], or engage in both channels simultaneously. Manufacturers typically operate with limited production capacity, and this information is often not disclosed to online platforms.
Methodology/results - Our study develops a screening game framework where the manufacturer has private information about its capacity, and the online platform presents a menu of quantity-price pairs contracts to the manufacturer. The manufacturer then determines the quantity allocated to the reselling channel and to the agency channel. Our finding highlights that the manufacturer is more likely to use both [agency and reselling] channels under the asymmetric information game. We also show that a manufacturer with high capacity benefits from keeping its capacity information private, but a manufacturer with low capacity does not. Furthermore, the platform and the manufacturer may not benefit from the manufacturer having more production capacity. Lastly, we find that if the manufacturer’s capacity is sufficiently small, then the platform may be better off not knowing the manufacturer's capacity. 
Managerial implications - Our study offers insights and guidance for manufacturers and online platforms, assisting them in making strategic decisions and navigating the dynamics of information sharing within the online retail.",Channel Structure of Online Retail Platforms - Impact of Asymmetric Information and Capacity Constraints,"[79019, 23431, 68884]",934,"[138, 50, 12]",3899,"Online, Omnichannel, and Pricing",30,15,61,Retail Operations,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S10 [building - 101],"['Supply Chain Management', 'Game Theory', 'Capacity Planning']",WD-61
"The present study proposes a Territory Design Strategy for the Multiperiod Vehicle Routing Problem with Simultaneous Pickup and Delivery and Time Windows [TD-MPVRPSPDTW]. This problem deals with the design of compact and contiguous territories to simultaneously collect and deliver orders from a warehouse to customers who have time windows requirements in a multi-period approach. This problem is motivated by the circularity of the supply chain because it helps to integrate the delivery and collection of products. Besides, the proposed territory design strategy lets generate consistent routing plans due to each driver being assigned to one territory. We proposed a mixed-integer linear programming model [MILP] for the problem, which is solved using a metaheuristic algorithm. We evaluate the performance of the proposed algorithm through benchmark instances.
",Territory Design Strategy for the Multiperiod Vehicle Routing Problem With Simultaneous Pickup and Delivery and Time Windows,"[79020, 71223, 38450]",749,"[145, 65, 74]",3900,Vehicle Routing Problems With Time Windows,5,13,64,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S16 [building - 101],"['Vehicle Routing', 'Logistics', 'Metaheuristics']",WB-64
"Offshore off-grid power-to-X [PtX] processes are gaining research interest due to their potential in unlocking untapped renewable energy sources and meeting the growing demand for green hydrogen and its derivatives. However, the inflexibility of the chemical processes involved, coupled with the stochastic nature of wind power production as the sole energy source, poses significant operational challenges. Despite the buffering function of energy and material storages included in the system, the inherent uncertainty in wind power generation complicates the assurance of safe and continuous operation, given the narrow operational windows and low ramp rates. Commonly proposed predictive control solutions, such as model predictive control [MPC], often overlook the uncertainty from wind power forecast deviations, which is particularly significant offshore. We propose a novel MILP-based MPC framework that incorporates two-stage stochastic programming to address this uncertainty. The framework generates scenarios using an ARMA model based on deterministic wind forecasts and reported statistical error characteristics. This approach aims to enhance the operational efficiency of off-grid PtX processes, thereby increasing their load factor, product yield, and competitiveness. It is expected to facilitate wide-scale deployment of these processes, contributing in the transformation towards a fossil-free economy.",Incorporating two-stage stochastic programming in the optimal scheduling of an offshore off-grid power-to-X system,"[79022, 79029, 79030]",472,"[82, 93, 136]",3902,Distributed energy systems,21,10,22,Energy Management,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,81 [building - 116],"['Optimal Control', 'OR in Energy', 'Stochastic Optimization']",TD-22
"MILP resolution is strongly impacted by symmetries, namely the existence of solutions identical up to a permutation. In this presentation, we focus on MILP with structured symmetries, ie any column permutation of the solution matrix is a symmetry. Various types of symmetry-breaking techniques exist to handle such symmetries, from specialized branching disjunctions, lexicographic ordering or symmetric variables aggregation. We focus on the latter technique which has shown to be very efficient on problems with specific structure, namely featuring the disaggregation property. We study in which case the aggregation technique can be used and how to benefit from its efficiency in more general cases. Case studies on variants of the Unit Commitment problems are proposed.",Structured [sub-]symmetry breaking in MILP by variable aggregation,[79023],882,"[14, 109, 103]",3903,Topics in Combinatorial Optimization I [Contributed],64,14,25,Combinatorial Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,011 [building - 208],"['Combinatorial Optimization', 'Programming, Integer', 'Polyhedral Combinatorics']",WC-25
"This study presents a novel approach to schedule surveillance operations for critical undersea infrastructure. The problem addresses the vulnerability of undersea infrastructure to deliberate attacks, natural disasters, accidents and anchor droppage incorporating their associated risks. The aim is to allocate and order the surveillance operations across different levels, including undersea, surface, and aerial activities. There is a trade-off between minimising surveillance operations cost and maximising incident detection. Therefore, the problem is formulated as a novel bi-objective flexible job-shop scheduling model [BO-FJSS] considering operational constraints for multiple types of surveillance assets, such as remotely operated underwater vehicles [ROVs], autonomous underwater vehicles [AUVs], unmanned aerial vehicles [UAVs] and surface vessels. The study also emphasises on the significance of various risk factors, achieved by utilising the Analytic Network Process [ANP] to assess the risks at different depths and incorporating them as weights in the BO-FJSS model. The computational experiments on a specific gas pipeline between the UK and Norway, offer valuable insights into the diverse levels of risks associated with various events within different depth ranges. This information can assist in prioritising risk management strategies and efficiently allocating resources. ",Integrated Risk-based Bi-objective Surveillance Scheduling for Undersea Infrastructure Safety,"[22501, 77549, 8301, 3018]",793,"[129, 77, 126]",3905,"Military, Defense, and International Security I",65,5,20,"Military, Defense, and International Security","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,45 [building - 116],"['Scheduling', 'Multi-Objective Decision Making', 'Risk Analysis and Management']",MD-20
"The Transmission Maintenance Scheduling [TMS] is a strategic problem in asset management for electrical utilities around the worlds. It consists in generating an annual plan for the preventive maintenance of electric power transmission equipment. The resulting maintenance plan must ensure a continuous power flow during maintenance operations. It must also satisfy various constraints such as the stability of the network and the availability of skilled teams to fulfill the planned maintenance tasks. The proposed TMS problem is part of an important project for asset management at Hydro-Quebec [HQ]. It is a variant addressing the human resource allocation constraints to ensure the feasibility of a maintenance plan [respecting the available number of technicians in each period]. The problem is solved using a constraint-satisfaction approach and experiments are carried out on a HQ power subnetwork, with limits on the capacity of available electricians. ",Resource-constrained transmission maintenance scheduling problem ,"[74751, 79025, 79310, 72598, 79311]",660,"[26, 151, 144]",3906,"Advancements of OR-analytics in statistics, machine learning and data science 17",16,13,06,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,1013 [building - 202],"['Decision Support Systems', 'Practice of OR', 'Utility Systems']",WB-06
"We consider different methods for generating cuts and solving mixed-integer semidefinite programming [MISDP] instances within an outer approximation framework. In fact, the main components of the classical outer approximation algorithm for convex mixed-integer nonlinear programming can easily be tailored towards MISDP such that similar convergence properties are obtained. We propose some new methods for generating cuts that have desirable theoretical and computational properties and we present a numerical comparison.",Mixed-Integer Semidefinite Programming by Outer Approximation,[70639],478,"[5, 14, 111]",3907,Algorithms for Mixed-Integer Nonlinear Programming and Nonconvex Optimization,86,12,04,MINLP,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1001 [building - 202],"['Algorithms', 'Combinatorial Optimization', 'Programming, Mixed-Integer']",WA-04
"Energy system models provide insight into future energy trends, with real-life applications for supporting strategic decision-making. In this study, we assess the impact of a rolling planning horizon methodology [i.e. myopic, limited foresight, and perfect foresight over the modelling horizon] in the comprehensive fully sector-coupled European energy system model, Balmorel. We compare imperfect foresight i.e. myopic and limited foresight with full decision-making foresight, with a particular focus on the role of hydrogen production in the Nordic region. We apply policy targets and visions, and assess the impact on the design of future energy systems as well as how the role of the Nordic region as a hydrogen exporter is impacted. In addition to analyzing the potential impacts on the design of the energy system, we find considerably longer computation time for solving the comprehensive large-scale energy system with limited or full foresight compared to a myopic approach. Finally, we discuss the implications for energy modellers and decision-makers when assessing future energy systems, which are optimized using different planning foresight horizons.",Impact of imperfect foresight in long-term energy planning - A focus on hydrogen production in Europe and the Nordics,"[58629, 77553, 62517]",261,"[37, 93]",3909,Hydrogen Modeling and Regulation I,22,14,09,Energy Markets,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,10 [building - 116],"['Energy Policy and Planning', 'OR in Energy']",WC-09
"Many operations rely on forecasts of future events, often determining the allocation of significant sums of money. Fortunately, algorithmic forecasts achieve high levels of accuracy, but many decision-makers reject algorithms upon observing they are not perfectly accurate. What determines whether decision-makers perceive an algorithm as inaccurate, and how are these perceptions related to their willingness to use algorithms when forecasting future events? We propose that the tendency to dismiss algorithmic advice is partially due to the common practice of presenting forecasts as single points rather than intervals. In Study 1, we find that people perceive equally imperfect forecasts as more accurate when these forecasts are presented as intervals rather than points. In Study 2, we examine how these differential accuracy perceptions affect people’s willingness to use algorithms. Here, we find that people do not necessarily favor interval over point forecasts when deciding whether to rely on the algorithm’s forecasts. Instead, such preference only occurs when people are explicitly prompted to evaluate the algorithm’s accuracy before deciding whether to commit to its forecasts. In sum, these findings underscore the complex processes underlying people’s perceptions of algorithmic errors and their subsequent reliance on algorithms. Moreover, they show how managers can apply interval forecasts to effectively increase human forecasters’ reliance on forecasting algorithms.","Accuracy, Awareness, and Acceptance - How Attention Drives People’s Use of Point versus Interval Forecasting Algorithms","[78950, 36644]",111,"[10, 25, 26]",3911,Scenarios and foresight practices - Behavioural issues I,13,12,11,Behavioural OR,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,12 [building - 116],"['Behavioural OR', 'Decision Analysis', 'Decision Support Systems']",WA-11
"We consider a class of stochastic difference-of-convex-functions [DC] programs, which are optimization problems where the objective function is the expected value of a stochastic DC function based on a probability distribution, while the constraint functions are DC. There have been many methods developed for convex [nonconvex] stochastic optimization problems without constraints or with convex constraints; however, methods designed for nonconvex and nonsmooth constrained programs are rare. The DC algorithm [DCA] is acknowledged in the literature on deterministic optimization as one of the few efficient algorithms for solving large-scale nonconvex and nonsmooth optimization problems. Using penalty techniques, we transform nonconvex constrained stochastic DC programs into standard stochastic DC programs and introduce novel stochastic DCAs to solve the resulting stochastic DC programs. The convergence analysis of the proposed algorithms is thoroughly investigated, and numerical experiments are performed to evaluate their behaviors.",Solving Stochastic DC programs with DC constraints,"[78663, 10867, 79299]",452,"[136, 107]",3914,Stochastic and Deterministic Global Optimization,93,4,41,Stochastic and Deterministic Global Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,97 [building - 306],"['Stochastic Optimization', 'Programming, Constraint']",MC-41
"Personnel in the Australian Defence Force must complete a number of training courses throughout their career. Currently, timetabling is performed manually across hundreds of courses and associated sessions, resulting in inefficient or incorrect timetables. To address this, we have created an optimisation algorithm to improve timetabling efficiency and administrative workloads. For this problem, there are a number of hard constraints, including - a maximum number of students per session, students cannot attend overlapping courses, and a number of fixed training pathways or course sequences. Soft constraints such as prioritisation of students is also important. In this domain efficiency is defined in terms of the total time delays in students being able to complete their training.

The algorithm uses a combination of Dynamic Programming [DP] and Genetic Algorithms [GA] to create optimal timetables where DP is initially used to find the optimal timetable for each student over course pathways and session options - which decrease as each timetable is generated over the student priority queue. The GA is then used to, where possible, improve upon these initial solutions by considering more efficient permutations and combinations of specific course and sessions over all the student DP solutions. The results showed a 45% increase in efficiency, reducing the average student delay by approximately half compared to the manual solution, and in a fraction of the time.",Student Timetabling Optimisation for the Australian Defence Force,"[77453, 53127]",980,"[142, 84]",3917,"Military, Defense, and International Security II",65,7,20,"Military, Defense, and International Security","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,45 [building - 116],"['Timetabling', 'Optimization Modeling']",TA-20
"The Clustered-State Markovian Arrival Process [CS-MAP] is introduced as an extension of the Markovian Arrival Process [MAP]. This new model can be employed to model marked point processes with a finite mark space, but our focus lies in its application for modeling recurrent processes with terminal events, which are prevalent in the biomedical context where temporal sequences of recurrences are observed, preceded by the death of the patient. We present novel results regarding this stochastic process, including explicit expressions for the marginal and joint densities and for the moments and correlations of the inter-event times, as well as for the probability mass function of the number of recurrences before death. Furthermore, we provide an explicit expression for the likelihood function, incorporating right-censoring, which is common in survival analysis. Maximizing the log-likelihood function poses a challenge due to the large number of parameters in the model; however, we propose a heuristic approach for this purpose and we employ an appropriate local maximization algorithm. Additionally, we introduce some methods to enhance computational efficiency, such as a simplification of the likelihood function. Finally, we demonstrate the effectiveness of the proposed maximum likelihood approach using simulated data and apply it to model real data concerning patients with oncological diseases.",The Clustered-State Markovian Arrival Process - A Framework for Dependent Recurrence and Mortality Modeling,"[78949, 63237, 50114]",450,"[135, 0]",3918,Analysis of Stochastic Models I,50,10,39,Stochastic Modelling,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,35 [building - 306],['Stochastic Models'],TD-39
"We consider a provider of electric vehicle charging that operates a network of charging stations and uses time-varying pricing to maximize profit and reduce the impact on the electric grid. We propose a bilevel model with a single leader and multiple disjoint followers. The customers [followers] makes decisions independently from each the other. The provider [leader] sets the prices for each station at each time slot, and ensures there is enough energy to charge. The charging choice of each customer is represented by a combination of a preference list of [station, time] pairs and a reserve price. The proposed model takes thus into accounts for the heterogeneity of customers with respect to price sensitivity and charging preferences. We define a single level reformulation based on the reformulation for the rank pricing problem. Computational results highlight the efficiency of the new reformulation and the impact of the model on the grid peak.","Optimal Electric Vehicle Charging with Dynamic Pricing, Customer Preferences and Power Peak Reduction","[3287, 25372, 79039]",468,"[93, 0]",3920,Optimization for electric vehicles,21,8,22,Energy Management,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,81 [building - 116],['OR in Energy'],TB-22
"Optimization problems on graphs involve the search for graphs of a given size that minimize or maximize a certain invariant or a combination of invariants. These problems are usually NP-hard and the exhaustive enumeration of all possibilities is not acceptable. Efficient dealing with these problems can help theoreticians to get some useful hints and make analytical proof out of these results. Computer-assisted proofs are common in the graph theory for a while now. Several dedicated software packages have been developed, such as AutoGraphiX [https://www.autographix.ca], newGraph [https://www.mi.sanu.ac.rs/newgraph/], PHOEG [https://phoeg.umons.ac.be/phoeg], to mention a few. They are, however, too general and in some cases could require unacceptably long CPU time to provide useful results. Another popular approach is to develop metaheuristic tailored for each particular optimization problem. We illustrate the application of Variable Neighborhood Search [VNS] metaheuristic to Spectral Reconstruction of Graphs and Maximization of Spectral Radius of Threshold Graphs. The proposed algorithms are tested on a representative set of test graphs. Conducted experimental evaluation shows the superiority of the proposed approach over the application of dedicated software. The work has been partially supported by the Scientific Fund of the Republic of Serbia, Grant LZWK.",Metaheuristics for Optimization on Graphs,"[51528, 79390, 79043, 79041, 79042, 79044, 79045]",401,"[53, 74, 134]",3921,Parallel Optimization and Scalability,64,13,52,Combinatorial Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,8003 [building - 202],"['Graphs and Networks', 'Metaheuristics', 'Software']",WB-52
"Auctions with below-bid pricing [e.g., uniform-price, and ascending auctions] have remarkable theoretical properties, but practitioners are skeptical about their implementation. We present a dynamic model of collusion in multiunit auctions that explains this gap between theory and practice. To sustain collusion at the reserve price, bidders submit crank-handle bids. The cost of sustaining crank-handle collusion depends on the degree of below-bid pricing in the auction. Our model predicts that crank-handle collusion is easier to sustain in auctions with more below-bid pricing and when bidders are more symmetric. Evidence from auctions of fishing quota in the Faroe Islands supports our predictions.",EPIC Fail - How Below-Bid Pricing Backfires in Multiunit Auctions,"[78928, 79046, 79047]",639,"[9, 50]",3922,Market Design 1 - Auctions,87,10,43,Market Design,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,99 [building - 306],"['Auctions / Competitive Bidding', 'Game Theory']",TD-43
"We consider kernel based Interior-point methods [IPMs] for $P_*[\kappa]$-
Linear Complementarity Problems [LCP] that are based on the class of Eligible kernel functions [EKFs].  The importance of kernel-based IPMs stems from the fact that the iteration bounds of large-step IPMs is significantly improved for some instances of EKFs.  However, the derivation of the iteration bounds for particular EKFs is usually long and quite involved which was the motivation to investigate whether this process can be simplified and under what conditions. 

Hence, we introduce additional conditions on the class of EKFs, which are not very restrictive, however, they allow for the significant simplification of the analysis and calculation of iteration bounds. We derive a new simplified scheme to calculate iteration bounds and illustrate it with calculation of iteration bounds of most EKFs with polynomial and exponential barrier terms mentioned in the literature. In all cases we match the complexity obtained using the classical scheme.   
",Simplified Analysis of Kernel-Based Interior-Point Methods for $P_*[\kappa]$- Linear Complementarity Problems,"[33028, 68231, 38402, 67315, 57860]",139,"[60, 21, 113]",3923,Interior point methods,68,3,38,"Conic Optimization - Theory, Algorithms, and Applications","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,34 [building - 306],"['Interior Point Methods', 'Convex Optimization', 'Programming, Nonlinear']",MB-38
"We describe several methods for approximating chance constraints on the duration of surgeries assigned to a surgical session.
We use historical data to create an empirical distribution of the session duration and employ SVMs and Probit regression to learn the probability that a session runs overtime, given the surgeries scheduled in it. These models are chosen as linear constraints can easily be formed from the results.
Through numerical experiments, we demonstrate the accuracy of these models to act as binary classifiers, and their effectiveness as approximations to chance constraints. We compare them to a distributionally robust approach to approximating the chance constraints, and show that the they result in similar, in many cases exactly the same, schedules but require much less time to solve the optimisation problem.",Machine Learning of Chance Constraints for Scheduling Surgeries,[79048],599,"[56, 129, 136]",3924,Surgery Scheduling and Operating Room Planning,3,8,10,OR in Health Services [ORAHS],"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,11 [building - 116],"['Health Care', 'Scheduling', 'Stochastic Optimization']",TB-10
" Understanding how players adjust their strategies in games, based on past experience, is a crucial tool for policymakers.  It enables them to forecast the system's eventual behavior, exert control over the system, and evaluate counterfactual scenarios.  The task becomes increasingly difficult when only a limited number of observations are available or difficult to acquire.  In this work, we introduce the Side-Information Assisted Regression [SIAR] framework, designed to identify game dynamics in multi-player normal-form games only using data from a short run of a single system trajectory. To enhance system recovery in the face of scarce data, we integrate side-information constraints into SIAR, which restrict the set of feasible solutions to those satisfying game-theoretic properties and common assumptions around strategic interactions.   SIAR is solved using sum-of-squares optimization, resulting in a hierarchy of approximations that provably converge to the true dynamics of the system. 
We showcase that the SIAR framework accurately predicts player behavior across a spectrum of normal-form games, widely-known families of game dynamics, and strong benchmarks, even if the unknown system is chaotic.",Data-scarce identification of game dynamics via sum-of-squares optimization,[79055],187,"[50, 21, 20]",3929,Experimental economics and game theory 2,73,14,40,Experimental economics and game theory,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,96 [building - 306],"['Game Theory', 'Convex Optimization', 'Control Theory']",WC-40
"The objective of this study is to clarify the impact of market characteristics of the mobile telecom market in Japan, the Republic of Korea [South Korea], the United Kingdom [UK], and the United States [USA] on the efficiency of management of the Mobile Network Operators [MNOs] in each market. The MNOs are said to be affected by their government policies. In comparison to the efficiencies of MNOs internationally, it is necessary to consider not only the external factors that the MNO can NOT control but also the internal factors that the MNO can control. This study employs meta-frontier to separate those factors and clarify the efficiency gap [i.e., the Technology-Gap] between the countries. The Technology-Gap reveals a range of characteristics across international mobile network operator markets - the platform migration strategy [3G to 4G, 4G to 5G], the mobility of customers, the smartphone [device] sales and customer acquiring strategy, the government regulations, and so on.
One of the results shows that the global MNO markets can be divided into two categories given the mobility of customers. The markets that have high mobility of customers migrate their generation aggressively and sell smartphones [devices] at lower prices to acquire new customers. The markets that have low mobility of customers tend to cut their service costs and lower their service fees to retain customers.
",The international Efficiency Gap Analysis of Mobile Network Operator markets,"[57491, 28177]",942,"[24, 35, 151]",3930,DEA applications in Policy Making and Planning I,89,10,48,Data Envelopment Analysis and its Application,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,60 [building - 324],"['Data Envelopment Analysis', 'Efficiency Analysis', 'Practice of OR']",TD-48
"The first draft of the human genome project was published more than two decades ago. It was incomplete, especially in the centromeric and telomeric regions, which are highly repetitive. With the development of third-generation technology, it has become possible to fill most of the gaps in the reference genome [T2T consortium]. However, long-read technologies can cover large portions of the genome, but this involves higher error rates. Thus, de novo sequencing projects often use different sequencing technologies to, on the one hand, obtain long DNA fragments and, on the other hand, improve the sequence quality with short reads.

In the Genomic Map of Poland project, we used a pipeline to construct de novo a diploid human genome based on the trio - mother, father and child. We used several technologies in the pipeline - short reads, PacBio HIFI, Hi-C and ultra-long Nanopore. The resulting chromosome-wide scaffolds were compared to reference genomes [GRCH38 and CHM13]. We then assessed the quality of our reference genome assembly by analyzing the consistency of k-mers appearing in the genome sequence and in short and long reads. It was also checked whether k-mers specific to only one of the parents [mother or father] occur in the copy of the chromosome inherited from one parent [each copy of the chromosome contains small differences in the sequence, specific to the individual]. Various k-mer features indicate the high quality of the assembled genome.",Assesment of the quality of de novo assembled genome,"[5423, 79364, 11807, 5390]",381,"[17, 0]",3932,Advancements in AI and Genomics - Bridging Technology and Biology for Future Healthcare Solutions,2,10,20,"Computational Biology, Bioinformatics and Medicine","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,45 [building - 116],"['Computational Biology, Bioinformatics and Medicine']",TD-20
"Power sector plays a crucial role towards decarbonization for many economies, especially in line with the net-zero targets to limit global warming by 1.5 Celsius degrees. Technical constraints intrinsic to the sector, penetration of new technologies, investment and operational costs, as well as its links with the rest of the economy make the power sector a complex system to analyze. Although there are numerous studies to integrate bottom-up power sector technology models with top-down macroeconomic models, this study is the first attempt to link the three separate and interrelated models within a single framework - an electricity market simulation model, a generation expansion planning model, and an applied general equilibrium model. The proposed framework is implemented to analyze alternative scenarios aiming at successful phasing-out of coal-fired power plants in Turkey. The results suggest that, given the existing capacity and potential of renewables, Turkey can achieve her coal-phase out by early 2030s. The results also show, while installed capacity and generation of coal-fired power plants are reduced, real GDP and electricity demand could be maintained; and the CO2 emissions from power sector could be reduced by as much as 50% in 2030 compared to their 2018 level. ",A New Integrated Assessment Framework for Energy-Economy-Environment Modeling - Insights from Coal Phase-out in the Turkish Power Sector towards Net-zero Emission Targets,"[24342, 79059, 80061, 79394, 79393, 79396]",711,"[37, 33, 28]",3933,Environment and climate change,21,14,22,Energy Management,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,81 [building - 116],"['Energy Policy and Planning', 'Economic Modeling', 'Developing Countries']",WC-22
"This research focuses on the robust vehicle routing problem with time windows [VRPTW] under uncertain travel times. More specifically, we assume all travel times belong to a known budget uncertainty set. To solve large-scale instances, one often resorts to local search algorithms. These algorithms involve checking whether a route is feasible regarding the time window; if it is not, evaluate its infeasibility. Our goal is to study the computation of the infeasibility of the robust VRPTW with budget uncertainty for two classical cases of functions. We prove that the computation of both cases is polynomially solvable by providing dynamic programming algorithms to solve them.",Polynomial-time algorithms to compute violation in the robust vehicle routing problem with time windows and budget uncertainty,"[74938, 23058, 75011, 67821]",573,"[127, 145, 108]",3935,Location and transportation problems under uncertainty,49,10,35,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,44 [building - 303A],"['Robust Optimization', 'Vehicle Routing', 'Programming, Dynamic']",TD-35
"We consider an unconstrained optimization problem whose objective function is expressed by the sum of a continuously differentiable function and an L1-regularizer. One approach for solving such problems, the Newton-type proximal gradient method, has gained attention. In Newton-type proximal gradient methods, we need to solve a sub-problem to obtain the scaled proximal mapping at each iteration. However, the computational cost of solving this subproblem is sometimes very high. In our study, we address this difficulty by approximating the sub-problem in a more easily solvable form. Concretely, we approximate the L1-norm within the objective function of the subproblem using a quadratic function. This method allows the solution to the subproblem to be found using only simple calculations. However, the obtained solution requires a system of linear equations to be solved. In response to this issue, we calculate an approximation to the Hessian in the Newton-type proximal gradient method using a memoryless quasi-Newton method. In addition, we use the Sherman-Morrison-Woodbury formula to obtain the closed-form solution to the inverse matrix with reduced computational costs. Thus, we easily obtain an approximate solution for the scaled proximal mapping. Based on the proposed method for the subproblem, we give an algorithm of Newton-type proximal gradient methods. In addition, we show the global convergence of the proposed algorithm under some standard assumptions. ",Newton-type proximal gradient method with quadratic subproblem approximation for unconstrained optimization with L1-regularizer,"[73744, 79061]",955,"[19, 81, 5]",3936,Nonsmooth optimization algorithms II,70,15,41,Nonsmooth Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,97 [building - 306],"['Continuous Optimization', 'Non-smooth Optimization', 'Algorithms']",WD-41
"In 1962, Gale and Shapley initiated the study of stable matching problems, introducing the Stable Marriage Problem [SMP]. The aim is to match men and women based on their preferences for all members of the opposite gender. A matching is deemed stable if there are no pairs of unmatched individuals who prefer each other to their partners in the matching. We say that there are no blocking pairs. A natural generalization of SMP is to consider non-bipartite models. For instance, consider a set of n individuals where each one ranks all the others in order of preference. In this context, a matching can be assimilated to pairing individuals to share living spaces, leading to the formulation of the Stable Roommate Problem [SRP]. While there always exists a stable matching in SMP, it is not the case in some instances of SRP. However, Irving provided a polynomial algorithm that, for any instance of SRP, determines whether a stable matching exists or not. If it exists, it produces one. We focus on a different definition of stability, namely local stability, recently introduced in the context of kidney exchange programs [KEPs]. The decision problem's complexity for non-empty locally stable exchanges remains open. When the exchange is a matching, this is equivalent to a local version of SRP. Here, a matching achieves local stability if no blocking pairs intersect with it. In this talk, we explore the trade-offs between global and local stability and discuss the complexity of local SRP.",Locally Stable Roommate Problem,"[78988, 66779, 1344]",871,"[14, 5]",3937,Exact Algorithms and Formulations for Combinatorial Optimization Problems,64,10,29,Combinatorial Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,157 [building - 208],"['Combinatorial Optimization', 'Algorithms']",TD-29
"The consistent travelling salesman problem looks for a minimum-cost set of Hamiltonian routes, one for every day of a given period. When a customer requires service over several days, the service times on different days must differ by no more than a given threshold [for example, one hour]. We analyze two variants of the problem, depending on whether the vehicle is allowed to wait or not at a customer location before its service starts. There are three mathematical models in the literature for the problem without waiting times, and this paper describes a new model appropriated to be solved with a branch-and-cut algorithm. The new model is a multi-commodity flow formulation on which Benders’ Decomposition helps manage a large number of flow variables. There were no mathematical models in the literature for the variant with waiting times, and this paper adapts the four mathematical models to it. We analyze the computational results of the formulations on instances from the literature with up to 100 customers and three days.",Solving the Consistent Travelling Salesman Problem,"[1090, 71345]",725,"[145, 11, 72]",3940,Vehicle Routing II,64,5,29,Combinatorial Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,157 [building - 208],"['Vehicle Routing', 'Branch and Cut', 'Mathematical Programming']",MD-29
"The objective of the work is to help a large international transport operator in the optimized planning of its fleet, which constantly performs full and break-bulk routes, as well as groupages adapted to any requirement of its customers. We are faced with a periodic vehicle routing problem with capacities and time windows [PCVRPTW] with some additional requirements and constraints such as - multiple-week planning, time windows for available days and hours, maximum daily and weekly driving and working times, daily, weekly and bi-weekly breaks for drivers, maximum distance between an unloading and the next assigned load, the first and last load of each week must be the least possible distance from the drivers' homes, the customer wants all vehicles to travel at least 3500 km weekly and reached this value wants to balance the distances, among others. One of the main challenges lies in the logistics operator's desire to reschedule all pending tasks every 15 minutes, since, on the one hand, they are constantly receiving new orders and, on the other hand, unforeseen incidents arise that alter the original schedule. Therefore, the algorithm must be fast enough to provide quality solutions in approximately 4 to 5 minutes. This paper will show the mathematical programming model, the implementation performed with Hexaly, and the solutions obtained in real instances, obtaining the first feasible solutions in a few seconds and quality solutions in a few minutes.",Route Optimization of Freight Transportation Under Multiple Real Operational and Legal Constraints,"[12745, 79067, 79066, 73644]",758,"[145, 72, 112]",3941,Real-Life Applications in Routing,5,5,58,VeRoLog - Vehicle Routing and Logistics,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S07 [building - 101],"['Vehicle Routing', 'Mathematical Programming', 'Programming, Multi-Objective']",MD-58
"Sophisticated decision-making becomes necessary when weighing the value of accepting and simultaneously scheduling or rejecting a random incoming request now versus the value of waiting for another request in the future. This work proposes the Dynamic Request Acceptance and Unsplittable Scheduling Problem [DRAUSP], which considers a single resource with a given capacity over multiple homogeneous time slots. During each decision period within a fixed time horizon, a single random request arrives that demands capacity for some consecutive time slots and yields positive revenue if accepted. The objective becomes to accept or reject the incoming requests and, if accepted, to schedule them into available time slots to optimize the revenue collected over the time horizon. We use a Stochastic Dynamic Programming [SDP] algorithm to optimize the DRAUSP, which we model as a Markov decision process. In addition, we present a dimension compression technique to compute upper and lower bounds using SDP. Our computational study shows that we can derive appropriate bounds for some DRAUSP instances in reasonable computational time. Moreover, we conclude that basic heuristics, such as First Come First Serve, are insufficient to handle the complexity of DRAUSP and that more sophisticated heuristics are necessary to obtain adequate results.",Deriving Upper and Lower Bounds with Stochastic Dynamic Programming for Request Acceptance and Scheduling Decisions under Uncertainty ,"[79065, 67427, 24583]",401,"[14, 84, 117]",3943,Parallel Optimization and Scalability,64,13,52,Combinatorial Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,8003 [building - 202],"['Combinatorial Optimization', 'Optimization Modeling', 'Programming, Stochastic']",WB-52
"    Nowadays, whether  corporate can effectively implement and develop ESG has become a basic public evaluation indicator. 
However, the implementation and development of ESG has a significant impact on business performance. How to develop ESG to enhance  corporate performance has become a focal point  global companies stress.
    Therefore, this study develops a knowledge-driven hybrid model to explore the key path to the development of  ESG in improving corporate operating performance.
First, we define the target company and use m-VIKOR technology to conduct a comprehensive assessment of the company's ESG and operating performance.
Second, we use ESG and business performance analysis to differentiate target companies.
Third, a study is conducted based on the core and rules of data mining exploration influence.
Finally, we develop an effective development path based on the rules.
This research will help companies implement ESG development and help with  climate change.",Developing knowledge-driven hybrid models for  corporate towards ESG,"[78816, 78219, 80316]",3,"[1, 22, 49]",3945,OR in Accounting - Performance and ESG,7,14,59,OR in Financial and Management Accounting,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S08 [building - 101],"['Accounting', 'Critical Decision Making', 'Fuzzy Sets and Systems']",WC-59
"Recent geopolitical factors have a considerable effect on the economic environment of firms. We study abrupt business environment changes that affect demand growth and demand uncertainty in combination with time-to-build in a dynamic investment model of the firm. The business environment can either be a regime of relative prosperity or a regime of geopolitical turmoil. The firm has to decide on the timing and size of the investment. We find that time-to-build by itself does not affect the size and only delays the investment resulting in a constant project value at the moment of the investment. Time-to-build also does not affect the size greatly if regime shifts are included. The effect of the shift rates on the investment decision is twofold. The firm prioritizes the current regime in the investment decision if the probabilities of regime shifts occurring are small. If the probabilities of regime shifts occurring are large, the firm invests based on the weighted average of the two regimes.",Investment under Uncertainty - Abrupt Business Environment Changes and Time-to-Build,"[78987, 53631, 61089]",292,"[25, 83, 117]",3947,Dynamics of the Firm I,90,3,33,Optimal Control Theory and Applications,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,42 [building - 303A],"['Decision Analysis', 'Optimization in Financial Mathematics', 'Programming, Stochastic']",MB-33
"Exploiting synergies between the offshore wind and oil and gas [O&G] sectors can support decarbonisation goals, whilst exploiting disused hydrocarbon infrastructure for long-term economic value. Incorporating greater sectoral integration and diversity of energy carriers enables energy models to evaluate the impact of different policy levers more realistically. We therefore seek to quantify the economic and environmental benefits achievable by exploiting future synergies between the O&G and offshore wind sectors. This paper analyses scenarios with competing decarbonisation and repurposing alternatives and the resulting impacts on brownfield electrification and offshore hydrogen/ammonia production feasibility, as well as energy demands and emissions in the UK North Sea. Based on a mixed-integer linear programming [MILP] approach, the model determines optimal pathways for each of 242 O&G platforms while minimising total system costs up to 2060. Operational constraints ensure technical viability of proposed technologies and through consideration of multiple energy carriers, a more realistic assessment of energy system integration is undertaken compared to previous studies. Offshore heat and power demands are calculated on a platform-specific level, and wind deployment scenarios are devised using a spatially-constrained bottom-up LCOE model. Future work should focus on integrating this tool within large-scale energy models, to improve the representation of the offshore system.",A new modelling framework for offshore energy systems considering decarbonisation and decommissioning options for oil and gas infrastructure - a UK North Sea case study,"[79070, 79072, 79073, 79074, 79076, 68618]",465,"[93, 84]",3948,Multi-energy systems,23,4,19,OR in Energy,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,44 [building - 116],"['OR in Energy', 'Optimization Modeling']",MC-19
"A bi-level model is proposed to explore efficient policies for supporting negotiations on financial crisis resolution. In a principal-agent framework, this model minimizes a welfare loss function of a central authority [social planner, SP] by simultaneous choice of subsidy levels and potential pairs of banks to merge. The SP’s choice of mergers needs to be incentive compatible with autonomous choices of banks and the evaluation of the financial network must obey standard accounting principles. Incentive compatibility is enforced by two options of conditions based on stable matching or competitive bidding. For the evaluation of a financial network, we employ an extended Eisenberg-Noe clearing payment equilibrium by considering bankruptcy costs and seniority levels of liabilities. Additionally, liabilities are not cleared among solvent banks and corporate bonds may be used for clearing payments. The bi-level model states conditions for the clearing equilibrium. For demonstration we use major European banks and a scenario which is linked to the adverse economic scenario used in 2016 EU-wide stress testing.",Stabilizing financial networks via mergers and acquisitions,"[27947, 54607]",693,"[44, 126]",3949,Portfolio Optimization - Models and Methods,4,14,02,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,Glassalen [building - 101],"['Finance and Banking', 'Risk Analysis and Management']",WC-02
"Wildfires cause significant losses in both material value and human lives. Thus, preventing their occurrence or minimizing their impact when they do happen is paramount. Decision models have thus become increasingly popular in assisting the operational management of firefighting units. The management of wildfires can be divided into - [i] deployment, [ii] dispatch and [iii] positioning. Deployment decisions are made before the fire is detected, dispatch decisions are taken after the ignition is detected, and positioning decisions describe where the dispatched resources should suppress the fire.
The study presented here is part of a project that combines two stages - the prepositioning [deployment] of resources, and the resource movement during the suppression [positioning]. Naturally, the problem involves underlying uncertainty, two of its main sources being the ignition location and the wind properties.
The study aims to obtain statistical information about the behaviour of wind speed and wind direction by modelling historical data of a case study region in Portugal. Several statistical distributions were tested using goodness of fit tests, in order to select the most appropriate probability distributions.
The results were then used in the ambiguity set of a mixed integer programming distributionally robust optimization model.
",Modelling wind behaviour - using statistical distributions to represent wind speed and direction,"[79016, 79103, 23986, 19905, 430]",209,"[26, 0]",3950,Analytics and the link with stochastic dynamics I,17,7,31,Analytics,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,54 [building - 208],['Decision Support Systems'],TA-31
"Following sustained cost reductions and rapid diffusion into the power generation mix, renewable energy technologies such as wind power and solar PV may serve as the cornerstone for the future electricity system. An electricity system dominated by wind and solar is susceptible to uncertainty in weather conditions, particularly long periods of low wind and sunlight, known as Dunkelflaute. Previous studies investigating how to design a renewable electricity system typically employ a deterministic approach, which assumes perfect foresight about future weather conditions. In this study, we develop an adaptive robust optimization model to analyze a future renewable electricity system for Europe while considering the uncertainties in weather conditions. We analyze 40 years of historical weather data using the Finkelstein-Schafer statistical method to identify extreme weather conditions for wind and solar power. Our results show that considering Dunkelflaute events for the entire Europe may increase the installed generation capacity by 16% for wind power and 36% for solar PV, compared to the base case results calculated with typical weather data. Additionally, the transmission grid is more than doubled, while the hydrogen storage capacity is eight times greater than the base case. Consequently, the electricity system cost may increase by more than 20%. These results highlight the importance of accounting for weather-related uncertainties in designing future electricity systems.",Design a robust renewable electricity system for Europe considering extreme weather conditions,[79071],471,"[37, 127, 93]",3951,Stochastic models in energy systems planning and operations,21,9,22,Energy Management,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,81 [building - 116],"['Energy Policy and Planning', 'Robust Optimization', 'OR in Energy']",TC-22
"Multi energy systems optimal planning main challenge is the equilibrium between model accuracy and computational feasibility. This is due to the need of large-scale, nationwide, generation and transmission expansion model that incorporates aspects such as optimal units and storage scheduling, space constraints, and renewable energy sources availability. Furthermore, the deep decarbonization needs of the future energy systems implies to have coupled energy infrastructures, e.g. electric, heat, gas, and long duration energy storage including all the possible interactions between them through multi year periods, yields an increasing model complexity. It is modeled as a mixed integer linear problem and aims to include all the key features of state-of-the-art models obtainable from a linear formulation. The objective function is the minimization of the overall system cost, and it includes investment, maintenance, network expansion, transmission hourly prices, energy regulations, and fuel prices. 

The model has been implemented in Python language using Gurobi solver with an hourly time step. It can tackle in few minutes more than a year depending on the problem size - a yearly instance of 200 nodes yielded about 320 million continuous and tens of binary variables. The model flexibility enabled to run various sensitivity analyses to tackle uncertainty considering fuel availability, natural resources, different technologies, regulations, load profiles and time evolution.  ",Multi energy mixed integer linear programming model for generation and network expansion planning with coupled electricity and heat sectors considering renewable sources,"[77151, 58302, 33188, 72494]",640,"[12, 37, 111]",3952,Capacity expansion planning for energy systems,21,13,22,Energy Management,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,81 [building - 116],"['Capacity Planning', 'Energy Policy and Planning', 'Programming, Mixed-Integer']",WB-22
"We present a simple to implement multiscale entropy regularized Optimal mass transport solver using wavelet decomposition. The framework iteratively gives the user approximations to the optimal solution at a coarse to fine resolution and is particularly of interest for large scale problems with continuous costs and marginals. We discuss convergence, computational cost and show some examples.",Multiscale optimal mass transport ,[78989],721,"[111, 63, 110]",3954,Applications of Mixed-Integer and Nonconvex Optimization 1,86,14,04,MINLP,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,1001 [building - 202],"['Programming, Mixed-Integer', 'Large Scale Optimization', 'Programming, Linear']",WC-04
"Enterprises increasingly recognize the potential of business intelligence [BI] systems to enable data-informed decision-making. This study offers a novel, large-scale analysis of online customer testimonials to illuminate successful BI adoption patterns. Our research leverages natural language processing [NLP] techniques for systematic and thorough analysis of these rich, yet unstructured, data sources. Employing a qualitative approach, we used Python to scrape approximately 1000 testimonials from 'Customer Stories' sections of BI system websites [e.g., Power BI, Tableau, Knime]. A large language model [LLM], Llama 2, was used to systematically extract and analyse key variables - motivations, implementation considerations, challenges, security concerns, monetization, collaboration, dynamic capabilities, and behaviour change. Our findings reveal common motivations for BI adoption, typical implementation pathways, and recurring challenges that organizations encounter. Additionally, the study uncovered how successful BI implementations monetize their data-driven insights and foster collaboration across teams. This customer-centric perspective will provide actionable guidance for businesses considering BI investments, enabling them to anticipate potential roadblocks and maximize the return on their BI initiatives.",Analysing business intelligence adoption through natural language processing - a study of online customer testimonials ,"[56974, 79075, 79124]",235,"[8, 26, 66]",3955,Fairness and responsible AI,16,7,28,"Advancements of OR-analytics in statistics, machine learning and data science","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,065 [building - 208],"['Artificial Intelligence', 'Decision Support Systems', 'Machine Learning']",TA-28
"In order to increase the travel speed of trains or buses in public transport, it may be beneficial
to skip some stops on their routes. In such skip-stop services, a line is divided into two
sublines [let's say a green and a red one] which are operated alternatingly. The stations are
also divided into two sets with usually non-empty intersection, a set of green and a set of red stations. 
The green line only stops at green stations and the red line only stops at red station. 
With such a pattern, it is possible to increase the travel speed of the trains.
However, there now may exist pairs of stations which have no direct connection any more forcing
passengers to transfer between the red and the green line. It may even happen that passengers have
to take a detour to travel from an origin to a destination.

We consider the problem of designing skip-stop patterns from a passenger-oriented point of view.
Given a number p of stops to be left out [together in both lines] and a set of origin-destination
pairs representing the passengers' demand, we aim at finding stopping
patterns that minimize the number of transfers, the number of passengers who need a detour, or
the traveling time of the trains. For all three objectives we provide IP formulations and characterize
optimal solutions. We then combine these objectives and show which of them are in conflict and
which of these goals can be reached simultaneously. The results are illustrated at examples.
",Optimizing stopping patterns in public transport,"[1601, 78340, 72792]",817,"[119, 143, 72]",3956,Network Design for Public Transport,85,10,51,Public Transport Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,M5 [building - 101],"['Public Local Transportation Systems', 'Transportation', 'Mathematical Programming']",TD-51
"The progress of digitization and the emergence of resource-sharing business models have brought manufacturing companies with new possibilities to enhance their competitiveness and resilience. Shared manufacturing systems consist of agents such as manufacturing resources, and customer orders which pursue their individual objectives. Hence, despite its benefits, shared manufacturing systems come with computational challenges for effectively planning and scheduling this multi-agent ecosystem. Our study addresses this issue by introducing novel mixed integer programming [MIP] and constraint programming [CP] models as exact methods for representing and solving the scheduling of shared manufacturing resources. Additionally, a greedy constructive heuristic is developed. The effectiveness of the exact and heuristic methods is demonstrated through benchmarking with randomly generated instances.",Exact and Heuristic Methods for Multi-agent Scheduling of Shared Manufacturing Systems,"[79077, 77948]",810,"[69, 14, 3]",3957,Lot-sizing with game theory aspects,32,9,49,"Lot Sizing, Lot Scheduling and Production Planning","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,M1 [building - 101],"['Manufacturing', 'Combinatorial Optimization', 'Agent Systems']",TC-49
"Single-period dynamic assortment planning refers to the retailer’s problem of deciding the set of products to offer and their initial inventory levels with stochastic demand and dynamic substitution. The goal of the retailer is to maximize its expected revenue subject to a capacity constraint on the total number of items offered. This problem is notoriously difficult to solve. We propose an efficient heuristic for this problem considering an online retailer that has hundreds of possible SKUs to choose from and has the possibility to stock thousand units in total. We impose no assumptions on preference lists and substitution patterns. Compared to the most recent heuristic, our algorithm is competitive in the average revenue obtained but it is significantly faster especially for large instances. We show that our approach performs well in a variety of settings. ",Large-scale dynamic assortment planning problem - a heuristic approach,"[22831, 67625, 57929]",481,"[138, 5]",3958,Assortment Management,30,9,50,Retail Operations,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,M2 [building - 101],"['Supply Chain Management', 'Algorithms']",TC-50
"This article models the inherent cooperative and non-cooperative incentives of stakeholders in investment projects in a novel way by combining concepts from cooperative game theory and real options theory. As stakeholders have outside options, in the sense that they may terminate negotiations with the current coalition and join another, we introduce and analyze a coalitional and dynamic stability concept. We show that investment projects, in which cooperation between stakeholders is necessary, are more prone to coalitional instability when there are insufficient synergies between the stakeholders. We characterize the proportional investment scheme as the investment scheme that maximizes the total project value and that results in the earliest investment timing. A failure to implement proportional investing leads to the formation of a smaller, less efficient, coalition. The vulnerability to fail is exacerbated in a market that is characterized by high profit growth and low profit uncertainty, or vice versa. Finally, we explicitly consider one-leader investment projects and characterize the prioritized investment scheme that maximizes the value of the leader. We show that the same market conditions govern the stability of the prioritized investment scheme, which contributes to the robustness of our results.  ",Dynamic Stability of Cooperative Investment under Uncertainty,[79080],854,"[25, 50, 82]",3961,Dynamics of the Firm II,90,4,33,Optimal Control Theory and Applications,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,42 [building - 303A],"['Decision Analysis', 'Game Theory', 'Optimal Control']",MC-33
"In this work, we first discuss how evaluating two sources of risk [market risk and trend risk] using a unique measure of risk. In particular, we combine two risk measures with alternative correlation matrix in order to get a unique risk measure. Then we extend the inverse optimization problem proposed by Black and Litterman [1992] adapted to this context. Finally, we propose an ex post empirical analysis, where we compare the proposed models with other different portfolio selection models.",Portfolio selection with two risk sources and inverse optimization,"[79079, 74165]",142,"[126, 83]",3963,Risk management in finance,9,2,51,Risk management in finance,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,M5 [building - 101],"['Risk Analysis and Management', 'Optimization in Financial Mathematics']",MA-51
"Recently bike-sharing systems begin to employ diverse pricing and subscription policies to cater to the commuting needs of heterogeneous users. Frequent riders choose to purchase subscriptions to reduce their commuting costs if the system is easily accessible. To maintain a high market share, operators should improve their service level to retain a large number of subscribers. In addition, there is also a small proportion of general customers who use the system occasionally. These customers contribute to the operational profits, and providers need to fulfill as much customer demand as possible to ensure high profitability. Thus, maintaining the service level and obtaining profits are both important operational objectives in bike-sharing systems. However, jointly optimizing these objectives for the heterogeneous users in bike-sharing systems is challenging, especially when demand is highly uncertain. To address these issues, this study presents a robust satisficing optimization model that jointly optimizes the service level and profits in bike-sharing systems under demand uncertainty. To avoid over-conservation of solutions, side information like weather and weekend is integrated into the model to describe the relationship between demand and these exogenous factors. Extensive numerical experiments show that our model achieves higher robustness, lower violation probability and degree, and higher average-case out-of-sample performance than other benchmarks. ",A robust satisficing multi-objective optimization approach for bike-sharing systems with heterogeneous user types,"[79083, 69533, 64298]",828,"[127, 143]",3966,Transportation and Logistics under Uncertainty,49,14,34,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,43 [building - 303A],"['Robust Optimization', 'Transportation']",WC-34
"Coherent probabilistic forecasting, where the goal is to forecast multivariate time series that have hierarchical aggregation is important for accurate decision making in real world application. Recent work has focused on end-to-end fashion that simultaneously learns from all time series in the hierarchy and comprise the reconciliation step in single trainable model. However, this approach often neglects that distinct advantages of several reconciliation methods, where is adept to identifying temporal characteristics. To address this gap, our study proposes a novel coherent probabilistic forecasting framework with mixture-of-experts [MoE]. Employing the MoE framework, our approach effectively captures both regular and irregular temporal dependencies to integrate three diverse strategies - top-down, bottom-up and fully coherent. By introducing different experts, this model can generate forecasts that are probabilistically coherent as well as produce sophisticated forecasts that encompass diverse temporal dynamics. An empirical evaluation on real world hierarchical time series datasets demonstrates comparable advantages of the proposed approach. ",Mixture of Experts-based Coherent Probabilistic Forecasts for Hierarchically related Time Series,"[77239, 25193]",416,"[47, 8, 7]",3967,Recent Methodologies in Explainable AI [XAI] 1,71,2,04,Recent Advancements in AI ,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,1001 [building - 202],"['Forecasting', 'Artificial Intelligence', 'Analytics and Data Science']",MA-04
"We address a gradual covering competitive facility location problem where two competing retailers must decide on opening various types of facilities by taking into account customer preferences represented by probabilities for visiting different facility types. It is assumed that customers at a demand point are entirely willing to travel to a specific type of facility when it is located within a full coverage radius. However, if a facility is positioned beyond this full coverage distance, customers are either partially covered or not covered at all. It is also assumed that customers at a demand point can visit several facilities of different types of a retailer. However, among facilities of the same type, customers at a demand point can visit more than one facility of the same type if each facility belongs to a different retailer's chain. In such a case, the captured proportion of the buying power by a single facility is determined by both its own coverage extent and the total coverage provided by all facilities, which may lead to a loss of some buying power. Thus, the revenue generated by a facility depends not only on the visiting probabilities but also on the coverage strength in the presence of competitors. We formulate a nonlinear integer bilevel programming model and propose tabu search heuristics for the solution.",A bilevel gradual covering facility location model with customer preferences and different facility types  ,[18916],767,"[64, 72, 74]",3968,Location with Multiple Actors,29,9,61,Locational Analysis,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S10 [building - 101],"['Location', 'Mathematical Programming', 'Metaheuristics']",TC-61
"A rise in traffic accidents has led to both traffic congestion and subsequent secondary accidents. Effectively addressing this issue requires rapid accident investigation and management. In this paper, we aim to improve the efficiency of traffic accident assessment and investigation with the aid of drone technologies. Our approach involves strategically pre-positioning drones, enabling traffic supervisory agencies to dispatch drones immediately upon receiving an accident report. Methodology-wise, we present a data-driven robust stochastic optimization [RSO] model, which encapsulates the uncertainty of traffic accidents within a scenario-wise Wasserstein ambiguity set. To the best of our knowledge, this is the first study that incorporates covariates, i.e., weather conditions, into the Wasserstein ambiguity set with the CVaR metric. We demonstrate that the proposed RSO model can be reformulated into a mixed-integer programming [MIP] model, allowing an efficient solution approach. Via a real-world dataset of London traffic accidents, we validate the practical applicability of the RSO model. Across various parameter settings, our RSO model exhibits superior out-of-sample performance compared with various benchmark models. The numerical results yield valuable insights for traffic supervisory agencies.",Data-driven Drone Pre-positioning for Traffic Accident Rapid Assessment,"[79078, 64298]",831,"[127, 64, 111]",3969,New problems in logistics under uncertainty,49,15,34,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,43 [building - 303A],"['Robust Optimization', 'Location', 'Programming, Mixed-Integer']",WD-34
"In this talk, we consider the development of a new BRT line and the redesign of the surrounding bus network to best match the new line. Here, we consider the integrated problem of selecting a route for the BRT line as well as line planning and frequency setting for the surrounding bus lines. The route for the BRT line can be chosen from a selected set of edges within a graph, while the surrounding bus lines are chosen based on a given line pool. Moreover, travel time and operational cost benefits can be obtained for surrounding bus lines due to the possibility of using BRT segments, which allow for increased speed and reliability. The objective of our problem is to minimize the passenger travel time and operational costs, where we integrate the routing of the passengers as well as line capacities into the model. We propose a column-generation approach to solve this problem, where columns correspond to possible travel paths for passengers. Our approach is tested both for artificial instances as well as a case study for a new BRT line in Greater Copenhagen.",Integrated route selection and redesign of the surrounding bus network for a new BRT line,"[70927, 53331]",574,"[143, 119, 13]",3970,Network Design and Line Planning for Public Transportation 2,85,12,54,Public Transport Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S01 [building - 101],"['Transportation', 'Public Local Transportation Systems', 'Column Generation']",WA-54
"In response to the high cost of returns, some firms stop a free returns policy, but give their customers the option to purchase return insurance, which provides customers with the option to return items and receive a refund. We study how to design the return policy and insurance optimally.",Optimal Product Return Insurance,"[56517, 79088, 20952]",918,"[33, 0]",3971,Managing product returns,18,3,23,"Circular Economy, Remanufacturing and Recycling ","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,82 [building - 116],['Economic Modeling'],MB-23
"Hydrogen has gained significant attention as a potential fuel for the future. It is a versatile energy carrier that can be produced from a variety of low carbon sources and used in different sectors. Norway is uniquely positioned to take advantage of the growing interest, having significant natural resources, including hydropower, new renewable energy, as well as abundant natural gas resources and necessary infrastructure for carbon capture and storage. Furthermore, Norway is strategically located to key markets such as Germany and the Netherlands, which are looking to import hydrogen to meet their energy needs. 

Large plans for offshore wind deployment are planned, both in Norway and in Northern Europe, enabling business opportunities for producing hydrogen offshore. Benefits relate to reduced grid expansion, shorter transport distances to Europe and mitigated land use for onshore facilities. Hydrogen produced from offshore wind can also fulfill the requirements as an RFNBO. This study analysis under which conditions offshore electrolysis in the North Sea can be competitive to onshore hydrogen production, from electrolysis and natural gas with CCS, in Norway and in Germany. A mixed integer optimization model is developed using the TIMES framework, with a detailed modelling of the production, distribution and storage of electricity and hydrogen. In addition, the cost-competitiveness of different transport concepts is assessed, including pipelines and transport by ship. ",The cost-competitiveness of offshore hydrogen electrolysis in the North Sea,"[73014, 79092]",261,"[84, 93, 36]",3972,Hydrogen Modeling and Regulation I,22,14,09,Energy Markets,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,10 [building - 116],"['Optimization Modeling', 'OR in Energy', 'Electricity Markets']",WC-09
"Abstract - In the operation of high-frequency buses, it is important to maintain regularity in the service, reduce user waiting times, and maintain a low cost of operation. A key aspect to achieve this is to dispatch buses at regular intervals and control the interval during the routes, through strategies such as holding buses at stops. In systems where different lines operate that begin their operation at a common terminal, it is usual for buses to remain on the same line each time they arrive at the terminal after finishing a route, which limits the flexibility of the system. In this work, we seek to minimize user waiting times and the operating costs of a bus system by implementing bus spacing strategies at the terminal and a route control strategy to maintain regularity in the service. Our preliminary results show that when it is possible to interline buses in the terminal, waiting times are reduced between 10% and 20% compared to the case without interlining.
",Dispatch and route control of an urban public transportation system composed of buses with a mixed fleet,[79087],333,"[119, 0]",3973,Public transportation ,6,13,55,Transportation,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S02 [building - 101],['Public Local Transportation Systems'],WB-55
"Historical analysis of data related to the pick-up [i.e. the connection of the tugboat to the vessel] and drop-off [i.e. consequent disconnection] locations at ports such as the Port of Rotterdam shows a divergence from the scheduled plans, hypothetically related to factors such as weather, other scheduled activities for the tugboat, or other unknown factors. This poses challenges for the scheduling of tugboats and vessel-related activities, calling for an effective approach to predict the locations where vessels are picked up and dropped off. In this work, we propose an inverse optimization model to emulate the decisions of pilots in selecting the actual drop-off locations, aiming at learning the implicit decision models used by them. To this end, the proposed inverse optimization approach takes the actual decisions as inputs and determines an objective and/or constraints that replicate these decisions. In the model, ‘signals’ are defined as the important factors that impact the decision of pilots such as vessel-, towage-, and weather-specific information. The model is constructed based on an orienteering problem, in which the objective function aims at maximizing the total score of a path calculated as the sum of the visited node scores. The output of the model minimizes a loss function defined as the difference between the predicted and actual locations. The results enable advanced anticipatory tugboat scheduling that outperforms conventional myopic approaches. ",Pick-up and Drop-off Prediction for Tugboat Operations – An Inverse Optimization Approach ,"[35365, 79091, 75770]",671,"[66, 14, 143]",3974,Machine Learning and Optimization in Ports I,52,12,62,OR in Port Operations,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S12 [building - 101],"['Machine Learning', 'Combinatorial Optimization', 'Transportation']",WA-62
"The maritime shipping industry, responsible for 3% of global greenhouse gas emissions, is facing increasing pressure to decarbonization and ultimately zero emission due to the escalating threat of climate change. This urgent need has inspired the conceptualization of green maritime corridors—a designated network of shipping routes, ports, and associated infrastructure designed to advocate for shipping practices with low or zero emissions. Initial empirical studies have underscored the potential of green maritime corridors in advancing decarbonization. However, the shipping network and the requisite refueling stations to support alternative-fuel ships within the network remain undeveloped. Additionally, establishing green maritime corridors requires collaborative efforts from multiple stakeholders, wherein shipping lines, port operators, and governments are pivotal contributors, jointly ensuring the safeguarding of their respective interests. We first define the corridor network design and refueling station location and propose an optimization model to maximize the overall benefits,  with and without an emission trading system such as EU ETS. We further design a cooperative game for profit sharing among involved stakeholders such that they are incentivized to cooperate in establishing the corridors.  In this way, we provide a first optimization approach for green maritime corridor design, guiding policymakers and industry players on the way to successful implementations. ",The Network Design and Refueling Station Location Problem for Green Maritime Corridors and Emission Trading ,"[73738, 35365]",83,"[70, 79, 94]",3975,Port-Hinterland Transportation & Corridors,52,4,62,OR in Port Operations,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S12 [building - 101],"['Maritime applications', 'Network Design', 'OR in Environment and Climate change']",MC-62
"The problem of preparing mixed pallets is one of the main challenges in automating tasks in distribution centers. Whether it's the intrinsic complexity of packaging problems or the need to incorporate physical constraints such as stability and interference between cargo and robots in solving methods, traditional approaches to the problem tend to fall short of real-world needs.
In this work we propose an approach based on Digital Twins. A Digital Twin is a dynamic virtual representation of a physical object or system that uses real-world data, simulation models and/or machine learning.
The framework of our Digital Twin is based on three main components - classic optimization tools; physics engines; and machine learning.
A physics engine is a software system that simulates Newtonian physics in a simulated environment, including collision detection, rigid body dynamics and soft body dynamics.
In our Digital Twin framework, packing solutions are generated using classical optimization tools, the physical behavior of which is then simulated in the physics engine. Both results feed the machine learning model so that it acquires the knowledge needed to build solutions that can be used in the real world.
",A Digital Twin-driven approach for the mixed load palletizing problem.,"[37579, 27761, 36154, 38123]",544,"[65, 59, 131]",3977,Cutting and Packing 3 - 3D loading,81,4,07,Cutting and Packing [ESICUP],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,1019 [building - 202],"['Logistics', 'Industrial Optimization', 'Simulation']",MC-07
"The rising shift towards renewable energy sources to decarbonize the power sector highlights the need to create advanced optimization tools that capture variability and complementarity of renewable resources and incorporate infrequent but critical extreme weather conditions planning models. A clustering-optimization framework is introduced to address extreme weather conditions when planning for a multi-sector [including power, water, and heat] system, aiming to mitigate the effects of detailed variability. This framework was applied to assess the financial and resilience outcomes of adding concentrated solar power [CSP] and geothermal energy into a system already utilizing wind, solar photovoltaics, and batteries. When planning for fully renewable energy systems, extreme weather conditions necessitated higher capacity investments to reduce reliance on external power sources. Whereas technologies like CSP and geothermal have been shown to lower costs across the system, the inclusion of CSP could lead to decreased reliability during extreme weather events. This underscores the importance of designing energy systems that not only embrace renewable technologies but also robustly withstand and adapt to the challenges posed by extreme weather conditions, ensuring both sustainability and reliability in the face of climate change.","Planning of Power-Water-Heat Systems Using an Integrated, Extremes-Aware, Clustering-Optimization Framework","[78613, 36714, 54778]",840,"[93, 37]",3978,OR in Heating Systems,23,9,19,OR in Energy,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,44 [building - 116],"['OR in Energy', 'Energy Policy and Planning']",TC-19
"Changes in the correlation structures between asset returns strongly reflect the systemic events in the financial context. Commonly, correlation is used in absolute value. However, the question remains as to what information about the systemic risk is contained in the correlation signs, especially when interpreted as the weights of the edges in a signed network. From this perspective, balance theory can provide an interesting insight into catching signals of systemic crises and a meaningful tool to manage portfolio risk. The crucial point is that in balanced systems it is easier to predict the behaviour of a single entity. Hence, the balance contributes to increasing the level of predictability of the network behaviour. In a financial context, this is essential in managing risk to face systemic crises.  We show how the global balance index can be interpreted as a possible indicator of systemic risk. Moreover, we compare the global balance with some systemic risk indicators from the literature. We focus on the indicators that well-capture the loss of linear independence in the matrix of observations through the analysis of the minimum eigenvalues of the correlation matrix, interpreted as the distance from an appropriate space of rank-defective matrices.
Preliminary results are promising and they create a new bridge between systemic risk measures and signed networks.",Balance in financial signed networks - a new systemic risk measure,"[28327, 62431, 79094, 13128]",694,"[27, 53, 126]",3979,Complexity and Financial Patterns,4,15,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S14 [building - 101],"['Decision Theory', 'Graphs and Networks', 'Risk Analysis and Management']",WD-63
"In this work, we study the relationship between the mean-variance-based portfolio optimization algorithms' inputs parameters mu and Sigma and the rank-size function's law's estimated parameters from the best fit on the resulting sorted weights distribution. The objective is to explore the way weights describe a unified framework once they are ranked according to their sizes and, at the same time, the implications on the optimal allocation outcomes. Indeed, characterizing portfolio weights through rank-size curve parameters allows us to understand how portfolio risk and return vary as these parameters change.",Optimal portfolios' weights and rank-size shapes,"[79082, 7142, 25697, 57291]",693,"[126, 44, 84]",3981,Portfolio Optimization - Models and Methods,4,14,02,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,Glassalen [building - 101],"['Risk Analysis and Management', 'Finance and Banking', 'Optimization Modeling']",WC-02
"The Internet of Vehicles [IoV] serves as a foundational infrastructure for intelligent transportation systems. For service providers in the IoV and edge computing sectors, the meticulous design of edge computing networks is pivotal for securing a competitive advantage through enhanced cost efficiency and superior service quality. We introduce a robust stochastic model designed to tackle the network design challenge for edge servers effectively. This model incorporates critical considerations, such as the strategic placement of edge servers, capacity planning, congestion mitigation, accounting for stochastic demand, and guarding against unpredictable cyberattacks.Our primary objective is optimizing the network configuration to minimize total costs, including setup, capacity acquisition, congestion management, and data transmission. To represent this multifaceted issue accurately, we employ an M/M/1 queuing framework for assessing congestion in steady-state scenarios and refine our approach with a mixed-integer second-order cone programming framework.To manage uncertainties associated with network attacks, we incorporate an uncertainty set based on Renyi divergence, allowing us to adjust the model's conservativeness by modifying the divergence order. Furthermore, we propose a novel two-layer Benders decomposition approach to efficiently navigate the computational complexities inherent in solving this problem.",A Robust Stochastic Approach to Edge Computing Service Network Design with Network Attacks Uncertainties,[79085],828,"[127, 79, 5]",3982,Transportation and Logistics under Uncertainty,49,14,34,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,43 [building - 303A],"['Robust Optimization', 'Network Design', 'Algorithms']",WC-34
"The Industrial Engineering and Management [IEM] undergraduate students, from the University of the Aveiro, have to develop a project in their last semester. The purpose is for them to apply the knowledge and skills acquire throughout the IEM program to a complex problem, while also developing other skills like critical thinking, Problem solving and team work.
One of the problems, addressed by a group of four students, originated from a wind blade production unit, specifically in the part of the process where the blade is molded using different sized fibers. The fibers have to be transported in carts that are positioned near the blade, so the goal is to assign them to the carts, as well as the carts to the locations near the blade, so as to minimize the distance of the fiber to the respective point of use.
The work developed allowed the students to comprehend a complex industrial problem, while also using OR and thus understanding how it can be put in practice, in order to help organizations with their problems.
The results of the work were very useful for the company, helping them improve their performance in this process. It was also very positive for the students since it contributed to a more practical vision of the use of OR. It is also expected that this work, and other similar developed by undergraduate students, may become examples for future students, helping them cross that same bridge between theory and practice.",OR in an wind blade production unit - an assignment problem formulation developed by undergraduate students,"[23986, 79267, 79268, 79269, 79270]",790,"[69, 72, 151]",3983,OR Education II,48,3,16,OR Education,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,19 [building - 116],"['Manufacturing', 'Mathematical Programming', 'Practice of OR']",MB-16
"We present a case study focused on the elderly population residing in Paris, aiming to model their capabilities, defined as the set of choices available to them. We propose using a multi-objective mathematical program to measure the welfare of individuals. This methodology is grounded in data collected through questionnaires and GPS tracking of activities observed over a one-week period.

The study begins by establishing a detailed model of the current capabilities of the elderly, integrating various dimensions of their daily lives and decision-making processes. This model serves as a foundation for assessing the individual welfare of the elderly.

Following the modeling phase, we proceed to simulate the impact of various public policies on the capabilities of these individuals. The simulations aim to identify potential improvements or drawbacks in their welfare resulting from policy interventions. ",Modeling Capabilities of the Elderly in Paris,[67684],906,"[77, 112]",3984,"How to support complex decisions. Negotiating the trade-off between Social, Environmental and Economic values 3",44,7,47,Multiple Criteria Decision Analysis,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,50 [building - 324],"['Multi-Objective Decision Making', 'Programming, Multi-Objective']",TA-47
"Our study presents a novel approach to optimizing the Periodic Timetabling Problem [PESP] and Vehicle Circulation in the Rhätische Bahn Network [RhB-Network] using Satisfiability Modulo Theories [SMT]. Like traditional Mixed Integer Programming [MIP], our methodology utilizes the Event Activity Network [EAN] to model railway timetable events and activities. However, our approach encodes the PESP and vehicle circulation constraints within an SMT framework, extending SAT with difference logic. This extension allows for high-resolution time calculations, addressing the scalability limitations of existing SAT methods. Furthermore, we use selectable activities, enhancing the model's capability to incorporate vehicle circulation effectively during train scheduling and routing. Empirical analysis on a large scale, Swiss Railway Network reveals that although MIP and SMT solvers show comparable performance on smaller instances, the SMT solver significantly outperforms on larger scales, consistently finding optimal solutions. In particularly challenging instances, we achieve a reduction in computation time by a factor of 100 or more, illustrating the value of SMT-based methods in solving complex scheduling problems and advancing public transport optimization.",Integrated Vehicle Circulation and Periodic Timetabling Optimization with SMT,"[67433, 79098, 58406]",574,"[142, 143, 5]",3985,Network Design and Line Planning for Public Transportation 2,85,12,54,Public Transport Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S01 [building - 101],"['Timetabling', 'Transportation', 'Algorithms']",WA-54
"A renewable energy community is a non-profit legal entity involving prosumers which produce, consume, and exchange energy. In these communities, prosumers can cooperate to maximize the community's social welfare. In practice, this naturally raises the question of the community's social welfare sharing, as the members may have different contributions to the social welfare. In this presentation, we first empirically highlight the benefits of cooperation for the community and the individual members. Then, we present some cost-sharing mechanisms [from the literature] that guarantee the stability of the grand coalition of the prosumers in the community. Finally, we present some results on instances built with real-world data from our partner's demonstrator, Smart Lou Quila, located in South France.",Cooperative game theory and cost allocation - the case of renewable energy,[79086],641,"[50, 37, 93]",3986,"Game Theory, Solutions and Structures VI",88,8,36,"Game Theory, Solutions and Structures","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,32 [building - 306],"['Game Theory', 'Energy Policy and Planning', 'OR in Energy']",TB-36
"Human-centricity is central to the transition to Industry 5.0, prioritizing diverse human needs from health and safety to personal growth. In Industry 5.0, digital twins [DT] play a key role in seamlessly bridging the physical and virtual realms. While these virtual replicas enhance process safety, they often short in providing a comprehensive overview of safety issues. Various human factors contribute to the emergence of risky behaviours among workers, thereby posing significant threats to health and safety.  Recognizing this reality underscores the imperative for integrating human elements within digital twins. 
In the industrial field, there's a noticeable lack of comprehensive exploration into the digital twins of workers, mainly stemming from a prioritization of production efficiency over human needs. The proposal of the Human Digital Twin [HDT] emerges as a critical approach to realizing human-centricity in Industry 5.0. HDTs, serving as digital counterparts of humans, aim to revolutionize human-system integration by establishing a direct connection between human characteristics and system design and performance.
However, the exploration into advancing HDT development to redefine human roles and unlock full potential remains limited. This research aims to investigate the role of the human digital twin as an enabler of human-centricity in the industry. It proposes an HDT framework from a human factors perspective, promoting human safety and well-being in Industry 5.0.",Human Digital Twin for Improving Occupational Health and Safety,"[77586, 67324, 79099, 79100, 79101]",851,"[131, 8, 3]",3988,Simulation of organizations II,77,3,43,"Agent-based Models in Management, Economic and Organisation Sciences","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,99 [building - 306],"['Simulation', 'Artificial Intelligence', 'Agent Systems']",MB-43
"In thematic investing, investors rely on long-term trends to detect and subsequently invest in what they assess as innovative companies with promising technologies. The established method for investors is to identify such companies via qualitative assessments or business sector classifications. Due to the established links between innovations, [new] technologies and patents, patent analytics has recently become an information source in thematic investing. However, there seems to be a mismatch between business sector- and technology-based classifications, which potentially affects [1] assessments in which technologies companies are active in and [2] the grouping of companies with supposedly similar technology profiles. With our research, we address these issues by analyzing the patent portfolios of the companies attributed to two Global Standard Classification Standard sectors. We examine the patent attribution profile vector of the companies in these sectors and compare them via the cosine distance metric and use clustering to group companies of high technological similarity and compare these to the S&P sectors. Further, we employ artificial intelligence tools to identify names for these newly generated clusters to ease comparability and interpret the results with a Sankey diagram analysis and a network graph analysis. Our findings yield implications for various decision-makers involved in thematic investment, including investors, corporate management, and policymakers.",A Novel Approach to Thematic Portfolio Construction - Patent-based Investment Decision Making,"[79097, 67004, 67786]",138,"[66, 7, 44]",3991,Market dynamics and implications for portfolio decisions,74,9,57,Modern Decision Making in Finance and Insurance,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S06 [building - 101],"['Machine Learning', 'Analytics and Data Science', 'Finance and Banking']",TC-57
"Food banks play an important role in fighting food waste as well as alleviating hunger, yet often struggle to effectively collect all food donations due to their highly uncertain operating environment. Addressing this issue, this research introduces the capacitated vehicle routing problem with travel time restrictions and stochastic demand, service and waiting times, which can be generalized to other routing applications. The problem aims to determine a minimum number of vehicles, while cost-effectively planning routes for these vehicles so that each route violates the vehicle capacity and the travel time limit only with a very small probability. The resulting problem is highly complex and thus solved by means of a matheuristic, which decomposes the problem into its natural decision components. As such, it first determines the number of districts into which the service area should be partitioned, before allocating each customer to exactly one district and then plans a route for each district. Extensive numerical experiments, involving both randomly generated and real-life instances, demonstrate the matheuristic’s effectiveness in
solving instances with up to 100 customers. In addition, we present valuable managerial insights by applying our matheuristic to real-life instances from Dutch and Canadian food banks.
","Vehicle Routing With Stochastic Demand, Service and Waiting Times – The Case of Food Bank Collection Problems","[67701, 71050, 29273, 70828]",741,"[145, 136, 58]",3993,Vehicle Routing Under Uncertainty 1,5,5,64,VeRoLog - Vehicle Routing and Logistics,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S16 [building - 101],"['Vehicle Routing', 'Stochastic Optimization', 'Humanitarian Applications']",MD-64
"After a mass casualty incident, numerous victims endure severe injuries with different chances of survival, requiring timely rescue operations and prioritization in transportation. We develop a Mixed-Integer Linear Programming [MILP] model for a multi-period routing and scheduling problem for a heterogeneous rescue fleet with the objective of maximizing the expected number of survivors. Recognizing the uncertainties inherent in such situations, such as fluctuations in the victim population and their conditions over time, we propose an online optimization algorithm. This allows for real-time adjustments to the rescue operations based on incoming data. Our framework favors equity in healthcare service delivery through various fairness metrics. Validation of proposed approaches is achieved through computational tests and offline comparisons with real-world scenarios.",Efficient and equitable transportation of casualties by Online Optimization in mass casualty incidents,"[79102, 50712, 22044]",775,"[58, 143, 145]",3995,"Efficiency, equity and fairness in humanitarian operations",38,9,21,OR in Humanitarian Operations [HOpe],"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,49 [building - 116],"['Humanitarian Applications', 'Transportation', 'Vehicle Routing']",TC-21
"Recently a novel self-exciting point process has been introduced in the literature, featuring a continuous-time autoregressive moving average intensity process. Such a model, named CARMA[p,q]-Hawkes, extends the traditional Hawkes process by integrating a CARMA[p,q] framework instead of an Ornstein-Uhlenbeck intensity. As a matter of fact, the proposed model maintains the same level of mathematical tractability as the Hawkes process [e.g., infinitesimal generator, backward and forward Kolmogorov equations, joint characteristic function]; but it shows enhanced capability in reproducing complex time-dependent structures evident in several market data. Based on this framework we propose a compound CARMA[p,q]-Hawkes model incorporating a random jump size independent of both the counting and intensity processes, which serves as a key component for a new option pricing model. We conduct an analysis to assess the effectiveness of this pricing model in replicating the volatility surface observed in market option data.

References:
Mercuri, L., Perchiazzo, A., & Rroji, E. [2024]. A Hawkes model with CARMA [p, q] intensity. Insurance - Mathematics and Economics.","Pricing Options with a Compound CARMA[p,q]-Hawkes model","[70064, 46812, 70066]",277,"[44, 117, 135]",3996,Risk Management and Cryptoassets,4,8,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S14 [building - 101],"['Finance and Banking', 'Programming, Stochastic', 'Stochastic Models']",TB-63
"In recent years there has been growing attention to interpretable machine learning models which can give explanatory insights into their decision-making process. Thanks to their interpretability, decision trees have been intensively studied for classification tasks. Due to the remarkable advances in mixed-integer programming [MIP] of the last years, various MIP models for training an Optimal Classification Tree [OCT] have been proposed. Along the research line of Margin Optimal Trees [1], we present an alternative mixed-integer quadratic formulation to construct multivariate OCTs. The proposed model defines the decision rules of the tree as maximum margin hyperplanes following the Support Vector Machine [SVM] paradigm along the binary tree structure. Further, we exploit the decomposable structure of the model to develop a tailored algorithmic approach able to leverage the SVM nature of each subproblem. First, the algorithm has been tested on non-linearly separable synthetic datasets in a 2-dimensional feature space to provide a graphical representation of the maximum margin approach. Finally, the method has been tested on benchmark datasets from the UCI repository in order to evaluate its computational efficiency and the generalization capabilities of the generated trees.

References
[1] F. D’Onofrio, G. Grani, M. Monaci, and L. Palagi. Margin optimal classification trees. Computers & Operations Research, 161:106441, 2024.
",Margin Optimal Trees - a novel formulation and a SVM-based decomposition approach,"[72523, 65962]",863,"[111, 66, 72]",3997,Mixed Integer Optimization II,64,8,52,Combinatorial Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,8003 [building - 202],"['Programming, Mixed-Integer', 'Machine Learning', 'Mathematical Programming']",TB-52
"Signed graphs are extensively used across various real-world social networks, where positive edges represent favourable relationships [friends, colleagues] and negative edges denote unfavourable ones [enemies, adversaries]. Some previous research in signed graphs has focused on defining properties to evaluate clustering tendencies in the sense that the positive edges join vertices in the same cluster and the negative edges join vertices in different clusters. This work gives theoretical and experimental results of the so-called social disruption games. These games form a subclass of the simple games defined in the classical game theory. Social disruption games satisfies that a set of vertices is a winning coalition if and only if the subgraph induced by the coalition contains a cycle with only one negative edge. In this work, we explore properties such as properness, strongness, and decisiveness, among others. We also give some results on parameters like length, width, strict length, and strict width, etc. Finally, we present signed graphs that define social disruption games with specific properties and parameters [vetoers, critical players, symmetric players, etc.].

[Supported by Spanish AEI grant MICINN PID2020-112581GB-C21]",Properties and parameters of social disruption games,"[19815, 70944, 61912, 36990]",641,"[50, 53, 132]",3998,"Game Theory, Solutions and Structures VI",88,8,36,"Game Theory, Solutions and Structures","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,32 [building - 306],"['Game Theory', 'Graphs and Networks', 'Social Networks']",TB-36
"Energy capacity planning faces numerous uncertainties, especially regarding future demand and changes in generation costs. The development of new technologies has challenged previous assumptions about the affordability of renewable energy sources. As a result, the changing cost dynamics disrupt the traditional patterns, leading to significant impacts on expansion decisions and the future composition of the generation portfolio, influenced by the shifting dynamics between renewables and conventional sources. Additionally, the construction of new electricity generation facilities powered by gas or nuclear typically entails lengthy construction periods, with operational lifespans expected to extend beyond 15 years. Nevertheless, capacity expansion extends beyond long-term projects. Short-term strategies, including extending the lifespan of existing generation capacity that would otherwise be retired, and implementing demand response initiatives, have significant influence over the overall capacity landscape. In this study, we examine the socially optimal capacity expansion decisions, taking into account future carbon targets, demand uncertainty, and the integration of long-term and short-term generators. ",Models for electricity capacity expansion with demand uncertainty and a carbon budget,"[76232, 79105, 79106]",640,"[37, 110]",4002,Capacity expansion planning for energy systems,21,13,22,Energy Management,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,81 [building - 116],"['Energy Policy and Planning', 'Programming, Linear']",WB-22
"In this contribution, we discuss how to handle financial and
sustainable investment goals, focusing on greenness and ESG features.
Among thematic investments, green and energy-related ones have emerged, capturing investors’ attention. This contribution introduces a novel ESG-focused tracking error model to provide ESG-tailored-made allocations. We consider two reference benchmarks, accounting for a financial target and an ESG one, respectively. The objective function results in a convex linear combination of the two goals where the combining parameter accounts for the investor’s financial and ESG preferences. A symmetric tracking error measure is proposed to replicate the financial benchmark passively, while an asymmetric measure is used to track and possibly outperform the thematic ESG benchmark.
Identifying the benchmarks for the two components represents a crucial
step and, jointly with the choice of the combining parameter, accounts
for the portfolio’s overall risk-return and ESG profiles. In the model,
the sustainability feature is handled not only with the presence of the
ESG benchmark but also with the introduction of dedicated constraints.
An application to the EUROSTOXX 600 equity market is presented and
discussed for different choices of the combining parameter, representing
different sustainability preferences and risk-return profiles.",Tracking-based green portfolio optimization,"[19535, 55558, 79107]",693,"[100, 45]",4003,Portfolio Optimization - Models and Methods,4,14,02,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,Glassalen [building - 101],"['OR in Sustainability', 'Financial Modelling']",WC-02
"This study focuses on flexible job shop scheduling problems with multiple resource constraints. It introduces a comprehensive approach for modeling various resource types like limited capacity machine buffers, tools, utilities, and work in progress buffers. The proposed methodology involves a Constraint Programming [CP] model alongside a CP-based Adaptive Large Neighbourhood Search [ALNS-CP] algorithm. The ALNS-CP algorithm incorporates long-term memory structures to retain valuable information about individual and paired operations assigned to machines, gathered from high-quality and diverse solutions during the search process. These memory structures aid in formulating additional constraints for the CP solver, and therefore guiding the search towards promising areas of the solution space. To evaluate its efficacy, extensive experiments were conducted on established benchmark sets, comparing the performance of ALNS-CP with existing state-of-the-art methods. Furthermore, new instances of varying sizes were utilized to explore how different resource types impact the makespan. The computational findings highlight the competitiveness of the proposed framework, notably yielding 39 new best solutions across well-known problem instances of the literature.",Solving flexible job shop scheduling problems with multiple resource constraints using a unified CP-based solution framework,"[78178, 79110, 23864, 23636]",881,"[14, 74, 129]",4004,Topics in scheduling [Contributed],64,14,26,Combinatorial Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,012 [building - 208],"['Combinatorial Optimization', 'Metaheuristics', 'Scheduling']",WC-26
"The 2-dimensional Bin Packing Problem addressed in this presentation arises within the ornamental stone industry, where heavy stone panels of irregular shapes, often with defects, must be cut to meet the demand for rectangular items. The cutting process in this industry is subject to practical constraints, primarily influenced by the thickness of the stone panels and the cutting technology employed, resulting in an effect known as overcut. Overcutting involves cutting the raw material beyond the edge of the intended piece. This effect can damage pieces placed on top of the cut unless there is sufficient separation between the pieces or alignment with another cut in the same direction.

To the best of our knowledge, this constraint has never been explicitly addressed in the literature, making it a non-trivial challenge to overcome.

In response to these challenges, a novel approach is proposed. An extreme point-based heuristic has been developed to solve the problem, enhanced by a local search procedure based on the two-exchange heuristic. The irregularity of the panels is managed through a transformation process that converts irregular shapes into regular ones, simplifying the structure. Computational experiments using real-world data have been conducted to validate the effectiveness of the proposed approach, and the results and practical applications are thoroughly discussed.
",Extreme Points-Based Heuristic for Irregular Panel 2D-Bin Packing with Defects,"[78941, 79109, 1999, 663, 78851]",501,"[23, 14, 151]",4005,Cutting and Packing 2 - 2D irregular,81,3,07,Cutting and Packing [ESICUP],"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,1019 [building - 202],"['Cutting and Packing', 'Combinatorial Optimization', 'Practice of OR']",MB-07
"Bertani Trasporti Spa is one of Italy's leading players for logistics services in the automotive sector, delivering more than 5000 vehicles daily. One of the critical tasks in their planning process is the definition of the delivery trips for their fleet of auto-carriers.
The problem consists in selecting the best subset of orders that will be served the next day and building a set of optimal routes to deliver them. The routing of the auto-carriers must take into account constraints related to the loading of the vehicles on the carriers. The goal is to use the carriers in the most efficient way [at present there is more demand than transport capacity]. This means that we must saturate auto-carriers capacity and create trips visiting few and geographically close delivery points. At the same time we have to minimize the orders not delivered on time.
In this work we propose a heuristic method to build optimized routes, based on an Adaptive Large Neighborhood Search [ALNS] scheme, where loading constraints combine explicit rules with coefficients dynamically learned from historical data and feedback from expert planners.",Optimization of auto-carrier routes - a real-world case study in Italy,"[58611, 79111]",868,"[145, 151, 14]",4006,Combinatorial Optimization models and applications in Logistics and Transportation II,64,3,29,Combinatorial Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,157 [building - 208],"['Vehicle Routing', 'Practice of OR', 'Combinatorial Optimization']",MB-29
"The aim of the paper is to examine the dependence between uncertainty indices and the uncertainty of cryptocurrencies. The focus is on volatility spillovers between different risk proxy indices - geopolitical events, economic policy, commodity, equity and bond markets, and cryptocurrency uncertainty indices. The spillovers are used in networks, and various centrality measures are calculated to test which risk index or cryptocurrency is leading the market changes. Based on weekly data from April 2014 to March 2024, covering several market ups and downs, it is found that the transmission from uncertainty indices to cryptocurrency uncertainties is rather weak on average, but accelerates during turbulent periods such as the Covid-19 outbreak or the start of the Ukraine war. Overall, we conclude that while cryptocurrency pricing is largely decoupled from economic risk, the ability of the asset class to serve as a portfolio hedge may be limited.",Spillovers between risk indices and the cryptocurrency market,"[70695, 80304]",246,"[45, 53, 66]",4007,"AI in Eco-Finance - Time, Space, and Networks",53,3,08,AI & Innovation in Sustainable Finance,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,1020 [building - 202],"['Financial Modelling', 'Graphs and Networks', 'Machine Learning']",MB-08
"In a typical [forward] optimization problem, a decision-maker uses given values of model parameters to compute the values of decision variables. The goal in inverse optimization [IO] is instead to infer parameters that render given values of decision variables optimal. Most papers on IO utilize duality to impute objective function parameters. A corresponding literature for imputing constraint parameters is essentially non-existent, even for linear programs. The difficulty is that these IO problems include nonconvex bilinear constraints and/or objectives. We will discuss models and solution algorithms designed to tackle these difficult problems. We will illustrate key ideas through the motivating problem of imputing transition probabilities in Markov decision processes [MDPs]. If time permits, we will also extend these ideas to inverse semi-definite programs [SDPs] and inverse Quadratic Programs [QPs], where matrices on the left-hand-sides of constraints are unknown.",Inverse Optimization for Imputing Constraint Parameters in Mathematical Programs,[79113],452,"[19, 21]",4008,Stochastic and Deterministic Global Optimization,93,4,41,Stochastic and Deterministic Global Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,97 [building - 306],"['Continuous Optimization', 'Convex Optimization']",MC-41
"This paper considers the development of a Problem Structuring Method [PSM] WASAN over an action research programme and what we can learn about the applicability of PSMs to multiple contexts. The programme focusses on WASAN’s developmental journey from a single use approach, only applicable to a limited context, to a PSM that has generic applicability, that is can be used across multiple problem contexts. The project tracks the changes made to WASAN through 11 Learning Loop cycles across two new problem contexts with five different researchers using the approach to collect and analyse data. 
The unit of analysis for the paper is the philosophical, theoretical and methodological development of WASAN, to achieve this the paper focuses on the learning from a second level analysis of the Action Research Programme concerned with what we can learn about PSMs more broadly. Here the focus is the different classes of elements within WASAN which are required for it to be generically applicable, that is applicable to a variety of problem contexts. Here the paper distinguishes between two classes of elements, those which required consistent replication regardless of context; termed methodological elements. Second, those which allow an approach to be applicable to the local context thereby bridging the gap between the problem context and methodology; we term these contextual elements.
","The philosophical, theoretical and methodological development of a PSM through Action Research - From the Specific to the Generic",[28169],130,"[149, 0]",4010,Methodological Developments in Soft OR and PSMs,26,5,13,Soft OR and Problem Structuring Methods,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,15 [building - 116],['Problem Structuring'],MD-13
"With decarbonization, numerous and heterogenous new generation units are installed. Decisions on such investments depend on multiple, partly not observable factors. Many long-term energy models are yet formulated as linear programs and are therefore prone to abrupt changes in primal variables [“penny-switching”]. Discrete choice models deal with heterogeneity in individual decisions and avoid penny-switching. Here an approach is presented to incorporate such decision modelling in a non-linear, convex optimization problem, along with technological and system constraints. The problem is reformulated using the entropy function of information theory and transformed to an essentially linear program with a few exponential cone restrictions.
A first application investigates wind and PV investments across Germany based on the novel approach and compares the outcomes to the results of standard LP formulations. It is shown that the regional spread of investments increases when unobserved heterogeneity is taken into account.
",Heterogenous investors in energy markets,[24773],407,"[21, 36, 37]",4011,Market-based analyses in long-term energy system models,22,9,09,Energy Markets,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,10 [building - 116],"['Convex Optimization', 'Electricity Markets', 'Energy Policy and Planning']",TC-09
"Since the 1980s, Data Envelopment Analysis [DEA] has been a staple in management accounting for performance efficiency by converting inputs into outputs, typically within production processes. However, recent access to extensive accounting databases has shifted DEA applications to financial data [FinDEA], contrasting firms across industries. This evolution prompts two inquiries - the influence of chosen performance metrics on FinDEA's business assessment and the impact of industry traits on these evaluations. Our study assesses the coherence of ten FinDEA models across six sectors, revealing that FinDEA scores are consistent within specific performance metrics. Additionally, industry nuances distinctly influence FinDEA outcomes. These insights inform the application of FinDEA in future management accounting research using large-scale financial data.",Data Envelopment Analysis in Management Accounting - New Frontiers Exploring Industry Characteristics,[79115],3,"[1, 24, 45]",4012,OR in Accounting - Performance and ESG,7,14,59,OR in Financial and Management Accounting,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S08 [building - 101],"['Accounting', 'Data Envelopment Analysis', 'Financial Modelling']",WC-59
"Persistent high glucose levels, a hallmark of diabetes mellitus [DM], lead to widespread cellular damage. Atherosclerosis, occurring alongside glucose metabolism disturbances, significantly complicates the condition and accelerates in DM, making it critical to slow its progression. Identifying intervention targets within the complex network of molecules and processes, without disrupting essential functions, is challenging. Here, computational testing, like in silico analysis, becomes crucial for identifying potential therapeutic targets effectively. In our study, we used a Petri net model to identify key network nodes whose blockade could impede atherosclerosis in hyperglycemia. Our findings suggest that inhibiting isoforms of protein kinase C in diabetic patients could help slow the progression of atherosclerosis. Additionally, we observed that inhibiting aldose reductase could decelerate atherosclerosis advancement and reduce PKC expression in DM. Targeting oxidative stress by inhibiting the AGE-RAGE axis emerges as a promising therapeutic strategy for managing hyperglycemia-induced atherosclerosis. While blocking NADPH oxidase, the primary enzyme responsible for generating reactive oxygen species [ROS] in blood vessels, only marginally impeded atherosclerosis development, it effectively halted the increased production of mitochondrial ROS associated with mitochondrial dysfunction. These findings lay the groundwork for more in-depth studies.",Identifying Key Therapeutic Targets for Treating Hyperglycemia-Induced Atherosclerosis Using a Petri Net-Based Model,"[18684, 24774, 5420]",309,"[17, 0]",4013,Integrative Approaches in Health and Disease - From Molecular Structures to Clinical Outcomes,2,9,20,"Computational Biology, Bioinformatics and Medicine","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,45 [building - 116],"['Computational Biology, Bioinformatics and Medicine']",TC-20
"We present an evacuation network design problem under cost and travel congestion considerations while incorporating worst-case evacuation time for the evacuees. Employing a time-expanded network, we model for effective and controlled evacuation by determining active shelter locations and evacuee routes that can be implemented in the preparedness stage in response to foreseen extreme events, such as hurricanes and flooding. To solve our model, we devise an efficient Benders Decomposition [BD] framework with convergence enhancements for solving large-scale instances while also taking advantage of specific characteristics of the problem. We design and implement an experimental study to test our BD technique and to conduct sensitivity analysis for various parameters and model validation using data from Central Texas.
",Network Design for Evacuation Preparedness under Cost and Travel Congestion Considerations,"[59508, 72061]",523,"[79, 30, 150]",4014,Optimization in transportation infrastructure design and management,6,5,56,Transportation,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S04 [building - 101],"['Network Design', 'Disaster and Crisis Management', 'Network Flows']",MD-56
"Machine Learning has increasingly been used for algorithmic decision-making, often through what are considered black-boxes, i.e., models which do not provide an explanation for their outputs. In this talk, we consider the task of explaining the output and decisions from a given black-box to provide an interpretable explanation. Additionally, we tackle fairness concerns that might arise from using black-box models, where discrimination can be present. Our approach aims to give a local explanation for an output, while taking into account global fairness and accuracy.",Making Black-Boxes Explainable and Fair Through Mathematical Optimization,"[71558, 4607, 22145]",121,"[66, 72, 8]",4017,On Mathematical Optimization for Explainable and Fair Machine Learning,15,3,27,Mathematical Optimization for XAI,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,047 [building - 208],"['Machine Learning', 'Mathematical Programming', 'Artificial Intelligence']",MB-27
"This analysis utilizes a deterministic optimization model to determine the optimal operational production of electrolytic hydrogen. The objective is to compare different operational strategies for electrolyzers considering electricity prices, subsidies for certified renewable hydrogen production, and regulatory constraints on GHG emissions. Using electricity grid data from Northern Italy, hourly grid emission factors are calculated through various methodologies, including European regulations for Renewable Fuels of Non-Biological Origin production, the IPCC approach, and short-run marginal emission factors. Different emission accounting methods, such as Scope 2 and 3 from the Greenhouse Gas Protocol and the EU methodology are also considered. The comparison between different emission accounting methods shows that emissions calculated with Scope 3 are around 38% higher than those calculated considering Scope 2, and time-varying emissions calculations show a significant difference from yearly average methods, particularly when applied to a flexible load. The study highlights the temporal alignment of low prices and high emission factors; therefore, emission accounting methodologies that promote hydrogen production during low-price hours do not meet the need for GHG emission reduction. The findings indicate that different accounting methods impact the optimal operational behaviour of the electrolyzer and suggest the need of different strategies to reduce emissions.",Optimal operation of a grid connected electrolyzer and greenhouse gases emission accounting ,"[79049, 76651]",252,"[93, 110, 139]",4019,Impacts of transitioning to green gases,22,2,14,Energy Markets,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,16 [building - 116],"['OR in Energy', 'Programming, Linear', 'Sustainable Development']",MA-14
"We study parcel consolidation within the context of last-mile delivery, employing both a vehicle and a companion drone to efficiently serve customers across multiple time periods. We present a mixed-integer program and an optimization-based heuristic to address large-scale instances. The efficacy of our methodology is assessed through computational experiments on randomly generated instances, focusing on a residential network in Amherst, Massachusetts.",Parcel Consolidation In Last Mile Delivery by Vehicle and Drone,"[78550, 33084, 79122]",735,"[145, 111, 79]",4020,Routing Unmanned Aerial Vehicles 1,5,3,64,VeRoLog - Vehicle Routing and Logistics,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S16 [building - 101],"['Vehicle Routing', 'Programming, Mixed-Integer', 'Network Design']",MB-64
"We consider a tactical shift scheduling problem, consisting of assigning health care workers in a hospital to shifts for handling a given number of tasks spread out over a fixed time horizon. We will implement a branch-and-price algorithm with the master problem assigning schedules to workers and the subproblem generating columns [shift schedules] to add to the master problem. The master problem will minimize the total cost of the chosen shift schedules and assigning outside workers to handle understaffing. The novelty in our approach is the use of a network flow model in the shift subproblem. Instead of formulating the subproblem as a MIP, we will make use of a novel network formulation. By traversing the network from the starting period to the end period a path, corresponding to a valid shift schedule, will be achieved. As such the subproblem can be solved, and a maximum reduced cost schedule can be found, by solving the shortest path problem for the graph. We will implement Dijkstra's algorithm with a labelling and readout step to find the shortest path through the network. In addition we will evaluate the possibility of scheduling flexible and in-flexible workers to achieve a greater flexibility in the generated schedules. The fraction of flexible workers will be evaluated with varying degrees of flexibility to ensure a feasible application of the method. The method will be applied and tested on a number of test instances generated from real-world data.",Flexible shift scheduling of healthcare workers using column generation,"[78651, 78783]",607,"[13, 150, 56]",4021,Staffing and workforce planning and scheduling,3,9,15,OR in Health Services [ORAHS],"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,18 [building - 116],"['Column Generation', 'Network Flows', 'Health Care']",TC-15
"Recent research highlights that supply chains [SC] need to be resilient against disruptions and adaptable to long-term shifts in demand and supply environment. Moreover, there is a growing recognition of the need to transition towards a circular economy to protect the environment while promoting economic growth. This has led to a call for developing SCs that are sustainable, resilient, and adaptable, called viable SCs. Motivated by a manufacturing company in Norway as the case study, this study proposes a scenario-based stochastic programming framework to measure resilience and adaptability through qualitative scenarios. It also suggests integrating the core principles of the circular economy, which are called R-imperatives, into product development processes and supply chain design decisions to make SCs viable. Our results reveal the crucial role of implementing R-imperatives into the integrated PD and SC design for establishing the foundations of design for viability.","A Scenario-Based Stochastics Programming Framework to Enhance Viability - Aligning R-imperatives, Product Development, and Supply Chain Design","[79026, 74754, 70680, 79126]",923,"[136, 138, 139]",4022,Optimization for the Circular Economy,18,9,23,"Circular Economy, Remanufacturing and Recycling ","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,82 [building - 116],"['Stochastic Optimization', 'Supply Chain Management', 'Sustainable Development']",TC-23
"Cluster-based service systems are used to provide parallel processing of jobs. A specific feature of such systems is that jobs are divided into tasks that should run in parallel on different servers. A job does not start execution unless the required number of idle servers is available. This paper proposes a new analytical model based on Markov chain theory that captures the typical dynamics of such cluster-based system with finite buffer size. Through our analysis, key performance measures of a cluster-based service system such as the job blocking probability, the average job delay in the queue or the average utilization of the cluster servers can be estimated. The application of the model to cloud centers with thousands of servers and a heterogeneous workload is shown possible.",Performance analysis of a cluster-based service system,"[59550, 79123]",799,"[121, 135, 18]",4024,Stochastic Models in Service Operations II,50,12,39,Stochastic Modelling,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,35 [building - 306],"['Queuing Systems', 'Stochastic Models', 'Computer Science/Applications']",WA-39
"Mixed cropping systems present a promising solution to conventional agriculture by combining multiple crops within one field, thus increasing biodiversity and soil health, yet pose new planning challenges in terms of plant assignment and crop scheduling. To address this issue, we propose a novel decision problem in which crops need to be assigned to different farms and fields over time, taking into consideration a range of diverse factors, including plant interactions, field characteristics, and growth conditions, as well as the demand and geographical location of the customers. In this research, we formulate this problem as a mixed-integer quadratically constrained optimization model aimed at maximizing the total yield and propose a column generation-based approach to solve larger instances. Preliminary results show that our approach outperforms commercial solvers in terms of speed and solution quality. In addition, we present a managerial analysis on the effects of companion planting and the broader implications for farmers, enriching the discussion on mixed cropping systems and sustainable agriculture.",Quadratic Optimization for Sustainable Agriculture – A Study of Mixed Cropping Systems,"[79121, 72182, 53556, 55094]",380,"[89, 13, 100]",4026,Sustainable Food Supply,78,14,13,Secure & Sustainable Food Supply,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,15 [building - 116],"['OR in Agriculture', 'Column Generation', 'OR in Sustainability']",WC-13
"The manufacturing sector is considered one of the most significant consumers of energy. Given the energy crisis, manufacturing industry is paying increasing attention to energy-efficient scheduling as a short/medium-term strategy for reducing energy consumption without requiring significant investments. In this paper, the Hybrid Flowshop Scheduling Problem with Blocking Constraints and Sequence Dependent Setup Times [BHFS-SDST] is addressed for the minimization of makespan and total energy consumption [TEC]. A novel constraint programming [CP] model as an exact method is developed and its performance is compared with another exact method, mixed-integer linear programming formulation [MILP] and a multi-objective iterated greedy [MOIG] metaheuristic. The effectiveness of the proposed approaches is tested with randomly generated small, medium, and large benchmarks. Computational experiments demonstrate the efficiency of the developed methods in solving the BHFS-SDST problem.",Exact and metaheuristic methods for multi-objective energy aware hybrid flow shop scheduling problem with blocking and sequence-dependent setup times,"[79120, 77948, 79050]",856,"[93, 69, 14]",4027,OR in Energy II,23,14,19,OR in Energy,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,44 [building - 116],"['OR in Energy', 'Manufacturing', 'Combinatorial Optimization']",WC-19
"Data sovereignty has long been a focal point for enterprise customers, cloud service providers [hyperscalers], end-users, and national and international policymakers. The cloud distributed computing model has been presented as a means to control and apply data sovereignty aspects. This presentation delivers a comprehensive review of data sovereignty aspects within the context of public cloud computing environments. While multiple technologies supporting data sovereignty in the public cloud already exist, they also impose certain limitations and increased costs due to the necessity of adhering to specific frameworks and leveraging additional cloud cryptography, thereby increasing computational overhead. This paper provides a comparison of reference architectures for solutions reflecting data sovereignty in the public cloud. Appropriate computational experiments were conducted, and conclusions have been formulated.",Modeling Data Sovereignty in Public Cloud – A Comparison of Existing Solutions,"[79118, 4224, 79297]",391,"[18, 24]",4028,DEA and its application,89,2,48,Data Envelopment Analysis and its Application,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,60 [building - 324],"['Computer Science/Applications', 'Data Envelopment Analysis']",MA-48
"Zhou and Schoelkopf [2005] note that Many real-world machine learning problems are situated on finite discrete sets. Typically we may assume that there exist pairwise relationships among data.
In this talk we consider the general setting of classification on data sets in finite metric spaces - these include a broad range of cases, including pairwise relationships satisfying the triangle inequality.
Instead of embedding the data in an Euclidian space and using kernel techniques, we embed it in a real vector space endowed with the infinity norm. This embedding is necessarily isometric which means that the data in the normed space can be studied without introducing any distortion to the original metric space setting. While the target feature space is no longer an inner product space, and so the idea of kernel does not carry forward, there is still potential for a linear class boundary using the Hahn-Banach Theorem.
We discuss the advantages and disadvantages of such an embedding in the context of classifiers such as support vector machines.
In particular, we are interested in the problem of adjusting an already-trained binary classifier in response to new data. New training set data may become available after training a support vector machine - will the same trained machine still work or does it need to be adjusted; and if so, how?
",Classification in a vector space with the infinity norm,"[22643, 72394]",673,"[7, 66]",4030,"Advancements of OR-analytics in statistics, machine learning and data science 19",16,15,06,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,1013 [building - 202],"['Analytics and Data Science', 'Machine Learning']",WD-06
"In digital control rooms, controllers' experience significantly impacts their performance. They accumulate expertise through repetition, termed local knowledge, given the stable and repetitive nature of their tasks. Alternatively, they broaden their skills by working across various workstations, termed neighbor knowledge. Our study investigates the role and impact of these types of knowledge on operational performance, and examines the portability of neighbor knowledge within digital control room settings. For this purpose, leveraging a purposefully built comprehensive dataset from a major European rail system operator, we analyze how workstation-based and train-based experiences influence controllers' operational performance, measured by train delay. Our findings reveal that accumulated work at the same workstation improves operational performance, underscoring the importance of local knowledge. Further, when focusing on operational performance regarding passenger trains, we observe higher performance for controllers with more experience in passenger trains, but also lower performance for controllers with more experience in freight trains, indicating potential drawbacks of neighbor knowledge on operational performance.",Local knowledge and operational performance - Evidence from a digitized setting,"[62482, 77872, 41201, 65425]",102,"[10, 0]",4031,Behavioral Decision Analysis IV,13,7,11,Behavioural OR,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,12 [building - 116],['Behavioural OR'],TA-11
"The interest in the use of Deep Neutral Networks [DNNs] has grown rapidly over the last few years. As an increasing number of people and businesses are using DNN-based systems and governments start to take actions to regulate the use of artificial intelligence, there is a growing demand for methods to analyze the trustworthiness of a DNN and the limits of its application. One classical illustration for showing weaknesses of DNNs, especially in the context of image recognition, are Adversarial Examples. These are slightly modified versions of input data, that lead a DNN into wrong classifications. As Fischetti and Jo [2018] have shown, Adversarial Examples can be generated by using mathematical programming methods. Thus, these Adversarial Examples are provably optimal in respect to a given criterion, e.g. the distance to some given input data. However, the structure of these examples highly depends on the parameters of the network. To address this point, we will present a mixed-integer programming model for generating Adversarial Examples, that are robust with respect to small changes in the weights and biases of a DNN. For relaxations of the model, we will illustrate the impact of robustification on Adversarial Examples. Furthermore, we present experimental results on the influence of training data on the distance of Adversarial Examples and on the transferability of our examples.",Towards analyzing DNNs by robust adversarial examples created with MILPs,"[58332, 16606]",72,"[8, 111, 127]",4032,"Advancements of OR-analytics in statistics, machine learning and data science 1",16,2,28,"Advancements of OR-analytics in statistics, machine learning and data science","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,065 [building - 208],"['Artificial Intelligence', 'Programming, Mixed-Integer', 'Robust Optimization']",MA-28
Fixed interval scheduling deals with the problem of assigning jobs with fixed processing intervals to available machines. We consider the problem where the finishing times are subject to random delays. We elaborate the problem with expected number of overlaps objective which penalizes the number of additional machines needed to process the jobs when the delays appear and some jobs cannot be processed by the assigned machines. We consider preventive maintenance operations which can improve machine reliability and positively influence the distribution of the random delays of the jobs assigned to the same machine. This leads to a two-stage stochastic optimization problem with endogenous uncertainty for which we propose an extended robust coloring reformulation. We design a Benders decomposition algorithm to solve larger instances to optimality and perform a numerical study on simulated instances. ,Preventive maintenance in fixed interval scheduling problems under endogenous uncertainty - a stochastic optimization approach and exact decomposition algorithm,[24470],830,"[117, 129, 109]",4035,Optimization under Uncertainty in Manufacturing and Supply Chain Management,49,15,35,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,44 [building - 303A],"['Programming, Stochastic', 'Scheduling', 'Programming, Integer']",WD-35
"In this paper, we introduce three heuristic algorithms to investigate their in soilving Generalized Nash Equilibrium Problems [GNEP]. The first is an evolutionary-inspired algorithm which utilizes competitive selection and [linear \& nonlinear] regression to motivate generations of new points. The second involves stochastic gradient descent of the Shadow Point function across mass numbers of agents to find game solutions. Last but not least, we present an ANN-based model with intelligent agents who solve a GNEP. These algorithms are evaluated on 2 and 3 player games in 2 and 3 dimensions, with both linear and non-linear shared constraints. The success of these algorithms is discussed and compared vis-a-vis existing, optimization-based methods [such as VI/QVI and KKT-based methods], and the limitations of the algorithms are explored. Finally, we introduce two measures of performance for all the algorithms in terms of their capability of finding and describing the entire solution set of a GNEP.

This is joint work with - Benjamin Benteke, Kira Tarasuk, Nick Hoover and Mihai Nica [at U of Guelph]",Heuristics and optimization-based methods for solving generalized Nash games - a comparison,[8786],30,"[50, 5, 8]",4037,Tools and algorithms for equilibrium detection,63,10,40,"Interfaces Between Optimization, Hierarchical Problems and Equilibrium Detection with Applications","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,96 [building - 306],"['Game Theory', 'Algorithms', 'Artificial Intelligence']",TD-40
We consider a strategy for generating cuts for set partitioning and set packing constraints. We perform an exploitative wide branching to detect infeasible and suboptimal integer combinations and tighten continuous relaxation. The idea originates from results that even restarting branch-and-bound solver can be beneficial fo overall performance.,Generating cuts with phantom branching,"[79095, 70639]",478,"[11, 111]",4039,Algorithms for Mixed-Integer Nonlinear Programming and Nonconvex Optimization,86,12,04,MINLP,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1001 [building - 202],"['Branch and Cut', 'Programming, Mixed-Integer']",WA-04
"Autonomous trucking is expected to increase transportation productivity, especially for long-haul transportation. Providing infrastructure and enablers with enough capacity to support this type of transportation is essential for the successful deployment of autonomous trucks. In this paper, we investigate the capacitated transportation network design problem for autonomous trucks. We assume that the autonomous network requires several types of enablers with limited capacity to support autonomous truck transportation. A new mixed-integer linear programming model is proposed to locate various capacitated infrastructures and enablers required for long-haul autonomous transportation over multiple periods. This model strategically integrates the location decisions of multiple enablers over the planning horizon. Enablers, such as refueling stations, emergency maintenance stations, and transfer hubs, are characterized by unique operational features and capacity considerations. The model is implemented numerically on benchmark cases, such as the Sioux Falls network, and sensitivity analysis for key parameters is provided. ",Capacitated Multi-Enabler Transportation Network Design for Autonomous Trucks,"[67882, 19182, 1024]",523,"[143, 79, 111]",4040,Optimization in transportation infrastructure design and management,6,5,56,Transportation,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S04 [building - 101],"['Transportation', 'Network Design', 'Programming, Mixed-Integer']",MD-56
"Island waste transportation involves moving waste material generated within these remote regions, often characterized by limited infrastructure and unique geographic constraints. These conditions frequently create substantial barriers to sustainable waste management, resulting in
inefficient collection and disposal methods, elevated transportation expenses, and detrimental environmental impacts. This research is motivated by previous studies on island transportation planning, which focused on optimizing transportation routes in these regions. Despite existing
literature on island waste transportation, there is a notable gap in addressing the sustainability aspects of waste transportation. This study aims to contribute to the literature by incorporating sustainability criteria into the transportation planning model for waste collection in island and remote regions, and also other issues that might arise in waste transportation, such as community engagement, ecological preservation, and public health safety, among others. ",Sustainability in Island Waste Transportation,"[74324, 54768, 53496]",625,"[139, 143, 65]",4041,Advancing mobility towards sustainable solutions IV,6,13,56,Transportation,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S04 [building - 101],"['Sustainable Development', 'Transportation', 'Logistics']",WB-56
"Randomized Quasi-Monte Carlo [RQMC] sampling provides unbiased estimators whose variance often converges at a faster rate than the standard Monte Carlo estimators as a function of the sample size n. But computing valid confidence intervals is not obvious because the n observations are dependent and the central limit theorem does not apply in general as n increases. A popular heuristic is to replicate the RQMC process r times [say, about 10] to obtain r independent realizations of the estimator, and use these r observations to compute a confidence interval based on the normal or Student distribution. Alternatives include various types of nonparametric bootstrap methods for which normality assumptions are not required. We report extensive numerical experiments that compare these methods in terms of the width and coverage probability of the confidence intervals, for a variety of examples. The results were not exactly as we expected. Based on these results, we give our recommendations.",How to compute confidence intervals for quasi-Monte Carlo estimators,"[5117, 79131, 79132, 79133]",673,"[131, 117]",4045,"Advancements of OR-analytics in statistics, machine learning and data science 19",16,15,06,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,1013 [building - 202],"['Simulation', 'Programming, Stochastic']",WD-06
"In Chile, the Environmental Impact Service [SEA] mandates assessments for industrial projects falling under specific categories, necessitating either an Environmental Impact Study [EIA] or an Environmental Impact Declaration [DIA]. Central to the evaluation process is the invitation of the correct state administration agencies for participation. This investigation presents a novel approach leveraging pre-trained transformer models, tailored to predict the involvement of various state bodies in project assessments. Through the analysis of text within the project's primary form of declaration, we curated a dataset capturing participation patterns. Secondly, we fine-tuned the transformers to yield multi-label predictions, this was done by the modification of the final transformer layer, which facilitated the output of probabilistic estimates for each agency's participation. To address class imbalance in the dataset, we integrated class weights techniques into the loss function during model training. The experimental results showcase the effectiveness of this approach in accurately predicting multi-label participation, thereby enhancing the decision-making process surrounding environmental impact assessments. This research contributes to refining the SEAs procedures, ensuring comprehensive participation and evaluations of industrial projects by state agencies.",Enhancing Environmental Impact Assessment Procedures in Chile - Multi-Label Transformer Models for Predicting State Agencies Participation,"[74286, 57734, 18420]",392,"[66, 7, 8]",4046,Analytics for Decision Making,17,12,31,Analytics,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,54 [building - 208],"['Machine Learning', 'Analytics and Data Science', 'Artificial Intelligence']",WA-31
"The importance of transportation in society and its significant contribution to polluting emissions from internal combustion engines using fossil fuels highlight the necessity of transitioning to a renewable energy matrix. Green hydrogen emerges as a viable option to mitigate this impact. This transition aims to reduce emissions in maritime transportation progressively. 
The maritime sector plays a crucial role in global trade and transportation, accounting for a substantial portion of fossil fuel consumption and associated emissions. The adoption of cleaner and sustainable technologies in maritime transport not only aids in emission reduction but also fosters innovation and the development of eco-friendly solutions. Enhancing research and development in energy-efficient technologies and environmentally sustainable practices within maritime transport is essential for combating climate change on a global scale.
Transitioning towards green hydrogen and other renewable energy sources in maritime transport signifies a significant step towards a cleaner and more sustainable future. This shift prioritizes emission reduction and environmental protection as key objectives. By leveraging green hydrogen and renewable energy sources, the maritime industry can pave the way for a greener, more efficient, and environmentally conscious approach to transportation, setting new standards for sustainability and emission reduction in the sector.
",PROPOSAL TO REDUCE CARBON FOOTPRINT THROGH THE USE OF H2V IN TUGBOATS IN PORTS ,"[54768, 79134, 54786, 74324]",173,"[139, 143, 70]",4047,Energy Management in Ports and Shipping I,52,8,62,OR in Port Operations,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S12 [building - 101],"['Sustainable Development', 'Transportation', 'Maritime applications']",TB-62
"Efficiently solving a vehicle routing problem [VRP] is critical for delivery management companies. This work explores the theoretical and experimental connection between the Capacitated Vehicle Routing Problem [CVRP] and Constrained Centroid-Based Clustering [CCBC]. Transitioning from CVRP to CCBC represents a shift from exponential to polynomial complexity, utilizing commonly known clustering algorithms, such as K-means.

Our analysis highlights the intrinsic relationship between these problems through illustrative examples and deduced mathematical properties, emphasizing how achieving optimal or near-optimal CVRP solutions strongly depends on starting from specific CCBC centroid regions.

We then propose a CCBC-based approach enhanced to address CVRP, operating in three stages - a constrained centroid-based clustering algorithm generates customer clusters, integrating multi-start centroids, a refined customer assignment metric, and dynamic cluster count adjustment. Subsequently, a Traveling Salesman Problem [TSP] solver optimizes customer sequencing within clusters.

Moreover, our study explores the role of reinforcement learning in identifying centroid regions leading to optimal or near-optimal CVRP solutions. Demonstrating improvements in solution quality and computational efficiency, our findings suggest a transformative approach for delivery management challenges, offering a significant milestone in solving CVRP.",Exploring the intersection of Capacitated Vehicle Routing Problem and Centroid-Based Clustering,"[79130, 53863, 24885]",879,"[145, 14]",4048,Combinatorial optimization issues in transportation [Contributed],64,15,26,Combinatorial Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,012 [building - 208],"['Vehicle Routing', 'Combinatorial Optimization']",WD-26
"INTRODUCTION
In Chile, every year in fall and winter, respiratory diseases increase in pediatric population [due to different factors] and this produces an increase in the number of hospitalizations, sometimes collapsing the health system. To anticipate, a system has been created to predict when the peak will be.

METHOD
Data from Luis Calvo Mackenna Hospital were obtained from the open access database DEIS of the Ministry of Health. The created system was used to predict the peak date of pediatric hospitalizations from 2017 to 2023, except 2020 and 2021 due to the pandemic. The curve produced were smoothed by moving averages. As the curve ascends, alerts created with machine learning begin to appear. The model was tested using antecedents from 2 to 7 previous years.

RESULTS
Tests performed with 7 and 6 years of historical data had 100% results within the prediction interval [RMSE = 7.0 with respect to the exact day], while with 5 and 4 years were 67% [RMSE = 10.2] and 50% [RMSE = 11.3], respectively. Finally, with 3 years it was 20% [RMSE = 11.4] and with 2 it was 0% [RMSE = 12.6]. In all cases, the prediction was approximately one month in advance.

CONCLUSIONS
The system created has promising results if sufficient historical data is added to train the model. A future challenge is to adjust it for other pediatric hospitals.

ACKNOWLEDGMENTS
This work was funded by the ANID FONDEF IDeA I+D 2023 ID23I10423.
",Peak prediction of pediatric hospitalizations due to respiratory diseases,"[75797, 79135]",611,"[66, 73, 7]",4049,Machine learning and game theory in healthcare,3,2,15,OR in Health Services [ORAHS],"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,18 [building - 116],"['Machine Learning', 'Medical Applications', 'Analytics and Data Science']",MA-15
"In the dynamic and complex realm of the airline industry, efficient and reliable scheduling and execution of aircraft maintenance are crucial. Significant challenges in the optimal scheduling of aircraft line maintenance arise from the need to balance maintenance requirements with the limited availability of resources and dynamic operational constraints. Traditional approaches typically employ optimization-based methods to address these issues. However, these methods often struggle to scale up in real-world applications, along with the uncertainties and non-linearities prevalent in such scenarios. Recent advancements in airline scheduling signal a paradigm shift, with simulation-based, data-driven, and AI-enhanced approaches superseding conventional optimization models. This paper introduces an innovative approach that capitalizes on machine learning by integrating a hybrid simulation model with a reinforcement learning [RL] algorithm and dynamic programming search.

Utilizing a hyper-heuristic learning approach, the RL agent explores and learns the optimal scheduling of maintenance tasks, encompassing a criterion-based action space and considering various factors such as aircraft availability, resource allocation, and maintenance requirements. The results surpass the performance of existing optimization-based methods and offer valuable insights that may be applicable to other industries facing similar scheduling challenges in the transportation and supply chain sectors.",Optimizing Aircraft Line Maintenance Scheduling - A Reinforcement Learning Approach,[41500],879,"[131, 4, 8]",4052,Combinatorial optimization issues in transportation [Contributed],64,15,26,Combinatorial Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,012 [building - 208],"['Simulation', 'Airline Applications', 'Artificial Intelligence']",WD-26
"We consider the problem of locating multiple facilities on a network. It is assumed that all facilities are owned by a single decision maker, and each facility has an attractiveness. The users' choice behavior of facilities is described by multinomial logit model where the utility of each user is given by an increasing function of the attractiveness of each facility and a decreasing function of the distance to the facility. We consider the problem of finding facility locations that maximize the sum of the expected utilities for all users. This problem is a natural extension of the p-Median problem by assuming probabilistic choice behavior. The proposed model is formulated as a nonlinear integer programming problem. We present two solution approaches. One is based on a formulation in which the objective function is approximated by a set of linear functions, and the other is a heuristic method based on vertex substitution. Using example road networks, we investigate how the distance decay coefficient affects the locational patterns of facilities. In addition, we present a generalized problem involving optimization of the facility size under the budget constraints.",Locating multiple facilities on a network to maximize the sum of expected utility,[20142],614,"[64, 0]",4053,Location under uncertainty,29,5,61,Locational Analysis,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S10 [building - 101],['Location'],MD-61
"The court system, pivotal for social justice, is overflowed by congestion affecting social welfare, economic development, and access to justice. Despite its significance and operational complexity, being characterized by constrained resources, increasing demand, long in-process waiting time, impatient customers, and LOS measured in years, empirical research on court operations remains limited. This scarcity is largely attributable to challenges in accessing reliable and comprehensive data. This study employs NLP and AI tools to transform two decades of US Federal District Court case dockets into a detailed event log, leveraging process- and queue-mining to explore judicial congestion. By scraping, labeling and analyzing millions of docket entries through operations management lenses, we are able to observe the case flow in the system and assess congestion impacts on case processing. Our findings illuminate the judicial workflow, offering insights for reducing congestion and enhancing system efficiency. This pioneering approach underscores the potential of data-driven analysis in court systems operations.",From docket to data - unpacking judicial congestion using process- and queue-mining,[61529],672,"[7, 101, 130]",4054,"Advancements of OR-analytics in statistics, machine learning and data science 18",16,14,06,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,1013 [building - 202],"['Analytics and Data Science', 'OR/MS and the Public Sector', 'Service Operations']",WC-06
"In this research we intend to review port tariff literature. In our analysis we consider strategic pricing,  infrastructure cost recovery, market conditions and external costs. We are interested in identifying the optimization models and methods used to define tariff structures, and their relation with the strategies, the infrastructure and the market. Strategic pricing schemes are based on a different tariff between cargo owners due to segment demand or freight traffic. Infrastructure tariffs are usually managed by the port authorities and may be influenced by policies and regulations. The market is influenced by the global supply chain. We expect to identify which are the main elements in literature to decide pricing strategies, infrastructure and the market characteristics as well as the mathematical modeling problems that have been considered.",Port fariff considerations ,"[13432, 50208]",154,"[65, 25, 84]",4056,Port Performance,52,15,62,OR in Port Operations,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S12 [building - 101],"['Logistics', 'Decision Analysis', 'Optimization Modeling']",WD-62
"Ride-hailing has evolved from the decentralized model in which taxis operate independently to the centralized model with on-demand matching of customer travel requests with the available drivers.
The existence of a coordinator is a difference between the decentralized model of hailing taxis and the centralized matching in ride-hailing platforms. More research needs to be conducted to quantify the differentiated value provided by the platform and how this value changes in different contexts. 
In this paper, we quantify the value of coordination through matching in ride-hailing platforms by comparing differences in performance metrics between centralized and decentralized models. The performance metrics
include supply-side metrics like driver utilization, demand-side metrics like customer search time, and platform-side metrics like the number of unmatched customers. 
Using an agent-based model, we show that the value of coordination is different for the two sides of the platform. We explore how the value of coordination changes with variations in [i] demand and supply characteristics like the spatiotemporal distribution of demand, [ii] driver characteristics like knowledge of high-demand zones, [iii] customer characteristics like patience levels, and [iv] network characteristics like topography. Our analysis offers insights to the platforms in understanding the role of demand, supply, behavioral, and network characteristics in the value generated through their operations.",Platform coordination in ride-hailing through matching and pricing,"[59110, 59111]",850,"[3, 131, 143]",4059,Simulation in economics II,77,5,43,"Agent-based Models in Management, Economic and Organisation Sciences","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,99 [building - 306],"['Agent Systems', 'Simulation', 'Transportation']",MD-43
"The library users’ experience depends on the library’s performance on their operations. Material handling processes are crucial to improve those library’s operations and service levels, assuring books and items availability. Even if a flow of research has flourished into implementing new solutions, e.g. Autonomous Mobile Robots [AMRs], in response to operational challenges that limit library performance, scientific literature still lacks modeling such solutions. Hence, this study aims to investigate such solutions and their impact on improving the overall performance of library logistics operations.
With the Trondheim Public Library as a case study, several scenarios are analyzed. We considered two layouts - centralized and decentralized with different modes to transport the materials - manual and AMRs. We modeled the scenarios through Queueing Theory. For manual movements, we used Open Queueing networks [OQN], while scenarios used with AMRs are Semi-open Queuing networks [SOQN]. SOQN has shown promising applicability in similar contexts to support the decision-making processes for planning and controlling the material flow. The proposed Queueing networks are validated through simulation. 
Based on this and varying critical parameters such as flow volume, intensity, variability, library layout, and operational modes, we obtain performance measures to characterize the analyzed scenarios for library logistics operations as well as the advantages and disadvantages.
",Library Logistics Operations - An Analytical Study on Material Handling Processes,"[70680, 78828, 74045, 70286]",761,"[121, 65, 146]",4062,Warehouse Operations,5,7,58,VeRoLog - Vehicle Routing and Logistics,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S07 [building - 101],"['Queuing Systems', 'Logistics', 'Warehouse Design, Planning, and Control']",TA-58
"Transportation causes roughly 25% of global greenhouse gas emissions due to its reliance on fossil fuels [European Commission, 2011. In Turkey, road transport alone emits a staggering 93% of the sector [DEK, 2017]. To combat this, electrifying public buses offers a promising solution for sustainable cities. Electric buses boast zero tailpipe emissions, significantly improving air quality [Hensher et al., 2022]. They outperform diesel buses, which emit 1.35 kg of CO2 per mile, highlighting their potential to reduce emissions.

This study explores the optimal electric bus fleet conversion planning problem [EBFT problem] aligned with the Paris Agreement's emission reduction goals. We propose a linear programming model for EBFT, considering direct/external costs alongside salvage value. The model optimizes for both economic [minimizing total fleet conversion cost] and environmental [minimizing total emissions] objectives. We incorporate four scenarios representing different electrification levels.

A case study using Istanbul's Metrobus data demonstrates a scenario-dependent optimal conversion plan. For Istanbul, gradual conversion achieves a 21% emissions reduction by 2030. This study employs a comprehensive optimization approach, considering multiple objectives, timeframes, and vehicle ages.
",A Multi-Objective Optimization Approach for Electric Bus Fleet Conversion on the Istanbul Metrobus Line,"[79138, 52248]",956,"[110, 77, 143]",4065,Optimization of sustainable urban mobiltiy II,79,8,18,Sustainable Cities,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,42 [building - 116],"['Programming, Linear', 'Multi-Objective Decision Making', 'Transportation']",TB-18
"This research aims to develop and evaluate a model that explores the relationship between perceived overqualification, job crafting and employee cyberloafing. Drawing on job-demand resource theory, the model proposes that employee engagement serve as an explanatory factor in this association. Additionally, it has been suggested that the ambidextrous leaders also positively impact employee engagement which in turn reduce cyberloafing.",Role of perceived over qualification and job crafting in mitigating cyberloafing - A moderated-mediation model,[78937],692,"[10, 0]",4072,Developing Countries and Sustainable Humanitarianism ,67,15,18,OR for Development and Developing Countries,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,42 [building - 116],['Behavioural OR'],WD-18
"An urban regeneration project brings the creation of tangible and intangible effects that arise from the design choices adopted, as well as from the sustainability goals identified. The tangible effects of the intervention are traditionally intercepted by economic evaluation models and allow design choices to maximize the developer’s economic return. In addition, intangible effects are generated, such as, the impact on the quality of life, environmental impacts, and the social and cultural vibrancy of the area. Defining, and even more quantifying, the intangible effect of a project is complex, requiring effort in analyzing the intentionality and additionality that lead to the creation of added value for the area in which the project is located and for the community that benefits from it. 
The objective of the study is to identify and measure the impacts and the social additional value generated by the urban regeneration of the Porta Nuova district while considering the effects it has triggered on neighboring districts in the city of Milan and at larger scale. A multimethodological approach, based on Stakeholders’ Analysis, Multiple Criteria Spatial Decision Support Systems [MC-SDSS] and Community Impact Evaluation [CIE] has been developed to identify the purpose and the dimensions impacted by the urban project on the different stakeholders categories. The final result highlights the spatial distribution and the intensity of impacts from a purpose perspective. 

",Spatial multidimensional evaluation of impacts generated by the Porta Nuova district in Milan [Italy] - a purpose perspective.,"[46433, 51367, 79146]",905,"[26, 139, 137]",4073,"How to support complex decisions. Negotiating the trade-off between Social, Environmental and Economic values 2",44,5,47,Multiple Criteria Decision Analysis,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,50 [building - 324],"['Decision Support Systems', 'Sustainable Development', 'Strategic Planning and Management']",MD-47
"Predicting accurate Estimated Times of Arrival [ETA] for Inland Waterway Transportation [IWT] remains a challenge to coordinate port operations and reduce waiting. Captains report ETAs on inland vessels based on the remaining distance to the destination and the captain's experience.  A comprehensive literature review highlights the current methods for ocean-going vessel trajectory-based ETA prediction, ocean-going feature engineering for direct prediction, and predictions of ETA times in IWT. The experiments use historical position data [AIS] reported by vessels heading to the port of Rotterdam, as well as dimensions of the vessels, weather conditions, and water levels. Several machine learning methods using Neural Networks, Gradient-Boosted Decision Trees, and Light Gradient-Boosting Machines are proposed to predict ETA at the Port of Rotterdam. Predictions are then compared with the captain’s ETA and the Actual Arrival Times [ATA]. This paper shows that the predictions of ETA times are 98% more accurate than the captain’s ETAs. Compared to the ATA, our predictions are 99% accurate. These learning-based approaches can be further used in IWT research to create learning-informed optimization and simulation to reduce vessel turnaround times.  In practice, this improved accuracy of arrival time prediction will enhance the reliability of IWT and enable the port to develop an accurate terminal visit schedule for the inland barges.",Machine Learning Approaches for improved ETA Predictions and Turnaround Times of Inland Vessels at Sea Ports,"[78971, 79168, 35365]",166,"[66, 42, 65]",4074,Machine Learning and Optimization in Ports II,52,13,62,OR in Port Operations,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,S12 [building - 101],"['Machine Learning', 'Expert Systems and Neural Networks', 'Logistics']",WB-62
"We introduce a multi-factor stochastic volatility model aimed at describing the combined dynamics of both futures and option prices in commodity markets. The model is arithmetic, meaning that futures prices are affine functions of a finite set of state variables, which allows us to derive tractable formulas for vanilla options, and importantly, also for spread options. This dual tractability is a distinguishing feature of arithmetic models when compared to their multiplicate counterparts, and makes the model parameters amenable to efficient estimation using the Kalman filtering methodology on time series data consisting of futures and both types of options.
 
As an empirical application of the model, we apply the unscented Kalman filter on a data panel consisting of WTI crude oil futures, vanilla options, and calendar spread options. We demonstrate that the model can achieve a good fit to the data and describe how the inclusion of calendar spread options in the estimation provides additional information on the term structure of volatility and correlation. ",Calendar Spread Options and the Term Structure of Volatility,"[78879, 71189]",146,"[45, 0]",4075,Modelling commodity markets dynamics,74,2,57,Modern Decision Making in Finance and Insurance,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S06 [building - 101],['Financial Modelling'],MA-57
"Chile’s public healthcare system covers nearly 80% of the population. Despite its large coverage, it is characterized by long waiting times of its waiting lists. Our objective is to efficiently schedule hospital operating rooms for elective surgeries, to reduce the mean and median waiting time of the surgical waiting list. Our case study corresponds to a high complexity public pediatric hospital in Santiago, Chile. The hospital works based on morning and afternoon time blocks, each to be allocated to a single specialty for each operating room. Overtime is only allowed for long surgeries. 

For a given planning horizon, our strategy consists of two stages. First, we assign the medical specialties to the operating rooms, days, and blocks, by solving an integer linear programming problem that balances the available time in the planning horizon with the required surgery time demanded by each specialty. This considers variables such as the number of hours per block and the availability of surgeons per specialty and day. Second, we assign patients to the blocks based on one or more efficiency metrics. For instance, if only waiting time is considered, priority is given to patients with longer waiting times. 

The proposed scheduling model, although suboptimal, is faster than classical single-stage integer linear programming approaches, and shows more flexibility, as patients can be rescheduled without the need to rerun the scheduling once specialties are assigned to blocks.
",A two-stage operating room scheduling method for reducing hospital surgical waiting times for elective surgery lists,[78638],965,"[129, 56, 151]",4077,Surgery Scheduling and Operating Room Planning [2],3,15,15,OR in Health Services [ORAHS],"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,18 [building - 116],"['Scheduling', 'Health Care', 'Practice of OR']",WD-15
"Bicycle sharing systems have the potential to significantly alleviate traffic congestion and minimize demand for parking spaces in urban centers. One critical factor influencing the success of a bicycle sharing system is the effectiveness of rebalancing operations. These operations involve restoring the number of bicycles at each station to its expected demand through pickup and delivery activities performed by trucks. The Static Docked Bicycle Rebalancing Problem [SBRP] focuses on determining a cost-effective sequence of visited stations for a truck, along with the corresponding quantity of bicycles to be picked up or delivered at each station. Deviating from past studies, we propose a new formulation for the SBRP problem which minimizes the cost of rebalancing operations, while factoring in the cost of unmet passenger demand. The developed mathematical model is a mixed-integer nonlinear program, which is reformulated as a mixed-integer linear program. Then, our study introduces an exact branch-and-cut algorithm for addressing the SBRP and presents the results of computational tests. Lazy constraints and valid inequalities are introduced to improve performance and reduce computational time. The experimental findings demonstrate that we can solve to global optimality instances with up to 20 stations. A construction heuristic [nearest neighbor] is also introduced to provide a warm start to the branch-and-cut search, resulting in a more effective search of the solution space. ",A bicycle rebalancing model to minimize cost of operations and unmet demand in a docked network,"[79149, 79488, 79489, 67848]",526,"[5, 111, 143]",4078,Methods and models for sustainable transport solutions,6,7,56,Transportation,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S04 [building - 101],"['Algorithms', 'Programming, Mixed-Integer', 'Transportation']",TA-56
"Global greenhouse gas [GHG] emissions in 2023 have crossed more than 58GT, and transportation is one of the key contributors to these emissions. Supply chain play an important role in efficiently moving goods from manufacturers to end customers. However, the traditional approach to supply chain management often results in excessive carbon dioxide [CO2] emissions due to inefficient logistics processes. The supply chain landscape of automobile manufacturing organizations is characterized by complex networks involving multiple stakeholders, transportation modes, disparate inventory management systems, and complex supplier and dealer network. This complexity often leads to suboptimal transportation routes, underutilized capacity and excessive empty backhauls, all contributing to high CO2 emissions. Addressing Scope 3 CO2 emissions has become critical for organizations to align with sustainability goals.  
This paper attempts to address some of these challenges by focusing on shipment consolidation strategies to reduce the cost and GHG emissions in downstream logistics. By consolidating shipments, companies can streamline their distribution processes, reduce the number of vehicles on the road, and optimize delivery routes, leading to tangible reductions in CO2 emissions. The presentation will discuss a case study from the automotive industry, illustrating the significant impact of time-based consolidation on emissions offering practical insights for academic and industry leaders.",Optimizing Downstream Logistics - Shipment Consolidation Strategies for Sustainable Supply Chains,"[71771, 63134, 79150, 71784]",540,"[65, 138, 100]",4084,Sustainable Logistics,19,8,24,Sustainable Supply Chains,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,83 [building - 116],"['Logistics', 'Supply Chain Management', 'OR in Sustainability']",TB-24
"In the modern urban environment, the rise of e-commerce has led to significant last-mile delivery challenges, notably increasing congestion and air pollution. Estimates suggest that last-mile delivery accounts for half of the total CO2 emissions and delivery costs of the overall journey of a single parcel. A popular mitigation approach involves installing parcel lockers, promising yet demanding new models for optimal last-mile distribution network configuration. This study introduces a novel stochastic Mixed Integer Optimization [MIO] model for the optimal placement of lockers across various demand scenarios, incorporating a customer behavior model based on utility functions and queuing characteristics for the selected location. We present an efficient optimization strategy inspired by Benders Decomposition and explore a scenario where last-mile operators utilize a government-established locker network. Our findings, based on real courier company data to model demand and utility functions from regional surveys to design the utility model of customer. Lastly, the study presents a Continuous Approximation [CA] model guiding optimal locker quantity and the impact of the final locations and service level. The study discusses method performance, practical application, and improvement opportunities, offering insights into sustainable urban logistics.",Parcel Lockers as a Solution to Urban Logistics Challenges - A Stochastic MIO Model Incorporating Customer Utility ,"[67594, 51340, 67940]",274,"[130, 43, 32]",4087," Enhancement of circularity, inclusivity, and smartness in cities I",79,4,18,Sustainable Cities,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,42 [building - 116],"['Service Operations', 'Facilities Planning and Design', 'E-Commerce']",MC-18
"The rising prevalence of mental disorders significantly strains public healthcare services. The rapid growth of social media has created a solid foundation for research into public mental health. Artificial Intelligence [AI]-assisted methods have been developed to detect mental disorders in social media posts. However, existing studies have primarily focused on a limited array of predefined mental disorders, overlooking the continuous emergence of unknown mental disorders. This study presents an innovative method by establishing a comprehensive system based on pre-trained language models [PLMs], termed as systematic PLMs [sPLMs]. This system is adept at detecting known/in-domain [IND] and unknown/out-of-domain [OOD] mental disorders, and it further specifies the names of the mental disorders detected. Comprehensive experimental assessments in a public social media dataset, the Reddit dataset, demonstrate that sPLMs outperform pure PLMs [pPLMs] in detecting OOD mental disorders, achieving a 16.54% higher average F1 score. Furthermore, sPLMs exhibit a 91.58% accuracy in generating names for OOD mental disorders. This superior performance, both quantitatively and qualitatively, paving the way for advanced AI applications in healthcare domain. ",Employing artificial intelligence in healthcare - a systematic approach for the detection of mental disorders via social media analysis,"[78935, 79165, 79163]",632,"[8, 7, 26]",4088,"Advancements of OR-analytics in statistics, machine learning and data science 16",16,12,06,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,1013 [building - 202],"['Artificial Intelligence', 'Analytics and Data Science', 'Decision Support Systems']",WA-06
"The optimization of parcel pickup and delivery services aims to enhance efficiency, reliability, and cost-effectiveness across the logistics process, encompassing route optimization, delivery time reduction, and cost minimization.
This study delves into optimizing parcel services by integrating Unmanned Aerial Vehicles [UAVs], commonly referred to as drones, into logistics operations, with a specific focus on the last-mile delivery segment. The research endeavors to bolster the efficiency, dependability, and cost-efficiency of the entire delivery chain.
We examine a hybrid hierarchical system that amalgamates drones with conventional delivery methods to efficiently transport parcels from their origin points to final destinations, thereby curbing costs and enhancing delivery timelines. The study presents a constrained optimization problem along with its variational formulation.
Numerical simulations underscore how the incorporation of drones into multi-layered parcel delivery networks offers a promising avenue to elevate the efficiency, velocity, and adaptability of parcel delivery systems. This innovative approach empowers companies to overcome logistical hurdles, extend their operational footprint to remote areas, substantially diminish environmental emissions, and refine delivery timelines, particularly for last-mile deliveries.",Enhancing Parcel Pickup and Delivery Services with UAV Integration,"[79151, 22982, 79152]",900,"[84, 138, 112]",4091,Equilibrium detection in applications,63,12,40,"Interfaces Between Optimization, Hierarchical Problems and Equilibrium Detection with Applications","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,96 [building - 306],"['Optimization Modeling', 'Supply Chain Management', 'Programming, Multi-Objective']",WA-40
"At universities, academic staff performs many different tasks, resulting in heterogeneous demand for focused work, meetings, teaching, general collaboration and discussion with colleagues. These heterogeneous demands raise the question how appealing and effective working conditions can be guaranteed when shared workplaces are a necessity, i.e. when there are fewer desks than FTE’s available for a department. While large homogeneous office gardens with flex-work places maximize sharing, in practice they are disliked. We introduce the alternative idea of assigning clusters, groups of 7 to 9 employees, to a fixed number of desks and offices. Within a cluster coordination then occurs at a smaller scale, providing employees more agency in how to organize their office space usage. An optimization model is developed that assigns employees to clusters based on a short survey, previous office pairings and supervisory relations. In the model, we aim to have a fair distribution of “crowding” over the clusters for different times of the week. We develop several alternative objectives and ways to measure crowding and fairness in the model. The model was used during a pilot project with an academic department and some experiences from this project are discussed. We observe that for different clusters different coordination behaviour emerged. Also, junior employees more often found the opportunity to work in a private office compared to the situation with a fixed allocation.",Designing small-scale coordination clusters for academic office space usage,"[59535, 5079]",74,"[151, 112, 57]",4093,"Advancements of OR-analytics in statistics, machine learning and data science 3",16,4,28,"Advancements of OR-analytics in statistics, machine learning and data science","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,065 [building - 208],"['Practice of OR', 'Programming, Multi-Objective', 'Human Resources Management']",MC-28
"This paper contributes to the literature on predicting fuel poverty risk by applying clustering analysis to identify fuel-poor households. The study develops a novel approach to clustering-based XGBoost model by integrating data from a representative household survey in the UK [N=11'974]. The unobserved heterogeneity in fuel poverty households survey hides certain relationships between the contributory features in survey and fuel poverty. This study explores the application of the k-prototypes clustering method to group households into heterogenous clusters. The study segments the entire dataset into three clusters. Four XGBoost models were developed and applied to the entire dataset and each cluster to predict fuel poverty in households. The list of input features includes housing characteristics, socio economic features, and energy-cost variables. Additionally, for analyzing all features the shapley additive explanations [SHAP] method has applied, showing the different degrees of effects of each feature on fuel poverty in the entire dataset. The k-prototypes clustering-based XGBoost model is a promising approach to identifying the households at the risk of fuel poverty. Policymakers can gain new insights by identiying key socio-economic factors to alleviate fuel poverty. ","Algorithm-Enhanced Fuel Poverty Prediction Using Household Characteristics, Socio-Economic Factors, and Clustering Analysis","[21108, 78664]",340,"[7, 37, 93]",4094,Enhanced statistical methods for energy challenges,22,7,14,Energy Markets,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,16 [building - 116],"['Analytics and Data Science', 'Energy Policy and Planning', 'OR in Energy']",TA-14
"Prediction and optimization algorithms can be merged to address decision problems involving uncertain parameters linked to known contextual information. Within the framework of Contextual Stochastic Optimization, in this talk we explore the integration of learning models, such as neural networks, with optimization components. Our objective is to learn from a range of policies the most effective one, each parameterized by a neural network. In the specific, our setting is based on hybrid pipelines composed of a machine learning layer where we consider policies encoded by a neural network and concluded by a combinatorial optimization layer. Then, we aim to minimize the expected empirical risk loss on the data training set. To tackle this challenge, we introduce two novel algorithms - the Bregmann Progressive Hedging algorithm, based on a variant of the Progressive Hedging, and the Primal-Dual Mirror Descent algorithm. We present preliminary computational results on the minimum two-stage stochastic spanning tree problem and compare the performance of our learned policy with a well-tailored strategy designed for the problem, but heavily computationally. Our results show that our policy achieves good performance levels while being significantly less computationally demanding.",Contextual Stochastic Optimization using Mirror Progressive Hedging,"[68038, 79154, 79153, 68911]",141,"[66, 14, 136]",4095,Machine Learning for and with Mathematical Optimization,15,15,27,Mathematical Optimization for XAI,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,047 [building - 208],"['Machine Learning', 'Combinatorial Optimization', 'Stochastic Optimization']",WD-27
"In maritime search and rescue, an efficient search path is essential to maximize the success rate of the operation, typically indicated by the probability of containment and detection. This paper formulates a time-critical search problem, where a group of searchers moves through a discretized environment over time steps to locate the moving targets. Given the importance of finding drowning personnel timely, the decay factor of the survival rate is considered and incorporated into the objective function. To that end, we propose a novel approach specifically designed for this task, integrating Branch-and-Bound [B&B] with Reinforcement Learning [RL]. To obtain more effective and non-myopic policies than handcrafted heuristics, we guide the branching process in path planning via RL and design a novel set representation and reward function for the bounding procedures associated with a policy, speeding up the convergence of unsupervised exploration. Our experimental results demonstrate the effectiveness of the proposed method, particularly on large realistic instances, improving upon the current state-of-the-art. The method can be generalized for different problem settings in operational decision support for maritime search and rescue.",A novel approach for time-critical search planning in maritime search and rescue,"[78631, 79191, 76119, 10607, 79188]",389,"[58, 145, 113]",4100,"Disaster Response - Search and Rescue, Resource Allocation and Impactful Prepositioning",38,4,21,OR in Humanitarian Operations [HOpe],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,49 [building - 116],"['Humanitarian Applications', 'Vehicle Routing', 'Programming, Nonlinear']",MC-21
"Decision-making problems defined as global optimization problems of an objective function subject to a set of constraints arise in various application fields. The objective function and constraints can be black box and difficult to evaluate functions with unknown analytical representations. This means that there are black boxes associated with the functions which, given the values of the input parameters, return the values of the corresponding functions and the optimizer knows nothing about how these values are obtained. Therefore, one of the main goals in this context is to develop fast global optimization algorithms that produce reasonably good solutions with a limited number of function evaluations. We will discuss various deterministic approaches based on the Lipschitz continuity assumption to construct efficient and reliable numerical methods to solve the mentioned problems. Among these, the divide-the-best scheme for developing and studying numerical methods in a unitary way, the index scheme for managing non-convex constraints and the local tuning on the function’s behaviour will be examined in particular [see https://doi.org/10.1007/978-3-030-54621-2_764-1]. Some recently proposed high-precision techniques [including the infinite computing paradigm https://www.theinfinitycomputer.com] will also be considered.

This work was partially supported by the Italian INdAM GNCS Project 2023, number CUP_E53C22001930001.",Advanced numerical methods for Lipschitz global optimization,"[12851, 12956]",452,"[52, 5, 38]",4101,Stochastic and Deterministic Global Optimization,93,4,41,Stochastic and Deterministic Global Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,97 [building - 306],"['Global Optimization', 'Algorithms', 'Engineering Optimization']",MC-41
"We study duality and optimality conditions for general convex stochastic optimization problems. The main result gives sufficient conditions for the absence of a duality gap and the existence of dual solutions in a locally convex space of random variables. It implies, in particular, the necessity of scenario-wise optimality conditions that are behind many fundamental results in operations research, stochastic optimal control and financial mathematics. Our analysis builds on the theory of Frechet spaces of random variables whose topological dual can be identified with the direct sum of another space of random variables and a space of singular functionals. The results are illustrated by deriving sufficient and necessary optimality conditions for several more specific problem classes. We obtain significant extensions to earlier models e.g.\ on stochastic optimal control, portfolio optimization and mathematical programming.",Dual solutions in convex stochastic optimization,[79155],958,"[136, 21, 83]",4102,Stochastic optimization - theory and applications,49,13,35,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,44 [building - 303A],"['Stochastic Optimization', 'Convex Optimization', 'Optimization in Financial Mathematics']",WB-35
"The Digital Health Hub, funded by UK’s Engineering and Physical Sciences Research Council, is a collaborative initiative in the South West of England and Wales bringing together leading universities, supporting companies, National Health Service Trusts, and social care organizations to create a dynamic ecosystem for training, research, and collaboration.
The hub has three pillars. Firstly, the Hub's Skills and Knowledge Programme responds to the professional training needs of industry, health and social care providers, and academia, focusing on two key themes - Transforming Health and Care Beyond the Hospital, and Optimizing Disease Prediction, Diagnosis, and Intervention. The aim is to develop the necessary skills and expertise to drive innovation and deliver effective digital healthcare solutions.
Secondly, the Hub's Fellowship programme nurtures future leaders across academia, industry, health and care sectors, and the wider community. With five different schemes, it offers a platform for aspiring leaders to influence the future of digital health.
Finally, the Hub's Research programme focuses on advancing pre-competitive research within the transformative areas of healthcare. By collaborating with organizations, the Hub ensures that research efforts align with current health priorities and add value to the healthcare landscape. Our presentation provides an overview of the £3.2 million three-year programme.",Leadership Engagement Acceleration & Partnership [LEAP] Digital Health Hub - Driving Sustainable Digital Healthcare,"[63586, 79158, 79157, 79159, 28046, 79162, 79156, 79164, 42975, 45846]",608,"[7, 56]",4103,Healthcare Analytics,3,4,15,OR in Health Services [ORAHS],"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,18 [building - 116],"['Analytics and Data Science', 'Health Care']",MC-15
"Electric bikes have their niche in the last mile logistics. They offer certain advantages in terms of reduced emissions and infrastructure congestion. Sharing bikes between multiple depots provides for extra route flexibility and improved performance of the logistics system. In this talk, we address route optimisation for electric bikes in a shared multi-depot setting. We discuss modelling issues and present insights from computational experiments a.o. on the interplay between the locations of the depots and the performance of the transportation system.",Sharing e-bikes between multiple depots,[19186],503,"[145, 143, 65]",4104,Last mile delivery modeling,6,2,56,Transportation,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S04 [building - 101],"['Vehicle Routing', 'Transportation', 'Logistics']",MA-56
"Persons with severe mental illness who cannot live independently can make use of long-term residential facilities. The matching process between client and facility is complex, as both restrictions and client preferences play a crucial role. For this, we introduce the Knapsack-Based Routing model, which combines and extends two lines of research - Skill-Based Routing models and queues with resources. We design an effective rolling-horizon heuristic by combining key ingredients from different elementary models. For small instances, we compare the performance with a Markov Decision Process formulation. For larger instances, we find that our heuristic outperforms the best-known policy for Skill-Based Routing with preferences for a benchmark instance [Chen et al, 2020]. Finally, we apply our heuristic to a case study in Amsterdam and show how the current placement policy can be improved.",Allocation of Persons to Mental Health Homes by Knapsack-Based Routing,"[68630, 79167, 39439]",953,"[56, 135, 84]",4106,Healthcare logistics and routing,3,5,10,OR in Health Services [ORAHS],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,11 [building - 116],"['Health Care', 'Stochastic Models', 'Optimization Modeling']",MD-10
"Carbon border adjustment can be described as a taxing mechanism where a carbon-regulated market taxes materials imported from unregulated regions. In this study, we analyze how a manufacturer exporting to two regions—one developed [and regulated] and one developing [and unregulated]— responds to CBAM imposed by the developed region. We study two models. The first is a base model in which the manufacturer sets prices for both regions. In the CBAM model, the manufacturer considers investing in green technology to reduce its unit emission and then sets prices for both regions, taking into account the carbon border tax. We also consider a local government that may offer incentives to the manufacturer to invest in green technology at the initial stage. We find that under some market conditions, neither the government’s subsidy nor the carbon border tax causes a reduction in unit emission level.",Asymmetric Emission Regulations and Interventions of The Local Government,"[79161, 42083, 634]",486,"[50, 139]",4108,Carbon Regulation,19,13,24,Sustainable Supply Chains,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,83 [building - 116],"['Game Theory', 'Sustainable Development']",WB-24
"In this presentation, we will talk about the Mobile Production Vehicle Routing Problem, a new variant of the Vehicle Routing Problem. The objective of this problem is to determine a synchronized production and delivery schedule to minimize travel and delay costs. Previous work shows that CPLEX is unable to provide optimal solutions for most instances with more than 15 customers. This work aims to develop an exact algorithm to find optimal solutions for larger instances. We develop a branch-and-price algorithm for solving the problem. To accelerate the column generation procedure, an efficient dominance test and three heuristic pricing algorithms are developed to quickly find the columns with negative reduced costs for the pricing problem. Computational results on benchmark instances will be presented. ",Mobile Production Vehicle Routing Problem,"[22491, 19761, 65612]",755,"[145, 13, 111]",4109,Column Generation for Vehicle Routing,5,4,58,VeRoLog - Vehicle Routing and Logistics,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S07 [building - 101],"['Vehicle Routing', 'Column Generation', 'Programming, Mixed-Integer']",MC-58
"This Research deals with various principles that were examined and identified during Decision-Making processes and integrate them to a comprehensive methodology for modeling an analytical model which instructs how to design a DSS properly.
The analysis allows explaining a decision maker's behavior un-der the assumptions of bounded rationality, over time and it leads to a better understanding of the performances of DSS during their life-cycle. The model deals with the following principles:
The resolution of Information – The analysis demonstrates the trade-off between accuracy of information and time and costs re-sources to gain this information. 
The timing of decision-making - the analysis shows how to sup-port decision makers [e.g. - politicians] to take their decisions in the right moment. A model will be under the assumption that in-formation resources become more accurate over time, but opportunities sometimes vanish over time. 
Bounded Rationality of decision-makers - it is assumed that decision makers could not adopt a new set of decision rules and it takes to know how to handle with a new DSS platform. The analytic model will clarify how to consider this issue during a design process of DSS.
Multi Criteria approach - the analysis deals with decision situation where it is assumed that a decision-maker cannot formulate a joint utility function constituted from different criteria which are completely inestimable or estimable partly.
",Handling  Issues of Uncertainty while designing DSS ,[25473],657,"[26, 10, 25]",4113,Simulation and Modelling for Decision Support,45,14,45,Decision Support Systems,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,30 [building - 324],"['Decision Support Systems', 'Behavioural OR', 'Decision Analysis']",WC-45
"This work is on using Reinforcement Learning for Optimal Information Acquisition in Decision Processes. We use a model-based Deep Reinforcement Learning [DRL] technique to optimize large scale decision-intensive processes. We extend a basic model for decision-intensive processes with lead time constraints and parallel information acquisition. In many situations multiple sources of information can be requested in parallel, such that we can put a maximum throughput time constraint on the process while still minimizing for the total effort that is invested in the process. These extensions cause the state space of the problem to explode and we aim at showing the value of DRL for problems like this. For small size problems we make a comparison to the optimal solution and for larger size problems we introduce some heuristics against which we compare the performance of the DRL solution. For the small size problems our solution performs within a small range from the optimal solution. For the larger case we show significant improvements compared to the heuristics.",Scalable algorithms for throughput time constrained decision-intensive processes,[65938],473,"[26, 8, 67]",4114,Methods and Algorithms of Decision Support,45,4,45,Decision Support Systems,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,30 [building - 324],"['Decision Support Systems', 'Artificial Intelligence', 'Management Information Systems']",MC-45
"Production scheduling and vehicle routing are both well known and widely studied problems within the field of operations research. However, in many practical settings a supply chain consists out of a sequence of different stages that all require some scheduling or routing. Rather than just optimizing the planning within each stage separately, a good synchronization between the different stages is crucial to prevent inefficiencies or infeasibilities. Especially when there is none or limited storage capacity between subsequent stages, bad synchronisation might lead to delays or even deadlocks. An integrated solution method is required to prevent this and obtain good synchronization over the entire supply chain. We propose a general framework for modelling such multi-stage problems and compare different integration strategies

The issue of multi-stage scheduling problems with limited intermediate storage especially arises for industries that are making a transition from make-to-stock towards make-to-order. A good example is the compound-feed industry, where storage is usually designed for large quantities of universal products. Due to changing customer requirements, the production changes more and more towards small quantities of many different personalized products instead. Since intermediate storage is not suited for this, this increases the need for good synchronization between production and transportation. We also provide results based on realistic data from this industry","Integrated Production, Storage and Transportation Optimization in the Compound Feed Industry",[72501],676,"[138, 151, 5]",4115,DSS in Agriculture,20,9,12,OR in Agriculture and Forestry ,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,13 [building - 116],"['Supply Chain Management', 'Practice of OR', 'Algorithms']",TC-12
"The Ordered Weighted Averaging [OWA] method is a popular generalization
of robust optimization, which is most commonly applied to combinatorial
problems if they have an uncertainty set modeled by a discrete list of
scenarios. We introduce a novel approach, OWA for interval uncertainty,
which extends OWA to handle uncertainty represented by continous
intervals. We explore its properties, including its relationship with
OWA for discrete scenarios and sample convergence, and present both
theoretical results and solution methods.",OWA for Combinatorial Problems with Interval Uncertainty,"[72628, 29733]",882,"[14, 127, 25]",4116,Topics in Combinatorial Optimization I [Contributed],64,14,25,Combinatorial Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,011 [building - 208],"['Combinatorial Optimization', 'Robust Optimization', 'Decision Analysis']",WC-25
"The marginalisation of the inner areas due to increased social, material, economic, infrastructural vulnerability is a phenomenon that afflicts many countries today and is growing rapidly.In recent years, specific policies, measures, and funding have been proposed aimed at counteracting this phenomenon.To date, despite the increased awareness of the issue of marginal areas and the growing concern about the progressive decline and abandonment of large areas of territory, current policies are slow to produce the expected effects.The heritage of weak areas, if properly integrated, can become a resource, an opportunity for the whole territorial system. Decision-makers engaged in the prospect of the territorial rebalancing will have to be supported in identifying the residual values of these marginal areas, to recognise where and how it can be significantly emphasised in the prospect of an integrated long-run redevelopment process. The research, according to a multicriteria-based axiological approach, proposes the construction of a Reaction Capacity Index [RCI] of the internal areas of Sicily. The estimation and representation in QGIS of the RCI provides an overall framework based on which decision makers can identify specific policies and actions aimed at favouring a territorial rebalancing. The methodological approach for estimating the RCI index that provides an overall measure of territorial capital endowment is developed based on the Multi-attribute Value Theory [MAVT].",Territorial rebalancing in an axiological perspective. An Reaction Capacity Index of Sicily's inner areas,"[46428, 72264]",905,"[26, 77, 137]",4119,"How to support complex decisions. Negotiating the trade-off between Social, Environmental and Economic values 2",44,5,47,Multiple Criteria Decision Analysis,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,50 [building - 324],"['Decision Support Systems', 'Multi-Objective Decision Making', 'Strategic Planning and Management']",MD-47
"We consider any simple graph with finite vertex set and edge set.
Turán graph is defined as the complete r-partite graph formed by partitioning its vertex set into r subsets, with vertex number as equal as possible, and joining two vertices by an edge if and only if they are located in different subsets.In his paper, Haemers defined the Seidel energy of a graph as the sum of the absolute values of all Seidel eigenvalues of this graph. Heamers noticed that the Seidel energy of any graph is invariant when the Seidel switching and complement of that graph are taken. Tian showed  complete bipartite graph and the empty graph are switching equivalent. Thus Seidel energies of empty graph, complete graph and complete graph with same order are the minimum among all graph of same order. Tian proved that the Seidel energy increases when an edge is deleted in the tripartite Turán graph, and then gave the problem of whether this property would be hold for all Turán graphs.
Liu and Chen, proved the above problem for quinate Turán graphs. We will show that the Seidel energy always increases when an edge is deleted for almost every Turán graphs.
W.H. Haemers, Seidel switching and graph energy, MATCH Commun. Math. Comput. Chem..
G.X. Tian, Y. Li, S.Y. Cui, The change of Seidel energy of tripartite Turán graph due to edge deletion, Linear Multilinear Algebra.
Y.Y. Liu and X. Chen, The change of Seidel energy of quinate Turán graph due to edge deletion. Discrete Applied Mathematics.",The change of Seidel energy of Turán graph due to edge deletion,"[66374, 79202, 79210]",875,"[53, 0]",4121,Optimization issues on graphs I [Contributed],64,14,29,Combinatorial Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,157 [building - 208],['Graphs and Networks'],WC-29
"The paper addresses the Capacitated Vehicle Routing Problem [CVRP] in the context of fuel delivery to gas stations. The CVRP aims to minimize total travel distance for a fleet with limited capacity. Fuel delivery, however, introduces unique complexities within the CVRP framework.
We propose a novel approach that integrates the Simulated Annealing [SA] algorithm with a customized CVRP model specifically designed for gas station networks. This model incorporates real-world constraints like vehicle capacity, fuel demands at each station, and road network distances.
The paper outlines the design of the SA-based CVRP model for fuel delivery. We detail the objective function [minimizing distance] and the SA's exploration mechanism for generating candidate solutions. To assess its effectiveness, the proposed approach undergoes computational tests in Poland's gas station network serviced by the Samat transportation company. We compare the performance of our SA-based CVRP model with the conventional Integer Programming model for CVRP powered by Gourobi. The results aim to demonstrate the efficacy of SA in finding efficient fuel delivery routes, potentially leading to cost reductions and a lower environmental impact for transportation companies.",A Novel CVRP Model with Simulated Annealing for Fuel Delivery - A Case Study for Clients of Samat Transportation Company in Poland,[76686],978,"[143, 5, 65]",4122,Vehicle routing II,6,3,60,Transportation,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S09 [building - 101],"['Transportation', 'Algorithms', 'Logistics']",MB-60
"In this work, we address the Multi-vehicle Arc Routing Problem with Irregular Services, where some arcs and/or edges of a mixed graph must be serviced a certain number of times in some subsets of days of a given time horizon. The objective is to define a set of routes for each day of the horizon so to satisfy the required services at minimum cost. 

We introduce a matheuristic algorithm in which, during an initial phase, services are allocated to individual days within a given horizon. By solving a multi-vehicle arc routing problem for each day, we derive an initial feasible solution. Subsequently, we employ an Iterated Local Search [ILS] approach, wherein new routes are heuristically generated and then combined using a mathematical model to efficiently discover new, high-quality solutions.",An Iterated Local Search Matheuristic approach for the Multi-vehicle Arc Routing Problem with Irregular Services,[71408],206,"[145, 74]",4123,Graph and network optimization,64,9,25,Combinatorial Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,011 [building - 208],"['Vehicle Routing', 'Metaheuristics']",TC-25
"Given the increasing demand for freshness, the need to reduce food waste, and to ensure food safety, it is crucial to consider quality deterioration processes in the [re]design of food supply chains.
Past developments in food supply chains focussed foremost on efficiency and economic profitability, resulting in large-scale, centralized processing facilities. This has become a liability as efficiency in large-scale processing limits product differentiation and flexibility. Decentralised, [upstream] pre-processing, and improved pre-treatments can affect product deterioration in an early stage and will result in more diverse and intermediate product flows that can be used in existing and new value chains and provide new markets.
To identify the benefits of alternative supply chain structures, we propose a generic model based on mixed-integer linear programming. The modelling structures are applicable to a broad variety of different [food] supply chains. Both, network design aspects and gradual quality deterioration processes are incorporated.
We illustrate the applicability of the concept from an existing fruit supply chain in China. The results show that decentralized pre-processing can mitigate quality deterioration processes. In addition, a decentralised supply chain leads to improved sustainability from a social point of view since smallholder farmers in emerging economies can increase their income without affecting the profits of downstream stakeholders.
",Food network design with quality deterioration considerations - an application to a perishable food supply chain,"[79001, 79172, 71700, 10561]",852,"[79, 120, 89]",4124,Food Supply Chains,78,12,13,Secure & Sustainable Food Supply,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,15 [building - 116],"['Network Design', 'Quality Management', 'OR in Agriculture']",WA-13
"We consider convex game with discontinuous payoff functions. A good overview of some key results in the literature on the existence of Nash equilibria in discontinuous games is given in [3].

We will look at the discontinuity in a certain form - we will look at w-continuous functions, which means that the discontinuity measure is not greater than the specified positive number w. Functions of this type can be approximated by continuous functions [[1,2]]. If the approximation is also a concave or quasi-concave function, then a Nash equilibrium exists in the game. This equilibrium can be thought of as a quasi-Nash equilibrium with respect to the discontinuous payoff function of the original game. We would prefer that in the case where a Nash equilibrium exists in the original game, it is also found in a quasi-sense. If no equilibrium exists in the original game, then one would want the quasi-Nash equilibrium to provide the best possible solution. We will look at examples and conditions under which this is possible.

[1] Bula, I. On the stability of Bohl-Brouwer-Schauder theorem. Nonlinear Analysis, Theory, Methods, and Applications, V26, 1859—1868, 1996.
[2] Bula, I. Discontinuous functions in Gale economic model. Mathematical Modelling and Analysis, V.8[2], 93—102, 2003.
[3] Reny, P. J. Nash Equilibrium in Discontinuous Games. Annual Review of Economics, V.12, 439—470, 2020.
",Discontinuous Game and Nash Equilibrium,[78998],645,"[50, 21]",4126,"Game Theory, Solutions and Structures VII",88,9,36,"Game Theory, Solutions and Structures","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,32 [building - 306],"['Game Theory', 'Convex Optimization']",TC-36
"Amid the rapid growth of online retail, last-mile delivery faces significant challenges, including the cost-effective delivery of goods to all customers. Accordingly, the development and improvement of innovative approaches thrive in current research. Our work contributes to this stream by applying dynamic pricing techniques to effectively model the possible involvement of the crowd in fulfilling delivery tasks. The use of occasional drivers [ODs] as a viable, cost-effective alternative to traditional dedicated drivers [DDs] prompts the necessity to focus on the inherent challenge posed by the uncertainty of ODs’ arrival times and willingness to perform deliveries.
We introduce a dynamic programming framework that offers individualized pairs of delivery task and compensation to ODs as they arrive. This model, akin to a reversed form of dynamic pricing, accounts for ODs’ decision-making by treating their acceptance thresholds as a random variable. Thereby, our model addresses the dynamic and stochastic nature of OD availability and decision-making.
We analytically solve the stage-wise optimization problem, outline inherent challenges such as the curses of dimensionality, and present structural properties. Designed to cope with these challenges, our approximation methods aim to accurately determine avoided costs, which are a key factor in calculating optimal compensation.
",A Dynamic Compensation Strategy to Leverage Occasional Drivers in Last-Mile Delivery,"[55930, 74974]",699,"[124, 108, 65]",4128,Pricing and applications,11,10,59,Pricing and Revenue Management,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S08 [building - 101],"['Revenue Management and Pricing', 'Programming, Dynamic', 'Logistics']",TD-59
"Airlines serve different customers. Developing customized offer and pricing strategies have become a strategic priority to airlines and have attracted operations research accordingly. The present paper tests the fundamental hypothesis of the offer management system architecture of Schubert et al. [2021] that aims to combine the simplicity of branded fares with the flexibility of unbundled ancillaries. The hypothesis is that the wealth of data available to airlines enables highly granular and high-dimensional segmentation based on multiple features of a particular customer search.

The hypothesis is tested through inductive research based on 202 million flight coupons of a major network airline between 2018 and 2023. The research observes customer choice probabilities differ depending on weekday of the booking. Next, it identifies a pattern. Customer choice probabilities also differ based on sales channel, flight characteristics, and customer loyalty status. Generalizing these findings, this research concludes with a new hypothesis that airlines can improve prediction accuracy of customer choices and hence display more relevant offers when considering these search features. If true, it presents a low-cost process to increase prediction accuracy with data readily available to airlines. The research is relevant to both airline practitioners and researchers for follow-up studies alike.
",Customized airline offer management - inductive research as proof-of-concept of high-dimensional segmentation,[69680],700,"[124, 143, 47]",4131,Pricing and applications 2,11,12,59,Pricing and Revenue Management,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S08 [building - 101],"['Revenue Management and Pricing', 'Transportation', 'Forecasting']",WA-59
"The p-Laplacian is a non-linear generalization of the Laplace operator. In the graph context, its eigenfunctions are used for data clustering, spectral graph theory, dimensionality reduction and others problems, as non-linearity better captures the underlying geometry of the data.
We formulate the p-Laplacian nonlinear eigenproblem as an optimization problem under p-orthogonality constraints. The problem of computing multiple eigenvectors of the graph p-Laplacian is then solved by iteratively minimizing the constrained generalized Rayleigh quotient. 
We analyze two different optimization algorithms to solve the variational problem. The first is a manifold gradient descent which solves a smooth unconstrained minimization over the p-orthogonality subspace. The second is an Alternate Direction Method of Multipliers which leverages the scaling invariance of Rayleigh quotient to solve a constrained minimization under a p-orthogonality constraint.
We demonstrate the effectiveness and accuracy of the proposed algorithms and compare them in terms of efficiency.",A Variational Model for graph p-Laplacian eigenfunctions under p-orthogonality constraints,"[78977, 79179, 79180]",133,"[63, 19, 113]",4132,"Nonsmooth optimization and applications, Part I",84,7,32,Advances in large scale nonlinear optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,41 [building - 303A],"['Large Scale Optimization', 'Continuous Optimization', 'Programming, Nonlinear']",TA-32
"This study introduces a link-based traffic assignment and equilibrium model for multi-modal networks considering congestion effects on travel times. To seamlessly enable the mode switch at any step of the journey, we employ an intervention control modelling technique that utilizes a multi-layer network wherein each layer corresponds to a feasible mode choice. The approach also enables us to prescribe node-dependent mode switching costs. The solution technique relies on dynamic programming and allows us to implicitly consider all possible route and mode choice combinations without enumerating all of them explicitly. This model can be used to design, analyze, and operate transportation systems that include multiple modes such as a variety of sustainable options.", A link-based traffic equilibrium model for multi-modal network,"[79178, 79112, 43447]",151,"[143, 135]",4133,Transportation Network Modelling and Optimization I,6,2,55,Transportation,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S02 [building - 101],"['Transportation', 'Stochastic Models']",MA-55
"Within the Problem Structuring Methods literature, it is widely held that undertaking a Causal Mapping session on the ultimate goals of a set or prioritized actions contributes to a greater likelihood of achieving those goals. An alternative approach to understanding the nature of this link between group goals and individual actions undertaken in the pursuit of these group goals is offered by drawing upon the psychological concepts of “Indispensable” and “Identifiable” activities. These highlight the difference between self-serving and group-serving activities in a group environment. Participants of group assignments from an MBA program were surveyed during and after the production and submission of a group assignment, with some participating in collective agreement group activities before the commencement of the assignments.",Indispensable and Identifiable activities and the link between group goals and individual goals following Causal Mapping Interventions.,"[10269, 79183]",130,"[149, 92, 55]",4134,Methodological Developments in Soft OR and PSMs,26,5,13,Soft OR and Problem Structuring Methods,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,15 [building - 116],"['Problem Structuring', 'OR in Education', 'Group Decision Making and Negotiation']",MD-13
"The deviaton matrix is a measure of the cumulative deviation from the limiting probabilities and plays an important role in e.g. Markov [decision] processes, perturbed Markov processes, queueing theory,
and network centralities. It is well-known that the deviation matrix in the irreducible case exists if and only if the second moment of the return time from a fixed state to iself is finite.
We provide a necessary and sufficient condition for the deviation matrix to exist in the form of a system of two nested Lyapunov functions, the equivalent of which has been known for a long time for discrete time Markov chains.
However, it has not been realised earlier that finiteness of the deviation matrix implies that the absolute cumulative deviation from the limiting probabilities is finite as well. There is a relatively simple [but not simply proved] argument why this is true.
We further present a formula for the deviation matrix in terms of a rank one perturbation of the q-matrix. 
We will illustrate our results with some applications", On the Deviation matrix of an ergodic denumerable Markov process in continuous time  with applications,[31064],502,"[135, 108]",4135,Stochastic Modelling,47,5,40,Advances in Stochastic Modelling and Learning Methods,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,96 [building - 306],"['Stochastic Models', 'Programming, Dynamic']",MD-40
"Modelling, quantitative or qualitative, is part of many study programs. However, transferring trained methodology to application may depend on skills to communicate models in a multi-perspective context.
Based on multi-perspectivity in technology [Ropohl 2009], the study investigates how system mapping with causal loop diagrams supports a multi-perspective view on the energy transition. The study was conducted with five study groups and analysed builiding a typology based on qualitative content analysis [Kuckartz 2018].  
Five types were identified along the typology's axes modelling preconceptions and linkage of perspectives - The two central types group participants, who, despite lower modelling experience, connected several perspectives using the mapping methodology. Communicating the models to peers enhanced their learning. 
The explorative type transcended perspectives but did not follow the mapping approach closely, while the trained methodologist linked qualitative to quantitative modelling, remaining in a preferred perspective. The multi-learner was able to holistically integrate modelling approaches and perspectives. 
The findings suggest that short sessions combining soft model building techniques with phases of interaction may be suitable to develop a common-sense understanding for complex questions e.g. concerning sustainability transitions. Possible applications for interdisciplinary stakeholder dialogues are presented.
",System maps to foster a multi-perspective view on the energy transition,[78757],716,"[133, 37, 92]",4137,Supporting Planning and Sustainability,26,2,13,Soft OR and Problem Structuring Methods,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,15 [building - 116],"['Soft OR', 'Energy Policy and Planning', 'OR in Education']",MA-13
"Network interdiction models have been utilized to analyze a wide variety of real-life problems such as military operations, illegal drug trafficking, increasing the preparedness of infrastructure against natural disasters or terrorist attacks, social networking, and preventing the spread of epidemics. This study explores a novel shortest-path interdiction problem variant with a single defense node against multiple adversarial nodes. The defense has limited resources to mitigate the threat posed by the adversary. By interdicting nodes and all connected arcs from those nodes, the defense aims to minimize the threat. The length of arcs connected to the interdicted node will increase inversely with the change in weight of the interdicted node. In this talk, the proposed mathematical model, the solution approach, and computational results will be presented.",Resource-constrained network interdiction on nodes to mitigate threats,"[51350, 79189]",875,"[150, 14, 72]",4138,Optimization issues on graphs I [Contributed],64,14,29,Combinatorial Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,157 [building - 208],"['Network Flows', 'Combinatorial Optimization', 'Mathematical Programming']",WC-29
"In recent years there has been a renewed interest in ‘systems-based’, ‘systemic’, and ‘analytics-driven’ evaluation of public policy. This interest has been supported with contributions from systems, policy, and evaluation sciences communities [see Reynolds et al. 2016; OECD 2019; Blomkamp 2022; GO-Science 2022]. Often the dissemination and learning from these contributions remain largely focused within these disciplines. I argue the resulting disconnect between related work challenges OR researchers and practitioners in identifying where they can meaningfully contribute to address current needs. 

This paper aims to support OR contributions to future innovations in public policy evaluation by presenting a review of the current state of knowledge and practice in three parts. Three questions are used to explore OR’s relationship to policy evaluation:
 
1.	What makes an evaluation ‘systemic’?
2.	What challenges are currently faced in systemic policy evaluation?
3.	How is and can the field of OR contribute to resolving these challenges?

",Challenges for systemic public policy evaluation,[42228],718,"[101, 133, 151]",4141,OR Innovations in Policy Making - B,26,7,13,Soft OR and Problem Structuring Methods,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,15 [building - 116],"['OR/MS and the Public Sector', 'Soft OR', 'Practice of OR']",TA-13
"We introduce an open-source GitHub repository containing comprehensive benchmarks for a wide range of machine scheduling problems, including Job Shop Scheduling [JSP], Flow Shop Scheduling
[FSP], Flexible Job Shop Scheduling [FJSP], FJSP with Assembly constraints [FAJSP], FJSP with Sequence-Dependent Setup Times [FJSPSDST], and the online FJSP [with online job arrivals]. Our primary goal is to provide a centralized hub for researchers, practitioners, and
enthusiasts interested in tackling machine scheduling challenges.",Job Shop Scheduling Benchmark - Environments and Instances for Learning and Non-learning Methods,"[51230, 79175, 78889, 79190]",732,"[14, 8, 69]",4142,	[Deep] Reinforcement Learning for Combinatorial Optimization 3,14,7,03,Data Science Meets Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,1005 [building - 202],"['Combinatorial Optimization', 'Artificial Intelligence', 'Manufacturing']",TA-03
"There exists wide range of software products to support decision making. However, the main disadvantage of these products is their commercial nature, which makes them relatively expensive and inaccessible to small companies, students, and researchers. Addressing this gap, we introduce a novel desktop tool called Multicriteria Decision Assistant [MDA]. MDA emerges as an evolution of a previous software, the FDA, enhancing user experience by eliminating the need for an internet connection, thus addressing privacy and data security concerns by preventing sensitive data from being sent to external servers. This feature significantly increases the tool's appeal to users wary of data privacy issues.
What sets the tool apart is not just its cost-free accessibility but its educational potential. Unlike many decision support tools that operate as black boxes, MDA makes the decision-making process transparent, displaying the results of intermediate calculations. This transparency is invaluable for educational purposes, allowing students to grasp the underlying principles of multicriteria decision-making and encouraging a deeper understanding of the decision process itself. The proposed software package is demonstrated on an illustrating example of a real-life decision problem.",A Desktop Tool for Multicriteria Decision Analysis,[9368],893,"[26, 134, 6]",4144,Pairwise comparisons and preference relations 3,44,12,44,Multiple Criteria Decision Analysis,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,20 [building - 324],"['Decision Support Systems', 'Software', 'Analytic Hierarchy Process']",WA-44
"The remarkable success of sharing economy platforms in the B2C sector has led to increased attention from both practitioners and researchers seeking to apply these models to the B2B sector. Studies have already demonstrated that combined with cloud manufacturing and additive manufacturing, these concepts can considerably reduce costs while the responsiveness supply chains is increased. However, there is a need to design resource allocation mechanisms such that desirable properties are to be achieved. Promising approaches for such an environment are decentralized combinatorial auction frameworks that can be used to exchange jobs to decrease production costs of operations efficiently and effectively. In our approach, machines autonomously select jobs from an existing production plan to forward them to other suppliers that can produce these parts for lower costs. An auctioneer creates promising part bundles and manufacturing machines autonomously place bids on the packages via a combinatorial second price reverse auction. Costs of the reallocated bundles are shared throughout a Shapley value-based approach without the need to disclose critical information. We provide analytical and numerical insights on this approach and show if and how desirable properties such as budget balance, incentive compatibility, and individual rationality can be achieved. ",Mechanism design for job reassignments in collaborative manufacturing,"[39372, 69225]",810,"[69, 14, 9]",4145,Lot-sizing with game theory aspects,32,9,49,"Lot Sizing, Lot Scheduling and Production Planning","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,M1 [building - 101],"['Manufacturing', 'Combinatorial Optimization', 'Auctions / Competitive Bidding']",TC-49
"Our study is going to address a lost-sales inventory management problem spanning multiple echelons and products, aiming to maximize network profit while determining optimal quantities from upstream suppliers. We utilize deep reinforcement learning [DRL] to optimize inventory management in these complex supply networks.

We begin by constructing a complex inventory management model accommodating lost sales and intricate cost structures. Employing DRL, we develop a framework to tackle this problem and evaluate its efficacy.

Our investigation extends to various scenarios, encompassing changes in lead time, fixed and sourcing costs, holding expenses, lost sales penalties, and demand variations. Therefore, the overarching research question we will attempt to address is - Does deep reinforcement learning produce superior policies for the multi-echelon, multi-product, lost-sales inventory management problem, in comparison with standard reinforcement learning?
","Optimizing a Multi-echelon, Multi-Product, Lost-Sales Inventory Management System through Deep Reinforcement Learning","[75632, 18872]",732,"[8, 66, 138]",4147,	[Deep] Reinforcement Learning for Combinatorial Optimization 3,14,7,03,Data Science Meets Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,1005 [building - 202],"['Artificial Intelligence', 'Machine Learning', 'Supply Chain Management']",TA-03
"Product line design is one of the key problems that firms need to solve. Therefore, they employ techniques to predict customer choice behaviour and optimize the performance of the product assortment they aim to offer.In most cases, the prediction problem and the optimization problem are defined as separate problems and solved in a sequential manner where first, the choice model is estimated and second, the assortment optimization problem is solved, using the choice model parameters as an input. Integrating estimation and optimization  provides an opportunity for the empirical model to be more accurate where it matters – close to the optimal assortment. We develop a MILP formulation that is able to solve the two problems jointly. Using numerical experiments, we analyse under what conditions and to which extent the integrated approach is superior to the sequential approach. 

",Integrating Predictive and Prescriptive Analytics for Assortment Optimization - a Machine Learning-based Approach,"[75077, 79193, 52290]",414,"[124, 0]",4148,Pricing and learning 2,11,4,59,Pricing and Revenue Management,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,S08 [building - 101],['Revenue Management and Pricing'],MC-59
" With the advent of batteries and P2X technologies, energy producers can also participate as consumers on energy markets, which increases the flexibility of such prosumers. We consider a single time-step market equilibrium where a large player is an integrated producer and consumer under capacity constraints. The capacity constraints are relevant for emerging consumer-side technologies, such as limited electrolyzer capacity. We use conjectural variations to model different levels of market power, and we solve the model analytically. Under market power, we compare the equilibrium solution with the one for a separated producer and consumer. We calculate corresponding production and consumption equilibrium amounts, prices, and social welfare. The main finding is that the social welfare change of vertical integration is ambiguous and depends on the parameters, for example the limited capacity of the consumption technology.
Using the same analytical method, we consider also a symmetric two time-step duopoly of capaciated storage and production under market power. Also in this case, we show how the equilibrium solution in terms of prices and storage amounts depends on parameters, for example the intensity of market power measured by the conjenctural variation parameter. The findings may be used in energy-policy support to identify different cases for de-bundling energy technology assets. ",Large prosumers under capacity constraints - Market power effects of vertical integration and of storage,"[29367, 75192, 68618]",448,"[36, 37, 50]",4149,Decentralized multi-energy markets,22,7,09,Energy Markets,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,10 [building - 116],"['Electricity Markets', 'Energy Policy and Planning', 'Game Theory']",TA-09
"This paper addresses the integrated planning problem encompassing olive harvest and production within the olive oil industry. The aim is to optimize a mathematical model that seamlessly integrates both olive harvest and olive oil production processes. The primary goal is to maximize overall profit by determining the quantities of olives harvested from various groves, olives procured from external farmers, olive oil produced, and managing by-products to mitigate the adverse impacts of olive oil production. The problem is formulated as a mixed integer linear programming model [MILP]. Profit maximization comprises total sales revenue and total costs, covering harvesting, procurement, fixed and variable processing expenses. System constraints include harvest and production planning, capacity limitations, and processing restrictions. The proposed MILP model encompasses several distinctive aspects of the problem, such as olive ripeness, olive oil quality, farming methods [organic and conventional]. Backup facility assignment and coverage distance resilience strategies are incorporated. The effectiveness of the developed model is validated through a numerical experiment based on a real-world case study, indicating that simultaneous consideration of harvesting and production processes significantly enhances the profitability of the olive oil supply chain.",Resilient Olive-Oil Supply Chain Network Design ,[52589],590,"[89, 72, 111]",4150,OR in Agriculture,20,5,12,OR in Agriculture and Forestry ,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,13 [building - 116],"['OR in Agriculture', 'Mathematical Programming', 'Programming, Mixed-Integer']",MD-12
"We study the classical Frobenius problem, which seeks to find the largest integer that cannot be represented as a nonnegative integral combination of given relatively prime positive integers, termed the Frobenius number. The Frobenius problem is known by other names within the literature such as the money-changing problem, the coin problem, the chicken McNugget problem and the Diophantine problem of Frobenius.

The Frobenius number is an important quantity in operational research where, from a geometric viewpoint, it is the maximal right-hand side such that the corresponding knapsack problem is integer infeasible. Further, given that the knapsack problem is NP-hard, there is a rich history on this problem. Note that computing the Frobenius number in general is NP-hard and, as such, there has been a much research into producing upper bounds on the Frobenius number.

The main contributions are observations regarding a previously known upper bound. In particular, we observe that a previously presented argument features a subtle error that alters the value of the bound. Despite this, we show that the error does not impact upon on the bound’s validity, although it does impact on its tightness. We compare the relative tightness of the corrected upper bound with the original. In particular, we show that the updated bound is tighter in all but only a relatively “small” number of cases using both formal and simulation-based techniques. This is joint work with Daiki Haijima.",The McNugget-Knapsack Connection - Refining Upper Bounds on the Frobenius Number with Operational Research Applications,"[79182, 79195]",880,"[109, 110, 14]",4151,Exact methods in combinatorial optimization [Contributed],64,9,52,Combinatorial Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,8003 [building - 202],"['Programming, Integer', 'Programming, Linear', 'Combinatorial Optimization']",TC-52
"In this communication, we aim to present the solving of an MBDA's use case related to
optimal assignment, onto IBM online QPUs. The Quantum Approximate Optimization Algorithm
[QAOA] [Farhi et al. 2014] is the base of our Variational Quantum Algorithm developed.

We compare two methods to account for constraints, first primarily by integrating them
into the Cost Hamiltonian with Lagrangian multipliers and second, by adapting the Mixer
Hamiltonian according to [Wang et al. 2022] and [Fuchs et al. 2022].  For the former,
determining the optimal Lagrangian multipliers is generally a challenging task and the
integration of constraints into the Cost Hamiltonian can significantly increase the
associated circuit depth. The latter method aims to reduce the overall Hilbert space to
only feasible solutions, which lets get rid of Lagrangian multipliers but may
significantly enlarge the circuit associated to the Mixer Hamiltonian and make the
initial state harder.  It is then interesting to compare the circuit depth of both
methods with respect to how well they are able to statistically put forward optimal
solutions against non-optimal and non-feasible ones, still for relatively small sized
instances, to fit on current QPUs.",Solving an MBDA's use case related to optimal assignment on current IBM Quantum Computers,"[78886, 79204, 79208, 79207, 79206]",374,"[14, 109, 114]",4152,Hybrid Classical-Quantum Algorithms,83,2,42,Quantum Computing Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,98 [building - 306],"['Combinatorial Optimization', 'Programming, Integer', 'Programming, Quadratic']",MA-42
"Several countries have adopted awards as a stimulus and recognition for companies that stand out in the management of the quality of products and services. Stakeholder involvement must be infiltrated into the values and mission of companies to reinforce, mainly, corporate governance, providing transparency and accountability to the developed businesses. The evaluation of an organization's performance should be related to the multiple interests pursued by the corporation, including the expectations achieved by its stakeholders. This work shows how to evaluate the companies that apply for an international quality award, based on the management excellence model presented, using the Preference Selection Index Method [PSI] to find the weighting of the criteria and the Combined Compromise Solution [COCOSO] Method for the ordering of the alternatives. The criteria used to evaluate companies are Research and Development [R&D], People, Projects, Leadership, Processes, Information and Knowledge, Operations, and Sustainability in Business according to the stage scale created to represent the evolution of each company's criteria from the perspective of the internal environment. Company D is the winner of the Management Excellence Award. The result is quite coherent, as it is the only company that already presents 2 [two] criteria in the higher stage, that is, the R&D and Leadership criteria have already reached the stage of Criterion Implemented with Excellence [grade 5].",Decision-making for the International Management Excellence Award using  the PSI-COCOSO Hybrid Multicriteria Method ,"[78724, 79274, 67216, 201, 68928, 79227]",907,"[26, 120, 100]",4154,"MCDA and Composite Indicators - Issues, Advances and Applications 2",44,15,44,Multiple Criteria Decision Analysis,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,20 [building - 324],"['Decision Support Systems', 'Quality Management', 'OR in Sustainability']",WD-44
"The aggravation of agricultural land misuse, alongside climate change and population growth, intensifies the pressure on agriculture to fulfill food demand. To address this challenge, urgent action is needed to implement sustainable land and forest management practices, expand cultivation areas, and enforce effective agricultural policies. Agriculture has historically been pivotal in sustaining livelihoods and economies, necessitating optimization in production planning and productivity to unlock its full potential.

There is limited research on agricultural production planning, particularly in Turkey.
In this study, the focus is on conducting a 2-3 year simulation study to maximize revenue from agricultural products in a specific region. Using mixed integer programming, the study designs crop patterns for selected parcels and plans production accordingly. The GAMS software is utilized to solve the model. 

In conclusion, addressing agricultural challenges through optimization models and simulation studies is crucial for fostering sustainable and efficient agriculture. This study contributes to optimizing agricultural production in Eskisehir, Turkey, underscoring the importance of customized solutions in agricultural planning and management.",A Simulation Study for Agricultural Production Optimization,"[3550, 20847]",676,"[129, 111, 131]",4155,DSS in Agriculture,20,9,12,OR in Agriculture and Forestry ,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,13 [building - 116],"['Scheduling', 'Programming, Mixed-Integer', 'Simulation']",TC-12
"In this paper, we consider the network interdiction problem using the network flow as the performance measure. An evader wishes to maximize the expected flow of some illegal commodities such as drugs without being detected. On the other hand, a defender aims to minimize the objective of the evader by interdiction activities, such as adding some security controls, that can alter the probability measures.
In this study, flows are intercepted stochastically at each arc, and the expected flow is calculated by considering the penalty cost when the network flow is detected. We need to determine which path from the source to the sink in the network can most effectively maximize the expected flow considering the detection probability, and to determine which arc is the best to interdict. To minimize flow, the defender increases the detection probabilities of arc with a cost by interdiction activities. Under the total interdiction cost constraint, whether each arc is to be interdicted or not is the decision to be made. This paper proposes an efficient search method for optimal interdicted arcs in such situations.",Exploring Strategy for Stochastic Maximum Flow Network Interdiction Problem,"[59209, 1101]",151,"[150, 117, 75]",4156,Transportation Network Modelling and Optimization I,6,2,55,Transportation,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S02 [building - 101],"['Network Flows', 'Programming, Stochastic', 'Military Operations Research']",MA-55
"The practical traffic optimization model  and the development of scenarios are considered to choose the most advantageous variant for the parking lot areas organization in the car factory plant. Traffic should run smoothly, i.e. without stoppages at the entrance/exit to the plant and without traffic jams in the plant zone, taking into account the effective flow of vehicles including the scheduling model in the multi-storey car parking. The micro logistic simulation model was developed, what if analysis and computational experiments performed.",Optimization of vehicle traffic in parking lots and access roads around the car plant,[4224],227,"[84, 129, 131]",4157,Combinatorial Optimization in Scheduling,64,13,26,Combinatorial Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,012 [building - 208],"['Optimization Modeling', 'Scheduling', 'Simulation']",WB-26
"In today's rapidly changing world, organizations are confronted with an environment marked by constant shifts and uncertainty, driven by factors like rapid technological progress and geopolitical instability. In such circumstances, the ability to adapt and thrive becomes crucial for an organization's survival. While some organizations display impressive resilience in the face of challenges, others struggle to cope with disruptions. Organizations that can swiftly respond to emerging challenges and seize new opportunities are often better equipped to navigate turbulent times successfully. However, building resilience is not just about reacting to external pressures; it also involves proactively shaping organizational structures and processes to enhance adaptability.

This study seeks to shed light on the mechanisms that drive organizational resilience, with a particular focus on key design elements such as decision-making modes and incentive mechanisms. Using agent-based modeling and simulation, the study aims to explore how these organizational design factors interact and influence resilience. Preliminary findings suggest that these design factors significantly impact an organization's resilience, with the effects being moderated by the complexity of tasks. By exploring these dynamics, this research contributes to our understanding of organizational resilience and provides insights that can inform strategies for mitigating vulnerability.
",Designing Organizations for Resilience - Decision-Making Modes and Incentive Mechanisms,[26117],851,"[131, 68, 15]",4158,Simulation of organizations II,77,3,43,"Agent-based Models in Management, Economic and Organisation Sciences","Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,99 [building - 306],"['Simulation', 'Managerial Accounting', 'Complex Societal Problems']",MB-43
"We extend the convergence analysis of the Scholtes-type regularization method for cardinality-constrained optimization problems. Its behavior is clarified in the vicinity of saddle points, and not just of minimizers as it has been done in the literature before. This becomes possible by using as an intermediate step the recently introduced regularized continuous reformulation of a cardinality-constrained optimization problem. We show that the Scholtes-type regularization method is well-defined locally
around a nondegenerate T-stationary point of this regularized continuous reformulation. Moreover,
the nondegenerate Karush-Kuhn-Tucker points of the corresponding Scholtes-type regularization converge to a T-stationary point having the same index, i.e. its topological type persists. As consequence, we conclude that the global structure of the Scholtes-type regularization essentially coincides with that of CCOP.",Extended convergence analysis of the Scholtes-type regularization for cardinality-constrained optimization problems,"[66744, 20607]",527,"[52, 81, 72]",4159,Nonsmooth optimization algorithms I,70,14,41,Nonsmooth Optimization,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,97 [building - 306],"['Global Optimization', 'Non-smooth Optimization', 'Mathematical Programming']",WC-41
"Given the diversity and rapid pace of innovation in the medical device [MD] industry, there is an increasing need for Health Technology Assessment [HTA] agencies to prioritise which MDs to evaluate. The Portuguese HTA agency, INFARMED, National Authority of Medicines and Health Products, I.P. –, intends to develop a socio-technical framework to address such a challenge. Taking a step towards this goal, the present study develops an approach for structuring a multiple criteria decision analysis [MCDA] model to measure the value of MD assessments. More concretely, it arrives at a robust value tree that represents the relevant value dimensions to consider when prioritising MDs for HTA evaluation at INFARMED. The proposed methodological approach follows a top-down [”value-focused thinking”] approach, entailing a systematic literature review and social methods, such as a web survey and a workshop. Causal mapping, as a powerful visual tool capable of supporting structuring tasks, was applied as an intermediary method to build the final value tree. The resulting tree is structured into three levels, comprising criteria and attributes within two domains - disease-related and device-related dimensions. Looking forward to operationalising this structure in the future, selected MCDA techniques are discussed. All in all, the present study provides a sound operational basis for the framework sought by INFARMED, with potential relevance for future studies on HTA priority-setting. ",Selection and prioritisation of medical devices for HTA evaluation - Towards a socio-technical framework based on multiple criteria decision analysis,"[63390, 79197, 79356, 79353]",968,"[56, 133, 149]",4161,Decision support in healthcare,3,2,17,OR in Health Services [ORAHS],"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,40 [building - 116],"['Health Care', 'Soft OR', 'Problem Structuring']",MA-17
"Maximal Coverage Location Problem [MCLP] attempts to find a predetermined number of facilities to maximize the number of demand points that can be covered. In MCLP, while all demand points within a critical distance of a facility are completely covered, demand points outside this region are not covered at all. In Partial MCLP [MCLP-P], another critical distance is introduced, which allows coverage between two critical distances, monotonically decreasing with respect to demand points’ distance from facilities. In this study, we explore MCLP-P under coverage uncertainty. We utilize a robust optimization framework and introduce an approach to hedge against uncertainty. We present the model and the solution approaches and compare the performance of the proposed solution approaches on randomly generated datasets.",A robust maximal covering location model considering partial coverage,"[51317, 79198, 51350]",765,"[64, 14, 72]",4162,Covering Location Problems,29,7,61,Locational Analysis,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S10 [building - 101],"['Location', 'Combinatorial Optimization', 'Mathematical Programming']",TA-61
"An edge coloring of a graph G is an assignment of colors to the edges so that two incident edges receive distinct colors. 
In particular, one is often interested in determining an actual edge coloring with the minimum number of colors so to explicitly assign tasks to machines over time.
In the general case, this problem is known to be NP-Hard, however, if G is bipartite, one can solve this problem efficiently, through constructive algorithms.
One drawback of this algorithmic'' approach is that it often involves sophisticated implementation. Moreover, finding edge coloring that minimize a generic objective function on bipartite graph is known to be NP-Hard.

The edge coloring Polytope of a graph G is the convex hull of all the possible edge coloring of G. 
In this work, we prove that a natural formulation of the edge coloring polytope on forests is polynomial in size and is described by a totally unimodular matrix. The interest towards this result is twofold. On one hand, it allows to use linear programming techniques to solve edge-coloring problems. On the other, it gives a polynomiality proof for all edge coloring problems on forests.
Our results give an alternative proof to different known results such as the max-edge coloring problem on forests and the minimum-sum edge coloring on forests and multicycles.",Polynomial-size Formulation for Edge Coloring Problems on Forests,"[74246, 72388]",865,"[14, 53]",4163,Topics in Integer Programming I,64,12,25,Combinatorial Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,011 [building - 208],"['Combinatorial Optimization', 'Graphs and Networks']",WA-25
"Unsolicited in-kind donations, such as textiles, pose significant challenges to humanitarian organisations [HOs] due to the uncertainty of donor supply in terms of quantity and quality of items. To enable flexible operations, HOs deploy donation containers in urban areas. These are processed and the items which they collect are sold as second-hand textiles in retail outlets. However, labour-intensive processing, sorting and waste make it difficult for HOs to sell all donations through retail outlets. Instead, a portion is exported in bulk without sorting to reduce costs. This paper presents a simulation model to understand the processes behind unsolicited textile donations. The results show that the critical process factors are the interplay between local demand for used clothing, the holding costs of storage, and the proportion of waste in donations. The challenge is to avoid shortages in emergency situations without incurring additional storage or disposal costs. In this context, the paper uses an allocation problem to prioritise the sorting of donation containers. This would be facilitated by a platform that processes donor information before the items are dumped in the containers. Understanding sorting processes and donor behaviour is essential to optimise the impact of unsolicited donations on HOs. This study derives actionable implications for such operations and the critical role of donor education in reducing wasteful donations. It also highlights the need for policy ",Impact of Information Availability in Processing Unsolicited Donations,"[67777, 17105]",928,"[100, 110, 131]",4165,Information sharing in sustainable supply chains,18,15,23,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,82 [building - 116],"['OR in Sustainability', 'Programming, Linear', 'Simulation']",WD-23
"Project evaluation derives from operational research method and tools for formalizing decision-making processes. Among these, the Appraisal and valuation discipline privileges the ones aimed at measuring the added value, an aspect of the progress of social communities that implies considerations of distributive justice.
Although not universally endorsed, the appeal to the civic engagement of evaluation is increasingly compelling since the succession of economic crises and environmental fluctuations enlarges the frontline of countering the distress of territories and communities.
This paper addresses the redevelopment processes of historic urban fabric by proposing a “scenario-generating model” based on a structured and limited set of constraints [rules] able to outline a potentially unlimited number of consistent layouts. 
On the basis of two different experiences carried out with reference to a building-urban fabric in Italy, some critical reflections are proposed on - 1. the grammatical framework of this system, which bases its internal coherence on the morphological, semantic, and syntactic levels; 2. the projection of the ethics of constraint from the particular dimension of what is owed to things and people, to the general dimension of what is owed to earth and communities.
",The grammar of constraints for a things-based justice. Urban text[ure]s and the ulteriority of the local identities,"[46421, 46428, 67985, 72267, 72264]",891,"[12, 41, 107]",4167,MCDA and urban planning 3,44,10,47,Multiple Criteria Decision Analysis,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,50 [building - 324],"['Capacity Planning', 'Ethics', 'Programming, Constraint']",TD-47
"In this study, we work on a clustering problem where it is assumed that the features identifying the clusters may differ for each cluster. Number of clusters and number of relevant features in each cluster are given in advance. A center-based clustering approach is proposed. Finding the cluster centers, assigning the data points and selecting relevant features for each cluster are performed simultaneously. A non-linear mixed integer mathematical model is proposed which minimizes the total distance between data points and their cluster center by using the selected features for each cluster. Different linearization methods have been used for solving the problem. 
Besides, two different heuristic algorithms have been developed by taking into account the nature of the mentioned problem. Experimental results have been presented. 
",Mixed Integer Programming and Heuristics Approaches for Clustering with Local Feature Selection,[25550],730,"[66, 72]",4169,Optimization and Machine Learning - Methodological Advances,14,9,03,Data Science Meets Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,1005 [building - 202],"['Machine Learning', 'Mathematical Programming']",TC-03
"The most economical measure for tackling public health emergencies is mass immunization. During crises, policymakers heavily rely on global human vaccine product trade as it profoundly shapes the development of additional intervention measures. Drawing on global human vaccine product trade data from 2019 to 2022, this study employs social network analysis methods to explore the structure of the global human vaccine product trade network from various network statistical features such as community structure and evolution. In addition, through a comparative examination of global monthly trade networks for human vaccines, this study reveals the evolving structural dynamics amidst the global public health crisis [COVID-19], offering insights into how it affects national intervention strategies and vaccine market stability during pandemics. Overall, the global human vaccine product trade network has experienced an increase in its scale with an extended multipolar structure. Additionally, the alterations in network structure revealed by trade volume and frequency can also demonstrate competitive dynamics and behaviors between countries, and highlight the shifts in vaccine strategies among countries during public health emergencies. High-income countries with rapid vaccine rollout and introduction [such as the United States] exhibit stronger resilience by actively adopting comprehensive intervention measures.",Understanding the structure and dynamics of the global vaccine product trade network.,"[76042, 79225, 50319]",672,"[7, 53, 30]",4171,"Advancements of OR-analytics in statistics, machine learning and data science 18",16,14,06,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,1013 [building - 202],"['Analytics and Data Science', 'Graphs and Networks', 'Disaster and Crisis Management']",WC-06
"In this talk, we present an outer approximation algorithm for computing the Edgeworth–Pareto hull of multi-objective mixed-integer linear programming problems [MOMILPs]. It produces the extreme points [i.e., the vertices] as well as the facets of the Edgeworth–Pareto hull. The algorithm relies on a novel oracle that solves single-objective weighted-sum problems and we show that the required number of oracle calls is polynomial in the number of facets of the convex hull of the extreme supported non-dominated points in the case of MOMILPs. Moreover, for the special case of multi-objective linear programming problems, by generating the Edgeworth–Pareto hull the algorithm solves the problem to global optimality. A computational study on a set of benchmark instances from the literature is provided.",On computing the Edgeworth–Pareto hull of multi-objective mixed-integer linear programming problems via an outer approximation algorithm,"[36277, 45890, 20539, 19001]",584,"[112, 72, 109]",4173,Multiobjective Mixed-Integer Linear Optimization,34,15,37,Multiobjective Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,33 [building - 306],"['Programming, Multi-Objective', 'Mathematical Programming', 'Programming, Integer']",WD-37
"Inventory Routing Problem [IRP] arises from vendor-managed
inventory business settings where the supplier is responsible for
replenishing the inventories of its customers over a planning
horizon. In the IRP, the supplier makes the routing and inventory
decisions together to improve the overall performance of the
system. More specifically, the supplier decides [i] when to
replenish each customer, [ii] how much to deliver to each
customer, and [iii] how to route delivery vehicles between the
depot and the customers. In our setting, the supplier's goal is to
minimize total transportation cost over a planning horizon while
avoiding stock-outs at the customer locations. We assume that the
supplier has a fleet of homogeneous capacitated delivery vehicles
and abundant availability of the product to be delivered to the
customers. Each customer has a constant demand/consumption rate
and limited storage capacity to keep inventory. In order to
address this problem, we propose a novel integrated clustering and
routing algorithm. In the clustering phase, we partition the
customer set into clusters such that a single vehicle serves each
cluster. In the routing phase, we develop the delivery schedule
for each cluster. The novelty of the proposed approach is that it
takes the three main decisions [when to deliver, how much to
deliver and how to route] into account when partitioning the
customer set into clusters.",Inventory Routing Problem - A New Integrated Clustering and Routing Algorithm,"[23502, 41315]",978,"[143, 145, 109]",4174,Vehicle routing II,6,3,60,Transportation,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S09 [building - 101],"['Transportation', 'Vehicle Routing', 'Programming, Integer']",MB-60
"Energy arbitrage is a common practice in the European energy sector, which consists of taking advantage of differences in energy prices in the market. Companies use arbitrage to buy energy at a certain price, store it in batteries and later sell it at a higher price. Based on the same idea, hydrogen production can be considered renewable if the electricity supply mix contains a minimum amount of renewables. This work models the costs of an energy supply chain, identifying the best moments of energy purchase and sale over a time period, to control an industrial plan producing green hydrogen, taking into account a renewable share. Optimal conditions to operate the plant are unknown, as demand and production are uncertain variables at the moment to participate in the market. Our developed solution evolves from a deterministic model to an improvement with the introduction of stochasticity. The model is tested in realistic conditions with a custom package capable of solving the optimization problem for all the study cases. For a time horizon of 24 hours, the model is solved in less than half a second using state of the art solvers in an HPC cluster. To incorporate uncertainty, the variability of the energy price of the electrical grid is modeled by generating random scenarios. This is achieved by predicting time series using both traditional statistical methods and neural networks. It allows to reliably represent the variability of the operation over the energy market.",Dealing with stochasticity in the renewable hydrogen production,"[60332, 79216, 79215, 79214, 79212, 79213]",234,"[36, 7, 93]",4175,Production Optimization and Supply Chain Management of Green Hydrogen under Uncertainties,22,13,09,Energy Markets,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,10 [building - 116],"['Electricity Markets', 'Analytics and Data Science', 'OR in Energy']",WB-09
"In this paper, we consider a capacitated economic lot-sizing problem where companies determine the optimal joint order plan in order to meet their demands in the most cost effective manner for a given planning horizon. After determining the minimum cost solution of the collaboration, the resulting joint-cost is to be allocated among the collaborators appropriately in order to sustain the collaboration. We first prove that the core of the corresponding cooperative game may be empty. Next, we establish the necessary and sufficient conditions for this game to have a non-empty core and show that when these conditions are satisfied, we can obtain a core allocation by solving a dual problem of a tight LP-relaxation. We also establish the equivalency of the dual optimal solution set and the core allocations when they exist and satisfy equal treatment of equals. Finally, we propose five different cost allocation mechanisms - [i] marginal allocation, [ii] dual linear programming based mechanism, [iii] the Shapley Value, [iv] the Nucleolus, and finally [v] pattern-based mechanism.  Based on an extensive computational study, we observe that the pattern-based allocation mechanism significantly outperforms the other mechanisms with respect to the fairness of the cost allocations.",Cost Allocation Mechanisms in a Capacitated Economic Lot-Sizing Game,[79200],810,"[50, 61]",4177,Lot-sizing with game theory aspects,32,9,49,"Lot Sizing, Lot Scheduling and Production Planning","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,M1 [building - 101],"['Game Theory', 'Inventory']",TC-49
"We study the problem of finding the optimal location and price equilibria of a Stackelberg location game when price equilibria are used. First, the leader and then the follower choose their locations and then set delivered prices to maximise their profits. Previous work has solved problems of location-price equilibria on different settings, showing that the minimiser of the social cost gives a location equilibrium. Any solution to the Stackelberg problem is also a location equilibrium, and it can be shown that there are location equilibria which are not minimizers of the social cost. Our research question is whether there may exist a Stackelberg solution which is not an equilibrium provided by the social cost. Assuming that firms set the equilibrium prices after the locations are fixed, the problem can be modelled as a bilevel optimization problem. We propose an exact interval branch-and-bound algorithm, suitable for relatively small problems, to obtain rigorous solutions. Our main goal is to see how the Stackelberg solution can differ from the location-price equilibria provided by the minimization of the social cost.",A Stackelberg location problem on the plane using delivered price equilibria,"[3378, 3355]",767,"[64, 19, 52]",4179,Location with Multiple Actors,29,9,61,Locational Analysis,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S10 [building - 101],"['Location', 'Continuous Optimization', 'Global Optimization']",TC-61
"Geo experiments are a method of choice to estimate online advertising effectiveness. It is challenging to relate ad spend [treatment] in each geo and the ensuing time series responses, such as sales. The return on ad spends [ROAS] is the difference between responses obtained during treatment [after-data] and the counterfactual responses if treatment had not occurred [before-data]. Since we obtain only after-data for treated geos, evaluating the ROAS depends on how well the before-data are modelled. A typical approach for modelling before-data includes estimating the behavior of treated geos using the behavior of control geos. An inevitable problem is to find control geos homogeneous to treated geos so that any difference can be directly attributed to the treatment. To tackle the challenges mentioned above, we propose a time series based statistical matching framework. The selected time series features used as covariates vary naturally among all geos without being influenced by the treatment. We achieve a balanced covariate distribution between control and treated geos using Coarsened Exact Matching [CEM]. The weights obtained from CEM are used to build a time-varying negative binomial regression model for counterfactual estimation of before-data for treated geos. We demonstrate results using real-time online advertising data. We observe significant improvement in homogeneity between control and treated geos, thereby improving the accuracy of incremental impact measurement.",Estimation of Online Advertising Effectiveness using a Coarsened Exact Matching Framework,"[79205, 79217]",356,"[7, 47]",4181,Recommender systems,17,2,31,Analytics,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,54 [building - 208],"['Analytics and Data Science', 'Forecasting']",MA-31
"Complex networks representing structural properties of natural systems exhibit characteristics such as small-world or scale-free. However, the symmetry of such networks has not been studied much until recently, although some networks, such as the human brain, have symmetries directly embedded in their structure, or such symmetry is an indicator of their specific state. In a recent study, however, it was shown that non-trivial symmetries based on graph automorphisms exist in general complex networks representing real-world systems. Nevertheless, due to the uncertainty of determining the edges of such a network, it is necessary to consider approximate symmetries, as was shown in a recent study by Liu 2020, where the image of the resulting automorphism may show inconsistencies for several pairs. However, the proposed optimization method has shortcomings, such as, for example, the instability of the results, relatively small radius of search, and not considering fixed points. In this paper, we consider adopting the method recently developed for treating the Graph Matching Problem and propose an alternative method that addresses some of the shortcomings of the approach mentioned above. In addition, we propose a scheme for testing the approximate symmetry methods using different random networks reflecting the approximate symmetry in the network representing the complex networks with emphasis on the human brain.",Evaluating approximate symmetries in complex brain networks,"[79201, 79259, 79285, 79265]",880,"[53, 14, 17]",4185,Exact methods in combinatorial optimization [Contributed],64,9,52,Combinatorial Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,8003 [building - 202],"['Graphs and Networks', 'Combinatorial Optimization', 'Computational Biology, Bioinformatics and Medicine']",TC-52
"Integrating multiple channels and delivering a seamless experience to customers in an Omnichannel setting presents a set of interesting and challenging problems in fulfillment. In particular, when an order needs to be delivered to a customer’s home, a delivery executive can pick up the items from any of a set of stores which are in the vicinity [ship from store], or from any other node in the distribution network like a regional warehouse [order allocation]. We develop a combined order allocation and routing problem for a retailer who has a multi-tier distribution center network and a set of stores serving customers in a city. We consider real-world requirements like minimum inventory requirements at stores and service time for customers. Solutions are presented for generated test data. Some real-world policies [like batching windows] are discussed, and future research opportunities are identified.",Last-mile fulfillment in Omnichannel Retail - A combined allocation and routing problem,[78414],101,"[32, 130, 145]",4187,Retail Distribution II,30,13,50,Retail Operations,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,M2 [building - 101],"['E-Commerce', 'Service Operations', 'Vehicle Routing']",WB-50
"The deregulation of electricity markets has emphasized the importance of accurate daily electricity price forecasting [EPF] due to electricity's non-storability and the necessity for day-ahead bidding to align demand with production. 
Accurate EPF is crucial for utilities and consumers, enabling effective bidding strategies, adjusting consumption to benefit from lower prices, and allowing producers to manage generation assets more efficiently. 
The challenge in EPF arises from the complex dynamics of electricity prices, influenced by factors like weather conditions, business activity, and consumer habits, which introduce volatility, seasonality, and unexpected prices surges. 
Despite the success of deep learning methods in the literature on EPF, recent global events such as the COVID-19 pandemic and the Russo-Ukrainian war have shown the need for more adaptable forecasting models. 
This paper proposes an adaptive Long short-term memory [LSTM] model that responds to external shocks by monitoring a disruption signal based on the spillover of shocks across different consumer categories. This approach aims to enhance the accuracy of EPF by accounting for both direct and indirect effects of external shocks on electricity prices, using a dynamic threshold to update the forecasting strategy based on observed systemic changes.
We demonstrate the better forecast performance of our approach and provide an application with a model of energy storage arbitrage. 
",Enhanced electricity prices forecasting from spillovers among commodities,[79104],340,"[93, 47, 66]",4189,Enhanced statistical methods for energy challenges,22,7,14,Energy Markets,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,16 [building - 116],"['OR in Energy', 'Forecasting', 'Machine Learning']",TA-14
"The prevailing and anticipated rates of urbanization are recognized as substantial challenges to the society. Urban areas have been mapped as primary drivers of economic development, however, the rapid expansion of cities in recent decades has raised concerns regarding their sustainability, as evidenced by escalating infrastructural, ecological, and social challenges. Bearing in mind the benefits of smart urban development, this paper aims to assess the level of achieved smartness in European cities using a multicriteria analysis approach. By utilizing data from the latest Eurostat Urban Audit Perception Survey, the research assesses the perceived urban performance by inhabitants, encompassing various dimensions of smart and sustainable urban development. To analyse the diverse attributes of cities' urban performance, a multi-criteria model is developed. This model integrates entropy for determining criteria weights and employs the PROMETHEE method for ranking. Respondents' subjective preferences are measured using a 4-point Likert scale, and responses are quantified for analysis purposes. The resulting rankings are visually presented to identify European urban areas where inhabitants perceive the highest overall levels of smartness and sustainable urban development. The obtained results may serve local self-governments, by offering a diagnostic tool for evaluating the effectiveness of existing smart city initiatives and identifying areas for improvement.",Measuring Urban Smartness - A Comparative Study of European Cities Using Multi-Criteria Analysis,"[67122, 42388, 45294]",912,"[139, 0]",4192," Enhancement of circularity, inclusivity, and smartness in cities II",79,5,18,Sustainable Cities,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,42 [building - 116],['Sustainable Development'],MD-18
"Traditional supply chain [SC] focuses on the management of forward flows going from upstream members to downstream units. The Closed-Loop Supply Chain [CLSC] also manages the backward flows from downstream members to upstream units. The integration of upstream and downstream activities into one system has three main objectives - to meet environmental objectives, to create new economic opportunities and to provide competitive advantages. This paper analyses a CLSC consisting of a manufacturer producing new products and selling them through a retailer. The manufacturer recycles the products at the end of their use [used packaging] and generates unit production cost savings if it uses recycled material instead of new material. The retailer sells the products to the final customer, collects the used packaging and returns the deposit to the customer under a guaranteed buyback contract. Several models are proposed for this situation and the operating conditions are analysed.",Closed-loop supply chain coordination with a deposit-refund system,"[3797, 79218]",925,"[138, 25, 50]",4193,Game theory for the circular economy,18,12,23,"Circular Economy, Remanufacturing and Recycling ","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,82 [building - 116],"['Supply Chain Management', 'Decision Analysis', 'Game Theory']",WA-23
"Eurotransplant [ET] prioritizes candidates for deceased-donor kidney transplantation with the ET Kidney Allocation System [ETKAS] algorithm. ETKAS is a points-based system which awards points for patient and donor attributes, including dialysis time, HLA match quality, candidate age, relative location of the candidate w.r.t. the donor, and chances of finding a donor with at most 1 HLA mismatch in the ET donor pool.

The ET Kidney Advisory Committee [ETKAC] regularly points out that ETKAS is outdated. For instance, ETKAS still assigns equal priority to matches on HLA-A, -B and -DR loci, ETKAS does not include age matching, and ETKAS inadequately compensates immunized candidates for having a restricted donor pool. Despite recognition of such issues, the weights assigned to ETKAS attributes has changed little since ETKAS was introduced in 1996.

To facilitate ETKAC discussions on revising ETKAS, we developed the ETKAS simulator, which is a discrete event simulator based on ET registry data simulating ET kidney allocation. We find that the simulator accurately describes candidate dialysis times at transplant, mortality rates, and transplantation rates in ET member countries. To demonstrate utility of the ETKAS simulator for policy evaluation, we map the impact of assigning extra points to HLA-DR matching relative to HLA-A and HLA-B matching, as well as directly prioritizing immunized candidates by assigning points for virtual Panel Reactive Antibodies [vPRA].",Evaluating policy changes to Eurotransplant’s kidney allocation rules with Discrete Event Simulation,"[70328, 35964, 6251]",951,"[131, 56]",4194,Kidney Exchange II,3,10,10,OR in Health Services [ORAHS],"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,11 [building - 116],"['Simulation', 'Health Care']",TD-10
"Packing 3D soft objects in an optimized container is considered. The shapes of the objects can be varied in certain limits. However, the volumes of the objects remain constant. Free translation/rotation for objects is allowed. The soft objects must be arranged completely inside the container without overlapping. Corresponding nonlinear programming models are formulated for different shapes of objects using the phi-function technique. A solution strategy based on the multistart technique is proposed. It involves three main stages - search for feasible starting points; find a local-optimal solution for each starting point using a decomposition procedure; choose the best solution from those found at the previous stage. Computational experiments for packing soft polyhedra in a cuboidal container are provided. Packing soft objects has various applications in biology and mechanics, material science and logistics.",Optimized Packing Soft 3D Objects,"[11877, 24536, 6291, 36126, 79221]",622,"[23, 0]",4195,Cutting and Packing 4 - 3D irregular,81,5,07,Cutting and Packing [ESICUP],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,1019 [building - 202],['Cutting and Packing'],MD-07
"Container terminal operators reduce the energy costs in container terminals by implementing a demand response system, which can shift energy consumption typically through energy prices. We investigate operations planning and energy management through demand response to reduce energy-related costs. We propose a two-stage stochastic mixed integer programming model with energy-related costs as part of the objective function. The considered operations include vessel scheduling at berths, temperature control of refrigerated containers, and allocation of handling capacity of quay cranes, yard cranes, and automated guided vehicles to serve each vessel. Various scenarios for vessel arrival times and electricity prices are explored to represent the uncertainty of energy demand and supply, respectively. Moreover, onshore power supply [OPS] will become an increasingly important part of energy consumption in ports. We develop a machine-learning-based approach to determine realistic OPS demand scenarios in stochastic optimization. The suggested model is decomposed and solved using a progressive hedging algorithm. A case study of the Alterwerder container terminal in Hamburg is conducted to test the model. Results show a significant improvement when applying a varying real-time energy pricing scheme compared to a single energy pricing scheme. ",Container Terminal Energy Management and Operations Planning Under Uncertainty with Learning-based Onshore Power Scenarios,"[74633, 35365]",671,"[65, 136]",4198,Machine Learning and Optimization in Ports I,52,12,62,OR in Port Operations,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S12 [building - 101],"['Logistics', 'Stochastic Optimization']",WA-62
"Traffic crashes remain a major cause of preventable loss of life and injury. Many policy and intervention efforts aim to reduce the number and severity of collisions. Of these, the most direct is the identification and correction of hotspots of severe crashes. We present data from the 20 largest cities in Israel, spanning over a decade, showing that locations of clusters of light and property damage crashes have a risk of a severe crash in the following year that is comparable to that of clusters of severe crashes. This has immediate policy implications, as we should rethink the intervention strategy that currently predominantly considers severe hotspots. Next, we apply machine learning to model the probability of having a future severe event at a cluster, given the known characteristics of the location and the previous accidents that occurred. The quality of the model is shown to differ between types of clusters, such as severe vs. minor, intersection vs. non-intersection, etc.  Finally, the problem facing a decision maker is how to use a given budget and prioritize clusters for intervention, based on the risk scores provided by the model and their type-dependant uncertainties, as well as the type-dependant cost of intervention. The overall aim of the policy is assumed to be maximization of the total reduction of expected future severe crashes, but extensions that consider fair spread of resources are also considered.  ",Prioritization of traffic crash hotspots for intervention ,"[79219, 3132, 574]",334,"[143, 7, 8]",4200,Big data analysis and AI in transportation,6,14,55,Transportation,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S02 [building - 101],"['Transportation', 'Analytics and Data Science', 'Artificial Intelligence']",WC-55
"This study evaluates the three main representations methods of the RoRo stowage planning problem for heterogeneous cargo and stability constraints. The lane, grid and hybrid-slot model are formulated and implemented in an equal manner to compare them. All formulations accounts for vessel stability by ensuring that the vertical centers of gravity, shear force and bending moment are within their respective limits. The results show the hybrid-slot approach create the best solutions on all instances across test on a small, medium and large ship. ",Comparison of 3 modelling approaches for stowage of heterogeneous cargo on RoRo ships with stability requirements,"[79211, 71531, 67073, 31857]",669,"[70, 111]",4201,Seaside Planning I,52,2,62,OR in Port Operations,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S12 [building - 101],"['Maritime applications', 'Programming, Mixed-Integer']",MA-62
"The normalized cut function in networks was introduced by Shi and Malik in 2000 to solve some issues concerning the interpretability of the minimum cut problem, which is a classical problem in graph theory whose aim is to provide the bipartition that minimizes the number of edges between nodes from different subsets, applied to partitioning and districting problems. The minimum k-way normalized cut problem [MkWNCP] tries to minimize the external edge density of each subset of a k-partition, also considering the number of internal edges.
We develop several reformulations using mathematical programming tools to exactly obtain the minimum k-way normalized cut of a graph. This is a challenging task since the normalized cut function is non-linear because it is defined by the ratio of two linear functions of integer variables. We also propose a branch-and-price procedure for the MkWNCP which scales better than the compact formulations. Extensive computational experiments assess the usefulness of these methods to solve the k-way normalized cut problem on large graphs and random graphs. In addition, all methods have been analyzed and studied in order to try to improve them as much as possible.
",Mixed-integer linear programming formulations and column generation algorithms for the Minimum Normalized Cuts problem on networks,"[54425, 5876, 73721]",242,"[14, 53, 13]",4202,Exact Algorithms and Formulations for Network Optimization Problems,64,9,29,Combinatorial Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,157 [building - 208],"['Combinatorial Optimization', 'Graphs and Networks', 'Column Generation']",TC-29
"Efficient vehicle routing and delivery planning are critical challenges in logistics systems, impacting operational effectiveness and cost efficiency. Planners are tasked with devising effective delivery and pick-up schedules to meet customers' needs, considering factors such as pick-up and demand quantities per customer, available working hours, shift times, total distances between customer locations, cost parameters, and various transportation constraints. Delivery planners allocate vehicles, determine the start time of service, optimize vehicle routes within specified time windows, schedule shifts, and decide on the number of tours for each vehicle. These actions aim to maximize vehicle utilization and meet requirements at minimum cost. We propose mixed integer programming models that reduce the logistic costs and number of vehicle traffic by combining shipments from different customers. To solve large problems, we propose a decomposition-based heuristics that separates periods by considering time windows and setting an objective that first minimizes the number of used vehicles in each period. The results show that inefficient transportation between two nodes is avoided, the numbers of vehicles are decreased, the utilization rate of each vehicle is increased, and vehicle traffic is reduced in each customer area.","A Multi-Trip Periodic Vehicle Routing Problem with Time Windows, Split Delivery for Simultaneous Pick-Up and Delivery",[78839],623,"[145, 143, 72]",4205,Vehicle routing I,6,2,60,Transportation,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,S09 [building - 101],"['Vehicle Routing', 'Transportation', 'Mathematical Programming']",MA-60
"This article aims to estimate the systemic risk of the Romanian stock market, using the FRM [Financial Risk Meter] methodology. This research contribution is about applying a novel systemic risk index to the Romanian financial system [FRM@RO], to identify potential sources of systemic risk, and to understand network interconnections, thus increasing risk awareness of both managers and regulators. By using data for companies listed at the Bucharest Stock Exchange [BSE], our article highlights several aspects of the systemic risk of the Romanian stock market. First, our study reveals that the main driver of systemic risk, especially during financial crises, is the volatility index, VIX. However, local factors, such as ROBOR interest rate and sectorial indices for financial investment companies, in general, and energy sector companies, in particular, are extremely important in triggering systemic risk. Second, the system risk indicator for the Romanian stock market, FRM@RO, may capture both investor sentiment, measured via the Google Trends Search Volume Index, and stock market volatility. Third, FRM@RO can act as an early warning indicator for economic turmoil, being able to predict periods of technical recession one quarter in advance. Fourth, by using network analysis, we can identify, daily, the level of market interconnectedness and highlight the main companies triggering tail co-movements. ",Financial Risk Meter for The Romanian Stock Market,[79220],511,"[45, 7]",4209,Innovations in Digital Assets - IDA,17,14,31,Analytics,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,54 [building - 208],"['Financial Modelling', 'Analytics and Data Science']",WC-31
"Influence diagrams provide a modeling and inference framework for sequential decision problems, representing the probabilistic knowledge by a Bayesian network and the preferences of an agent by utility functions over the random variables and decision variables. Computing the maximum expected utility [MEU] and the optimizing policy is exponential in the constrained induced width of the model and it has been long recognized as one of the most difficult probabilistic inference tasks, especially for larger problem instances. We will overview a suite of exact and approximate inference algorithms for IDs developed over the past years, starting with variable elimination [VE] schemes that use local computations to solve IDs exactly, and continuing with variational approaches based on message-passing over join-graph decompositions and weighted mini-buckets that yield variational bounds on the MEU and sub-optimal decision policies. We will also show that the latter bounding schemes can be used as heuristics generators that can guide efficient search strategies such as AND/OR search for solving IDs exactly that exploit the underlying structure of the model. We will provide experimental results on various synthetic and real-world benchmarks for IDs to highlight the performance of the inference methods presented. Finally, we will discuss recent extensions of IDs for decision making with imprecise information, multiple objectives and the potential of augmenting IDs with causal semantics.",Algorithmic Advances for Influence Diagrams,"[71869, 79223, 79224, 79235, 79237]",483,"[8, 27, 5]",4210,Decision problems represented as influence diagrams,49,12,34,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,43 [building - 303A],"['Artificial Intelligence', 'Decision Theory', 'Algorithms']",WA-34
"Chile's susceptibility to natural catastrophes, encompassing earthquakes, tsunamis, and volcanic activities, is exacerbated by climate change, resulting in heightened occurrences and increased severity of wildfires, floods, landslides, and droughts. Within this context, optimizing logistics planning and routing is critical to ensure timely aid delivery to affected communities. Our study introduces a novel routing model tailored to address unique geographical and disaster-related challenges faced in Chile. Unlike existing approaches, our model integrates comprehensive inventory control and distribution hub placement, optimizing relief supply routes in natural disasters. We propose a mixed integer linear programming [MILP] model to formulate a supply chain network strategically allocating distribution centers, regulating their inventories, and planning for ground and aerial transportation of relief goods. The efficacy of our proposed model is assessed through experimentation using real-world data sourced from various locations in Chile that have experienced natural disasters.",Optimizing Disaster Relief Logistics in Chile - A Location-Inventory-Routing Framework,"[23074, 68787, 25305]",90,"[65, 70, 14]",4211,Humanitarian aid provision and disposal,38,7,21,OR in Humanitarian Operations [HOpe],"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,49 [building - 116],"['Logistics', 'Maritime applications', 'Combinatorial Optimization']",TA-21
"Determining the assortment and inventory levels based on their shelf life is essential for retailers to maximize profits while avoiding food waste. The limited shelf space and the handling of perishable products with limited shelf lives interrelate assortment and inventory decisions. A joint approach is needed that defines the assortment size and the maximum possible inventory levels while considering differing product ages and their implications on the planning. The product ages on the shelves are further impacted by the customers’ withdrawal behavior. For instance, customers intentionally selecting products with longer expiration dates change the order of depletion as older products are left on the shelf. These interferences need to be considered when deciding on the products within the assortment and their corresponding inventory levels. We develop the first multi-period approach to address the interrelation between assortment planning and inventory management by integrating product shelf life and outdating. The resulting planning problem is solved using an iterative approach that considers different customer withdrawal behaviors and established demand effects.",Optimizing assortments with age-dependent inventory,"[49049, 22691, 71571]",424,"[61, 138]",4212,Food Waste,30,2,50,Retail Operations,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,M2 [building - 101],"['Inventory', 'Supply Chain Management']",MA-50
"The rapid increase in the volume of parcels to be delivered pushed logistics providers to explore innovative solutions to streamline the more critical and expansive stage of the supply chain - the last-mile logistics. Among the solutions, the most explored is self-collection, by which customers autonomously collect parcels from manned or unmanned facilities. However, the success of the strategy depends on two main aspects - the individuation of the determinants affecting the customers’ choices; the location of such facilities in relation to the positions of the potential customers. 

In this work, we explore the possibility of describing these aspects through the formulation of mathematical models for the optimal location of self-collection facilities with the objective of maximizing the attractiveness of this system, as alternative to home delivery. In particular, the mechanism of patronization of potential facilities is regulated by an attractiveness function in which some determinants are considered. 

Thanks to the cooperation with the principal Italian logistics provider, we apply the model to some real case studies with the objective of designing efficient and robust self-collection networks in densely populated contexts. The obtained results suggest insightful managerial implications and prove that the model can be used as an appropriate tool to support the decision-making process concerning the design of a self-collection network in an urban context.
",Strategies for the redefinition of postal collection and delivery services in urban areas,"[3210, 59563, 32636, 73854]",768,"[64, 65, 43]",4215,Location in Logistics and Supply Chain Management,29,10,61,Locational Analysis,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S10 [building - 101],"['Location', 'Logistics', 'Facilities Planning and Design']",TD-61
"Bulk liquid terminals play a crucial role in timely [un]loading of liquids from tankers and facilitating transport to and from the hinterland via pipelines, trains or trucks. At an import terminal, arriving tankers berth to be unloaded via loading arms and pipelines connected to storage facilities. Whenever the storage is full, the unloading rate essentially decreases to the drain rate of the storage, which increases the sojourn times of tankers considerably. On the other hand, when the storage is empty, the demand on the landside cannot be met and throughput will decrease.
The performance of an import terminal serving a single liquid type can be analyzed using feedback fluid queue models, assuming that tankers arrive according to some random process, carrying random amounts of liquid, and experiencing delay when the berth is in use. Importantly, such models explicitly take into account the interaction between tankers and storage reservoir. In this talk we focus on the more complex case with two types of liquid that cannot be mixed, each with their own storage reservoir and tankers, while sharing the same [single] berth. A straightforward fluid queue model is then no longer applicable, but we apply an iterative approximation scheme that performs remarkably well. We present graphs showing how the various parameters influence the behavior of the main performance measures, viz. the expected sojourn times of both types of tankers and the throughputs of both types of liquid.
",Performance analysis of liquid bulk terminals,"[74399, 74183, 70286]",154,"[65, 70, 121]",4216,Port Performance,52,15,62,OR in Port Operations,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S12 [building - 101],"['Logistics', 'Maritime applications', 'Queuing Systems']",WD-62
"Until recently maintenance at offshore wind farms have been conducted using a preventive- or condition-based maintenance strategy. However, as more information becomes available, and better prediction models are developed, the goal is to further reduce the cost of energy from offshore wind farms by moving to a predictive maintenance strategy. In such a strategy, probabilistic models of the future conditions of each component of a wind turbine are used as a basis for planning the maintenance. 
Heavy maintenance where large components such as blades, generators, etc., are replaced, is one of the costliest types of maintenance as it, besides new components, requires chartering of a specialized jack-up vessel. 
In this work we investigate how a predictive maintenance strategy affect the cost of heavy maintenance compared to other maintenance strategies. To do this, different strategies are evaluated through a framework simulating the 25-year long lifetime of a wind farm. Each year in the simulation framework, an optimization problem determining how many components to order and the chartering length of a jack-up vessel is solved. As these decisions must be taken well in advance of the maintenance period where component failures and weather conditions governing when the vessel can operate are unknown, this problem is formulated as a two-stage stochastic program. The vessel and component deployment is then simulated as the component failures and weather conditions are realized.
",Investigating condition-based and predictive strategies for heavy maintenance at offshore wind farms,"[74584, 79233, 79232, 74651]",831,"[65, 136, 131]",4219,New problems in logistics under uncertainty,49,15,34,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,43 [building - 303A],"['Logistics', 'Stochastic Optimization', 'Simulation']",WD-34
"In the context of index volatility versus the volatility of the entire market, the following question becomes pertinent - Does the market volatility track the index volatility, or is it the other way around? To estimate the market's overall volatility, we employ the cross-sectional intrinsic entropy [CSIE] model as the underlying methodology for tracking the aggregate volatility of all the symbols listed and traded on a given stock market. We investigate the appropriate GARCH models to forecast the variance of market cross-sectional aggregate volatility estimates and the variance of market indices volatility estimates [VoV] forecasting. The CSIE model provides estimates of the daily aggregate volatility for the entire market. Alternatively, index volatility estimates are determined on n-day rolling windows. Therefore, CSIE market volatility estimates are integrated into the research methodology as comparable n-day moving averages. Our study uses historical end-of-day [EOD] data consisting of traded volume, opening, high, low, and closing prices [OHLCV] for all companies listed on the NYSE and NASDAQ and the corresponding market indices, S&P500 and NASDAQ Composite respectively. We conclude that the S&P 500 is not fully representative of the market, and particularly in the periods preceding recessions, it does not capture overall market immanent deep trends, specifically those revealed through the CSIE market volatility estimator. ",Volatility of Stock Market Aggregate Volatility [VoV] Forecast Based on Cross-Sectional Intrinsic Entropy’s Volatility Estimates,"[58269, 57323, 79251]",694,"[45, 33]",4220,Complexity and Financial Patterns,4,15,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S14 [building - 101],"['Financial Modelling', 'Economic Modeling']",WD-63
"Drones are becoming increasingly popular for various tasks in different domains, such as assessing infrastructure damage, conducting surveys, and responding to emergencies. Typically, users of drone services enter individual contracts with a service provider for each order who provides the necessary technical equipment [drones, sensors, pilots, operating license]. With the growing number of users, it has become inefficient to perform individual orders independently because their synergies are not considered. 

Therefore, a new business model called the drone-sharing economy is emerging. Sharing drones, equipment, pilots, and data among multiple users from a particular region can significantly increase efficiency. This is made possible by holistic and cross-task mission planning with a high degree of automation where the challenge lies in optimizing the schedule of all orders to maximize their synergies while adapting to dynamic conditions such as the weather and new or changing orders. 

We propose a scheduling algorithm that employs variable neighborhood search to generate a rolling-horizon schedule for both drones and pilots. Due to the various requirements of the orders such as operational area, time windows, and others, multiple flight plans are generated. The goal is to maximize the number of orders completed by selecting the most fitting flight plan while considering constraints like drone and pilot compatibilities, data quality, and changing weather conditions. ",A Scheduling Algorithm for Drone-Sharing,"[49166, 50068, 69781, 79245]",473,"[26, 31, 129]",4224,Methods and Algorithms of Decision Support,45,4,45,Decision Support Systems,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,30 [building - 324],"['Decision Support Systems', 'Dynamical Systems', 'Scheduling']",MC-45
"In this talk, we propose a global optimization method for a quadratic reverse convex programming problem [QRC] whose feasible set is expressed as the area excluded the interior of a convex set from another convex set. It is known that many global optimization problems can be transformed into such a problem. Iterative solution methods for solving [QRC] have been proposed by many other researchers. One of the difficulty for solving [QRC] is that all locally optimal solutions do not always satisfy KKT conditions. In order to overcome this drawback, we introduce a procedure by combining parametric optimization techniques and Lipschitz optimization methods for finding locally optimal solutions of [QRC]. Moreover, we propose an algorithm for finding a globally optimal solution of [QRC] by incorporating such a procedure into a branch and bound procedure.",A global optimization method for a quadratic reverse convex programming problem ,[27970],507,"[52, 113, 114]",4225,Convex optimization algorithms,70,12,41,Nonsmooth Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,97 [building - 306],"['Global Optimization', 'Programming, Nonlinear', 'Programming, Quadratic']",WA-41
"The aim of this research is to develop a fuzzy logic based technique for optimal control of dynamical systems under vague conditions which allows, among other, to solve optimal trajectory planning and contour modelling problems with uncertainty in location constraints. 
The goal is to find a control law that will drive the output trajectory in such a way that vague constraints are satisfied in the fuzzy logic sense.
In order to adapt methods for control under specific types of uncertainty, we develop and apply fuzzy logic based mathematical tools such as fuzzy optimization, fuzzy metrics, etc. Fuzzy relations of a special design are used to incorporate uncertainty of constraints into the classical setting of the optimal control problem under consideration. 
The proposed approach is illustrated with examples, the numerical part of the analysis of which is reduced to quadratic programming. 
",Fuzzy logic based approach for optimal control of dynamical systems under uncertainty,"[30166, 79406]",898,"[82, 49, 31]",4226,Optimal control theory,90,10,33,Optimal Control Theory and Applications,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,42 [building - 303A],"['Optimal Control', 'Fuzzy Sets and Systems', 'Dynamical Systems']",TD-33
"The problem of allocating long-running applications [LRAs] to cloud resources can be modelled as vector bin packing [VBP] with additional compatibility constraints. In that problem, cloud compute nodes correspond to bins and LRAs to items. The multidimensional nature of the problem is associated with characteristics of bins and items, such as CPU cores, RAM, GPUs, etc. Planning resource allocation can be performed before LRA deployment and it is aimed at minimizing the number of nodes to which LRAs are allocated. In our study, we model affinity restrictions on LRA deployment. They are defined for pairs of LRAs which replicas can be jointly co-located to the same node, but with some limits, or for pairs of incompatible LRAs, which cannot be co-located. In our work, we explore a broad range of VBP heuristics capable of producing high quality solutions to the affinity-aware version of the problem and find a solution within acceptable time. The findings are supported by extensive computational experiments performed on the Alibaba Tianchi dataset. ",Affinity Aware Scheduling in Distributed Computing via Vector Bin Packing ,"[24630, 79247, 79249, 79252, 79253, 79254, 79256, 79255]",227,"[129, 12, 54]",4228,Combinatorial Optimization in Scheduling,64,13,26,Combinatorial Optimization,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,012 [building - 208],"['Scheduling', 'Capacity Planning', 'Grid Computing']",WB-26
"We explore the edge improvement problem, relaxing the fixed edge traversal times assumption in traditional network flow problems. We consider two versions - discrete edge improvement, limited to a discrete set, and continuous edge improvement, where values can vary within a given range. Initially, we analyze both versions on a tree-shaped network, discussing their computational complexity. For more complex networks, we develop mixed-integer programming [MIP] formulations for both versions. This study is the first to propose and compare different formulations for the discrete edge improvement problem and to present one for the continuous version. Since our models underperform for medium and large problem instances, we introduce a Benders decomposition algorithm to solve the discrete problem. Additionally, we use it heuristically to find high-quality solutions for the continuous edge improvement problem efficiently. We also devise an MIP formulation to determine lower bounds for the continuous problem, leveraging McCormick envelopes and optimal solution properties. Our experiments show that the Benders decomposition algorithm outperforms other formulations for the discrete edge improvement problem, while the heuristic method proposed for the continuous problem provides quite good results even for large instances.",Formulations and Algorithms for the Edge Improvement Problems,"[79229, 68932]",865,"[111, 72, 150]",4229,Topics in Integer Programming I,64,12,25,Combinatorial Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,011 [building - 208],"['Programming, Mixed-Integer', 'Mathematical Programming', 'Network Flows']",WA-25
"Our research addresses the limited use of Learning Analytics tools and techniques in providing personalized learning experiences for MBA students. We propose a novel framework that incorporates Bayesian Knowledge Tracing [BKT] and other predictive modeling techniques to create customized learning tracks for MBA programs. Our approach leverages data from standardized admission tests and student performance to generate tailored recommendations, addressing a significant gap in the application of learning analytics in graduate business education.

Preliminary results, based on a small sample, indicate that the use of BKT can improve the learning experience for MBA students. By developing a clear framework for implementing BKT and other predictive modeling techniques, this research aims to provide academic directors, deans, and key decision-makers in graduate business schools with a practical tool for enhancing personalized learning in their programs.

The proposed framework has the potential to be adapted for a broader audience beyond MBA programs, contributing to the advancement of personalized learning in various educational contexts. This research aligns with the growing need for data-driven approaches to optimize learning outcomes and student success in higher education.
",Enhancing Business Education through Bayesian Knowledge Tracing - A Framework for Personalized Learning,[78794],415,"[7, 26, 28]",4230,Learning Analytics and other Text Analytics tasks,17,13,31,Analytics,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,54 [building - 208],"['Analytics and Data Science', 'Decision Support Systems', 'Developing Countries']",WB-31
"The Traveling Salesman Problem [TSP] stands as a quintessential challenge in combinatorial optimization, with vast implications across industries. Given the absence of efficient classical algorithms to solve this problem, there's a great interest in quantum computing to address such challenge. However, the current quantum computing landscape is dominated by Noisy Intermediate Scale Quantum [NISQ] computers. Hence, the development of efficient encodings becomes paramount in harnessing the potential of these computers to tackle complex problems like the TSP. While previous encoding techniques have been introduced, they lacked the essential characteristic of proximity between neighboring solutions. We propose an algorithm capable of efficiently computing an encoding with proximity. Through experimentation, we show that our encoding facilitates superior performance of variational quantum algorithms compared to existing encodings. Furthermore, our approach requires fewer gates and qubits compared to conventional algorithms such as QAOA, highlighting its potential for practical quantum computing implementations.",Efficient Encoding for the Traveling Salesman Problem using Quantum Variational Algorithm,"[79236, 9446]",375,"[14, 0]",4231,Decomposition methods for Quantum Optimization,83,3,42,Quantum Computing Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,98 [building - 306],['Combinatorial Optimization'],MB-42
"The objective of a discrete robust optimisation approach e.g., budget uncertainty [Bertsimas and Sim, 2003&2004] is to find a solution that optimises against all scenarios in which up “a” number of coefficients that maximally influence the objective vary. Therefore, a solution produced by such method is optimal when exactly “a” coefficients are subject to change and it is feasible for other cases.  This view of robustness could be regarded as “pessimistic” robust optimisation approach as the value of the objective reflects the worst-case scenario for a given budget of uncertainty. Building upon the concept of pessimistic robust optimisation, in this research we present an approach that we call “optimistic” robust optimisation. For a given problem with uncertain data in the objective function, the optimistic robust optimisation approach aims to find a solution in which exactly “b” number of coefficients with “minimum” impact on the objective are subject to change. We present examples and discuss opportunities and challenges pertain to the use of such [optimistic]robust optimisation approach.",Optimistic versus Pessimistic Discrete Robust Optimisation Approaches - Opportunities and Challenges,[63492],826,"[127, 136, 64]",4232,Robust Optimization - Theory and Applications,49,14,35,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,44 [building - 303A],"['Robust Optimization', 'Stochastic Optimization', 'Location']",WC-35
"It is well known that the use of Rockafellar's perturbation method
is very efficient for getting duality results in convex analysis.
The use of this method in linear programming is less spread. It is
our aim to show that, using Rockafellar's perturbation method, one
can obtain duality results in infinite-dimensional conic linear
programming under very general conditions. Moreover, we analyze a
sample of Kretschmer's gap example used in N.E. Gretsky, J.M. Ostroy
and W.R. Zame's paper Subdifferentiability and the duality gap,
Positivity 6 [2002], 261-274.
",Duality gap and examples in infinite-dimensional conic linear programming problems,[75417],164,"[110, 21]",4234,Special Classes of  Convex Conic Optimization problems,68,7,38,"Conic Optimization - Theory, Algorithms, and Applications","Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,34 [building - 306],"['Programming, Linear', 'Convex Optimization']",TA-38
"Polyhedral conic functions, introduced by Gasimov and Öztürk, are defined with graphs depicting convex polyhedra as their level sets. They have become vital in machine learning, particularly for addressing classification problems. Derived by augmenting a hyperplane equation with the l1 norm, a polyhedral conic function relocates within the space through the vertex point.
The PCF [Polyhedral Conic Function] algorithm efficiently separates two sets of points in multidimensional spaces by solving a linear programming subproblem. It computes parameters and yields a function partitioning the space into two regions, thereby separating points effectively.
PCFs find utility in classification by formulating error functions to minimize discrepancies between sets. They have applications in image processing and psychiatry, aiding tasks like object recognition and disease identification.
Various PCF-based algorithms, like clustering-based PCF and Incremental Conic Functions, use clustering approaches to find vertex points. These extend traditional PCF capabilities by incorporating clustering techniques.
Mathematical programming and optimization play a crucial role in PCF algorithms, providing a framework for fine-tuning performance. Their termination in finite steps underscores their efficiency, making them appealing in real-world applications.
In conclusion, PCFs offer a versatile approach to classification problems, supported by mathematical principles. ",Conic Functions - Theory and Applications in Machine Learning,"[20847, 31920, 3550]",678,"[8, 66, 72]",4236,Machine Learning and Ensemble Learning with optimization methods,15,14,27,Mathematical Optimization for XAI,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,047 [building - 208],"['Artificial Intelligence', 'Machine Learning', 'Mathematical Programming']",WC-27
"The Video Assistant Referee [VAR] is introduced in football to guarantee consistency and accuracy in the appliance of the official playing rules. The VAR must avoid appraisals that are prone to interpretability, and must ensure fair play. However, as with every subjective judgment procedure, differences in perception might arise.
This work has gathered statistical evidence that even with use of the VAR, conclusions on the same game situations will still differ among more referees judging it behind the VAR screens. This was achieved by studying the performance of the Dutch football referees who are asked to judge game situations based on video replays. For all game situations, the referees are requested to classify the violation of the fouls rules into 11 ordered categories [from 0 = no violation of the rules up to 10 = strict violation of the rules resulting in a red card]. The discrete scores given by the referees are transformed into continuous scores, as required by the Analysis of Variance models, using linear optimization techniques. The statistical methods show enough evidence that there is difference in perception between several referees. Subsequently, groups are formed such that each group contains referees that have similar perception for more uniformity in decision making.",Analysis of Perception Differences in Football Video Refereeing,[79240],948,"[99, 7, 110]",4237,Fairness in sports,37,12,16,OR in Sports,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,19 [building - 116],"['OR in Sports', 'Analytics and Data Science', 'Programming, Linear']",WA-16
"The interpretability of models has become a crucial issue in Machine Learning because of algorithmic decisions' growing impact on real-world applications. Tree ensemble methods, such as Random Forests or XgBoost, are powerful learning tools for classification tasks. However, while combining multiple trees may provide higher prediction quality than a single one, it sacrifices the interpretability property resulting in “black-box” models. In light of this, we aim to develop an interpretable representation of a tree-ensemble model that can provide valuable insights into its behavior. First, given a target tree-ensemble model, we develop a hierarchical visualization tool based on a heatmap representation of the forest's feature use, considering the frequency of a feature and the level at which it is selected as an indicator of importance. Next, we propose a mixed-integer linear programming [MILP] formulation for constructing a single optimal multivariate tree that accurately mimics the target model predictions. The goal is to provide an interpretable surrogate model based on oblique hyperplane splits, which uses only the most relevant features according to the defined forest's importance indicators. The MILP model includes a penalty on feature selection based on their frequency in the forest to further induce sparsity of the splits. The natural formulation has been strengthened to improve the computational performance of mixed-integer software.",Unboxing Tree ensembles for interpretability - A hierarchical visualization tool and a multivariate optimal re-built tree,"[79222, 72523, 65962]",648,"[66, 111, 72]",4238,Feature attribution and selection for XAI,15,10,27,Mathematical Optimization for XAI,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,047 [building - 208],"['Machine Learning', 'Programming, Mixed-Integer', 'Mathematical Programming']",TD-27
"The resilience of the solution for routing problems may be affected if the solutions share many network resources.
The problem of finding sets of K paths between given origin and destination nodes in a network which minimize a total cost function and maximize the paths dissimilarity intends to promote the search for low cost solutions that simultaneously have little in common to one another.
That problem was addressed recently based on bi-objective single-commodity and discretized flow formulations which model the similarity based on two assumptions - the minimization of the number of arc reuses and the minimization of the number of pairwise arc overlaps.
In the present work we review the bi-objective linear integer formulations and compare this bi-objective approach with single-objective formulations where the similarity measure is weighted differently depending on the sensitivity of the area of the network where arcs may be shared.
",On finding dissimilar paths with different dissimilarity weights - a comparative approach,"[2201, 24919]",225,"[14, 112, 109]",4239,Integer Programming and Combinatorial Optimization - Complexity Questions and Algorithms,64,10,52,Combinatorial Optimization,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,8003 [building - 202],"['Combinatorial Optimization', 'Programming, Multi-Objective', 'Programming, Integer']",TD-52
"The study is related to raw material procurement and product shipments of a steel mill. These materials arrive at the factory's port and are transported from the port to the factory by trucks. The aim is to utilize cranes efficiently, reduce costs, and minimize queues. The system reads parameters related to ports, logistics, and factories from Excel and uses them for the simulation model for different scenarios. This includes ship details such as arrival dates, loading/unloading information, coil/sheet numbers and weights, and assigned port numbers. For facilities, it involves crane-ship assignments, operational details for various processes, and compatibility between coils and trucks. The system allows users to input data for different scenarios. The model is transformed into a functional system using Python with the SimPy library. Key performance indicators are analyzed through graphs and tables by comparing various parameters. This includes calculating queue lengths, waiting times, ship completion durations, demurrage rates, and costs, and exporting results to Excel. Additionally, some artificial intelligence methods such as SVM, Random Forest, Artificial Neural Networks, or PCFs will be added to the system to make predictions related to these key indicators.",A Simulation based Decision Support System for Logistics,"[31920, 20847, 13439]",334,"[131, 26, 8]",4240,Big data analysis and AI in transportation,6,14,55,Transportation,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S02 [building - 101],"['Simulation', 'Decision Support Systems', 'Artificial Intelligence']",WC-55
"This work focuses on the [k,l]-anonymity of some networks as a measure of their privacy against active attacks. 
Two different types of networks are considered. The first one consists of graphs with a predetermined structure, namely cylinders, toruses, and 2-dimensional Hamming graphs, whereas the second one is formed by randomly generated graphs. In order to evaluate the [k,l]-anonymity of the considered graphs, we have computed their k-metric antidimension. To this end, we have taken a combinatorial approach for the graphs with a predetermined structure, whereas for randomly generated graphs we have developed an integer programming formulation and computationally tested its implementation. The results of the combinatorial approach, as well as those from the implementations indicate that, according to the [k,l]-anonymity measure, only the 2-dimensional Hamming graphs and some general random dense graphs are achieving some higher privacy properties.

The main contributions of this research are the following:
1. Some theoretical results concerning cylinders, toruses and Hamming graphs are given.
2. We have developed an integer programming formulation for finding the k-metric antidimension of a given graph G. The formulation can be used as a tool to deal with randomly generated graphs as well as with classes of graphs for which their k-metric antidimension is not known theoretically.

Extensions of these results to multiset distances are currently under development.","[k,l]-anonymity of networks via their k-metric antidimension","[68454, 1116, 79257, 79258]",876,"[14, 72, 132]",4243,Optimization issues on graphs II [Contributed],64,15,29,Combinatorial Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,157 [building - 208],"['Combinatorial Optimization', 'Mathematical Programming', 'Social Networks']",WD-29
"In the middle of the season, apparel retail chains can increase sales by trans-shipment of unsold commodities between different stores. Planning of such an operation may be described with the help of a many-to-many transportation model. However, the product and its size variety are tremendous, while the amount of any product in a specific size that any store may send to another one is small. In the considered company, all shipments should go directly from one store to another [drop-ship] rather than through the central warehouse. A forecast of the demand is unavailable, first, because the product collection changes very often, and second, because the sales of the same product differ significantly among different stores. Therefore, decisions must be based on rules proposed by managers and verified over several seasons by experimentation. Moreover, the trans-shipment plan must keep employees' workload acceptable. This paper presents the general concept of the trans-shipment approach, mixed integer programming models of the considered problem, and the proposed hierarchical planning approach.",Lateral Trans-Shipments within an Apparel Retail Chain,[18856],101,"[150, 111]",4245,Retail Distribution II,30,13,50,Retail Operations,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,M2 [building - 101],"['Network Flows', 'Programming, Mixed-Integer']",WB-50
"A DSS has been developed to support B2B marketing strategy regarding agrofood products based on previous buying decisions. The DSS collects data directly from a blockchain B2B platform, which supports the whole procurement process from farm to fork. These marketing decisions refer to the determination of values of products’ characteristics, such as price, quality, certifications, logistics, for a better market placement of a specific product. Following the aggregation-disaggregation approach the preference models of different buyers are elicited based on their buying behavior of competitive products. The present research discusses the modeling and application in the DSS of a specific form of UTAdis method, which encompasses the notions of Stochastic UTA in order to deal with several evaluations of products characteristics in various B2B negotiations and transactions that lead to different buying decisions. The modified method is called Stochastic UTAdis and its application allow the users of the DSS to evaluate different marketing mix scenarios resulting to the allocation of a product into three predefined buying decision categories.",Modeling of Stochastic UTAdis and its application in a DSS addressing agrofoood marketing decisions,[79234],887,"[26, 71, 135]",4246,Robustness analysis in MCDA 2,44,7,44,Multiple Criteria Decision Analysis,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,20 [building - 324],"['Decision Support Systems', 'Marketing', 'Stochastic Models']",TA-44
"Financial institutions play a major role in the economy and the society as a whole. Securing their stability and success is utmost important in the current unpredictable financial markets. Recent bank failures have demonstrated that financial crises tend to move fast in the global economy. Managing uncertainty is the key issue in volatile operating environments.  

The talk will compare stochastic dynamic optimization methodology with simulation-based approaches which are still widely used in financial institutions. Mathematical optimization offers great benefits in solving complex real world strategic financial management problems.  
Only mathematical optimization is able to prepare institutions for diverse economic and financial market scenarios simultaneously and produce concrete decision recommendations. Simulation methodologies are insufficient for today`s business decision making.    
",Stochastic dynamic optimization in strategic financial management,[79184],138,"[137, 26, 44]",4247,Market dynamics and implications for portfolio decisions,74,9,57,Modern Decision Making in Finance and Insurance,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,S06 [building - 101],"['Strategic Planning and Management', 'Decision Support Systems', 'Finance and Banking']",TC-57
"Given that operations research provides its learners with an exposure to an array of problem-solving and decision-making analytical techniques that can be applied to all areas and domains for solving complex decision-making problems, its importance in higher education cannot be understated. The purpose of this paper is to first investigate the factors that influence students’ choices of OR courses at Universities of Higher Education in India. Next, we apply MARS method to predict students’ choices of these courses.",Application of MARS Method to analyze students’ choices of OR courses,"[57516, 3524]",914,"[92, 96, 84]",4249,"OR Initiatives for Education, Sustainability, and Developing Countries",48,5,16,OR Education,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,19 [building - 116],"['OR in Education', 'OR in Mathematics Education', 'Optimization Modeling']",MD-16
"The Failure-Directed Search [FDS] was introduced as a state-of-the-art algorithm for proving optimality and lower bounds for scheduling problems such as Job Shop Scheduling Problem [JSSP] and Resource-Constrained Project Scheduling Problems [RCPSP]. We re-implemented the FDS algorithm in a new CP solver called OptalCP to understand its notable performance better and use these insights to improve its performance further. Despite its genericity and simplicity, FDS proved optimality or solved optimally numerous scheduling instances that were open for decades. In this paper, we investigate the properties of FDS. Experimental results show the improved FDS is 1.7x faster on JSSP and 2.1x faster on RCPSP benchmarks and is 3.5x times faster on JSSP and 2.1x times faster on RCPSP than the state-of-the-art FDS algorithm in IBM CP Optimizer 22.1. The new FDS improved existing lower bounds of 66 out of 78 and 43 out of 78 randomly picked JSSP and RCPSP instances, respectively.",Improved Results on RCPSP Using Failure-Directed Search in Constraint Programming,"[15091, 79261, 79266]",960,"[129, 66, 5]",4250,RCPSP and extensions,35,5,60,Project Management and Scheduling,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S09 [building - 101],"['Scheduling', 'Machine Learning', 'Algorithms']",MD-60
"Multi-objective problems are common real-world problems. However, computing the whole Pareto front and analyzing it to make a decision is often impractical. Therefore, we propose an interactive algorithm to help the decision-maker [DM] reach the most preferred solution. The interactive algorithm alternates between generation stages, using a representation method based on the epsilon-constraint algorithm, and analysis stages, in which pairwise comparisons are required. Convex preference cones are used to adapt the search region based on the pairwise comparison. To that end, the H-representation of the 3-pointed convex cone is developed so that those types of preference cones can be included as inequality constraints in the generation phase. Moreover, two boosting strategies are put forward to increase the algorithm's convergence. The algorithm is tested on different sets of multi-objective problems, namely binary and integer problems with multiple constraints. The DM’s preferences are simulated through linear and Chebyshev value functions with different weight vectors. The algorithm has proven effective, converging to the most preferred point in the Pareto front, and the boosting strategies improved the efficiency of the algorithm. Furthermore, the advantage of increasing the size of the solution subset shown to the DM in each iteration is made clear in those instances in terms of iterations and computational time.",An interactive multi-objective algorithm using preference cones,"[62188, 53033, 48740]",584,"[77, 111, 84]",4251,Multiobjective Mixed-Integer Linear Optimization,34,15,37,Multiobjective Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,33 [building - 306],"['Multi-Objective Decision Making', 'Programming, Mixed-Integer', 'Optimization Modeling']",WD-37
"In this talk we will present recent experiences of the CQIIA-MatNet center of the University of Bergamo [https://cqiia.unibg.it/it/ricerca/matnet]. MatNet is a center which is focused on teaching of mathematics and its applications and which develops collaborations between universities and schools. We will present the experience of the San Pellegrino Summer School on Mathematics and Artificial Intelligence and the seminars series devoted to high school teachers, organized in collaboration with Mathesis Bergamo under the umbrella of Piano Lauree Scientifiche [PLS]. ",The experience of the CQIIA-MatNet center of the University of Bergamo,[24015],455,"[34, 0]",4253,Experienced Routes for the Teaching Students' Problem,48,4,16,OR Education,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,19 [building - 116],['Education and Distance Learning'],MC-16
"As the global demand for sustainable water management solutions continues to escalate, the prioritization of Nature-Based Solutions [NBS] has emerged as a critical strategy for addressing water challenges. This study presents a comprehensive approach to prioritize NBS using Multiple Criteria Decision Analysis [MCDA] within the context of a water utility case study in São Paulo, Brazil. São Paulo, a megacity grappling with water scarcity and quality issues, serves as an ideal setting to explore the effectiveness of NBS in mitigating water-related challenges. Through an integrated MCDA framework, various criteria such as environmental impact, total cost, integration with existing infrastructures, flexibility, simplicity and transparency, supply safety, among others are evaluated to assess the suitability of NBS alternatives. The study employs a participatory approach involving stakeholders from diverse sectors to ensure transparency and inclusivity in the decision-making process. By synthesizing qualitative and quantitative data, the MCDA facilitates the identification of optimal NBS interventions tailored to the unique socio-environmental context of São Paulo. The findings of this study not only contribute to advancing knowledge on NBS prioritization methodologies but also offer practical insights for water utilities and policymakers seeking sustainable solutions to water management challenges in urban settings.","Prioritizing Nature-Based Solutions for Sustainable Water Management - A  water utility case study in São Paulo, Brazil",[61695],158,"[25, 26, 94]",4254,"How to support complex decisions. Negotiating the trade-off between Social, Environmental and Economic values 1",44,4,47,Multiple Criteria Decision Analysis,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,50 [building - 324],"['Decision Analysis', 'Decision Support Systems', 'OR in Environment and Climate change']",MC-47
"In various sectors like energy, finance, and supply chain management, decision-makers optimize their decisions using forecast-informed optimization. In particular, energy market participants base their decisions on price forecasts. In this context, Decision-Focused Learning [DFL] has emerged, where the downstream decision problem is explicitly considered in the training procedure of the forecaster. This forecasting model is typically a neural network, which is trained using a gradient descent-based procedure. A pivotal challenge in DFL training is ensuring that the optimal decisions are differentiable with respect to the input parameters, being the price forecasts. A promising solution involves implicit differentiation of KKT conditions, albeit this significantly slows down training since it requires solving many optimization problems with every forward pass during training. We propose an innovative workaround using a Machine Learning [ML] proxy to replace the optimization problem in the forward pass. This approach is applied to Energy Storage Systems [ESS] participating in day-ahead and real-time balancing markets. Applying duality theory to the ESS optimization program allows us to simplify the ML proxy model, limiting its output to the dual variable of the energy balance constraint.  We observe similar out-of-sample profit performance as traditional DFL methods, while reducing training time by 70% to 95%, illustrating the improved scalability of the proposed method.",Decision-focused learning with machine learning proxies for energy storage system optimization in energy markets,"[78939, 57455, 71332, 62333, 75814, 56837]",320,"[47, 82, 36]",4255,Machine Learning in Applied Optimization,14,8,03,Data Science Meets Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,1005 [building - 202],"['Forecasting', 'Optimal Control', 'Electricity Markets']",TB-03
We present a data-driven approach that integrates machine learning prediction models with optimization under uncertainty in the presence of contextual information by utilizing residuals from the learning models.  We discuss an application of this approach for making unit commitment decisions under uncertain net load conditions to improve short-term power system operations. We propose several enhancements to the basic methodology and assess their effectiveness on several case studies conducted using real-world data collected from the California and New York Independent system operators. Results show that the proposed approach can significantly improve the out-of-sample performance.  ,A Data-Driven Methodology for Contextual Unit Commitment Using Regression Residuals,"[55453, 79286]",535,"[136, 93, 66]",4256,Risk Averse and Contextual Stochastic Optimization,49,8,35,"Stochastic, Robust and Distributionally Robust Optimization","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,44 [building - 303A],"['Stochastic Optimization', 'OR in Energy', 'Machine Learning']",TB-35
"Adaptive reuse in architecture and urban planning refers to the repurposing of existing buildings for new uses. Several and diverse sustainability benefits are associated with this practice, including prevention of soil consumption and natural resources depletion, limiting construction and demolition waste production, and saving costs.
With specific reference to social outcomes, adaptive reuse of buildings can generate positive effects such as revitalizing local communities, valorizing cultural heritage, providing new services for the inhabitants, creating new jobs, to name a few. These effects are even more relevant in the case of reuse projects concerning public buildings, where there is a strong need to guarantee the legitimacy of the decisions made according to a social purpose.
The present contribution explores the evaluation of social impacts in adaptive reuse of public buildings, and it proposes a multidimensional framework based on Multi-Attribute Value Theory [Keeney and Raiffa, 1976] for the construction of a synthetic impact score to rank alternative projects.
",Evaluating social impacts of adaptive reuse of public buildings - a Multi-Criteria Decision Analysis approach,"[7119, 46433, 79146, 72128, 57936]",890,"[26, 139, 15]",4257,MCDA and urban planning 2,44,9,47,Multiple Criteria Decision Analysis,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,50 [building - 324],"['Decision Support Systems', 'Sustainable Development', 'Complex Societal Problems']",TC-47
"A two-sided market is an economic platform that enables direct interactions between two distinct customer groups, providing each other with network benefits. Matching is the fundamental function that ensures the viability and service quality of such markets. While the Gale-Shapley algorithm and its variants can be used to compute stable matchings for two-sided markets effectively, these methods typically require participants to rank all potential partners, posing a significant cognitive burden to them and rendering it impractical for large-scale markets involving hundreds or thousands of options on each side. This paper presents an efficient framework for computing approximately stable matchings in large-scale and dynamic market settings. Instead of explicitly eliciting preferences, we employ probabilistic choice behavior models trained on participants’ historical interaction data with the platform. These models serve as incremental preference elicitation tools, which adapt themselves through continuous machine learning techniques. Additionally, we devise procedures for robust matching in the presence of probabilistic and evolving preference information provided by the behavior models. We conduct a simulation study to experimentally quantify the impact of enforcing approximate stability within a dynamic crowd-sourced delivery services setting, utilizing preference survey data collected from crowd-sourced drivers.",Attaining Approximate Stability in Large Two-Sided Matching Markets,"[61735, 79313, 79318]",850,"[3, 9, 32]",4258,Simulation in economics II,77,5,43,"Agent-based Models in Management, Economic and Organisation Sciences","Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,99 [building - 306],"['Agent Systems', 'Auctions / Competitive Bidding', 'E-Commerce']",MD-43
"Water supply systems are critical infrastructure and will be forced to adapt to threats posed by climate change. In Literature optimization of these systems is often done for either steady state, or single source one year operation. Most studies focus on the water side of the operation while incorporating mostly limited energy aspects. Therefore, fully joint planning of water-energy systems is still incipient, as is operation under a fully renewable regime.
In this work, we use multi-objective optimization methods to model water-energy systems with a high temporary resolution. We focus on the operation of existing water supply systems and expansion planning to optimally incorporate renewable energies by striking the balance of less efficient operation against cheaper and cleaner energy sources and storage. This is done for different operation regimes, including a wide range of prediction horizons, from greedy short-term decision making comparable to current operation up to global [in time] optimization as a reference bound.
We validate the model of the water supply system using data provided by our German project partner, Landeswasserversorgung.
In conclusion, we show how the operation of existing water supply networks should change and how to adapt the network to integrate renewables in a combined water-energy system in preparation for our future work on long-term predictions using simulations of future climate scenarios.",Coordinated Optimization of Water-Energy System Operation,"[78917, 74115, 79272]",711,"[147, 94, 112]",4259,Environment and climate change,21,14,22,Energy Management,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,81 [building - 116],"['Water Management', 'OR in Environment and Climate change', 'Programming, Multi-Objective']",WC-22
"This paper investigates the integration of the Synthetic Control Method [SCM], a proven method to analyze an impact of policy interventions, and Data Envelopment Analysis [DEA], a well-known mathematical technique for measuring relative efficiency of decision-making units with multiple inputs and multiple outputs, to evaluate the effect of policy interventions on the efficiency of decision-making unit. Specifically, this paper applies these methodologies to assess the efficiency of Serbian banks over a given period, focusing on the influence of a merger between two principal banks on their operational performance, and answering a counterfactual question whether efficiency of a bank would be the same if a merger did not happen. The study demonstrates that SCM can be an effective tool for analyzing the repercussions of policy measures on decision-making unit efficiency. In other words, we suggest combining these two methods in the future, in order to conclude whether an intervention really has an impact on efficiency. ",Adding counterfactual interpretation to DEA,"[14213, 77536, 69366]",945,"[24, 0]",4260,DEA methodological developments I,89,14,48,Data Envelopment Analysis and its Application,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,60 [building - 324],['Data Envelopment Analysis'],WC-48
"Student outcomes in an undergraduate management science course are hypothesized to depend on prior preparation and student engagement.  A predictive model is presented that combines these factors using data gathered from large, online course sections of business students.  The results help to inform instructors of actions to take to create equity of opportunity in the STEM classroom.",Towards Equity in Teaching Management Science,[65062],791,"[34, 91, 92]",4261,OR Education I,48,2,16,OR Education,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,19 [building - 116],"['Education and Distance Learning', 'OR in Distance Learning', 'OR in Education']",MA-16
"We consider a queueing system with competitive revenue maximizing servers who charge for priority. Customers are strategic and decide which server to seek service from and whether to pay for priority. Through an analysis of the equilibrium in this sequential two-stage game, we demonstrate that the system exhibits self-regulation. Specifically, contrary to systems lacking priority mechanisms, our findings reveal that socially optimal joining rates are attained.",Priority queues with competitive servers,"[68606, 71215]",896,"[121, 50]",4262,"Game Theory, Solutions and Structures X",88,13,36,"Game Theory, Solutions and Structures","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,32 [building - 306],"['Queuing Systems', 'Game Theory']",WB-36
"Empirical evidence identifies a large proportion of retailers using algorithmic pricing for making price decisions. This opens several questions surrounding tacit collusion and its detrimental effects on competition and consequently consumers in the form of supracompetitive prices. As an initial step towards answering whether or not tacit collusion is happening among pricing algorithms, one need to first identify which retailers are using such technology. We have defined two new statistical markers for identifying pricing algorithms in low-resolution data. One of the markers adapts counting the number of price changes by instead computing the average rate of price change per retailer over all its products both within and across countries. The other, novel, marker computes the persistence of a retailer in the marketplace. Using robust standardization of the two markers and applying a heuristic based on appropriate thresholds of these standardized markers, we have identified varying numbers of retailers within and between countries for the product categories employing algorithmic pricing. We have therefore created a new heuristic for identifying pricing algorithms in low-resolution data sets. Furthermore, this heuristic allows for further research in empirically identifying whether or not tacit collusion exists among algorithms and the effects such pricing has on the profitability of the retailers who employ such technology compared to those who do not.",Algorithmic pricing and collusion on a price comparison website,"[62021, 36137, 79295, 79392]",356,"[7, 32, 5]",4263,Recommender systems,17,2,31,Analytics,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,54 [building - 208],"['Analytics and Data Science', 'E-Commerce', 'Algorithms']",MA-31
"In this work, we study a complex variant of the feeder network design problem [FNDP] arising in the field of maritime transportation. The problem is about designing routes for ship services between several shipping ports, to deliver cargoes between these ports based on a fixed weekly demand. In many cases, this is done using disjoint networks of ships services, called feeder networks.

Each feeder network is connected to a main port by a long ship service called a mother route. Some of the paired demands are delivered by the mother routes themselves, and the remaining demands using daughter routes, which are shorter ship services that branch off from some of the ports in the mother routes.

In the variant of the FNDP studied, we also allow split deliveries of the demands and the rejection of some of the demands. There are time constraints on the routes and on the travel time for delivery of each cargo. We formulate a mathematical model for the problem and propose a matheuristic framework for obtaining high quality solutions.",Matheuristic for Feeder Network Design Problem With Optional Paired Demands and Split Delivery,"[79260, 58501]",785,"[14, 70, 145]",4264,Heuristics for Vehicle Routing 2,5,15,64,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S16 [building - 101],"['Combinatorial Optimization', 'Maritime applications', 'Vehicle Routing']",WD-64
"Emergency room overcrowding remains a significant challenge in healthcare systems worldwide, often involving several-hour-long waiting times for patients and creating a stressful, intense environment for physicians and nurses. Traditional strategies, such as redirecting arriving ambulances to alternative emergency departments, have limitations in fully optimizing patient flow and resource allocation. This study presents a novel approach to address this issue by systematically redirecting patient demand to specific emergency departments, integrating real data analysis with system modeling. The primary objectives include minimizing system time for patients, ensuring equitable emergency room crowding levels, and maximizing emergency physician and nurse welfare by accommodating their preferences regarding location and shift schedules across a network of public emergency medical centers within the city. By considering factors such as travel times, patient volume, and service durations, the proposed methodology aims to optimize system performance using multi-objective computing budget allocation for non-dominated sorting particle swarm optimization. Additionally, the study explores the use of genetic programming as an alternative approach. This research offers a comprehensive solution to the complexities of patient flow management in emergency departments, with promising implications for practical implementation in healthcare systems.",Optimizing Patient Flow and Personnel Welfare in Emergency Departments - A Multi-objective Approach,[68932],594,"[56, 131, 77]",4265,ED logistics,3,12,10,OR in Health Services [ORAHS],"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,11 [building - 116],"['Health Care', 'Simulation', 'Multi-Objective Decision Making']",WA-10
"Insect farming has recently gained notoriety as a more sustainable alternative in the food and feed industry. Insects need less land, water, and feed than conventional livestock and can be fed on residual by-products from the food transformation industry that would otherwise be considered waste. By capitalizing on these factors, insect farming demonstrates a substantially smaller environmental footprint compared to conventional protein sources like cattle rearing and intensive agriculture. It is also important to design a sustainable supply chain, when defining the facility locations and suppliers selected. 
Thus, this work studies the network design problem of an insect-based supply chain, which needs to account for the by-products selected as feed and their distinct conversion rate into protein and processing costs. 
",Network Design of an Insect-based Supply Chain,"[43398, 73154]",919,"[79, 84, 100]",4266,Sustainable food supply chains,18,4,23,"Circular Economy, Remanufacturing and Recycling ","Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,82 [building - 116],"['Network Design', 'Optimization Modeling', 'OR in Sustainability']",MC-23
"Computing, together with computational tools supported by machine learning, plays a key role in the creation of national genomic maps by enabling the analysis, interpretation and use of vast amounts of genetic information from multiple sources. Operations research methods are essential for understanding the genetic diversity of populations and for advances in personalised medicine and disease inheritance research. They influence the modelling processes of genome sequencing, comparison with reference genomes, identification of genetic variants and assessment of their functional consequences. Data processing enables the integration of genetic information from different sources, enabling the understanding of the relationships between genetic variation and phenotypic traits and diseases, as well as the identification of population structure and the analysis of genetic relationships between individuals in a population. The Genomic Map of Poland is a set of unique databases of genomic sequences reflecting the genetic picture of the Polish population, together with computational and visualisation tools that allow the analysis and interpretation of large amounts of genetic data and the integration of different sources of information, leading to an understanding of the genetic diversity of the population and the identification of relationships between genes and phenotypic traits and diseases.","Genomic Map of Poland versus artificial intelligence - opportunities, challenges and threats","[11807, 5423, 79328, 5390]",381,"[17, 7, 18]",4268,Advancements in AI and Genomics - Bridging Technology and Biology for Future Healthcare Solutions,2,10,20,"Computational Biology, Bioinformatics and Medicine","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,45 [building - 116],"['Computational Biology, Bioinformatics and Medicine', 'Analytics and Data Science', 'Computer Science/Applications']",TD-20
"Data-driven multicriteria analysis is a challenging research area integrating data analysis from two sources - first, raw data used for development of a model or a set of alternatives, and second, the data provided by the model simulation or its interactive analysis. The presentation focuses on the second issue.

First stage of interactive multicriteria decision analysis generates several sets of solutions. The corresponding data can be then analyzed for supporting selection of a final decision, as well as providing information guiding specification of parameters of the user preferences in further interactions. Applying data analysis in the process of multicriteria analysis significantly improves the decision-making process and reduces the required computation time. Use of the cluster-based surrogate in discrete decision-space helps in reducing the discrete decision space dimension, and thus the computational resources needed for solving the underlying optimization tasks.

The effectiveness of the approach is illustrated by analysis of an energy model supporting decisions of a system operator of an onshore wind farm equipped with a hydrogen energy storage. The objective is to maximize the cumulative profit by deciding on - [1] a constant hourly amount of electricity to be delivered, [2] configuration of the storage system components, and [3] control of flows to/from the storage to deal with the variable renewable energy generation.   ",Integrating multicriteria analysis with advanced data analysis methods,[19892],886,"[77, 7, 93]",4271,MCDA in energy,44,13,47,Multiple Criteria Decision Analysis,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,50 [building - 324],"['Multi-Objective Decision Making', 'Analytics and Data Science', 'OR in Energy']",WB-47
"Solving strategic-level decision problems necessitates integrating advanced data analytics and decision support models, which is particularly crucial in the data-driven Industry 4.0. This paper offers an overview of strategic decision challenges across industries, exploring various approaches, optimization problems, and solving techniques. Rather than following the conventional path of reviewing solely peer-reviewed academic publications, this study also systematically surveys industry literature. The nuanced needs and preferences driving strategic decision-making processes are revealed with a deliberate emphasis on decision-makers perspectives over intricate tool presentations. Through this dual review of academic and industry literature, the paper delineates the utilization landscape of decision support methods and tools, elucidating the rationale behind the choices made. This approach illuminates prevailing practices and reveals latent requirements, empowering decision-makers with the insights needed for informed method and tool selection.",Decision Support Models for Strategic Decision Making in Industry 4.0,[60848],659,"[26, 27, 137]",4273,Planning Techniques for Decision Support,45,15,45,Decision Support Systems,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,30 [building - 324],"['Decision Support Systems', 'Decision Theory', 'Strategic Planning and Management']",WD-45
"Design of large healthcare facilities like hospitals and nursing homes that can accommodate patients with wide-ranging disabilities is a challenge that is poorly understood. More elusive is design for healthcare workers with disabilities to prevent workplace injury. Across the vast range of facility types and designs, the posed problem becomes even more complex.  Here, we study the implication of facility design for healthcare workers suffering with low vision, and develop and validate a machine-learning driven approach for optimal facility redesign. 
We consider 7 facilities of varying sizes in the U.S. and collect a corpus of observational and telemetric data. We propose a method for fusing the output of two sets of models – a series of ML models to predict facility shortcomings from feedback and fall incidence data; and a computer vision model fine tuned on images of each facility to infer and predict facility features, locations, and workflows – to classify design choices that could again pose meaningful dangers to visually impaired employees of each facility. After conducting a series of real-world offline experiments to validate this approach [by comparing model’s predictions to the labels of an expert] and refine the model, we characterize the particular sets of conditions, and size and range of facility types, workforce composition profiles, and work conditions under which different variants of our method most closely replicate an expert’s assessment.
",Machine Learning-Guided Optimal Design of Facilities for Healthcare Worker Safety,[79228],966,"[66, 43, 56]",4274,Machine learning and analytics in healthcare,3,3,15,OR in Health Services [ORAHS],"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,18 [building - 116],"['Machine Learning', 'Facilities Planning and Design', 'Health Care']",MB-15
"Anticipating the strategies of potential attackers is crucial for protecting critical infrastructure. We can represent the challenge of the defenders of such infrastructure as a Stackelberg security game. The defender must decide how to allocate limited resources to protect specific targets, aiming to maximize their expected utility [such as minimizing the extent of damage] and considering that attackers will respond in a way that is most advantageous to them.

We present novel and stronger valid inequalities to find a Strong Stackelberg Equilibrium in both Stackelberg games and Stackelberg security games. We also present a new Stackelberg security game that aims to protect targets with a defined budget. We use branch-and-price in this new game to show that our approach outperforms the standard formulation in the literature, and we conduct an extensive computational study to analyze the impact of various branch-and-price parameters on the performance of our method in different game settings. ","Branch-and-price with novel cuts, and a new Stackelberg Security Game",[69008],895,"[50, 5, 111]",4276,"Game Theory, Solutions and Structures IX",88,12,36,"Game Theory, Solutions and Structures","Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,32 [building - 306],"['Game Theory', 'Algorithms', 'Programming, Mixed-Integer']",WA-36
"In this work we focus on linear parametric programming, where the constraint matrix is an affine function of an external parameter lambda; the goal is to assess the impact of lambda on the objective value of the optimal solutions.

Computing the objective for multiple values of lambda is usually cumbersome and involves solving the related linear program in full multiple times.

We show that by reformulating the problem in term an optimal basis for a given lambda*, we can use an eigendecomposition and Schur’s decomposition to obtain an upper bound for the objective value for other values of lambda. Moreover, the bound is actually tight when the basis stays optimal.

Given a solution for lambda*, the bound can be computed in quadratic time [in the number of constraints] for another lambda. The range for the bound is exact can also be computed in quadratic time - this opens the way to an iterative algorithm for computing the full function, changing base each time the current one becomes invalid.
",Efficient exact recomputation of linear modifications of the constraint matrix in Linear Programming,"[78892, 74052, 74437, 54116]",192,"[110, 0]",4279,Modern techniques for network optimization,68,9,38,"Conic Optimization - Theory, Algorithms, and Applications","Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,34 [building - 306],"['Programming, Linear']",TC-38
"Regulators in Europe are pushing for increased synchronization between European electricity markets. While this market coupling promises higher market efficiency and welfare gains, it also presents challenges for market administrators and grid operators who must manage congestion between market areas. 

In this paper, we propose several novel hybrid pricing models for congestion management in electricity markets. Hybrid models manage congestion by allowing certain market areas to use nodal pricing and others to use zonal pricing and have been previously shown to offer significant welfare improvements over zonal pricing models. We present two classes of hybrid models - simultaneous and sequential. Simultaneous models clear the market in a single step where nodal and zonal pricing areas are both cleared at once. Sequential models first clear a fully zonal model, fix the resulting power flow quantities between market areas, and then re-clear the nodal pricing areas in a second modelling step. We also present several subvariants of these models where we aggregate, partially disaggregate, and fully disaggregate the transfer capacities of lines connecting market areas.
",Hybrid Electricity Market Models - Combining Zonal and Nodal Pricing for Congestion Management,"[5551, 79278, 3516]",644,"[36, 0]",4281,Electricity Market Design,22,3,09,Energy Markets,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,10 [building - 116],['Electricity Markets'],MB-09
"Labour scheduling takes on special relevance in the healthcare field as most professionals work alternate shifts, which makes it difficult to balance work and personal life. One of the common practices is the allocation of schedules according to workers' preferences for each planning period. This results in models where professionals are assigned shifts in order to maximise their preferences while taking into account labour constraints. Usually these models include such constraints in their formulation, which results in excessively complex models. Thus, they can only be applied in very small instances and require heuristic and metaheuritic techniques to solve real instances such as Variable Neighbourhood Search, Simulated Annealing, Tabu Search, Iterated Search and Genetic Algorithms, or even hybrid approaches, mate-heuristic or hyperheuristic strategies. In this work we propose a strategy applied to a real case of an elderly people's home, which consists of the following steps - a] to generate all possible sets of shifts [patterns] that can be assigned to a professional that meet the labour constraints; b] to propose a model for assigning professionals to patterns. The labour constraints are satisfied, even if they do not appear explicitly, and the resulting model can be solved exactly on real instances of large size. This efficient strategy maximises the satisfaction of healthcare staff preferences, improving their productivity and the quality of patient care.",Labour scheduling for healthcare workers using efficient strategies,"[79147, 10441, 62763, 10443]",607,"[129, 56]",4285,Staffing and workforce planning and scheduling,3,9,15,OR in Health Services [ORAHS],"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,18 [building - 116],"['Scheduling', 'Health Care']",TC-15
"
In one-way flexible car-sharing systems, customers can drop-off the rented car at any location inside the operation area of the company and price of the rental is proportional to distance or duration of the rental. Rules of this system result in a supply and demand imbalance since supply of each zone changes dynamically. High-demand zones tend to have lower supply, and low-demand zones tend to have higher supply. Therefore, car-sharing companies relocate the cars between zones to match the supply and demand. This relocation process is costly and requires optimization for a profitable business. In this study, we formulate a two-stage stochastic programming model to optimize the relocation decisions in a dynamic one-way flexible car-sharing system. The model decides inventory levels of each zone at the beginning of each time period, number of cars to be relocated between each pair of zones, and unsatisfied requests at each zone to minimize total cost of relocation and lost sales.This model is very effective for small and average size instances. To solve the large instances, we developed an approximate clustering method where zones are clustered into multiple clusters. This approximate clustering method can solve instances up to 40 zones and 36 time periods. To enhance the implementation, we have also proposed an extended model where the staff movement was taken into account. Results show that inventory levels at each zone tend to reach a steady-state.
",A Stochastic Shared Car Relocation Problem to Optimize Long Term Strategic Inventory Decisions,[79284],526,"[136, 143, 61]",4288,Methods and models for sustainable transport solutions,6,7,56,Transportation,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,S04 [building - 101],"['Stochastic Optimization', 'Transportation', 'Inventory']",TA-56
"Offering a generic approach to obtaining both upper and lower bounds, decision diagrams [DDs] are becoming an increasingly important tool for solving discrete optimization problems. In particular, they provide a powerful alternative to other well-known generic bounding mechanisms such as the LP relaxation. A standard approach to employ DDs for discrete optimization is to formulate the problem as a Dynamic Program and to compile a DD top-down in a layer-by-layer fashion. To limit the size of the resulting DD and to obtain bounds, one typically imposes a maximum width for each layer which is then enforced by either merging nodes [resulting in a relaxed DD providing a dual bound] or by dropping nodes [resulting in a restricted DD providing a primal bound]. The quality of the DD bounds obtained from this top-down compilation heavily depends on the heuristics used for the selection of the nodes to merge or drop. While it is sometimes possible to engineer problem-specific heuristics for this, a generic approach relies on sorting the layer’s nodes based on objective function information. In this talk, we propose a generic and problem-agnostic approach that relies on clustering nodes based on the state information. In a set of experiments with different knapsack and scheduling problems, we show that our approach outperforms the classical generic approach, and often achieves drastically better bounds both with respect to the size of the DD  and the time used for compiling the DDs.",Using Clustering to Strengthen Decision Diagram Bounds for Discrete Optimization,"[17038, 72586]",730,"[14, 66]",4289,Optimization and Machine Learning - Methodological Advances,14,9,03,Data Science Meets Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,1005 [building - 202],"['Combinatorial Optimization', 'Machine Learning']",TC-03
"Multi-class data classification is a supervised machine learning problem that involves assigning data to multiple groups. Thanks to the recent advances in computation power and solution algorithms, optimization of learning methods became feasible. Mixed-Integer Linear Programming [MILP] models have already been proposed for Hyperbox, Decision Tree and Polyhedral Region Classifiers which are among the methods that split the data space into disjoint subspaces. In this study, we use cluster analysis as an aid to reduce the complexity of MILP learning models. Clustering algorithms partition the data points into dissimilar groups while the points within each group rest similar. We cluster sample sets of each class and modify the MILP model for cluster assignment replacing the samples. Alternatively, we generate low number of clusters and remove overlapping samples to produce disjoint subspaces. We present the proposed approach on Polyhedral Region Classifiers.",Clustering to Aid Optimization Based Learning Methods in Multi-Class Data Classification,[36941],730,"[66, 0]",4291,Optimization and Machine Learning - Methodological Advances,14,9,03,Data Science Meets Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,1005 [building - 202],['Machine Learning'],TC-03
"Generative Artificial Intelligence [AI] presents a compelling opportunity for advancing digital service platforms by enabling the creation of dynamic, personalized content and experiences. This paper aims to explore and analyze the critical success factors [CSFs] essential for the effective integration of generative AI within digital service platforms. Through an extensive review of existing literature and expert insights, key factors influencing the successful implementation of generative AI in digital service contexts were identified. Subsequently, employing qualitative analysis methodologies, a robust framework of CSFs was developed, encompassing aspects such as algorithmic sophistication, data quality, user engagement, platform scalability, and ethical considerations. Moreover, the study investigates the intricate interplay between these factors to elucidate their collective impact on the performance and sustainability of digital service platforms leveraging generative AI capabilities. The findings underscore the significance of addressing diverse challenges inherent in the adoption of generative AI, including algorithmic biases, privacy concerns, and user acceptance. This research contributes valuable insights into the critical factors shaping the effective deployment of generative AI in digital service platforms, offering actionable guidance for platform developers, service providers, and policymakers seeking to harness the potential of this transformative technology.",Leveraging Generative AI for Enhanced Digital Service Platforms - A Comprehensive Analysis of Critical Success Factors,"[79209, 79287, 79333]",907,"[5, 49, 28]",4292,"MCDA and Composite Indicators - Issues, Advances and Applications 2",44,15,44,Multiple Criteria Decision Analysis,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,20 [building - 324],"['Algorithms', 'Fuzzy Sets and Systems', 'Developing Countries']",WD-44
"We consider a firm that provides offline services through many branches to a population spread over a large geographic area. The firm is interested in opening additional satellite facilities in addition to its branches to improve its reach and attract new customers. Such facilities are also intended to relieve existing branches of low-value services so that they can focus on activities more valuable to the firm. Based on data available from a large bank in India, we create a model for deciding which locations are suitable for adding more satellite facilities. The model considers the footfall data of the existing branches, their geographical information and transactional data. ",Extending service locations optimally,[61138],769,"[12, 130, 64]",4296,Applications of Location Methods,29,12,61,Locational Analysis,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,S10 [building - 101],"['Capacity Planning', 'Service Operations', 'Location']",WA-61
"We introduce an innovative methodology for social network analysis that clusters similar criminal activities without initially identifying the perpetrators involved. This approach leverages an optimization model to facilitate the optimal grouping of past crimes, utilizing a newly developed similarity measure. Drawing from existing models where a delinquent is represented as a node and a mutual crime as a connecting link, we propose an “inverted social network” framework. In this novel network, each node symbolizes a crime event, and a link between two nodes signifies a similarity exceeding a predetermined threshold. This methodology employs a recently introduced similarity measure, specifically designed to encapsulate the unique characteristics inherent to this application domain.
An optimization model is utilized to aggregate crimes based on their similarity, showcasing the method’s applicability across various instances where the involved criminals are yet to be identified. This technique proves invaluable for deciphering patterns within crime databases and serves as an investigative tool for examining recent crimes with minimal information on the culprits. We will present the most recent findings from applying this methodology in collaboration with the Chilean Prosecutor's Office, illustrating its potential and outlining avenues for future exploration.",Optimization in Social Networks to Identify Related Crimes ,"[11616, 57734, 70827, 18420, 72370, 79387, 79388]",62,"[132, 7, 84]",4297,Crime Analytics,17,3,31,Analytics,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,54 [building - 208],"['Social Networks', 'Analytics and Data Science', 'Optimization Modeling']",MB-31
"Hexaly Optimizer is a new kind of global optimization solver. Its modeling interface is nonlinear and set-oriented. It also supports user-coded functions, thus enabling black-box optimization and, more particularly, simulation optimization. In a sense, Hexaly APIs unify modeling concepts from mixed-linear programming, nonlinear programming, and constraint programming. Under the hood, Hexaly combines various exact and heuristic optimization methods - spatial branch-and-bound, simplex methods, interior-point methods, augmented Lagrangian methods, automatic Dantzig-Wolfe reformulation, column and row generation, propagation methods, local search, direct search, population-based methods, and surrogate modeling techniques for black-box optimization.

Regarding performance benchmarks, Hexaly distinguishes itself against the leading solvers in the market, like Gurobi, IBM Cplex, and Google OR Tools, by delivering fast and scalable solutions to problems in the spaces of Supply Chain and Workforce Management like Routing, Scheduling, Packing, Clustering, and Location. For example, on notoriously hard problems like the Pickup and Delivery Problem with Time Windows or Flexible Job Shop Scheduling with Setup Times, Hexaly delivers solutions with a gap to the best solutions known in the literature smaller than 1% in a few minutes of running times on a basic computer.","Hexaly, a new kind of global optimization solver",[45600],701,"[134, 72, 52]",4299,Optimization Solvers,76,5,30,Software for Optimization,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,53 [building - 208],"['Software', 'Mathematical Programming', 'Global Optimization']",MD-30
"Amid growing concerns about climate change and the urgency to lower emissions, there's a significant shift towards sustainable public transportation. Electric bus fleets emerge as a pivotal solution to decrease reliance on fossil fuels and reduce greenhouse gas emissions.

This study aims to integrate an electric fleet into a public transit system transitioning from diesel to electric propulsion. The goal is to formulate an efficient charging schedule strategy, optimizing charger and bus usage.

Our methodology involves data collection from a transitioning company, developing an optimization model considering operational constraints like current fleet schedules, charger availability at depots, electric bus range limits, and electrical grid capacity.

Using mixed-integer programming, we aim to create optimal charging schedules and amounts, reducing the need for numerous chargers and daily charging, thereby cutting costs, boosting operational efficiency, extending battery life, and ensuring efficient energy use at depots.

This approach promises significant implications for sustainable transit, showcasing how careful planning and technological solutions can address the logistical challenges of electrifying public transportation fleets.",An Analytical Approach to Designing an Efficient Strategy for Scheduling Chargers and Electric Charging Times for a Public Bus Company.,"[25835, 79288]",815,"[119, 143, 109]",4300,Electric Busses,85,8,51,Public Transport Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,M5 [building - 101],"['Public Local Transportation Systems', 'Transportation', 'Programming, Integer']",TB-51
"We examine alternative approaches for dealing with excess demand through the lens of a retailer whose risk-attitude is modeled explicitly. With representative risk-neutral and risk-averse newsvendor models, we pursue a comparison between two basic stockout policies [lost-sales versus backorders] in terms of financial metrics as well as metrics that relate to inventory availability. Depending on the risk attitude and particular cost parameter relationships, we explore settings that reveal the superior way for handling stockout scenarios. We discover analytical takeaways that hold in general and report numerical findings that reveal resiliency of the superior stockout policy. 


",On the Impact of Risk-Attitude when Dealing with Stockouts,[57676],910,"[61, 138, 135]",4303,Retail Inventory Management III,30,14,61,Retail Operations,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S10 [building - 101],"['Inventory', 'Supply Chain Management', 'Stochastic Models']",WC-61
"Hexaly Optimizer is a new kind of global optimization solver. Its modeling interface is nonlinear and set-oriented. It also supports user-coded functions, thus enabling black-box optimization and, more particularly, simulation optimization. In a sense, Hexaly APIs unify modeling concepts from mixed-linear programming, nonlinear programming, and constraint programming. 
Regarding performance benchmarks, Hexaly distinguishes itself against the leading solvers in the market, like Gurobi, IBM Cplex, and Google OR Tools, by delivering fast and scalable solutions to problems in the spaces of Supply Chain and Workforce Management like Routing, Scheduling, Packing, Clustering, and Location. For example, on notoriously hard problems like the Pickup and Delivery Problem with Time Windows or Flexible Job Shop Scheduling with Setup Times, Hexaly delivers solutions with a gap to the best solutions known in the literature smaller than 1% in a few minutes of running times on a basic computer.
In addition to the Optimizer, we provide an innovative development platform called Hexaly Studio to model and solve rich Vehicle Routing and Job Shop Scheduling problems in a no-code fashion. The user can define its problem and data, run the Optimizer, visualize the solutions and key metrics through dashboards, and deploy the resulting app in the cloud – without coding. This web-based platform is particularly interesting for educational purposes; it is free for faculty and students.  
",Hexaly Studio - a no-code platform for mathematical optimization,"[22910, 45600]",432,"[52, 14, 72]",4305,Modeling tools,76,9,30,Software for Optimization,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,53 [building - 208],"['Global Optimization', 'Combinatorial Optimization', 'Mathematical Programming']",TC-30
"We consider classes of dynamic decision problems where an investor maximizes utility but faces random preferences. We consider three versions of the problem. In one version, the investor optimizes expected utility where the expectation is taken with respect to both financial and preference uncertainty. That is based on Steffensen and Søe [2023]. Another version is based on certainty equivalents. We tackle the time-consistency issues arising from that formulation by applying the equilibrium theory approach. The case where the investor learns nothing about his preferences as time passes is based on Desmettre and Steffensen [2023]. We also discuss another case where risk aversion is an observed stochastic process.

References - 

Desmettre, S., & Steffensen, M. [2023]. Equilibrium investment with random risk aversion. Mathematical Finance, 33, 946–975. https://doi.org/10.1111/mafi.12394

Steffensen, M., & Søe, J. [2023]. Optimal consumption, investment, and insurance under state-dependent risk aversion. ASTIN Bulletin - The Journal of the IAA, 53[1], 104-128. doi:10.1017/asb.2022.25","Optimal consumption, investment, and insurance under state-dependent risk aversion",[77092],280,"[45, 20, 19]",4306,Decision making in Insurance and Pensions,74,3,57,Modern Decision Making in Finance and Insurance,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S06 [building - 101],"['Financial Modelling', 'Control Theory', 'Continuous Optimization']",MB-57
"Time series shapelets are a state-of-the-art data mining technique for supervised classification tasks in time series domains. Shapelets are time series subsequences that retain the most discriminating power in time series. The main advantage of shapelet-based methods is their great interpretability. We propose a novel Mixed-Integer Programming model to optimize shapelets discovery based on optimal binary decision trees. Computational results for a large class of datasets show that our approach achieves performance comparable with state-of-the-art shapelets-based classification methods. To the best of our knowledge, this is the first approach based on optimal decision tree induction for time series classification.",Optimal Shapelets Tree for Time Series Interpretable Classification,"[72032, 73058]",207,"[66, 111, 84]",4308,Combinatorial Optimization for Machine Learning,64,4,26,Combinatorial Optimization,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,012 [building - 208],"['Machine Learning', 'Programming, Mixed-Integer', 'Optimization Modeling']",MC-26
"A linking pin organization is a structure in which relations between members of the same section are added to a pyramid organization where there exist only relations between each superior and his direct subordinates. This study proposes a model of adding relations between a delegate and every other member of the same level in a complete K-ary linking pin organization structure where every pair of nodes which have the same parent in a complete K-ary tree is adjacent. When edges between one node and every other node of depth N in a complete K-ary linking pin organization structure of height H are added, the total shortening distance which is the sum of shortening lengths of shortest paths between every pair of all nodes by adding edges is formulated. An optimal depth N such that the communication of information between every member in the organization becomes the most efficient is obtained by maximizing the total shortening distance.",Adding Relations between a Delegate and Every Other Member of the Same Level in a Complete K-ary Linking Pin Organization Structure,[19032],876,"[84, 53, 14]",4310,Optimization issues on graphs II [Contributed],64,15,29,Combinatorial Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,157 [building - 208],"['Optimization Modeling', 'Graphs and Networks', 'Combinatorial Optimization']",WD-29
"There is a family of related optimization problems, commonly denoted as Order Batching Problems, which are tackled as part of the supply chain management. This family, groups those problems consisting in the optimization of the picking process in a warehouse, when the picking policy follows a batching strategy. The batching strategy consists of grouping the items into batches before starting the picking process. The picking of the items within the same batch is then performed on a single picking route and all items in the batch are picked together. Tackling an Order Batching Problem usually involves solving several activities, necessary to perform the picking process, such as - batching, routing, assigning, sequencing, or waiting, among others. In this review we focus on manual picking systems performed in rectangular-shaped warehouses, which is probably the most common warehouse configuration in the literature. We review and classify the problems in the literature on a new taxonomy, examining the most outstanding heuristic and metaheuristics algorithms proposed to tackle the different tasks of the problem. Finally, we outline future research directions related to the studied topic.",A review of order batching optimization problems,"[58750, 79298]",878,"[138, 65, 74]",4311,Heuristic Algorithms for Combinatorial Optimization Problems II [Contributed],64,15,52,Combinatorial Optimization,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,8003 [building - 202],"['Supply Chain Management', 'Logistics', 'Metaheuristics']",WD-52
"This study explores tramp shipping operations, where efficient fuel management is important for financial success, environmental responsibility, and global sustainability goals. The paper focuses on integrating speed-dependent non-linear fuel consumption curves into the basic tramp ship routing and scheduling problem, recognizing the need for precise fuel consumption estimation. A non-linear mixed integer programming model is proposed for routing, scheduling, and speed optimization. The model is to study the impact of variable elasticities across different speed intervals, aligning closer to the complexities of a per vessel-specific fuel consumption. Finally, we will examine the impact of introducing speed and specially speed-dependent fuel-speed elasticities and calculate the time charter equivalent as an economic measure, based on the outputs of the model with constant elasticity and the model with speed-dependent elasticity.",A Variable Fuel Consumption Curve in the Tramp Ship Routing and Scheduling Problem,"[79293, 79471, 50651]",975,"[65, 70, 72]",4313,Transportation Network Modelling and Optimization II,6,3,55,Transportation,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,S02 [building - 101],"['Logistics', 'Maritime applications', 'Mathematical Programming']",MB-55
"The efficiency of air cargo terminals is crucial for the success of the supply chain. Freight forwarders deliver cargo to cargo terminal operators [CTOs] who manage handling, customs, and loading onto airliners. Delays in truck unloading can cause bottlenecks, impacting aircraft departures. Research focuses on manpower, truck scheduling, cargo routing, and aircraft loading. Existing studies address truck scheduling challenges, adapting insights from container terminals. This study proposes a multi-server queuing system to optimize truck arrivals and unloading bay usage, aiming to minimize costs. Methodology involves modeling truck arrivals and service times, with results expected to reduce costs and alleviate congestion at unloading bays. Limitations include potential applicability only to specific airports.",Optimizing Air Cargo Terminal Operations - A Model for Alleviating Unloading Bay Bottlenecks through Truck Scheduling,"[79294, 77373]",650,"[4, 74, 5]",4314,Airline Applications II,6,5,55,Transportation,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,S02 [building - 101],"['Airline Applications', 'Metaheuristics', 'Algorithms']",MD-55
"This study develops and analyzes non-Euclidean gradient methods for minimizing relatively smooth nonconvex functions using Bregman distances. We explore both constant and dynamic step-size strategies, examining their convergence and complexity, particularly under relative strong convexity.  We demonstrate the methods' effectiveness in matrix factorization, matrix completion, and image inpainting on real datasets, validating our theoretical findings.
",Non-Euclidean Gradient Methods for matrix completion,"[75022, 75023, 66489]",507,"[81, 21, 5]",4315,Convex optimization algorithms,70,12,41,Nonsmooth Optimization,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,97 [building - 306],"['Non-smooth Optimization', 'Convex Optimization', 'Algorithms']",WA-41
"Harnessing the potential of machine and deep learning architectures within Predictive Process Analytics [PPA] has yielded promising results, particularly in predicting future process outcomes. However, the opacity inherent in these algorithms poses a significant challenge for human decision-makers, limiting their understanding of the rationale behind the predictive outputs. In response, counterfactual explanations, serving as human-understandable ‘what if’ scenarios, offer intuitive insights into the decision-making process behind undesirable predictions. Nevertheless, generating counterfactual explanations encounters specific challenges when dealing with the sequential nature of business process cases in PPA. To address this, our paper introduces REVISED+, a novel data-driven approach aimed at generating more feasible and plausible counterfactual explanations tailored to the complexities of sequential process data. The counterfactual generation algorithm is guided through high-density regions of the process data distribution, and Declare language templates are used to capture sequential patterns among the process activities. This enables REVISED+ to improve the feasibility and plausibility of the generated counterfactual explanations. Finally, we assess the validity of our counterfactuals with metrics that define the key properties of a good counterfactual. 
",Leveraging Manifold Learning for Constrained Counterfactual Explanations in Process Outcome Prediction,"[75662, 74401]",124,"[8, 66, 7]",4319,XAI in Business Processes,15,4,27,Mathematical Optimization for XAI,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,047 [building - 208],"['Artificial Intelligence', 'Machine Learning', 'Analytics and Data Science']",MC-27
"Most studies on condition-based maintenance optimization consider the deterioration process of an industrial system to be known. In contrast, we make the more realistic assumption of an unknown deterioration process and develop approaches for determining when to carry out maintenance based on limited obtained condition data. We also assume that the parametric form of the deterioration process is unknown; our approaches are therefore fully data-driven. We analyze how the quality of the decisions depends on the amount of data that is available.",Data-Driven Condition-Based Maintenance Optimization,[37356],797,"[135, 0]",4321,Reliability Models,50,13,39,Stochastic Modelling,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,35 [building - 306],['Stochastic Models'],WB-39
"Location selection and assignment problems, which are of great importance in healthcare systems, are strategic decision problems. In location selection, topics such as the capacity of the sectors, the market area where the sector is located, the performance of the system and efficiency affect the decision maker's choice. This study initializes to determine the location of the service center where the most preferred hemodialysis treatment is going to be offered in the treatment of Chronic Renal Failure [CRF], which is increasing rapidly, and on the assignment of patients to hemodialysis centers. Decision problems have been identified by different decision makers regarding CRF, which has morbidity and mortality values. Among the decision problems listed from the perspective of different decision makers, the mathematical model proposal for the new hemodialysis location determination determined as the subject of the study. In the problem, it is aimed to optimize service access to hemodialysis patients with the hemodialysis center planned to be opened by determining the area where hemodialysis treatment is most needed with the land allocation model. The aim of the study was to provide hemodialysis services at a minimum distance with different variations to hemodialysis patients in Ankara, Turkey which was selected as the pilot region. Four different objective functions are designed as the decision maker may have different preferences.",Mathematical Models for New Hemodialysis Center Location Determination and Patient Scheduling ,"[40698, 79304, 79305, 79306]",952,"[56, 110, 72]",4322,Location planning in healthcare,3,7,15,OR in Health Services [ORAHS],"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,18 [building - 116],"['Health Care', 'Programming, Linear', 'Mathematical Programming']",TA-15
"When a user opens a website or app, there are often several advertisements interspersed. Generally, whenever an advertisement appears, loads, and is seen on a user's screen, it is referred to as an impression. For advertisers, an impression signifies an opportunity for their ads to be viewed. Therefore, real-time bidding [RTB] has emerged as a prominent paradigm, allowing advertisers to procure impressions through instantaneous automatic auctions. In RTB, the goal for advertisers is to maximize the total revenue generated from the impressions they win while considering constraints such as budget. Due to the difficulty in estimating the value of impressions and the bids of competitors, determining the optimal bid for each impression becomes a challenging problem for advertisers. 
In our work, we first propose a novel definition of impression value under the background of the mobile gaming market, which takes into account players' in-game purchase, conversion revenue, and publisher characteristics. In addition, to enhance the interpretability of the strategy, we introduce a two-part method - the first part employs reinforcement learning techniques to obtain a two-dimensional vector based on an optimization problem, and the second part utilizes this vector to guide modifications to existing generic bidding paradigms in the industry. Real-world experiments demonstrate an increase in return on investment by over 80% compared to current practices after applying our method.
",Bidding in Online Display Advertising - A Deep Reinforcement Learning Model for Mobile Gaming Market,"[79300, 43173, 79307, 73319, 79308]",726,"[9, 7, 66]",4324,Optimization in Online Environments,14,3,03,Data Science Meets Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,1005 [building - 202],"['Auctions / Competitive Bidding', 'Analytics and Data Science', 'Machine Learning']",MB-03
"In recent decades, the avocado fruit has gained popularity so many countries including United States, area among the biggest importer of this fruit, which implies a growing demand for this market. In Mexico context, micro and small producers are the ones who sustain most of the avocado exports in Michoacán. Decision making for the management of avocado production is a complex issue due to the large number of variables associated with developing a feasible production function and business plan. Although studies have been conducted by authors, the use of Expert Systems to modelling and simulation the farmer decision making process on Hass avocado production is still insufficiently explored. This paper addresses the need to design and implement an expert system for simulation modelling of Hass avocado farming for solving complex issues affecting the avocado production in Mexico context.  First, we present the literature review about the use of Expert System tool for modelling and simulation the farmer decision making process on Hass avocado production. Second we describe the formulation and implementation of an Expert System for simulation modelling of Hass avocado farming using Netlogo software. In particular, we discuss a case study in Mexico. Then, the main results of our analysis are showed. Finally, we present our conclusions.",Using Expert Systems for simulation modelling of Hass avocado farming,"[37306, 74631, 76763]",676,"[89, 26, 42]",4325,DSS in Agriculture,20,9,12,OR in Agriculture and Forestry ,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,13 [building - 116],"['OR in Agriculture', 'Decision Support Systems', 'Expert Systems and Neural Networks']",TC-12
"This project focuses on advancing the field of Explainable Predictive Process Monitoring [XPPM] by developing novel techniques for generating explanations in PPM, and developing novel evaluation metrics. Furthermore, we aim to understand the interplay between different types of explanations and how they can be leveraged to improve the explainability process.

The first aspect of this research concerns the generation of counterfactual explanations in PPM, which are explanations that suggest what should be different in the input instance to change the outcome of a PPM model. Firstly, an evaluation protocol measuring the quality of the generated counterfactual examples and the evaluated method's performance is proposed.
In the second part, a methodology is proposed, that involves the use of genetic algorithms, incorporating temporal background knowledge at runtime. The methodology aims to generate counterfactual explanations that guarantee the satisfaction of the background knowledge, a critical aspect of counterfactual generation in XPPM. 
A second aspect of this research is developing novel explainability method by using previously proposed Business Process Management [BPM] techniques to adapt explanation generation mechanisms.
A third aspect of this research project endeavours to combine various explanation techniques to streamline the search process and improve the overall quality of explanations, by leveraging multiple explanation types proposed in the literature.
",        Exploring Explainable Predictive Process Monitoring,[79303],124,"[26, 77]",4326,XAI in Business Processes,15,4,27,Mathematical Optimization for XAI,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,047 [building - 208],"['Decision Support Systems', 'Multi-Objective Decision Making']",MC-27
"The u-prenucleolus is a generalization of the prenucleolus using utility functions. The u-prenucleolus can also be considered as a generalization of the per-capita prenucleolus. We prove the generalizations of some important theorems about the prenucleolus to the u-prenucleolus, such as Kohlberg’s theorem and the theorem of Katsev and Yanovskaya about a sufficient and necessary condition on the unicity of the u-prenucleolus. We also prove a generalization of Huberman’s theorem by defining the u-essential coalitions and showing that these coalitions characterize the u-prenucleolus of u-balanced games. Considering the dual of the game, we define the u-anti-prenucleolus, and show that the u-anti-essential coalitions characterize it. Using these results, we get that in the primal game the u-dually-essential coalitions – which generalize the dually essential coalitions defined by Solymosi and Sziklai - characterize the u-prenucleolus. This way, we get a characterization set for the u-prenucleolus that differs from the set of u-essential coalitions.",TU-games with utility - characterization sets for the u-prenucleolus,"[67434, 18989]",385,"[50, 0]",4330,"Game Theory, Solutions and Structures I",88,2,36,"Game Theory, Solutions and Structures","Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,32 [building - 306],['Game Theory'],MA-36
"We describe a quantum algorithm based on an interior point method for solving a linear program with n inequality constraints on d variables. The algorithm explicitly returns a feasible solution that is eps-close to optimal, and runs in time O[sqrt{n} poly[d,log[n],log[1/eps]] which is sublinear for tall linear programs [i.e., n >> d]. Our algorithm speeds up the Newton step in the state-of-the-art interior point method of Lee and Sidford [FOCS '14]. This requires us to efficiently approximate the Hessian and gradient of the barrier function, and these are our main contributions.

To approximate the Hessian, we describe a quantum algorithm for the spectral approximation of A^T A for a tall n-by-d matrix A. The algorithm uses leverage score sampling in combination with Grover search, and returns a delta-approximation by making O[sqrt{nd}/delta] row queries to A. This generalizes an earlier quantum speedup for graph sparsification by Apers and de Wolf~[FOCS '20]. To approximate the gradient, we use a recent quantum algorithm for multivariate mean estimation by Cornelissen, Hamoudi and Jerbi [STOC '22]. While a naive implementation introduces a dependence on the condition number of the Hessian, we avoid this by pre-conditioning our random variable using our quantum algorithm for spectral approximation.",Quantum speedups for linear programming via interior point methods,[79317],667,"[19, 5]",4331,Quantum Computing for Continuous Optimization,83,7,42,Quantum Computing Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,98 [building - 306],"['Continuous Optimization', 'Algorithms']",TA-42
"Unit commitment [UC] is one of the fundamental optimization problems in power system research, on which many essential grid applications are built, such as operations, planning, and markets. Many power system research projects involve the development of a UC  model or its variant as a building block. Currently, most researchers prefer to write their own UC codes and use public data with their own tweaks. Although this approach allows authors more control over the code, the disadvantages are significant - it wastes time on reinventing the wheel, does not incorporate the state-of-the-art UC models/algorithms, and makes it difficult to benchmark with other work due to differences in implementation and datasets. In this work, we introduce an open-source Julia software package for UC-related research. The package not only implements state-of-the-art models/algorithms for UC problems but also provides great extensibility for secondary development and numerous test instances for benchmarking. This presentation will introduce the UC package and illustrate how it can be used to accelerate power system research.",Unitcommitment.jl – An Open Source Julia Software Package to Accelerate Power System Research,[79320],282,"[93, 36, 134]",4333,Advancements in energy system optimization and analysis tools,21,4,22,Energy Management,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,81 [building - 116],"['OR in Energy', 'Electricity Markets', 'Software']",MC-22
"In the U.S., over 3 million are estimated to have hepatitis C, with many cases undiagnosed. Early detection of hepatitis C is crucial but hindered by high screening and treatment costs. The paper discusses how Netflix-style contracts could incentivize increased screenings and treatment by offering fixed payments for unlimited drugs",The “Netflix Model” - A New Payment Model for Asymptomatic Disease Management,[79331],979,"[56, 0]",4336,Medical decision making,3,5,17,OR in Health Services [ORAHS],"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,40 [building - 116],['Health Care'],MD-17
"In addition to various behavioral biases and contextual factors affecting AI-augmented decision-making, emotions play an important role in such tasks. Across different emotions, the fear of being replaced by AI systems is one of the most reported ones among those who are working with AI. Our study explores how the fear of being replaced by an AI-algorithm for operations decision-making affects individuals' reliance on AI suggestions and subsequent decision-making performance. Using an online experiment, we manipulate this emotion by varying the likelihood of the AI to replace participants in a demand-forecasting task. Participants are provided with additional information compared to the AI, mirroring real-life scenario of private information, and informed that they may lose their job and the opportunity for bonuses in additional tasks if they fail to improve on the algorithm's suggestion. Our pre-test’s results suggest a marginally significant negative effect of the fear on the adoption of AI recommendation through decision-making process. However, there is no significant effect observed on decision-making performance.",Keeping your enemies closer? The effect of the fear of replacement by AI on AI-augmented decision-making,"[79332, 995, 79666]",105,"[25, 10, 26]",4337,Behavioral OR general papers,13,8,11,Behavioural OR,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,12 [building - 116],"['Decision Analysis', 'Behavioural OR', 'Decision Support Systems']",TB-11
"The focus of this paper is on demonstrating the creation of portfolio uncertainty bands using a bootstrapped procedure. We restrict attention to the model presented in Mezali and Beasley [1] and use re-sampling statistical techniques to build in-sample portfolio uncertainty bands. Further, we propose a number of ways in which the in-sample bootstrapped portfolios, which collectively form an uncertainty band, can be employed to improve out-of-sample portfolio performance for both index tracking and enhanced indexation.
Our formulation includes transaction costs, a constraint limiting the number of stocks that can be in the portfolio and a limit on the total transaction cost that can be incurred. Numeric results are presented for eight test problems drawn from major world markets, where the largest of these test problems involves over
2000 stocks.
Keywords - enhanced indexation; index tracking; portfolio optimisation; quantile regression; bootstrapping
",Bootstrap approach to quantifying uncertainty in index tracking and enhanced indexation,[38982],673,"[19, 45, 47]",4340,"Advancements of OR-analytics in statistics, machine learning and data science 19",16,15,06,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,1013 [building - 202],"['Continuous Optimization', 'Financial Modelling', 'Forecasting']",WD-06
"The venture capital market, a private alternative investments form, has gained increasing attention and importance in recent years. Key industry metrics such as assets under management, average funding valuation, and the number of funds and startups have risen sharply, making 2021 a record year for the sector.

Several studies have shown that this industry contributes greatly to the development of countries' economies [LERNER; TÅG, 2013]. The United States, a market where venture capital originated and which has been instrumental in its development, has long been able to benefit from the positive effects of a well-organized venture capital market - increased technological spillover effects, companies that are among the most highly capitalized on the stock market, and more employment [through VC backed companies] are just some of the benefits that the U.S. market has enjoyed [LERNER; NANDA, 2020; TOM, 2019]. However, in the post COVID 19 era, capital raising and performance metrics are no longer in line with the industry's former standards and show a somewhat different landscape.

Against a backdrop of complex and difficult global context, this paper first examines what the growth drivers of VC are in relation to regional and national macroeconomic variables in Europe and what changes the venture capital market drivers have experienced from external shocks [e.g. - due to the COVID-19] [GOMPERS; GORNALL; KAPLAN; STREBULAEV, 2020].
",Venture Capital - Drivers of Growth,[78693],276,"[44, 29]",4341,Optimal Portfolio Strategies,4,13,02,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,Glassalen [building - 101],"['Finance and Banking', 'Development']",WB-02
"This study aims to compare the performances of different forecasting techniques to predict electrical energy consumption of Türkiye. The literature on energy demand forecasting generally focuses on multiple regression analysis and time series methods such as ARMA, ARIMA, SARIMAX methods. More recently, machine learning methods have become popular and artificial neural networks are particularly successful in predictions made using time series data. 
In this study, the energy demand for Türkiye will be predicted by using the data set produced by the local authorities such as Load Dispatch Information System and Load Dispatch Directorates. In addition, economic and environmental factors affecting electricity use will be determined and included in the analysis. For example, economic indicators such as the number of companies, population, low income per capita, and indicators created from air temperature, business days, holidays, festivals, special days of the Hijri and Gregorian calendars will be included in the analysis.
As the forecasting method, multiple regression, time series methods, SARIMAX and machine learning methods will be used. Then the performance of these methods will be evaluated, and the best one will be selected.
",Electrical energy demand forecasting for Türkiye ,"[79342, 8542]",530,"[47, 66, 37]",4344,"Advancements of OR-analytics in statistics, machine learning and data science 11",16,15,28,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,065 [building - 208],"['Forecasting', 'Machine Learning', 'Energy Policy and Planning']",WD-28
"This project emerged from a collaboration between the CERMICS laboratory and Renault Group. Their challenge - optimizing reverse empty-packaging logistics for car parts transportation from suppliers to plants. Spanning an entire continent, this multi-attribute inventory routing problem [IRP] involves 30 commodities, 16 depots, and 600 customers—far surpassing existing literature instances. Traditional IRP algorithms do not scale, so we propose a large neighborhood search [LNS]. Our approach involves three key steps - [1] generalizing split delivery vehicle routing and IRP neighborhoods, [2] adapting a matheuristic for medium-scale IRP into a large neighborhood, and [3] introducing novel perturbations like customer and commodity reinsertion. Additionally, we establish a new lower bound based on flow relaxation. To foster large-scale IRP research, we release an industrial instance library and open-source our code. Rigorous numerical experiments validate the effectiveness of each LNS component.
The industrialization of our LNS, coupled with robust change management, has yielded substantial benefits. At Renault, we have achieved a 4−5% reduction in packaging logistics costs per vehicle produced, a 50% decrease in packaging shortages at suppliers, and a 10% reduction in CO2 equivalent transport emissions for 2023. These gains translate to millions of euros saved and kilotons of emissions reduced annually.",Renault Group Packaging Return Logistics in Europe,"[79154, 79354, 79355, 79153]",22,"[61, 65, 145]",4345,EEPA,56,4,02,EURO Excellence in Practice Award,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,Glassalen [building - 101],"['Inventory', 'Logistics', 'Vehicle Routing']",MC-02
"Vietnam's agricultural sectors have rapidly developed to become major contributions to the growth of economy [e.g., in 2019, Vietnam’s rice export revenue achieved US$1.4 billion]. Accompanying the development, however, agricultural waste management  in Vietnam [i.e., approximately 76 million tons including 45.22 million tons of rice straw, 8.73 million tons of rice husk, 4.04 million tons of sugarcane bagasse, 6.33 million tons of maize by-products, 1 million tons of coffee shell and 10 million tons of vegetable by-products] has become a challenging issue for rural planners due to the lack of an efficient planning tool and impacts of climate change. As a result, it has increasingly caused air and water pollution in many rural areas of the country. In this project, we thus collaborated with Vietnam institutions [including Hue University of Agriculture and Forestry, Institute of Meteorology, Hydrology and Climate Change, and Transport and Development Strategy Institute] to develop a proof-of-concept planning model for rural planners to optimally locate facilities that produce bio-organic fertilizer from waste, and to determine the optimal set of routes for a fleet of vehicles to collect and transport the waste from farms to facilities. In the novel location-assignment-routing problem, we investigated unreliability in facility location and vehicle routing planning due to the risk of disruptions to location sites and roads [i.e., unexpected failure of location sites and roads due to flood impact]. The model aims to minimise total expected cost of locating facilities and routing vehicles under the risk of disruptions [due to flood impact] such that the amount of waste at farms is totally collected. A robust mixed-integer nonlinear programming model for this problem was formulated. The model was further generalised to a multi-period planning problem given limited periodic budgets of locating facilities and deploying vehicles. The model was linearized so that it can be solved using a general-purpose solver. In addition, a meta-heuristic algorithm was developed to solve the large-scale instances within a reasonable computation time. Finally, the model was applied for agricultural waste management in Quang Tri province, Vietnam.",Optimal Planning Tool for Agricultural Waste Management in Vietnam [OPT-AWAMA],"[67933, 79359, 79357, 65508, 67936, 67937, 67938]",22,"[43, 64, 145]",4347,EEPA,56,4,02,EURO Excellence in Practice Award,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,Glassalen [building - 101],"['Facilities Planning and Design', 'Location', 'Vehicle Routing']",MC-02
"We present several analytical results for a chance constrained model of electric energy storage participating in both the energy and reserve markets, alongside generation, in the context of a day-ahead unit commitment model. The main source of uncertainty, and thus the reason for a reserve market, is assumed to be imperfect forecasts of wind generation. Although some intuitions in deterministic models remain valid under uncertainty but most of the intuitions and proofs become invalid in the presence of uncertainty, like buying energy at the lowest price and selling at the highest price by the battery when is available for reserves. Therefore, analyzing the unit commitment problem in the presence of wind power uncertainty and inclusion of storage batteries become crucial. We develop two chance constrained models with different features and applications. The results are discussed through a numerical example.",Chance Constraint Models for the Operations of Storage Systems in Energy and Reserve Markets,"[54945, 40076, 79368]",869,"[36, 93, 136]",4350,Flexibility in future energy systems,22,3,14,Energy Markets,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,16 [building - 116],"['Electricity Markets', 'OR in Energy', 'Stochastic Optimization']",MB-14
"Court congestion is a pressing issue worldwide, affecting social welfare, economic development, and access to justice. This research agenda aims to address the challenges posed by court congestion through the lens of operations management. By employing empirical research, the application of theoretical frameworks, and data-driven methodologies, we seek to unravel the operational complexities and identify solutions to improve judicial system efficiency. Utilizing comprehensive datasets from the US Federal District Courts and the Israeli court system, this project employs a variety of techniques including natural language processing, artificial intelligence, and econometrics to analyze the effects of judicial congestion. Our approach leverages operations management tools, process-mining, and queue-mining to propose and evaluate interventions designed to reduce congestion and enhance system performance. The significance of this research lies in its potential to contribute to both the operations management field and societal welfare by offering insights and interventions that facilitate access to justice. By bridging theoretical innovation and practical solutions, this research aims to forge new paths in operations management and social impact.",Leveraging Operations Management to Improve Access to Justice - A Research Agenda on Court Congestion,[61529],915,"[7, 0]",4351,YW4OR_2,39,13,12,WISDOM - Women in OR,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,13 [building - 116],['Analytics and Data Science'],WB-12
"Multi-objective decision-making often poses challenges due to the computational complexity of generating and analyzing the Pareto front. To address this, we propose a framework integrating two strategies to alleviate the burden on decision-makers [DMs]. Firstly, we introduce algorithms based on the epsilon-constraint method to derive a Pareto front representation balancing coverage, uniformity, and cardinality, thereby reducing the number of non-dominated vectors to be considered. Secondly, we incorporate DM preferences to refine the feasible region. We do so initially using criteria ranking and selecting a region of interest of the Pareto front to focus the representation. Then, after generating the initial representation, through pairwise comparisons we construct convex cones and eliminate inferior regions from consideration. We demonstrate the efficacy of our framework through its application to pharmaceutical supply chain design, illustrating its adaptability aid in tactical and strategic decision-making contexts. This methodology offers a practical approach to multi-objective optimization challenges, providing DMs with manageable decision support while maintaining focus on the proposed strategies",A methodology to tackle multi-objective linear programming problem,[62188],457,"[77, 0]",4352,YW4OR_1,39,12,12,WISDOM - Women in OR,"Wednesday, 8:30-10:00",W,A,8:30,10:00,2024-07-03,13 [building - 116],['Multi-Objective Decision Making'],WA-12
"The increased deployment of renewable energies introduces intermittency in power generation that was reliably dispatched in the past. The conventional Unit Commitment problem in energy system management consists in optimizing the operation schedule of thermal units to meet the load at minimum cost and CO2 emissions. However, Uncertain Unit Commitment [UUC] models extend this framework by integrating uncertainties stemming from generation, load fluctuations, pricing, etc. These extensions require decomposition-based computational methods to solve these problems efficiently and accurately. TotalEnergies collaborates with Ecole des Ponts [CERMICS lab] to focus on an isolated industrial energy-intensive site, initially with a known load. The energy demand must be met by internal resources - thermal generators, PV panels, and batteries. The complexity of this problem stems from the interplay of factors like storage and ramping constraints, interstage coupling, binary variables, and the intermittency of solar energy. Ensuring continuous load fulfillment remains imperative for safety and reliability. Yet, an overly cautious approach can result in high production costs and CO2 emissions. We study the risk-neutral and risk-averse approaches via the stochastic and robust settings, respectively. By separating the information structures behind binary and continuous variables, we can decouple them in the optimization problem and use cut-generation algorithms to solve both problems.",Uncertain Unit Commitment For The Operation Of Hybrid Power Plants Under Uncertainty,"[79369, 68911, 79370, 79371, 79372]",471,"[127, 135, 93]",4353,Stochastic models in energy systems planning and operations,21,9,22,Energy Management,"Tuesday, 12:30-14:00",T,C,12:30,14:00,2024-07-02,81 [building - 116],"['Robust Optimization', 'Stochastic Models', 'OR in Energy']",TC-22
"The field of building deconstruction is a significant contributor to waste generation. However, despite the availability of many waste recovery techniques to valorize this waste, much of it is currently lost due to poor waste stream management.
This study aims to propose a modeling of the deconstruction sector that optimizes dismantling operations and their associated waste flows to raise the recovery rate of the waste, thereby lowering both the financial and environmental costs of the field. This model of the deconstruction sector is based on a bi-level problem. Its upper-level part encompasses the scheduling of operations. The latter hinges on a weighted flexible job shop problem [wFJSP] with release dates and is assessed by a non-regular criterion.
We propose a new hybrid disjunctive graph genetic algorithm [HDGGA] for wFJSP, integrating into a genetic algorithm, based on a new encoding, a local search approach employing a novel extension of the disjunctive graph representation that takes account of the idle machine times.
The lower-level problem models the allocation of the waste induced.
Experiments are carried out on two sets of instances, including one derived from data collected amongst the actors of the field working in the Lille metropolis.
The first results exhibit the interest in the genetic-based hybrid method proposed, indicating that it outperforms the existing genetic algorithm-based approach, especially in medium-sized and large-sized instances.",A hybrid genetic algorithm for non-regular flexible job shop arising from deconstruction applications,[79376],530,"[129, 74, 18]",4358,"Advancements of OR-analytics in statistics, machine learning and data science 11",16,15,28,"Advancements of OR-analytics in statistics, machine learning and data science","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,065 [building - 208],"['Scheduling', 'Metaheuristics', 'Computer Science/Applications']",WD-28
"In the Kidney Exchange Problem [KEP], kidney exchanges can be modelled in a directed weighted graph as circuits starting and ending in an incompatible pair or as paths starting at an altruistic donor.
The weights on the arcs represent the medical benefit which is a measure of the quality of the transplantation.
For medical reasons, circuits and paths are of limited length.
The aim of the KEP is to determine a set of disjoint kidney exchanges of maximal medical benefit or maximal cardinality [all weights equal to one].
In this work, we consider two types of uncertainty in the KEP which stem from the estimation of the medical benefit [weights of the arcs] and from the failure of a transplantation [existence of an arc].
Both uncertainties are modelled via uncertainty sets with constant budget.
The robust approach entails finding the best KEP solution in the worst-case scenario within the uncertainty set.
We modelled the robust counterpart by means of a max-min formulation which is defined on exponentially-many variables associated with the circuits and paths.
We propose two exact approaches to solve it - one based on the result of Bertsimas and Sim and the other on a reformulation to a single-level problem.
In both cases, the core algorithm is based on a Branch-Price-and-Cut approach where the exponentially-many variables are dynamically generated.
The computational experiments prove the efficiency of our approach. ",Robust approaches for the Kidney Exchange Problem,"[71690, 23193, 22042]",826,"[127, 13, 17]",4362,Robust Optimization - Theory and Applications,49,14,35,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,44 [building - 303A],"['Robust Optimization', 'Column Generation', 'Computational Biology, Bioinformatics and Medicine']",WC-35
"The literature finds evidences of adverse selection all insurance businesses. Nevertheless, these
analyses remain inconclusive as regards the economic consequences for the insurance industry,
particularly for institutions bearing longevity by transferring the longevity risk exposure. The present research gives a contribution to the assessment of the adverse selection in the context under consideration and evaluate the change in the cost of transferring longevity to capital market, which can be pointed out as a sort safety loading, consisting in an optimal hedge, i.e. “the equilibrium hedge”, since it contains the market inefficiencies due to the adverse selection effects. For estimating. the longevity exposure, the projections are based on a  more informative stochastic model which is the frailty-based model proposed by the authors.",On the Adverse Selection in longevity risk Transfer,"[57420, 67280, 57649, 57721]",386,"[44, 45, 135]",4363,New Challenges for Risk Management ,4,10,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S14 [building - 101],"['Finance and Banking', 'Financial Modelling', 'Stochastic Models']",TD-63
"To tackle the challenge of optimizing middle-mile logistics, the crucial link between warehouses and final deliveries, we introduce a novel instance generator that aims to create a rich and adaptable dataset of diverse instances to empower researchers and developers. The instance defines a logistics network with hubs, vehicles, routes, lines, and rotations. Additionally, it specifies a list of shipments that need to be transported through this network. To customize the instance, the user can adjust various parameters, such as the number of hubs, density of the space graphs, distribution of shipment weights, or the maximum number of vehicles.
The generator reflects real-world complexities through variations in network size and structure. We developed a random graph generator to mimic real-world middle mile networks, by generating space graphs for hubs. Subsequently, lines and routes are randomly constructed on the generated space graphs, while adhering to user-defined constraints.
The tool is in the form of an optimized C++ library that enables the generation of instances with a large number of hubs and shipments. It offers the immense potential for advancing middle-mile logistics optimization by providing a comprehensive and adaptable dataset for benchmarking optimization approaches, training machine learning models, and analyzing the impact of network configurations and shipments characteristics on overall efficiency.",A Novel Instance Generator for Simulating Middle-Mile Logistics Networks,"[79363, 71690, 77216, 53206, 23193]",789,"[134, 53, 143]",4365,Logistics 3,5,15,58,VeRoLog - Vehicle Routing and Logistics,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,S07 [building - 101],"['Software', 'Graphs and Networks', 'Transportation']",WD-58
"We present the uses of Dynamic Condition Response [DCR] Graphs as Explainable AI in Business Process Management. DCR Graphs is a declarative process modelling notation that can be used by domain experts to model, simulate and support workflows and business processes in a flexible and maintainable way. The first version of the DCR notation was published in 2010 in a PhD project of Mukkamala at IT University of Copenhagen, as part of the Trustworthy pervasive healthcare services project lead by Hildebrandt. The notation was further extended and accompanied by the first graphical editor as part of an industrial PhD project by Slaats [2015], at the danish company Exformatics and IT University of Copenhagen. The development was significantly advanced during the EcoKnow.org research project [2017-2021]  lead by Hildebrandt, leading to major additions to the language, such as sub processes, decision modelling and data, and the development of tools for transformation of natural language textual regulations and requirements to formal models process models, lead by López [2018], award winning algorithms for process mining [lead by Back and Slaats], and the creation of the company DCR Solutions [DCRSolutions.net], offering industrial strength design, simulation and execution tools. Recently, the tool chain has been extended in the PhD project of Cosma [2024] with tools for mining hierarchical and timed DCR Graphs and for transforming DCR Graphs to Petri Nets.",Dynamic Condition Response Graphs as Explainable AI in Business Processes,[79399],124,"[53, 76]",4369,XAI in Business Processes,15,4,27,Mathematical Optimization for XAI,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,047 [building - 208],"['Graphs and Networks', 'Modeling Systems and Languages']",MC-27
"Energy generation is an area where four of the many problems facing humanity today - the energy crisis, climate change, scarcity of natural resources and global warming - interact in a vicious circle. In this scenario, it is not surprising that governments around the world are reviewing their energy policies. Agents such as Renewable Energy Sources [RES] and Local Energy Communities [LEC] can play a fundamental role in the energy transition. However, the optimization of operational costs continues to be a relevant factor. The Unit Commitment [UC] problem is one of the classical approaches to optimize these costs. This problem involves decisions related to the schedule of generating units as well as the power they must produce in order to meet the total power demand, where the last one can be deterministic or uncertain. By integrating RES, a new source of uncertainty is added to the problem. In this work, various formulations for the UC problem with uncertainty are presented and solved using benchmark data.",Unit Commitment problem with uncertain demand and renewable energy availability,"[71514, 1]",842,"[93, 14, 127]",4370,OR in Energy,23,13,19,OR in Energy,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,44 [building - 116],"['OR in Energy', 'Combinatorial Optimization', 'Robust Optimization']",WB-19
"In this talk, we present a comprehensive analysis of solving  two famous NP-hard problems - the Max-Cut and the Stable Set Problem. While the former is equivalent to Quadratic Unconstrained Binary Optimization [QUBO] problem, the latter needs to be reframed as a QUBO problem. We solve both using exact solvers like GUROBI or BiqBin, and approximately by  quantum annealing solvers such as the D-Wave quantum processing unit and D-Wave hybrid solver.

In cases where the capabilities of quantum annealing solvers are exceeded, we introduce for the Stable Set Problem a specialized decomposition technique, which effectively breaks down the input graph into smaller graphs, rendering them more manageable for quantum annealers.
Finally, we conduct a thorough assessment of both quantum solvers alongside a classical simulated annealing solver. Our extensive numerical evaluation on various families of benchmark graphs demonstrates that the simulated annealing approach remains highly competitive. This competitiveness is primarily attributed to its simplicity and computational efficiency compared to quantum solvers.",Solving Combinatorial Optimization Problems with Quantum Annealers,[75335],375,"[5, 14]",4371,Decomposition methods for Quantum Optimization,83,3,42,Quantum Computing Optimization,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,98 [building - 306],"['Algorithms', 'Combinatorial Optimization']",MB-42
"The data available on SMEs [small and medium-sized enterprises] has recently experienced a sharp increase as new sources have emerged, providing value added to improve the risk management models of banks, which can be processed using advanced ML techniques. The characteristics of this segment make the adoption of ML models particularly well-suited and strategic, supporting process automation and digitalisation. To achieve these goals, the rating model has been updated, making it available online and in real time, without easing risk management standards. ML algorithms provide a tool to complement pre-existing approaches enhancing their breadth and accuracy.",Evolution of IRB models through new data sources and machine learning,[79418],386,"[44, 126, 45]",4373,New Challenges for Risk Management ,4,10,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,S14 [building - 101],"['Finance and Banking', 'Risk Analysis and Management', 'Financial Modelling']",TD-63
"E-bikes, a sustainable micro-mobility solution, have surged in popularity in urban areas due to their user-friendliness and cost-effectiveness, reshaping urban mobility and contributing to sustainable cities. However, their increasing use has led to a rise in road crashes. This study aims to assess hazard perception training for novice e-bike riders to enhance their ability to anticipate road hazards.
Participants were assigned to two intervention groups and a control group. The Act and Anticipate group received a theoretical tutorial and viewed video clips of real-time hazardous situations from an e-bike rider's perspective, followed by a hazard detection task. The Predictive and Commentary Training group engaged in a theoretical tutorial, anticipated potential hazards, and viewed video footage with expert commentary. A week later, participants completed a hazard perception test, identifying hazardous situations in video clips by pressing a response button.
Both intervention groups showed improved attentiveness to potential hazards compared to the control group, as indicated by enhanced response sensitivity and verbal descriptions. Actively engaging in hazard detection tasks with expert commentary effectively enhances hazard perception skills in complete-novice e-bike riders, contributing to safer traffic negotiation. The advantages of each training methodology are outlined, alongside implications for intervention strategies, licensing protocols, and policy formulation.",Enhancing hazard perception skills of e-bike riders in urban settings through training - Towards sustainable cities,[79424],274,"[143, 0]",4374," Enhancement of circularity, inclusivity, and smartness in cities I",79,4,18,Sustainable Cities,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,42 [building - 116],['Transportation'],MC-18
"It is widely acknowledged that healthcare providers' interests may diverge from those of patients. In and of itself, healthcare accessibility is a complex measure rooted in the ability of an individual to receive healthcare effectively with demonstrated anxiolytic effects. Annually, the Center for Medicare and Medicaid Services gathers survey data from current beneficiaries of Medicare in the United States and Puerto Rico [excluding healthcare facilities] to assess healthcare equity. These surveys delve into chronic risk factors associated with various diseases and mental health conditions. This study innovates extant Medicare patient care models by identifying the error-minimizing determinants underlying the probability of delivering a correct chronic condition diagnosis. We propose a novel multiobjective and multi-target shallow-learning radial basis function artificial neural network specification for classification. Using Explainable AI, we provide policy-based results, allowing providers to understand Medicare patients' disease pathology better. Diagnosticians are likely to have a lesser financial incentive to recommend profitable procedures regardless of appropriateness, reducing poor quality and costly care when providers are profit-incentivized instead of generous.",The Role of Ethics and Altruism - Mapping the Determinants of Healthcare Accessibility for Medicare Beneficiaries,"[25951, 11548, 79441]",692,"[66, 15, 58]",4375,Developing Countries and Sustainable Humanitarianism ,67,15,18,OR for Development and Developing Countries,"Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,42 [building - 116],"['Machine Learning', 'Complex Societal Problems', 'Humanitarian Applications']",WD-18
"Polling models are classes of queueing models in which a server attends to
several queues. By cyclically visiting the queues, the server serves awaiting customers. The analysis of these systems is involved and provides computationally involved results. Therefore, the approximation of these models by their continuous counterparts can be quite helpful. In this talk, we analyze continuous polling models with batch arrivals and deploy a novel mean value analysis to [i] find the average spread of customers and [ii] derive some key performance statistics of polling models; delivery times and batch sojourn times. The resulting approximations are shown to already be quite accurate for polling systems with small numbers of queues.",A Mean Value Approach to Continuous Polling Models with Batch Arrivals,[79486],159,"[65, 121, 146]",4376,Stochastic Models in Manufacturing,50,5,39,Stochastic Modelling,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,35 [building - 306],"['Logistics', 'Queuing Systems', 'Warehouse Design, Planning, and Control']",MD-39
"We consider the two-stage stochastic prize collecting traveling salesman problem, in which the objective is to maximize profit of served customers minus driving costs. The first-stage customers are to be served every day, while the second-stage customers fluctuate from day to day. The task is to select a subset of the first-stage customers, such that the expected earning is maximized.
We present a highly parallel heuristics based on consensus fixing - The problems are solved independently for each scenario using a variable neighborhood search heuristic. Then, the scenarios try to reach consensus about fixing a single first-stage variable, using various score functions. The process of alternating between heuristic solution and variable fixing is repeated until all first-stage variables have been fixed.
Computational results are reported for instances having up to 500 first-state and 500 second-stage customers.
",A consensus fixing heuristic for the stochastic prize collecting TSP,[64035],195,"[14, 136, 145]",4377,Combinatorial optimization topics in transportation,64,2,26,Combinatorial Optimization,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,012 [building - 208],"['Combinatorial Optimization', 'Stochastic Optimization', 'Vehicle Routing']",MA-26
"Fleet planning is a key success factor of retail management in the convenience store industry where a wide range of goods are replenished on a daily basis. The operational challenge in this industry is to decide the size of transport equipment – reefer truck and dry truck in an optimized manner. The idea to get some flexibility in the fleet pool decision is that the reefer truck can be used as a dry truck if the temperature control is disabled with limited cost increases. In this study, a simulation approach categorized as Digital SC twins is selected to demonstrate the benefits of increasing compatibility by opting for a mixed-truck policy including the optimal fleet size and timeline. The study optimizes the mixed vehicle in terms of fleet maximizing the service level while minimizing costs by applying the proposed policy. To set up a real-life simulation baseline, a convenience store supply chain in South Korea was modeled with some operation parameters. Initial simulation revealed the impact of time windows on reducing the number of vehicles used in standard operation. The second experiment investigated the effect of reefer trucks ratio to dry trucks under each time window standard operation. The finding is that the optimized fleet planning enables the supply chain to maintain the same level of on-time delivery even with fewer vehicles. It can effectively resolve the issue of delayed delivery due to insufficient capacity while reducing total costs and travel distances.",Simulation-based Multi-objective Optimization for Fleet Resource Planning in a Convenience Store Supply Chain,"[79493, 79513]",101,"[131, 77, 145]",4378,Retail Distribution II,30,13,50,Retail Operations,"Wednesday, 10:30-12:00",W,B,10:30,12:00,2024-07-03,M2 [building - 101],"['Simulation', 'Multi-Objective Decision Making', 'Vehicle Routing']",WB-50
"Evaluation of the importance of criteria using weights is usually an integral part of solving multi-criteria decision-making [MCDM] problems. MCDM methods often require independent criteria, but this may not be true for real problems. Significant dependence then leads to either a reduction or a strengthening of importance. There are several approaches that can reflect the dependency between criteria, e.g., CRITIC method or the methods based on the Choquet integral. Existing approaches, however, suffer from drawbacks. They are based on the correlation which does not reflect the direction of dependence or expert evaluations. This contribution focuses on the application of the fuzzy functional dependencies [FFD] to reflect the dependencies between criteria in their weights. The model is tested using several numerical examples.",Modelling Dependencies Between Criteria with Fuzzy Functional Dependence,[66052],66,"[25, 26, 27]",4379,Emerging Trends in Decision Analysis,45,5,45,Decision Support Systems,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,30 [building - 324],"['Decision Analysis', 'Decision Support Systems', 'Decision Theory']",MD-45
"The last decade has seen the development of new convergence-based derivative-free algorithms. Among them, DMulti-MADS is a direct search algorithm, which extends the single-objective algorithm MADS to multiobjective optimization. In this talk, we present recent improvements to this method, from the consideration of general inequality constraints and mixed-integer variables to the development of new search heuristics. We illustrate the performance of these new features in numerical experiments and engineering applications.","DMulti-MADS, a Multiobjective Algorithm for Constrained Derivative-Free Optimization",[71645],12,"[112, 81, 113]",4380,Objective Space-Based Approaches in Multiobjective Optimization,34,7,37,Multiobjective Optimization,"Tuesday, 8:30-10:00",T,A,8:30,10:00,2024-07-02,33 [building - 306],"['Programming, Multi-Objective', 'Non-smooth Optimization', 'Programming, Nonlinear']",TA-37
"The study addresses the supply chain network design problem [SCNDP], focusing on strategic, long-term decisions that integrate flexibility and demand information to enhance cost efficiency. We examine five policies that incorporate new flexibilities at strategic and operational levels to optimize the SCNDP. Initially, we explore a model where local production meets demand. We expand this to include scenarios where production capacity 
can satisfy demands across different locations and shared among facilities with preestablished links. Unlike traditional models that fix production capacity early, we also consider policies allowing production decisions to be deferred until demand is known, favoring a make-to-order strategy. Our analysis employs a two-stage robust optimization framework for which we introduce a novel computationally efficient exact [based on Column-and-Constraint Generation [C&CG]] and multiple approximation techniques [based on Linear Decision Rules]. Our findings reveal that capacity sharing significantly reduces costs and shortage risks, showcasing its superiority over delayed production decisions, particularly under moderate uncertainty. The study concludes by applying these insights to a real-world case, offering practical design recommendations. ",The Value of Flexibility in Robust Supply Chain Network Design,"[56012, 70637]",830,"[138, 127, 43]",4381,Optimization under Uncertainty in Manufacturing and Supply Chain Management,49,15,35,"Stochastic, Robust and Distributionally Robust Optimization","Wednesday, 14:30-16:00",W,D,14:30,16:00,2024-07-03,44 [building - 303A],"['Supply Chain Management', 'Robust Optimization', 'Facilities Planning and Design']",WD-35
"We focus on a stochastic Joint Replenishment Problem setting with a single 
item, one warehouse and multiple retailers facing compound Poisson demand. The novel aspect of this setting is that the retailers’ opportunities for replenishment are constrained by cyclical schedules. We model the underlying Markov Decision Process and determine the optimal joint replenishment policy for small instances via a value iteration approach. Our numerical analysis investigates the influence of schedule attributes on the structure of the final solution. To address larger instances, we propose an iterative solution method that makes use of a reduced state space.",The Scheduled Joint Replenishment Problem,"[79349, 54033, 67696, 55094]",910,"[61, 138, 139]",4382,Retail Inventory Management III,30,14,61,Retail Operations,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,S10 [building - 101],"['Inventory', 'Supply Chain Management', 'Sustainable Development']",WC-61
"Operational research specialists at the OCP Group, the Mohammed VI Polytechnic University, and the Polytechnique Montreal operationalized a system optimizing OCP’s supply chain downstream activities. The system
simultaneously schedules production, inventory, and vessels while 
ensuring the highest demand fulfillment. Therefore, it has become central to the OCP planning process. Planners use the optimizer’s solutions and insights to improve plans in different OCP sites. Initially, the system was a bottleneck, curbing the use of other supply chain management tools. OCP management now credits the system operationalization with providing operational benefits, contributing to over a $240 million increase in annual turnover.",OCP Optimizes Its Supply Chain For Africa and The World,"[73660, 24885, 73820, 10966, 73814, 73815, 73827, 73821, 73823, 73822, 73826]",22,"[138, 0]",4383,EEPA,56,4,02,EURO Excellence in Practice Award,"Monday, 12:30-14:00",M,C,12:30,14:00,2024-07-01,Glassalen [building - 101],['Supply Chain Management'],MC-02
"The global urgency to mitigate climate change and reduce greenhouse gas emissions has intensified efforts to transition towards renewable energy sources. Isolated electrical grids, characterized by their detachment from larger networked systems, present unique challenges in this transition. This study, focusing on Israel’s grid as a model for the year 2050, aims to address these challenges by employing a holistic approach that integrates advanced modeling techniques to assess the potential of renewable energy integration effectively.
Employing a comprehensive methodology, this research incorporates a detailed Performance Model [PM] and an Energy Management System [EMS] model. The PM evaluates the hourly electrical output from diverse renewable sources, including solar [photovoltaics and agrivoltaics], onshore and offshore wind, sea waves, and organic waste, taking into account Israel's specific geographic and climatic conditions. The EMS model then optimizes the alignment between this renewable energy supply, the anticipated demand, and necessary storage solutions. This approach includes extensive data collection on renewable energy potentials, area mapping for solar installations, and evaluations of wind and sea wave energy potentials based on geographical and meteorological data. The innovative aspect of considering organic waste adds an additional layer of resource diversification.
The findings reveal a significant potential for Israel to increase its renewable energy share to between 80.1% and 99.5% by the year 2050, contingent upon the implementation scenario. Achieving such a high level of renewable integration hinges on developing substantial energy storage capacities. This research identifies a feasible combination of the Vehicle to Grid [V2G] concept, stationary batteries, and pumped hydroelectric plants as viable solutions for meeting the projected total electric demand of 183.3 TWh in 2050, with renewable resources providing an installed capacity of 180.6 GW.
A key outcome of the study is the emphasis on agrivoltaics and the V2G concept as instrumental in enhancing the grid's renewable capacity. These innovations not only conserve land resources but also offer the requisite storage to manage the inherent variability of renewable energy sources. Moreover, the study underscores the importance of adopting a comprehensive energy management approach capable of accommodating the dynamic nature of renewable energy production and consumption.
This research contributes to the broader discourse on renewable energy potentials in isolated grids, providing critical insights for policymakers, energy planners, and researchers. It underscores the feasibility and necessity of employing advanced modeling techniques to evaluate and optimize the integration of renewable energy sources, highlighting the need for significant storage capacities and innovative solutions like agrivoltaics and V2G.
The implications of this study for energy policy and planning are profound, especially for regions with isolated grids similar to Israel. It encourages policymakers and energy planners to consider the methodologies and findings presented to make informed decisions regarding renewable energy investments and infrastructure development. Furthermore, the research highlights the pivotal role of innovation in energy storage and management technologies in maximizing the potential of renewable energy sources.
In conclusion, the integration of renewable energy into isolated grids, exemplified by the case study of Israel in 2050, is both a viable and essential strategy for moving towards a sustainable energy future. This study lays the groundwork for future research to explore emerging technologies and storage solutions, ensuring that renewable energy can adequately meet growing demands while addressing environmental concerns.",The potential of renewable electricity in isolated grids - The case of Israel in 2050.,"[79643, 79644, 79645, 80295, 80296, 80297]",23,"[37, 139]",4384,EPOCG,57,5,02,EURO Prize for OR for the Common Good,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,Glassalen [building - 101],"['Energy Policy and Planning', 'Sustainable Development']",MD-02
"Adapting to the consequences of climate change is undeniably one of the paramount challenges of our time. Among these consequences, intense heavy rain events pose a significant threat by endangering human lives and causing severe damage to infrastructure through flash floods. Recent events such as the heavy rainfall and flooding in Western Europe in 2021, Pakistan 2022, and Libya 2023, have once more corroborated the enormous potential for damage and underscore the importance of developing high-quality mitigation concepts in order to reduce the impact of these events on communities and their infrastructure. Such mitigation concepts, however, are typically elaborated solely based on simulations resulting in a time-consuming trial-and-error approach that provides no quality guarantee for the obtained solutions. 

In this talk, an approach that combines mixed-integer programming and a combinatorial graph algorithm for the automatic computation of high-quality flood mitigation concepts consisting of retention basins, embankments, and ditches is presented. This, to the best of our knowledge, is the first automated decision support approach for this critical application that scales well enough to be applied to realistic scenarios. Apart from the developed optimization algorithms, the talk also presents a comprehensive web-application that has been designed to make the algorithms available to a broad variety of users beyond the OR community. The work has been conducted within the project “Incentive Systems for Municipal Flood Mitigation” in an interdisciplinary team consisting of two academic research groups, an engineering office, two municipalities, and a municipal water association. By now, the resulting software has been used in practice by over 30 institutions from all over Germany reaching from local authorities over engineering offices to academic research groups. The empowerment of users with limited technical knowledge to design high-quality precautionary measures for pluvial flash floods sets the developed software apart from state-of-the-art approaches and marks a significant innovation showcasing the OR community’s contribution to societally vital topics.",Rethinking Municipal Flood Mitigation - A Real-World Application of Operations Research Methods for Improving Climate Resilience,"[67297, 36955]",23,"[53, 147, 40]",4385,EPOCG,57,5,02,EURO Prize for OR for the Common Good,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,Glassalen [building - 101],"['Graphs and Networks', 'Water Management', 'Environmental Management']",MD-02
"Traditional smart homes emphasize the convenience brought by information technology, yet the elderly face certain challenges in accepting and mastering technological capabilities. Thus, understanding what constitutes an elderly-friendly health-smart home according to their expectations becomes a focal point in housing policies for ultra-aged societies. This study aims to construct an evaluation framework for health-smart homes based on exploring the perspectives of the elderly. Firstly, a semi-open format will be utilized to collect indicators of concern for the elderly. Secondly, dynamic Z-number will be employed to quantify the inconsistency and uncertainty in the decision-making process of the elderly. Finally, fuzzy Delphi method will be utilized to gather decision consensus. The research outcome will establish standards for evaluating elderly-friendly health-smart homes, contributing to the overall development of housing policies in ultra-aged societies.
",Developing an Evaluation System for Health-Smart Homes for the Elderly Based on Dynamic Z-Number Fuzzy Delphi Method,"[79452, 79689, 78219, 53814]",578,"[25, 26, 139]",4387,Behavioural Studies in Health Care ,13,8,07,Behavioural OR,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,1019 [building - 202],"['Decision Analysis', 'Decision Support Systems', 'Sustainable Development']",TB-07
"Benefit Corporations [BC] pursue a dual purpose - maximization of profits and generating societal and environmental benefits. BCs are becoming popular across the globe, with BC laws being passed in different countries. To date, BC laws have been passed in 43 US states and several countries in Europe and South America. Using the staggered adoption of BC law across US states, we examine how investors reacted to this law. We find that investors reacted negatively to the passing of the law. Our results are robust in using alternate measures of abnormal returns and excluding firms in the financial services sector.
Moreover, our results are robust regarding entropy balancing the three moments. This suggests that investors in the US do not favor corporations adopting a dual purpose [as opposed to a solo focus on profit maximization]. We further find that the negative reaction is predominantly for smaller and loss-making firms. Since most of the BCs in the US are, in fact, small in size, investors expect smaller firms to adopt the BC status. We also find that investors’ negative reactions softened after the onset of the COVID pandemic when the focus on ESG saw an exponential increase. As an additional analysis, we are replicating our work in the Italian context, as it was the first European country to adopt the BC law. Our results are significant for stakeholders, including investors, boards, regulators, and analysts.",Investors’ perception of firms’ ESG focus - Evidence from staggered implementations of  Benefit Corporations law in the US,"[79730, 79729, 79742]",57,"[139, 0]",4388,AI and ESG for the small economy SDG agenda [EWG-ORD Workshop 2],67,10,18,OR for Development and Developing Countries,"Tuesday, 14:30-16:00",T,D,14:30,16:00,2024-07-02,42 [building - 116],['Sustainable Development'],TD-18
"We take it for granted that an optimization package accepts both minimization and maximization problems, recognizes them as equivalent, and converts all minimizations to maximizations [or vice-versa] before solving. This is only the very simplest example of the many conversions carried out routinely by large-scale optimization software. The range of expressions recognized by modeling languages and solvers has been steadily extended in ways that make optimization models easier to describe, validate, and maintain — but that make conversion possibilities ever more numerous and complex.

Continuing this trend, automatic conversions have been a central feature in the design of a new solver interface framework for the AMPL modeling language. This presentation describes a range of challenges that have been faced in detecting formulations that solvers can handle, and in implementing conversions to forms that solvers require. Examples combining a variety of discrete and nonlinear expressions lead to some general recommendations for design and implementation of automated conversions.",Advances in Automated Conversion of Optimization Problems,"[38248, 3753]",240,"[76, 63, 84]",4390,Modeling Languages,76,8,30,Software for Optimization,"Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,53 [building - 208],"['Modeling Systems and Languages', 'Large Scale Optimization', 'Optimization Modeling']",TB-30
"In this study, a new core allocation formula using the tau-value and Shapley-value for the big boss games class has been proposed, referred to as the weight function, and a generalization has been provided. The tau-value consistently belongs to the core, whereas the Shapley value may not be part of the core in the big boss game class. While the tau-value might favor the big boss, it might not meet the expectations of other players. Conversely, the Shapley-value could satisfy other players but might not suffice for the big boss. To resolve this dilemma, we proposed a new core allocation family and a Solomonic solution procedure for the big boss games class.",A Solomonic Approach to Core Allocations For Big Boss Games,"[71170, 5971]",187,"[50, 27, 33]",4392,Experimental economics and game theory 2,73,14,40,Experimental economics and game theory,"Wednesday, 12:30-14:00",W,C,12:30,14:00,2024-07-03,96 [building - 306],"['Game Theory', 'Decision Theory', 'Economic Modeling']",WC-40
"The session starts with two presentations done by authors of representative and highly cited papers published recently in EJOR. They represent two categories - Innovative Application of OR, and Theory & Methodology. Some further research developments and practical implications that followed these publications will be given by their authors. Then, the editors of EJOR will explain their editorial policy and will give some current characteristics of the journal. They will also describe their approach to the evaluation and selection of articles and will point out topics of OR that recently raised the highest interest. In the last part of the session, the editors will answer some general questions from the audience.",Policy and facts about the European Journal of Operational Research [EJOR],"[1473, 18483, 663, 56298, 35181, 53737]",870,"[26, 0]",4393,"EJOR - policy, facts and highlights",96,5,08,OR Journals,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,1020 [building - 202],['Decision Support Systems'],MD-08
"We consider a consistent vehicle routing problem for the delivery of parcels with electric vehicles. Stemming from a real-world problem, we assume that vehicles can only be charged with electricity between their delivery tours in the morning and their pickup tours in the afternoon. For this purpose, a charging station with a limited amount of charging slots is available at the depot. We aim at generating a set of vehicle routes that are driver- and time-consistent and efficiently use limited charging resources, while optimizing the sum of vehicle fixed cost, vehicle/driver operating time, arrival time consistency and driver consistency. We present a mathematical model to describe the problem in detail. For solving the real-world problem, a template-based Adaptive Large Neighborhood Search is developed, complemented with constraint programming for charging management and quadratic programming for delivery and pickup trip scheduling. Computational experiments for different settings and scenarios, based on data from an Austrian parcel delivery company, are presented and analysed.",The consistent electric-vehicle routing problem with backhauls and charging management,"[79834, 16596, 20773, 79835]",870,"[14, 74, 143]",4395,"EJOR - policy, facts and highlights",96,5,08,OR Journals,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,1020 [building - 202],"['Combinatorial Optimization', 'Metaheuristics', 'Transportation']",MD-08
"New technologies such as deep learning should always be approached with scientific rigor.  Using a unique and extensive collection of tabular data sets, we wanted to explore the potential of deep learning for credit scoring.  We believe one of the key novelties of this paper is the robust empirical comparison framework we set up, using multiple datasets, evaluation metrics, confidence intervals, frequentist and Bayesian testing of significance. What we often encounter when reviewing papers, is that researchers either don’t give baseline methods a fair chance or cherry pick them when comparing against their own newly proposed method[s].   Furthermore, something rather basic but not unimportant relates to the title and message of the paper.  We opted for a simple, easily Google-able title properly reflecting the research question which not only draws attention but also allowed readers to very easily grasp the key takeaways of our work. In this presentation, we elaborate more on this as well as how this paper helped shape our future research.", Deep Learning for Credit Scoring - Do or Don’t – A retrospective look,"[80000, 47369, 47240, 80001, 57466]",870,"[7, 8, 66]",4396,"EJOR - policy, facts and highlights",96,5,08,OR Journals,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,1020 [building - 202],"['Analytics and Data Science', 'Artificial Intelligence', 'Machine Learning']",MD-08
"This seminar presents a usage of operational research methodologies during the ARCSAR [arcsar.eu] and AI-ARC EU-H2020 projects for enhancing safety and avoiding environmental damage in the Arctic and North Atlantic [ANA] maritime domain. The ARCSAR and AI-ARC [ai-arc.eu] projects arose due to concerns regarding the effects of increased maritime activity in the multi-national ANA region, caused by the melting ice due to climate change. The authors principal focus in the ARCSAR project has been working with search and rescue [SAR] and marine environmental response [MER] practitioners and other ANA stakeholders to classify and prioritise innovation needs. The overall aim is to lower the likelihood and social, environmental and economic consequences of large-scale incidents in ANA waters such as [i] a cruise ship sinking, [ii] a significant oil spill or [iii] a nuclear leakage. The AI-ARC project focused on developing and demonstrating an AI-based virtual control room that can improve decisions related to situational awareness, and assessment of risks against maritime incidents such as grounding, collision, oil spill, etc.

The seminar will first describe the holistic operational research process undertaken. This first involved a hierarchical classification of current and future ANA innovation needs, based at the highest level around the international maritime organisation [IMO]’s polar code. To achieve this, a series of workshops, questionnaires and interviews and literature review were used. These drew responses from 131 ANA stakeholders, including SAR and MER practitioners, environmentalists, NGO and charity representatives, indigenous peoples group representatives, industrialists and academics. This resulted in a hierarchical network of 20 need and 75 sub-need categories. A quantitative methodology using a combined PICK chart and Multi-Criteria Decision Making was then used to develop a priority set of 17 sub-needs. The PICK chart classified the needs according to importance and difficulty whilst a novel balanced knapsack goal programming model was developed to choose a high importance set of sub-needs, whilst considering the balance between long-term [more challenging] and shorter term [more implementable] innovation needs and across the six polar need categories.

The process of disseminating the methodology to ANA stakeholders and its usage to inform ARCSAR activities will be described. This involved a series of stakeholder workshops, where SWOT analyses around a set of the priority list innovations were undertaken. The priority needs were also used to inform significant activities, including a LiveEX ship evacuation exercise in Svalbard Norway. With respect to AI-ARC, the development of the risk index using AI based FL rule-based system and MCDM techniques to analyse data related to real-time AIS [ship conditions and position] combined with weather and human judgements will also be presented.

Finally, some highlights of the policy and practice impacts of the development of innovations to tackle the identified needs will be detailed. These include:
 The development of a set of training materials and procedures to use by ANA communities in the case of a nuclear incident
 An Arctic lessons learned arena for SAR and MER practitioners to share good practice
 Development of an AI-based virtual control room for safer ANA maritime navigation
 Enhanced training exercises and protocols for handling oil-on-ice in the case of an ANA oil spill
 Evidence given to, and cited by, a UK parliamentary enquiry on future Arctic policy
 Enhanced SAR operations in the Antarctic maritime region due to participation and knowledge transfer by Maritime New Zealand

Overall conclusions will then be drawn, including transferability of the developed methodology to other complex, multi-disciplinary situations where innovations to enhance safety and ensure wellbeing are needed."," Formulation of Strategy Towards the ARCTIC in terms of Search and Rescue – Needs, Innovations, and AI/OR based solutions","[8301, 3018]",23,"[70, 8]",4397,EPOCG,57,5,02,EURO Prize for OR for the Common Good,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,Glassalen [building - 101],"['Maritime applications', 'Artificial Intelligence']",MD-02
"Large-scale convex optimization is integral to several application domains, given its amenability to efficient solution methods and its versatile modeling capabilities. Interior Point Methods [IPMs] represent a widely adopted class of optimization techniques for solving convex problems due to their ability to yield accurate solutions with a polynomial complexity guarantee. However, challenges such as numerical inaccuracy and ill-posedness of the underlying optimization problem can impede their performance.

To address these challenges, researchers have explored regularized versions of IPMs, which exhibit improved robustness in practical scenarios. Despite the practical success of regularized IPMs, a comprehensive theoretical understanding has been lacking until recently.

In this talk, we present an infeasible IPM combined with the Proximal Method of Multipliers [PMM]. The resulting algorithm [IP-PMM] is interpreted as a primal-dual regularized IPM, suitable for solving convex programming problems. We apply few iterations of the interior point method to each sub-problem of the proximal method of multipliers. Once a satisfactory solution of the PMM sub-problem is found, we update the PMM parameters, form a new IPM neighbourhood, and repeat this process.

Crucially, we demonstrate the polynomial complexity of the algorithm for a broad class of convex problems under standard assumptions, marking a significant advancement as the first polynomial complexity result for a primal-dual regularized IPM. By inheriting the polynomial complexity of IPMs and the numerical stability of PMMs, IP-PMM offers a promising solution.

To enhance the applicability of our approach, we discuss general-purpose preconditioning strategies for efficiently solving the associated linear systems within IP-PMM. Subsequently, we present numerical results spanning a diverse range of convex programming problems, showcasing the benefits of regularization in IPMs and affirming the reliability and efficiency of the proposed IP-PMM algorithm.",Regularized Interior Point Methods for Convex Programming,[63240],21,"[60, 0]",4398,EDDA,55,2,02,EURO Doctoral Dissertation Award,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,Glassalen [building - 101],['Interior Point Methods'],MA-02
"Shared mobility systems like car sharing and bike sharing have become an attractive and wide-spread type of urban mobility over the past decades. The biggest challenge regarding the profitable operation of such systems is the occurring dynamic imbalance between supply and demand, which stems from fluctuating demand patterns and spatially unbalanced vehicle movements. To counter these imbalances, the scientific literature so far has focused on the supply-sided control approach by means of active vehicle relocation. In this dissertation, demand management is proposed as a cost-efficient alternative, in which the system’s demand side is influenced through pricing and availability control. On the one hand, specific practice-relevant problems are addressed and solved. On the other hand, general modeling and solution approaches are developed, which can be transferred to related optimization problems for tactical and operational control of shared mobility systems. Extensive numerical studies, including case studies of Europe’s largest car sharing company Share Now, demonstrate that demand management can be implemented successfully in shared mobility systems.",Demand Management in Shared Mobility Systems,[59177],21,"[143, 130]",4399,EDDA,55,2,02,EURO Doctoral Dissertation Award,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,Glassalen [building - 101],"['Transportation', 'Service Operations']",MA-02
"Contrary to classic classification and regression trees, built in a greedy heuristic manner, designing the tree model through an optimization problem allows us to easily include desirable properties in Machine Learning in addition to prediction accuracy. We present a Non-Linear Optimization approach that is scalable with respect to the size of the training sample, and illustrate this flexibility to model several important issues in Explainable and Fair Machine Learning. These include sparsity, as a proxy for interpretability, by reducing the amount of information necessary to predict well; fairness, by aiming to avoid predictions that discriminate against sensitive features such as gender or race; the cost-sensitivity for groups of individuals in which prediction errors are more critical, such as patients of a disease, by ensuring an acceptable accuracy performance for  them; local explainability, where the goal is to identify the predictor variables that have the largest impact on the individual predictions; as well as data complexity in the form of observations of functional nature. The performance of our approach is illustrated on real and synthetic data sets.",Enhancing classification and regression trees. A mathematical optimization approach,[56862],21,"[66, 19]",4400,EDDA,55,2,02,EURO Doctoral Dissertation Award,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,Glassalen [building - 101],"['Machine Learning', 'Continuous Optimization']",MA-02
"Air travel disruptions due to Covid recovery, strikes, and military presence highlight the need for efficient airspace management. The dissertation presents demand-capacity balancing mechanisms aimed at improving European air traffic management [ATM] performance. The first paper introduces dynamically priced flexible trajectory products to manage demand, based on an optimizing routing under capacity constraints. The second paper proposes a simulation-optimization approach for cross-border capacity planning, showing potential for significant delay reductions. Finally, the third paper analyzes measures to reduce aviation emissions through capacity planning and pricing initiatives. Impacting both research and practice, the findings offer innovative solutions for managing demand-capacity imbalances post-Covid, potentially reshaping European airspace management.",Advanced Demand-Capacity Balancing Mechanisms to Improve Performance of European Air Traffic Networks,[67052],982,"[4, 131]",4401,EDDA2,55,3,02,EURO Doctoral Dissertation Award,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,Glassalen [building - 101],"['Airline Applications', 'Simulation']",MB-02
"Over the last century, mathematical optimization has become a prominent tool for decision making. Its systematic application in practical fields such as economics, logistics or defense led to the development of algorithmic methods with ever increasing efficiency. Indeed, for a variety of real-world problems, finding an optimal decision among a set of [implicitly or explicitly] predefined alternatives has become conceivable in reasonable time. In the last decades, however, the research community raised more and more attention to the role of uncertainty in the optimization process. In particular, one may question the notion of optimality, and even feasibility, when studying decision problems with unknown or imprecise input parameters. 


In this talk, we study a class of optimization problems which suffer from imprecise input data and feature a two-stage decision process, i.e., where decisions are made in a sequential order and where unknown parameters are revealed throughout the stages. The applications of such problems are plethora in practical fields such as, e.g., facility location problems with uncertain demands, transportation problems with uncertain costs or scheduling under uncertain processing times. The uncertainty is dealt with a robust optimization [RO] viewpoint [also known as worst-case perspective] and we present original contributions to the RO literature on both the theoretical and practical side.",Adjustable Robust Optimization with Non-Linear Recourse,[67824],982,"[127, 0]",4402,EDDA2,55,3,02,EURO Doctoral Dissertation Award,"Monday, 10:30-12:00",M,B,10:30,12:00,2024-07-01,Glassalen [building - 101],['Robust Optimization'],MB-02
"This talk adds some professional and, not least, personal elements to the appreciation of Professor Jakob Krarup. They cover the period since 2000, address some of his important scientific achievements, some joint conferences and conference organizations, especially within EURO and EUROPT, and in particular his warm-hearted and loyal, caring and fatherly personality that manifested itself in the minds and hearts of many people and forever in EURO, IFORS and all Operational Research.
",Some thoughts of sincere gratitude to Professor Jakob Krarup - our mentor and friend,[3524],983,"[88, 0]",4403,Memorial Session for Jakob Krarup,66,5,65,Memorial Session for Jakob Krarup,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,R021 [building - 358],['OR History'],MD-65
"The story of the DAPS society and participation of other foreigners at the DAPS activities.
",Jakob Krarup. Very personal remarks,[23425],983,"[88, 0]",4404,Memorial Session for Jakob Krarup,66,5,65,Memorial Session for Jakob Krarup,"Monday, 14:30-16:00",M,D,14:30,16:00,2024-07-01,R021 [building - 358],['OR History'],MD-65
"In the past two decades, European banking sectors experienced a significant integration and expansion on global financial markets. This period, characterized by intense competition and the aftermath of the global financial crisis [2007-2009], sparked growing concern regarding the risk taken on by banking systems and its impact on financial performance. Although there is an abundant literature on this topic, uncertainty remains about the extent and nature of the effects of banking risk. Moreover, previous studies that examined this relationship often assume a uniform effect of risk on banking efficiency, without considering the possibility that the risk taken on by banks may have diverse effects depending on the characteristics of the bank and the conditions of the operating environment. To address this issue, we propose a Bayesian stochastic frontier model for unbalanced data. It includes commercial banks operating in European Union countries in 2004-2020 to estimate the evolution of their cost efficiency. We assume the efficiency depends on the size of the bank and the levels of risk it assumes. In a hierarchical model, we postulate that the influence exerted by these characteristics depends on the environment in which the bank operates, including the country and the industry. We obtained the economic and financial data from Orbis Bank Focus [2004-2016] and Orbis [2017-2020], while industry and country data from the World Bank's World Development Indicators database.",A Bayesian Hierarchical Approach to Analyze the Heterogeneous Influence of Banking Risk on Cost Efficiency,"[12255, 12256]",277,"[44, 45, 126]",4405,Risk Management and Cryptoassets,4,8,63,"OR in Banking, Finance and Insurance - New Tools for Risk Management","Tuesday, 10:30-12:00",T,B,10:30,12:00,2024-07-02,S14 [building - 101],"['Finance and Banking', 'Financial Modelling', 'Risk Analysis and Management']",TB-63
"During the talk, I will present both the conditions we have to take into account in selecting call topics for our funding effort, and present some of our funding opportunities, such as the mobility and network programme for PhD students and researchers, our Hydrogen Valleys programme and our Nordic Grand Solutions programme [Technology agnostic, must meet impact goals of importance for the green transition].
",Nordic Project Funding as a Means to Create Impact with Your Research,[80329],791,"[37, 0]",4406,OR Education I,48,2,16,OR Education,"Monday, 8:30-10:00",M,A,8:30,10:00,2024-07-01,19 [building - 116],['Energy Policy and Planning'],MA-16
